[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimentology",
    "section": "",
    "text": "Preface\nAs scientists and practitioners, we often want to create generalizable, causal theories of human behavior. As it turns out, experiments—in which we use random assignment to measure a causal effect—are an unreasonably effective tool to help with this task. But how should we go about doing good experiments?\nThis book provides an introduction to the workflow of the experimental researcher working in psychology or the behavioral sciences more broadly. The organization of the book is sequential, from the planning stages of the research process through design, data gathering, analysis, and reporting. We introduce these concepts via narrative examples from a range of subdisciplines, including cognitive, developmental, and social psychology. Throughout, we also illustrate the pitfalls that led to the “replication crisis” in psychology.\nTo help researchers avoid these pitfalls, we advocate for an open science–based approach in which transparency is integral to the entire experimental workflow. We provide readers with guidance for preregistration, project management, data sharing, and reproducible report writing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-story-of-this-book",
    "href": "index.html#the-story-of-this-book",
    "title": "Experimentology",
    "section": "The story of this book",
    "text": "The story of this book\nExperimental Methods (Psych 251) is the foundational course for incoming graduate students in the Stanford psychology department. The course goal is to orient students to the nuts and bolts of doing behavioral experiments, including how to plan and design a solid experiment and how to avoid common pitfalls regarding design, measurement, and sampling.\nAlmost all student coursework both before and in graduate school deals with the content of their research, including theories and results in their areas of focus. In contrast, our course is sometimes the only one that deals with the process of research, from big questions about why we do experiments and what it means to make a causal inference all the way to the tiny details of project organization, such as what to name your directories and how to make sure you don’t lose your data in a computer crash.\nThis observation leads to our book’s title. “Experimentology” is the set of practices, findings, and approaches that enable the construction of robust, precise, and generalizable experiments.\nThe centerpiece of the Experimental Methods course is a replication project, reflecting a teaching model first described in Frank and Saxe (2012) and later expanded on in Hawkins et al. (2018) . Each student chooses a published experiment in the literature and collects new data on a preregistered version of the same experimental paradigm, comparing their result to the original publication. Over the course of the quarter, we walk through how to set up a replication experiment, how to preregister confirmatory analyses, and how to write a reproducible report on the findings. The project teaches concepts like reliability and validity, which allow students to analyze choices that the original experimenters made—often choices that could have been made differently in hindsight!\nAt the end of the course, we reap the harvest of these projects. The project presentations are a wonderful demonstration of both how much the students can accomplish in a quarter and how tricky it can be to reproduce (redo calculations in the original data) and replicate (recover similar results in new data) the published literature. Often our replication success rate for the course hovers just above 50%, an outcome that can be disturbing or distressing for students who assume that the published literature reports the absolute truth.\nThis book is an attempt to distill some of the lessons of the course (and students’ course projects) into a textbook. We’ll tell the story of the major shifts in psychology that have come about in the last ten years, including both the “replication crisis” and the positive methodological reforms that have resulted from it. Using this story as motivation, we will highlight the importance of transparency during all aspects of the experimental process from planning to dissemination of materials, data, and code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-book-is-and-isnt-about",
    "href": "index.html#what-this-book-is-and-isnt-about",
    "title": "Experimentology",
    "section": "What this book is and isn’t about",
    "text": "What this book is and isn’t about\nThis book is about psychology experiments. These will typically be short studies conducted online or in a single visit to a lab, often—though certainly not always—with a convenience sample. When we say “experiments” here, we mean randomized experiments where some aspect of the participants’ experience is manipulated by the experimenter and then some outcome variable is measured.1\n1 We use bold to indicate the introduction of new technical terms.The central thesis of the book is that:\n\nExperiments are intended to make maximally unbiased, generalizable, and precise estimates of specific causal effects.\n\nWe’ll explore the implications of this thesis for a host of topics, including causal inference, experimental design, measurement, sampling, preregistration, data analysis, and many others.\nBecause our focus is on experiments, we won’t be talking much about observational designs, survey methods, or qualitative research; these are important tools and are appropriate for a whole host of questions, but they aren’t our focus here. We also won’t go into depth about the many fascinating methodological and statistical issues brought up by single-participant case studies, longitudinal research, field studies, or other methodological variants. Many of the concerns we raise are still important for these types of studies, but some of our advice won’t transfer to these less common designs.\nEven for students who are working on nonexperimental research, we expect that a substantial part of the book content will still be useful, including chapters on replication (3  Replication), ethics (4  Ethics), statistics (chapters 5  Estimation, 6  Inference, 7  Models), sampling (10  Sampling), project management (13  Project management), and reporting (chapters 14  Writing, 15  Visualization, 16  Meta-analysis).\nIn our writing, we presuppose that readers have some background in psychology, at least at an introductory level. In addition, although we introduce a number of statistical topics, readers might find these sections more accessible with an undergraduate statistics course under their belt. Finally, our examples are written in the R statistical programming language, and for chapters on statistics and visualization especially (chapters 5  Estimation, 6  Inference, 7  Models, 15  Visualization, 16  Meta-analysis), some familiarity with R will be helpful for understanding the code. None of these prerequisites are necessary to read the book, but we offer them so that readers can calibrate their expectations.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Experimentology",
    "section": "How to use this book",
    "text": "How to use this book\nThe book is organized into five main parts, mirroring the timeline of an experiment: (1) “Foundations,” (2) “Statistics,” (3) “Planning,” (4) “Execution,” and (5) “Reporting.” We hope that this organization makes it well suited for teaching or for use as a reference book.2\n2 If you are an instructor who is planning to adopt the book for a course, you might be interested in our resources for instructors, including sample course schedules, in appendix A — Instructor’s guide.The book is designed for a course for graduate students or advanced undergraduates, but the material is also suitable for self-study by anyone interested in experimental methods, whether in academic psychology or any other context—in or out of academia—in which behavioral experimentation is relevant. We also hope that some readers will come to particular chapters of the book because of an interest in specific topics like measurement (8  Measurement) or sampling (10  Sampling) and will be able to use those chapters as stand-alone references. And finally, for those interested in the “replication crisis” and subsequent reforms, chapters 3  Replication, 11  Preregistration, and 13  Project management will be especially interesting.\nUltimately, we want to give you what you need to plan and execute your own study! Instead of enumerating different approaches, we try to provide a single coherent—and often quite opinionated—perspective, using marginal notes and references to give pointers to more advanced materials or alternative approaches. Throughout, we offer:\n\ncase studies that illustrate the central concepts of a chapter\naccident reports describing examples where poor research practices led to issues in the literature\ndepth boxes providing simulations, linkages to advanced techniques, or more nuanced discussion\n\nWhile case studies are often integral to the chapters, the other boxes can typically be skipped without issue.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#themes",
    "href": "index.html#themes",
    "title": "Experimentology",
    "section": "Themes",
    "text": "Themes\nWe highlight four major cross-cutting themes for the book: transparency, measurement precision, bias reduction, and generalizability.3\n3 Themes are noted using small caps.\ntransparency: For experiments to be reproducible, other researchers need to be able to determine exactly what you did. Thus, every stage of the research process should be guided by a primary concern for transparency. For example, preregistration creates transparency into the researcher’s evolving expectations and thought processes; releasing open materials and analysis scripts creates transparency into the details of the procedure.\nmeasurement precision: We want researchers to start planning an experiment by thinking: “What causal effect do I want to measure?” and to make planning, sampling, design, and analytic choices that maximize the precision of this measurement. A downstream consequence of this mindset is that we move away from a focus on dichotomized inferences about statistical significance and toward analytic and meta-analytic models that focus on continuous effect sizes and confidence intervals.\nbias reduction: While precision refers to random error in a measurement, measurements also have systematic sources of error that bias them away from the true quantity. In our samples, analyses, and experimental designs, and in the literature, we need to think carefully about sources of bias in the quantity being estimated.\ngeneralizability: Complex behaviors are rarely universal across all settings and populations, and any given experiment can only hope to cover a small slice of the possible conditions where a behavior of interest takes place. Psychologists must therefore consider the generalizability of their findings at every stage of the process, from stimulus selection and sampling procedures to analytic methods and reporting.\n\nThroughout the book, we will return to these four themes again and again as we discuss how the decisions made by the experimenter at every stage of design, data gathering, and analysis bear on the inferences that can be made about the results. The introduction of each chapter highlights connections to specific themes.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-software-toolkit-for-this-book",
    "href": "index.html#the-software-toolkit-for-this-book",
    "title": "Experimentology",
    "section": "The software toolkit for this book",
    "text": "The software toolkit for this book\nWe introduce and advocate for an approach to reproducible study planning, analysis, and writing. This approach depends on an ecosystem of open-source software tools, which we introduce in the book’s appendices:4\n4 These appendices are available online at https://experimentology.io but not in the print version of the book, since their content is best viewed in the web format.\nthe R statistical programming language and the RStudio integrated development environment\nversion control using git and GitHub for allowing collaboration on text documents like code, prose, and data, storing and integrating contributions over time (appendix B — Git and GitHub)\nthe RMarkdown and Quarto tools for creating reproducible reports that can be rendered to a variety of formats (appendix C — R Markdown and Quarto)\nthe tidyverse family of R packages, which extend the basic functionality of R with simple tools for data wrangling, analysis, and visualization (appendix D — Tidyverse)\nthe ggplot2 plotting package, which makes it easy to create flexible data visualizations for both confirmatory and exploratory data analyses (appendix E — ggplot)\n\nWhere appropriate, we provide code boxes that show the specific R code used to create our examples.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#onward",
    "href": "index.html#onward",
    "title": "Experimentology",
    "section": "Onward!",
    "text": "Onward!\nThanks for joining us for Experimentology! Whether you are casually browsing, doing readings for a course, or using the book as a reference in your own experimental work, we hope you find it useful. Throughout, we have tried to practice what we preach in terms of reproducibility, and so the full source code for the book is available at https://github.com/langcog/experimentology. We encourage you to browse, comment, and log issues or suggestions.5\n\n\n\n\n5 The best way to give us specific feedback is to create an issue on our github page at https://github.com/langcog/experimentology/issues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citing-this-book",
    "href": "index.html#citing-this-book",
    "title": "Experimentology",
    "section": "Citing this book",
    "text": "Citing this book\n\nBibTeX citation:\n@book{experimentology2025,\n  author    = {Frank, Michael C. and Braginsky, Mika and Cachia, Julie and Coles, Nicholas A. and Hardwicke, Tom E. and Hawkins, Robert D. and Mathur, Maya B. and Williams, Rondeline},\n  title     = {{Experimentology: An Open Science Approach to Experimental Psychology Methods}},\n  year      = {2025},\n  publisher = {Stanford University},\n  doi       = {10.25936/3JP6-5M50},\n  note      = {Also published by MIT Press, ISBN 978-0-262-55256-1}\n}\nFor attribution, please cite this work as:\n\nFrank, M. C., Braginsky, M., Cachia, J., Coles, N. A., Hardwicke, T. E., Hawkins, R. D., Mathur, M. B., & Williams, R. 2025. Experimentology: An Open Science Approach to Experimental Psychology Methods. Stanford University. https://doi.org/10.25936/3JP6-5M50. (Also published by MIT Press, ISBN 978-0-262-55256-1)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Experimentology",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks first and foremost to the many generations of students and TAs in Stanford Psych 251, who have collectively influenced the content of this book through their questions and interests.\nThanks to the staff at the MIT Press, especially Philip Laughlin and Amy Brand, for embracing a vision of a completely open web textbook that is also reviewed and published through a traditional press. We appreciate your support and flexibility.\nWe adapt the Contributor Roles (CRediT) Taxonomy6 to describe our contributions to this manuscript, and we recommend that you do so in your work as well.\n6 Learn more at https://credit.niso.org.\nPreface\n\nPrimary writer: Michael C. Frank\nEditor: Tom E. Hardwicke\n\nChapter 1  Experiments\n\nPrimary writer: Michael C. Frank\nCowriter: Nicholas Coles\nEditor: Tom E. Hardwicke\n\nChapter 2  Theories\n\nPrimary writer: Michael C. Frank\nEditors: Nicholas Coles and Tom E. Hardwicke\n\nChapter 3  Replication\n\nPrimary writer: Michael C. Frank\nEditors: Maya B. Mathur, Tom E. Hardwicke, and Nicholas Coles\n\nChapter 4  Ethics\n\nPrimary writer: Rondeline Williams\nCowriter: Michael C. Frank\nEditors: Tom E. Hardwicke and Julie Cachia\n\nChapter 5  Estimation\n\nCowriters: Maya B. Mathur, Nicholas Coles, and Michael C. Frank\nEditors: Julie Cachia and Tom E. Hardwicke\n\nChapter 6  Inference\n\nPrimary writer: Michael C. Frank\nCowriter: Maya B. Mathur\nEditors: Julie Cachia and Tom E. Hardwicke\n\nChapter 7  Models\n\nCowriters: Maya B. Mathur and Michael C. Frank\nEditors: Tom E. Hardwicke and Robert D. Hawkins\n\nChapter 8  Measurement\n\nPrimary writer: Michael C. Frank\nEditors: Robert D. Hawkins, Tom E. Hardwicke, and Rondeline Williams\n\nChapter 9  Design\n\nPrimary writer: Michael C. Frank\nEditors: Nicholas Coles and Tom E. Hardwicke\n\nChapter 10  Sampling\n\nPrimary writer: Michael C. Frank\nEditors: Julie Cachia, Tom E. Hardwicke, and Maya B. Mathur\n\nChapter 11  Preregistration\n\nPrimary writer: Tom E. Hardwicke\nEditor: Michael C. Frank\n\nChapter 12  Data collection\n\nCowriters: Rondeline Williams and Michael C. Frank\nEditor: Tom E. Hardwicke\n\nChapter 13  Project management\n\nPrimary writer: Michael C. Frank\nEditor: Tom E. Hardwicke\n\nChapter 15  Visualization\n\nPrimary writer: Robert D. Hawkins\nEditors: Michael C. Frank, Tom E. Hardwicke, and Mika Braginsky\n\nChapter 14  Writing\n\nPrimary writer: Tom E. Hardwicke\nEditor: Michael C. Frank\n\nChapter 16  Meta-analysis\n\nCowriters: Nicholas Coles and Maya B. Mathur\nEditors: Michael C. Frank and Tom E. Hardwicke\n\nConclusion\n\nPrimary writer: Michael C. Frank\nEditor: Tom E. Hardwicke\n\nAppendix appendix A — Instructor’s guide\n\nPrimary writer: Julie Cachia\nEditor: Michael C. Frank\n\nAppendix appendix B — Git and GitHub\n\nPrimary writer: Julie Cachia\nEditor: Michael C. Frank\n\nAppendix appendix C — R Markdown and Quarto\n\nPrimary writer: Michael C. Frank\nEditor: Julie Cachia\n\nAppendix appendix D — Tidyverse\n\nPrimary writer: Michael C. Frank\nEditors: Julie Cachia and Mika Braginsky\n\nAppendix appendix E — ggplot\n\nPrimary writer: Michael C. Frank\nEditors: Julie Cachia and Mika Braginsky\n\nTechnical infrastructure\n\nDevelopers: Mika Braginsky and Natalie Braginsky",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001-experiments.html",
    "href": "001-experiments.html",
    "title": "1  Experiments",
    "section": "",
    "text": "1.1 Observational studies don’t reveal causality\nIf you’re reading this book, there’s probably something about psychology you want to understand. How is language learned? How is it that we experience emotions like happiness and sadness? Why do humans sometimes work together and other times destroy one another? When psychologists study these centuries-old questions, they often transform them into questions about causality.2",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#observational-studies-dont-reveal-causality",
    "href": "001-experiments.html#observational-studies-dont-reveal-causality",
    "title": "1  Experiments",
    "section": "",
    "text": "2 Defining causality is one of the trickiest and oldest problems in philosophy, and we won’t attempt to solve it here! But from a psychological perspective, we’re fond of Lewis’s (1973) “counterfactual” analysis of causality. On this view, we can understand the claim that money causes happiness by considering a scenario where if people hadn’t been given more money, they wouldn’t have experienced an increase in happiness.\n1.1.1 Describing causal relationships\nConsider the age-old question: Does money make people happy? This question is—at its heart—a question about what interventions on the world we can make. Can I get more money and make myself happier? Can I cause happiness with money?\nHow could we test our hypothesized effect of money on happiness? Intuitively, many people think of running an observational study. We might survey people about how much money they make and how happy they are. The result of this study would be a pair of measurements for each participant: [money, happiness].\nNow, imagine your observational study found that money and happiness were related—statistically correlated with each another: people with more money tended to be happier. Can we conclude that money causes happiness? Not necessarily. The presence of a correlation does not mean that there is a causal relationship!\n\n\n\n\n\n\n\nFigure 1.1: The hypothesized causal effect of money on happiness.\n\n\n3 In this chapter, we’re going to use the term “variables” without discussing why we study some variables and not others. In the next chapter, we’ll introduce the term “construct,” which indicates a psychological entity that we want to theorize about.Let’s get a bit more precise about our causal hypothesis. To illustrate causal relationships, we can use a tool called directed acyclic graphs (DAGs, Pearl 1998). Figure 1.1 shows an example of a DAG for money and happiness. The arrow represents our idea about the potential causal link between two variables: money and happiness.3 The direction of the arrow tells us which way we hypothesize that the causal relationship goes.\nThe correlation between money and happiness we saw in our observational study is consistent with the causal model in figure 1.1; however, it is also consistent with several alternative causal models, which we will illustrate with DAGs below.\n\n\n1.1.2 The problems of directionality and confounding\n\n\n\n\n\n\n\nFigure 1.2: Three reasons why money and happiness can be correlated.\n\n\nFigure 1.2 uses DAGs to illustrate several causal models that are consistent with the observed correlation between money and happiness. DAG 1 represents our hypothesized relationship—money causes people to be happy. But DAG 2 shows an effect in completely the opposite direction! In this DAG, being happy causes people to make more money.\nEven more puzzling, there could be a correlation but no causal relationship between money and happiness in either direction. Instead, a third variable—often referred to as a confound—may be causing increases in both money and happiness. For example, maybe having more friends causes people to both be happier and make more money (DAG 3). In this scenario, happiness and money would be correlated even though one does not cause the other.\nA confound (or several) may entirely explain the relationship between two variables (as in DAG 3), but it can also just partly explain the relationship. For example, it could be that money does increase happiness, but the causal effect is rather small, and only accounts for a small portion of the observed correlation between them, with the friendship confound (and perhaps others) accounting for the remainder.\nIn this case, because of the confounds, we say that the observed correlation between money and happiness is a biased estimate of the causal effect of money on happiness. The amount of bias introduced by the confounds can vary in different scenarios—it may only be small or it may be so strong that we conclude that there’s a causal relationship between two variables when there isn’t one at all.\nThe state of affairs summarized in figure 1.2 is why we say “correlation doesn’t imply causation.” A correlation between two variables is consistent with a causal relationship between them, but it’s also consistent with other relationships as well.4\n4 People sometimes ask whether causation implies correlation (the opposite direction). The short answer is “also no.” A causal relationship between two variables often means that they will be correlated in the data, but not always. For example, imagine you measured the speed of a car and the pressure on the gas pedal / accelerator. In general, pressure and speed will be correlated, consistent with the causal relationship between the two. But now imagine you only measured these two variables when someone was driving the car up a hill—now the speed would be constant, but the pressure might be increasing, reflecting the driver’s attempts to keep their speed up. So there would be no correlation between the two variables in that dataset, despite the continued causal relationship.5 In fact, DAGs are one of the key tools that social scientists use to reason about causal relationships. DAGs guide the creation of statistical models to estimate particular causal effects from observational data. We won’t talk about these methods here, but if you’re interested, check out the suggested readings at the end of this chapter.You can still learn about causal relationships from observational studies, but you have to take a more sophisticated approach. You can’t just measure correlations and leap to causal conclusions. The “causal revolution” in the social sciences has been fueled by the development of statistical methods for reasoning about causal relationships from observational datasets.5 As interesting as these methods are, however, they are only applicable in certain specific circumstances. In contrast, the experimental method always works to reduce bias due to confounding (though of course there are certain experiments that we can’t do for ethical or practical reasons).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#experiments-help-us-answer-causal-questions",
    "href": "001-experiments.html#experiments-help-us-answer-causal-questions",
    "title": "1  Experiments",
    "section": "1.2 Experiments help us answer causal questions",
    "text": "1.2 Experiments help us answer causal questions\nImagine that you (a) created an exact replica of our world, (b) gave $1,000 to everybody in the replica world, and then (c) found a few years later that everyone in the replica world was happier than their matched self in the original world. This experiment would provide strong evidence that money makes people happier. Let’s think through why.\nConsider a particular person—if they are happier in the replica that in the original world, what could explain that difference? Since we have replicated the world exactly but made only one change—money—then that change is the only factor that could explain the difference in happiness. We can say that we held all variables constant except for money, which we manipulated experimentally, observing its effect on some measure—happiness. This idea—holding all variables constant except for the specific experimental manipulation—is the basic logic that underpins the experimental method (as articulated by Mill 1843).6 Let’s think back to our observational study of money and happiness. One big causal inference problem was the presence of “third-variable” confounds like having more friends. More friends could cause you to have more money and also cause you to be happier. The idea of an experiment is to hold everything else constant—including the number of friends that people have—so we can measure the effect of money on happiness. By holding the number of friends constant, we would be severing the causal links between friends and both money and happiness. This move is graphically conveyed in figure 1.3, where we “snip away” the friend confound.\n6 Another way to reason about why we can infer causality here follows the counterfactual logic we described in an earlier footnote. If the definition of causality is counterfactual (“What would have happened if the cause had been different?”), then this experiment fulfills that definition. In our impossible experiment, we can literally see the counterfactual: if the person had $1,000 more, here’s how much happier they would be!\n\n\n\n\n\nFigure 1.3: In principle, experiments allow us to “snip away” the friend confound by holding it constant (though in practice, it can be tough to figure out how to hold something constant when you are talking about people as your unit of study).\n\n\n\n\n1.2.1 We can’t hold people constant\nThis all sounds great in theory, you might be thinking, but we can’t actually create replica worlds where everything is held constant, so how do we run experiments in the real world? If we were talking about experiments on baking cakes, it’s easy to see how we could hold all of the ingredients constant and just vary one thing, like baking temperature. Doing so would allow us to conduct an experimental test of the effect of baking temperature. But how can we “hold something constant” when we’re talking about people? People aren’t cakes. No two people are alike, and, as every parent with multiple children knows, even if you try to “hold the ingredients constant,” they don’t come out the same!\nIf we take two people and give one money, we’re comparing two different people, not two instances of the same person with everything held constant. It wouldn’t work to make the first person have more or fewer friends so they match the second person—that’s not holding anything constant; instead it’s another (big, difficult, and potentially unethical) intervention that might itself cause lots of effects on happiness.\nYou may be wondering: Why don’t we just ask people how many friends they have and use this information to split them into equal groups? You could do that, but this kind of strategy only allows you to control for the confounds you know of. For example, you may split people equally based on their number of friends but not their educational attainment. If educational attainment also impacts both money and happiness, you still have a confound. You may then try to split people by both their number of friends and their education. But perhaps there’s another confound you’ve missed: sleep quality! Similarly, it also doesn’t work to select people who have the same number of friends—that only holds the friends variable constant and not everything else that’s different between the two people. So what do we do instead?7\n7 Many researchers who have seen regression models used in the social sciences assume that “controlling for lots of stuff” is a good way to improve causal inference. Not so! In fact, inappropriately controlling for a variable in the absence of a clear causal justification can actually make your effect estimate more biased (Wysocki, Lawson, and Rhemtulla 2022).\n\n1.2.2 Randomization saves the day\n\n\n\n\n\n\n\nFigure 1.4: If you randomly split a large group of people into groups, the groups will, on average, be equal in every way.\n\n\nThe answer is randomization. If you randomly split a large roomful of people into two groups, the groups will, on average, have a similar number of friends. Similarly, if you randomly pick who in your experiment gets to receive money, you will find that the money and no-money groups, on average, have a similar number of friends. In other words, through randomization, the confounding role of friends is controlled. But the most important thing is that it’s not just the role of friends that’s controlled; educational attainment, sleep quality, and all the other confounds are controlled as well. If you randomly split a large group of people into groups, the groups will, on average, be equal in every way (figure 1.4).\nSo, here’s our simple experimental design: we randomly assign some people to a money group and some people to a no-money control group (we sometimes call these groups conditions). Then we measure the happiness of people in both groups. The basic logic of randomization is that, if money causes happiness, we should see more happiness—on average—in the money group.8\n8 You may already be protesting that this experiment could be done better. Maybe we could measure happiness before and after randomization, to increase precision. Maybe we need to give a small amount of money to participants in the control condition to make sure that participants in both conditions interact with an experimenter, and hence, that the conditions are as similar as possible. We agree! These are important parts of experimental design, and we’ll touch on them in subsequent chapters.Randomization is a powerful tool, but there is a caveat: it doesn’t work every time. On average, randomization will ensure that your money and no-money groups will be equal with respect to confounds like number of friends, educational attainment, and sleep quality. But just as you can flip a coin and sometimes get heads nine out of ten times, sometimes you use randomization and still get more highly educated people in one condition than the other. When you randomize, you guarantee that, on average, all confounds are controlled. Hence, there is no systematic bias in your estimate from these confounds. But there will still be some noise from random variation.\nIn sum, randomization is a remarkably simple and effective way of holding everything constant besides a manipulated variable. In doing so, randomization allows experimental psychologists to make unbiased estimates of causal relationships. Importantly, randomization works both when you do have control of every aspect of the experiment—like when you are baking a cake—and even when you don’t—like when you are doing experiments with people.9\n9 There’s an important caveat to this discussion: you don’t always have to randomize people. You can use an experimental design called a within-participants design, in which the same people are in multiple conditions. This type of design has a different set of unknown confounds, this time centering around time. So, to get around them, you have to randomize the order in which your manipulation is delivered. This randomization works very well for some kinds of manipulations but not so well for others. We’ll talk more about these kinds of designs in chapter 9.\n\n\n\n\n\ndepth\n\n\n\n\n\nUnhappy randomization?\nAs we’ve been discussing, random assignment removes confounding by ensuring that—on average—groups are equivalent with respect to all of their characteristics. Equivalence for any particular random assignment is more likely the larger your sample is, however. Any individual experiment may be affected by unhappy randomization, when a particular confound is unbalanced between groups by chance.\nUnhappy randomization is much more common in small experiments than larger ones. To see why, we use a technique called simulation. In simulations, we invent data randomly following a set of assumptions: we make up a group of participants and generate their characteristics and their condition assignments. By varying the assumptions we use, we can investigate how particular choices might change the structure of the data.\nTo look at unhappy randomization, we created many simulated versions of our money-happiness experiment, in which an experimental group receives money and the control group receives none, and then happiness is measured for both groups. We assume that each participant has a set number of friends, and that the more friends they have, the happier they are. So, when we randomly assign them to experimental and control groups, we run the risk of unhappy randomization—sometimes one group will have substantially more friends than the other.\n\n\n\n\n\n\nFigure 1.5: Simulated data from our money-happiness experiment. Each dot represents the measured happiness effect (vertical position) for an experiment with a set number of participants in each group (horizontal position). The dot color shows how uneven friendship is between the groups. The dashed line shows the true effect.\n\n\n\nFigure 1.5 shows the results of this simulation. Each dot is an experiment, representing one estimate of the happiness effect (how much happiness is gained for the amount of money given to the experimental group). For very small experiments (e.g., with one or three participants per group), dots are very far from the dashed line showing the true effect—meaning these estimates are extremely noisy! And the reason is unhappy randomization. The upper and lower points are those in which one group had far more friends than the other.\nThere are three things to notice about this simulation. First, the noise overall goes down as the sample sizes get bigger: larger experiments yield estimates closer to the true effect. Second, the unhappy randomization decreases dramatically as well with larger samples. Although individuals still differ just as much in large experiments, the group average number of friends is virtually identical for each condition in the largest groups.\nFinally, although the small experiments are individually very noisy, the average effect across all of the small experiments is still very close to the true effect. This last point illustrates what we mean when we say that randomized experiments remove confounds. Even though friendship is still an important factor determining happiness in our simulation, the average effect across experiments is correct and each individual estimate is unbiased.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#unhappy-randomization",
    "href": "001-experiments.html#unhappy-randomization",
    "title": "1  Experiments",
    "section": "Unhappy randomization?",
    "text": "Unhappy randomization?\nAs we’ve been discussing, random assignment removes confounding by ensuring that—on average—groups are equivalent with respect to all of their characteristics. Equivalence for any particular random assignment is more likely the larger your sample is, however. Any individual experiment may be affected by unhappy randomization, when a particular confound is unbalanced between groups by chance.\nUnhappy randomization is much more common in small experiments than larger ones. To see why, we use a technique called simulation. In simulations, we invent data randomly following a set of assumptions: we make up a group of participants and generate their characteristics and their condition assignments. By varying the assumptions we use, we can investigate how particular choices might change the structure of the data.\nTo look at unhappy randomization, we created many simulated versions of our money-happiness experiment, in which an experimental group receives money and the control group receives none, and then happiness is measured for both groups. We assume that each participant has a set number of friends, and that the more friends they have, the happier they are. So, when we randomly assign them to experimental and control groups, we run the risk of unhappy randomization—sometimes one group will have substantially more friends than the other.\n\n\n\n\n\n\nFigure 1.5: Simulated data from our money-happiness experiment. Each dot represents the measured happiness effect (vertical position) for an experiment with a set number of participants in each group (horizontal position). The dot color shows how uneven friendship is between the groups. The dashed line shows the true effect.\n\n\n\nFigure 1.5 shows the results of this simulation. Each dot is an experiment, representing one estimate of the happiness effect (how much happiness is gained for the amount of money given to the experimental group). For very small experiments (e.g., with one or three participants per group), dots are very far from the dashed line showing the true effect—meaning these estimates are extremely noisy! And the reason is unhappy randomization. The upper and lower points are those in which one group had far more friends than the other.\nThere are three things to notice about this simulation. First, the noise overall goes down as the sample sizes get bigger: larger experiments yield estimates closer to the true effect. Second, the unhappy randomization decreases dramatically as well with larger samples. Although individuals still differ just as much in large experiments, the group average number of friends is virtually identical for each condition in the largest groups.\nFinally, although the small experiments are individually very noisy, the average effect across all of the small experiments is still very close to the true effect. This last point illustrates what we mean when we say that randomized experiments remove confounds. Even though friendship is still an important factor determining happiness in our simulation, the average effect across experiments is correct and each individual estimate is unbiased.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#generalizability",
    "href": "001-experiments.html#generalizability",
    "title": "1  Experiments",
    "section": "1.3 Generalizability",
    "text": "1.3 Generalizability\nWhen we are asking questions about psychology, it’s important to think about who we are trying to study. Do we want to know if money increases happiness in all people? In people who live in materialistic societies? In people whose basic needs are not being met? We call the group we are trying to study our population of interest and the people who actually participate in our experiment our sample. The process of sampling is then what we do to recruit people into our experiment.\nSometimes researchers sample from one population but make a claim about another (usually broader) population. For example, they may run their experiment with a particular sample of US college students but then generalize to all people (their intended population of interest). The mismatch of sample and population is not always a problem, but quite often causal relationships are different for different populations.\nUnfortunately, psychologists pervasively assume that research on US and European samples generalizes to the rest of the world, and it often does not. To highlight this issue, Henrich, Heine, and Norenzayan (2010) coined the acronym WEIRD. This catchy name describes the oddness of making generalizations about all of humanity from experiments on a sample that is quite unusual because it is Western, Educated, Industrialized, Rich, and Democratic. Henrich and colleagues argue that seemingly “fundamental” psychological functions like visual perception, spatial cognition, and social reasoning all differ pervasively across populations—hence, any generalization from an effect estimated with a WEIRD subpopulation may be unwarranted.\nIn the early 2000s, researchers found that gratitude interventions—like writing a brief essay about something nice that somebody did for you—increased happiness in studies conducted in Western countries. Based on these findings, some psychologists believed that gratitude interventions could increase happiness in all people. But it seems they were wrong. A few years later, Layous et al. (2013) ran a gratitude experiment in two locations: the United States and South Korea. Surprisingly, the gratitude intervention decreased happiness in the South Korean sample. The researchers attributed this negative effect to feelings of indebtedness that people in South Korea more prominently experienced when reflecting on gratitude. In this example, we would say that the findings obtained with the US sample may not generalize to people in South Korea.\nIssues of generalizability extend to all aspects of an experiment, not just its sample. For example, even if our hypothetical cash intervention experiment resulted in gains in happiness, we might not be warranted in generalizing to different ways of providing money. Perhaps there was something special about the amount of money we gave or the way we provided it that led to the effect we observed. Without testing multiple different intervention types, we can’t make a broad claim. As we’ll see in chapter 7 and chapter 9, this issue has consequences for both our statistical analyses and our experimental designs (Yarkoni 2020).\nQuestions of generalizability are pervasive, but the first step is to simply acknowledge and reason about them. Perhaps all papers should have a Constraints on Generality statement, where researchers discuss whether they expect their findings to generalize across different samples, experimental stimuli, procedures, and historical and temporal features (Simons, Shoda, and Lindsay 2017). This kind of statement would at least remind researchers to be humble: experiments are a powerful tool for understanding how the world works, but there are limits to what any individual experiment can teach us.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#anatomy-of-a-randomized-experiment",
    "href": "001-experiments.html#anatomy-of-a-randomized-experiment",
    "title": "1  Experiments",
    "section": "1.4 Anatomy of a randomized experiment",
    "text": "1.4 Anatomy of a randomized experiment\n\n\n\n\n\n\nFigure 1.6: The anatomy of a randomized experiment.\n\n\n\nNow is a good time for us to go back and consolidate the anatomy of an experiment, since this anatomy is used throughout the book. Figure 1.6 shows a simple two-group experiment like our possible money-happiness intervention. A sample is taken from a larger population, and then participants in the sample are randomly assigned to one of two conditions (the manipulation)—either the experimental condition, in which money is provided, or the control condition, in which none is given. Then an outcome measure—happiness—is recorded for each participant.\nWe’ll have a lot more to say about all of these components in subsequent chapters. We’ll discuss measures in chapter 8, because good measurement is the foundation of a good experiment. Then in chapter 9, we’ll discuss the different kinds of experimental designs that are possible and their pros and cons. Finally, we’ll cover the process of sampling in chapter 10.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nAn experiment with very unclear causal inferences\nThe Stanford Prison Experiment is one of the most famous studies in the history of psychology. Participants were randomly assigned to play the role of “guards” and “prisoners” in a simulation of prison life inside the Stanford Department of Psychology building (Zimbardo 1972). Designed to run for two weeks, the simulation had to be ended after six days due to the cruelty of the participants acting as guards, who apparently engaged in a variety of dehumanizing behaviors toward the simulated prisoners. This result is widely featured in introductory psychology textbooks and is typically interpreted as showing the power of situational factors: in the right context, even undergraduate students at Stanford could quickly be convinced to act out the kind of inhumane behaviors found in the worst prisons in the world (Griggs 2014).\nIn the years since the study was initially reported, a variety of information has surfaced that makes the causal interpretation of its situational manipulation much less clear (Le Texier 2019). Guards were informed of the objectives of the experiment and given instructions on how to achieve these objectives. The experimenters themselves suggested some harsh punishments whose later use was given as evidence for the emergence of dehumanizing behaviors. Further, both guards and prisoners were coached extensively by the experimenter throughout the study. Some participants have reported that their responses during the study were exaggerated or fabricated (Blum 2018). All of these issues substantially undermine the idea that the assignment of participants’ roles (the ostensible experimental manipulation) was the sole cause of the observed behaviors.\nThe conduct of the study was also unethical. In addition to the question of whether such a study—with all of its risks to the participants—would be ethical at all, a number of features of the study clearly violate the guidelines that we’ll learn about in chapter 4. Participants were prevented from exiting the study voluntarily. The guards were deceived into believing that they were research assistants, rather than participants in the study. And to top it off, the study was reported inaccurately, with reports emphasizing the organic emergence of behaviors, the immersive nature of the simulation, and the extensive documentation of the experiment. In fact, the participants were instructed extensively, the simulation was repeatedly interrupted by mundane details of the research environment, and relatively little of the experiment was captured on video and analyzed.\nThe Prison Experiment is a fascinating and problematic episode in the history of psychology, but it provides very little causal evidence about the human mind.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#an-experiment-with-very-unclear-causal-inferences",
    "href": "001-experiments.html#an-experiment-with-very-unclear-causal-inferences",
    "title": "1  Experiments",
    "section": "An experiment with very unclear causal inferences",
    "text": "An experiment with very unclear causal inferences\nThe Stanford Prison Experiment is one of the most famous studies in the history of psychology. Participants were randomly assigned to play the role of “guards” and “prisoners” in a simulation of prison life inside the Stanford Department of Psychology building (Zimbardo 1972). Designed to run for two weeks, the simulation had to be ended after six days due to the cruelty of the participants acting as guards, who apparently engaged in a variety of dehumanizing behaviors toward the simulated prisoners. This result is widely featured in introductory psychology textbooks and is typically interpreted as showing the power of situational factors: in the right context, even undergraduate students at Stanford could quickly be convinced to act out the kind of inhumane behaviors found in the worst prisons in the world (Griggs 2014).\nIn the years since the study was initially reported, a variety of information has surfaced that makes the causal interpretation of its situational manipulation much less clear (Le Texier 2019). Guards were informed of the objectives of the experiment and given instructions on how to achieve these objectives. The experimenters themselves suggested some harsh punishments whose later use was given as evidence for the emergence of dehumanizing behaviors. Further, both guards and prisoners were coached extensively by the experimenter throughout the study. Some participants have reported that their responses during the study were exaggerated or fabricated (Blum 2018). All of these issues substantially undermine the idea that the assignment of participants’ roles (the ostensible experimental manipulation) was the sole cause of the observed behaviors.\nThe conduct of the study was also unethical. In addition to the question of whether such a study—with all of its risks to the participants—would be ethical at all, a number of features of the study clearly violate the guidelines that we’ll learn about in chapter 4. Participants were prevented from exiting the study voluntarily. The guards were deceived into believing that they were research assistants, rather than participants in the study. And to top it off, the study was reported inaccurately, with reports emphasizing the organic emergence of behaviors, the immersive nature of the simulation, and the extensive documentation of the experiment. In fact, the participants were instructed extensively, the simulation was repeatedly interrupted by mundane details of the research environment, and relatively little of the experiment was captured on video and analyzed.\nThe Prison Experiment is a fascinating and problematic episode in the history of psychology, but it provides very little causal evidence about the human mind.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "001-experiments.html#chapter-summary-experiments",
    "href": "001-experiments.html#chapter-summary-experiments",
    "title": "1  Experiments",
    "section": "1.5 Chapter summary: Experiments",
    "text": "1.5 Chapter summary: Experiments\nIn this chapter, we defined an experiment as a combination of a manipulation and a measure. When combined with randomization, experiments allow us to make strong causal inferences, even when we are studying people (who are hard to hold constant). Nonetheless, there are limits to the power of experiments: there are always constraints on the sample, experimental stimuli, and procedure that limit how broadly we can generalize.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nImagine that you run a survey and find that people who spend more time playing violent video games tend to be more aggressive (i.e., that there is a positive correlation between violent video games and aggression). Following figure 1.2, list three reasons why these variables may be correlated.\nSuppose you wanted to run an experiment testing whether playing violent video games causes increases in aggression. What would be your manipulation, and what would be your measure? How would you deal with potential confounding by variables like age?\nConsider an experiment designed to test people’s food preferences. The experimenter randomly assigns thirty US preschoolers to be served either asparagus or chicken tenders and then asks them how much they enjoyed their meal. Overall, children enjoyed the meat more; the experimenter writes a paper claiming that humans prefer meat over vegetables. List some constraints on the generalizability of this study. In light of these constraints, is this study (or some modification) worth doing at all?\nConsider the Milgram study, another classic psychology study (and our case study in chapter 4). Does this study meet our definition of an experiment?\n\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA basic introduction to causal inference from a social science perspective: Huntington-Klein, N. (2022). The Effect: An Introduction to Research Design and Causality. Chapman & Hall. Available free online at https://theeffectbook.net.\nA slightly more advanced treatment, focusing primarily on econometrics: Cunningham, S. (2021). Causal Inference: The Mixtape. Yale Press. Available free online at https://mixtape.scunning.com.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "002-theories.html",
    "href": "002-theories.html",
    "title": "2  Theories",
    "section": "",
    "text": "2.1 What is a psychological theory?\nThe definition we just gave for a psychological theory is that it is a proposed set of causal relationships among constructs that helps us explain behavior. Let’s look at the ingredients of a theory: the constructs and the relationships between them. Then we can ask about how this definition relates to other things that get called “theories” in psychology.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#what-is-a-psychological-theory",
    "href": "002-theories.html#what-is-a-psychological-theory",
    "title": "2  Theories",
    "section": "The cost of a bad theory",
    "text": "2.1.1 Psychological constructs\nConstructs are the psychological variables that we want our theory to describe, like “money” and “happiness” in the example from last chapter. At first glance, it might seem odd that we need a specific name for these variables. But in probing the relationship between money and happiness, we will have to figure out a way to measure happiness. Let’s say we just ask people to answer the question “How happy are you?” by giving ratings on a 1 (miserable) to 10 (elated) scale.\nNow say someone in the study reports they are an 8 on this scale. Is this really how happy they are? What if they weren’t concentrating very hard on the rating, or if they thought the researcher wanted them to be happy? What if they act much less happy in their interactions with family and friends?\nWe resolve this dilemma by saying that the self-report ratings we collect are only a measure of a latent construct, happiness. The construct is latent because we can never see it directly, but we think it has a causal influence on the measure: happier people should, on average, provide higher ratings. But many other factors can lead to noise or bias in the measurement, so we shouldn’t mistake those ratings as actually being the construct.\nThe particular question “How happy are you?” is one way of going from the general construct to a specific measure. The general process of going from construct to a specific instantiation that can be measured or manipulated is called operationalization. Happiness can be operationalized by self-report, but it can also be operationalized many other way—for example, through a measure like the use of positive language in a personal essay, or by ratings by friends, family, or a clinician. These decisions about how to operationalize a construct with a particular measure are tricky and consequential, and we discuss them extensively in chapter 8. Each different operationalization might be appropriate for a specific study, yet it would require some justification and argument to connect each one to the others.\nProposing a particular construct is a very important part of making a theory. For example, a researcher might worry that self-reported happiness is very different than someone’s well-being as observed by the people around them and assert that happiness is not a single construct but rather a group of distinct constructs. This researcher would then be surprised to know that self-reports of happiness relate very highly to others’ perceptions of a person’s well-being (Sandvik, Diener, and Seidlitz 1993).1\n1 Sometimes positing the construct is the key part of a theory. g (general intelligence) is the classic psychological example of a single-construct theory. The idea behind g theory is that the best measure of general intelligence is the shared variance between a wide variety of different tests. The decision to theorize about and measure a single unified construct for intelligence—rather than say, many different separate kinds of intelligence—is itself a controversial move.Even external, apparently nonpsychological variables like money don’t have direct effects on people but rather operate through psychological constructs. People studying money seriously as a part of psychological theories think about perceptions of money in different ways depending on the context. For example, researchers have written about the importance of how much money you have on hand based on when in the month your paycheck arrives (Ellwood-Lowe, Foushee, and Srinivasan 2022) but have also considered perceptions of long-term accumulation of wealth as a way of conceptualizing people’s understanding of the different resources available to White and Black families in the United States (Kraus et al. 2019).\nFinally, a construct can be operationalized through a manipulation: in our money-happiness example, we operationalized “more money” in our theory with a gift of a specific amount of cash. We hope you see through these examples that operationalization is a huge part of the craft of being a psychology researcher—taking a set of abstract constructs that you’re interested in and turning them into a specific experiment with a manipulation and a measure that tests your causal theory. We’ll have a lot more to say about how this is done in chapter 9.\n\n\n2.1.2 The relationships between constructs\nConstructs gain their meaning in part via their own definitions and operationalizations, but also in part through their causal relationships to other constructs. Figure 2.1 shows a schematic of what this kind of theory might look like—as you can see, it looks a lot like the DAGs that we introduced in the last chapter! That’s no accident. The arrows here also describe hypothesized causal links.2\n2 Sometimes these kind of diagrams are used in the context of a statistical method called structural equation modeling (SEM), where circles represent constructs and lines represent their relationships with one another. Confusingly, structural equation models are also used by many researchers to describe psychological theories. The important point for now is that they are one particular statistical formalism, not a general tool for theory building—the points we are trying to make here are more general.\n\n\n\n\n\nFigure 2.1: A schematic of what a theory might look like.\n\n\n\nThis web of constructs and assumptions is what Cronbach and Meehl (1955) referred to as a “nomological network”—a set of proposals about how different entities are connected to one another. The tricky part is that the key constructs are never observed directly. They are in people’s heads.3 So researchers only get to probe them by measuring them through specific operationalizations.\n3 We’re not saying these should correspond to specific brain structures. They could, but most likely they won’t. The idea that psychological constructs are not the same as any particular brain state (and especially not any particular brain region) is called “multiple realizability” by philosophers, who mostly agree that psychological states can’t be reduced to brain states, as much as philosophers agree on anything (Block and Fodor 1972).One poetic way of thinking about this idea is that the theoretical system of constructs “floats … above the plane of observation and is anchored to it by the rules of [measurement].” (Hempel 1952). So, even if your theory posits that two constructs (say, money and happiness) are directly related, the best you can do is manipulate one operationalization and measure another operationalization. If this manipulation doesn’t produce any effect, it’s possible that you are wrong and money does not cause happiness—but it is also possible that your operationalizations are poor.\nHere’s a slightly different way of thinking about a theory. A theory provides a compression of potentially complex data into much a smaller set of general factors. If you have a long sequence of numbers, say [2 4 8 16 32 64 128 256 …], then the expression \\(2^n\\) serves as a compression of this sequence—it’s a short expression that tells you what numbers are in vs out of the sequence. In the same way, a theory can compress a large set of observations (maybe data from many experiments) into a small set of relationships between constructs. Now, if your data are noisy, say [2.2 3.9 8.1 16.1 31.7 …], then the theory will not be a perfect representation of the data. But it will still be useful.\nIn particular, having a theory allows you to explain observed data and predict new data. Both of these are good things for a theory to do. For example, if it turned out that the money causes happiness theory was true, we could use it to explain observations such as greater levels of happiness among wealthy people. We could also make predictions about the effects of policies like giving out a universal basic income on overall happiness.4 Explanation is an important feature of good theories, but it’s also easy to trick yourself by using a vague theory to explain a finding post hoc (after the fact). Thus, the best test of a theory is typically a new prediction, as we discuss below.\n4 The relationship between money and happiness is actually much more complicated than what we’re assuming here. For example, Killingsworth, Kahneman, and Mellers (2023) describes a collaboration between two sets of researchers that had different viewpoints on the connection between money and happiness.One final note: Causal diagrams are a very useful formalism, but they leave the generalizability of the causal relationships implicit. For example, will more money result in more happiness for everyone, or just for people at particular ages or in particular cultural contexts? “Who does this theory apply to?” is an important question to ask about any proposed causal framework.\n\n\n2.1.3 Specific theories vs general frameworks\nYou may be thinking, “Psychology is full of theories but they don’t look that much like the ones you’re talking about!” Very few of the theories that bear this label in psychology describe causal relationships linking clearly defined and operationalized constructs. You also don’t see that many DAGs, though these are getting (slightly) more common lately (Rohrer 2018).\nHere’s an example of something that gets called a theory yet doesn’t share the components described above. Bronfenbrenner’s (1992) ecological systems theory (EST) is pictured in figure 2.2. The key thesis of this theory is that children’s development occurs in a set of nested contexts that each affect one another and in turn affect the child. This theory has been immensely influential. Yet, if it’s read as a causal theory, it’s almost meaningless: nearly everything connects to everything in both directions, and the constructs are not operationalized—it’s very hard to figure out what kind of predictions it makes!\n\n\n\n\n\n\n\nFigure 2.2: The diagram often used to represent Bronfenbrenner’s ecological systems theory. Note that circles no longer denote discrete constructs; arrows can be interpreted as causal relationships, but all constructs are assumed to be fully connected.\n\n\nEcological systems theory is not really a theory in the sense that we are advocating for in this chapter—and the same goes for many other very interesting ideas in psychology. It’s not a set of causal relationships between constructs that allow specific predictions about future observations. Ecological systems theory is instead a broad set of ideas about what sorts of theories are more likely to explain specific phenomena. For example, it helps remind us that a child’s behavior is likely to be influenced by a huge range of factors, such that any individual theory cannot just focus on an individual factor and hope to provide a full explanation. In this sense, EST is a framework: it guides and inspires specific theories—in the sense we’ve discussed here, namely a set of causal relationships between constructs—without being a theory itself.\nFrameworks like EST are often incredibly important. They can also make a big difference to practice. For example, EST supports a model in social work in which children’s needs are considered not only as the expression of specific internal developmental issues but also as stemming from a set of overlapping contextual factors (Ungar 2002). Concretely, a therapist might be more likely to examine family, peer, and school environments when analyzing a child’s situation through the lens of EST.\nThere’s a continuum between precisely specified theories and broad frameworks. Some theories propose interconnected constructs but don’t specify the relationships between them, or don’t specify how those constructs should be operationalized. So when you read a paper that says it proposes a “theory,” it’s a good idea to to ask whether it describes specific relations between operationalized constructs. If it doesn’t, it may be more of a framework than a theory.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nThe cost of a bad theory\nTheory development isn’t just about knowledge for knowledge’s sake—it has implications for the technologies and policies built off the theories.\nOne case study comes from Edward Clarke’s infamous theory regarding the deleterious effects of education for women. Clarke posited that (1) cognitive and reproductive processes relied on the same fixed pool of energy, (2) relative to men, women’s reproductive processes required more energy, and that (3) expending too much energy on cognitive tasks like education depleted women of the energy needed to maintain a healthy reproductive system. Based on case studies, Clarke suggested that education was causing women to become ill, experience fertility issues, and birth weaker children. He thus concluded that “boys must study and work in a boy’s way, and girls in a girl’s way.” (Clarke 1884, 19).\nClarke’s work is a chilling example of the implication of a poorly developed theory. In this scenario, Clarke had neither instruments that allowed him to measure his constructs nor experiments to measure the causal connections between them. Instead, he merely highlighted case studies that were consistent with his idea (while simultaneously dismissing cases that were inconsistent). His ideas eventually lost favor—especially as they were subjected to more rigorous tests. But Clarke’s arguments were used to attempt to dissuade women from pursuing higher education and hindered educational policy reform.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#the-cost-of-a-bad-theory",
    "href": "002-theories.html#the-cost-of-a-bad-theory",
    "title": "2  Theories",
    "section": "",
    "text": "Theory development isn’t just about knowledge for knowledge’s sake—it has implications for the technologies and policies built off the theories.\nOne case study comes from Edward Clarke’s infamous theory regarding the deleterious effects of education for women. Clarke posited that (1) cognitive and reproductive processes relied on the same fixed pool of energy, (2) relative to men, women’s reproductive processes required more energy, and that (3) expending too much energy on cognitive tasks like education depleted women of the energy needed to maintain a healthy reproductive system. Based on case studies, Clarke suggested that education was causing women to become ill, experience fertility issues, and birth weaker children. He thus concluded that “boys must study and work in a boy’s way, and girls in a girl’s way.” (Clarke 1884, 19).\nClarke’s work is a chilling example of the implication of a poorly developed theory. In this scenario, Clarke had neither instruments that allowed him to measure his constructs nor experiments to measure the causal connections between them. Instead, he merely highlighted case studies that were consistent with his idea (while simultaneously dismissing cases that were inconsistent). His ideas eventually lost favor—especially as they were subjected to more rigorous tests. But Clarke’s arguments were used to attempt to dissuade women from pursuing higher education and hindered educational policy reform.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#how-do-we-test-theories",
    "href": "002-theories.html#how-do-we-test-theories",
    "title": "2  Theories",
    "section": "2.2 How do we test theories?",
    "text": "2.2 How do we test theories?\nOur view of psychological theories is that they describe a set of relationships between different constructs. How can we test theories and decide which one is best? We’ll first describe falsificationism, a historical viewpoint on this issue that has been very influential in the past and that connects to ideas about statistical inference presented in chapter 6. We’ll then turn to a more modern viewpoint, holism, that recognizes the interconnections between theory and measurement.\n\n2.2.1 Falsificationism\nOne historical view that resonates with many scientists is the philosopher Karl Popper’s falsificationism. In particular, there is a simplistic version of falsificationism that is often repeated by working scientists, even though it’s much less nuanced than what Popper actually said! On this view, a scientific theory is a set of hypotheses about the world that instantiate claims like the connection between money and happiness.5 What makes a statement a scientific hypothesis is that it can be disproved (i.e., it is falsifiable) by an observation that contradicts it. For example, observing a lottery winner who immediately becomes depressed would falsify the hypothesis that receiving money makes you happier.\n5 Earlier we treated the claim that money caused happiness as a theory. It is one! It’s just a very simple theory that has only one hypothesized connection in it.For the simplistic falsificationist, theories are never confirmed. The hypotheses that form parts of theories are universal statements. You can never prove them right; you can only fail to find falsifying evidence. Seeing hundreds of people get happier when they received money would not prove that the money-happiness hypothesis was universally true. There could always be a counter-example around the corner.\nThis theory doesn’t really describe how scientists work. For example, scientists like to say that their evidence “supports” or “confirms” their theory, and falsificationism rejects this kind of talk. A falsificationist says that confirmation is an illusion; that the theory is simply surviving to be tested another day. This strict falsificationist perspective is unpalatable to many scientists. After all, if we observe that hundreds of people get happier when they receive money, it seems like this should at least slightly increase our confidence that money causes happiness!6\n6 An alternative perspective comes from the Bayesian tradition that we’ll learn more about in chapters 5 and 6. In a nutshell, Bayesians propose that our subjective belief in a particular hypothesis can be captured by a probability, and that our scientific reasoning can then be described by a process of normative probabilistic reasoning (Strevens 2006). The Bayesian scientist distributes probability across a wide range of alternative hypotheses; observations that are more consistent with a hypothesis increase the hypothesis’s probability (Sprenger and Hartmann 2019).\n\n2.2.2 A holistic viewpoint on theory testing\nThe key issue that leads us to reject strict falsificationism is the observation that no individual hypothesis (a part of a theory) can be falsified independently. Instead, a large series of what are called auxiliary assumptions (or auxilliary hypotheses) are usually necessary to link an observation to a theory (Lakatos 1976). For example, if giving some individual person money didn’t change their happiness, we wouldn’t immediately throw out our theory that money causes happiness. Instead, the fault might be in any one of our auxiliary assumptions, like our measurement of happiness, or our choice of how much money to give or when to give it. The idea that individual parts of a theory can’t be falsified independently is sometimes called holism.\nOne consequence of holism is that the relationship between data and theory isn’t always straightforward. An unexpected observation may not cause us to give up on a main hypothesis in our theory—but it will often cause us to question our auxiliary assumptions instead (e.g., how we operationalize our constructs). Thus, before abandoning our theory of money causing happiness, we might want to try several happiness questionnaires.\nThe broader idea of holism is supported by historical and sociological studies of how science progresses, especially in the work of Kuhn (1962). Examining historical evidence, Kuhn found that scientific revolutions didn’t seem to be caused by the falsification of a theoretical statement via an incontrovertible observation. Instead, Kuhn described scientists as mostly working within paradigms: sets of questions, assumptions, methods, phenomena, and explanatory hypotheses.\nParadigms allow for activities Kuhn described as normal science—that is, testing questions within the paradigm, explaining new observations, or modifying theory to fit these paradigms. But normal science is punctuated by periods of crisis when scientists begin to question their theory and their methods. Crises don’t happen just because a single observation is inconsistent with the current theory. Rather, there will often be a holistic transition to a new paradigm, typically because of a striking explanatory or predictive success—often one that’s outside the scope of the current working theory entirely.\n\n\nIn sum, the lesson of holism is that we can’t just put our theories in direct contact with evidence and think that they will be supported or overturned. Instead, we need to think about the scope of our theory (in terms of the phenomena and measures it is meant to explain), as well as the auxiliary hypotheses—operationalizations—that link it to specific observations.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#designing-experiments-to-test-theory",
    "href": "002-theories.html#designing-experiments-to-test-theory",
    "title": "2  Theories",
    "section": "2.3 Designing experiments to test theory",
    "text": "2.3 Designing experiments to test theory\n\n\n\n\n\n\n\nFigure 2.3: A roulette wheel. Betting on red is not that risky, but betting all your chips on a particular value (*) is much riskier.\n\n\n7 Even if you’re not a falsificationist like Popper, you can still think it’s useful to try and falsify theories! Although a single observation is not always enough to overturn a theory, it’s still a great research strategy to look for those observations that are most inconsistent with the theory.One way of looking at theories is that they let us make bets. If we bet on a spin of the roulette wheel in figure 2.3 that it will show us red as opposed to black, we have almost a 50% chance of winning the bet. Winning such a bet is not impressive. But if we call a particular number, the bet is riskier because we have a much smaller chance of being right. Cases where a theory has many chances to be wrong are called risky tests (Meehl 1978).7\nMuch psychology consists of verbal theories. Verbal theories make only qualitative predictions, so it is hard convincingly show them to be wrong (Meehl 1990). In our discussion of money and happiness, we just expected happiness to go up as money increased. We would have accepted any increase in happiness (even if very small) as evidence confirming our hypothesis. Predicting that it does is a bit like betting on red with the roulette wheel—it’s not surprising or impressive when you win. And in psychology, verbal theories often predict that multiple factors interact with one another. With these theories, it’s easy to say that one or the other was “dominant” in a particular situation, meaning you can predict almost any direction of effect.\nTo test theories, we should design experiments to test conditions where our theories make “risky” predictions. A stronger version of the money-happiness theory might suggest that happiness increases linearly in the logarithm of income (Killingsworth, Kahneman, and Mellers 2023). This specific mathematical form for the relationship—as well as the more specific operationalization of money as income—creates opportunities for making much riskier bets about new experiments. This kind of case is more akin to betting on a specific number on the roulette wheel: when you win this bet, it is quite surprising!8\n8 Theories are often developed iteratively. It’s common to start with a theory that is less precise and hence, that has fewer opportunities for risky tests. But by collecting data and testing different alternatives, it’s often possible to refine the theory so that it is more specific and allows riskier tests. As we discuss below, formalizing theories using mathematical or computational models is one important route to making more specific predictions and creating riskier tests.Testing theoretical predictions also requires precise experimental measurements. As we start to measure the precision of our experimental estimates in chapter 6, we’ll see that the more precise our estimate is, the more values are inconsistent with it. In this sense, a risky test of a theory requires both a very specific prediction and a precise measurement. (Imagine spinning the roulette wheel but seeing such a blurry image of the result that you can’t really tell where the ball is. Not very useful.)\nEven when theories make precise predictions, they can still be too flexible to be tested. When a theory has many free parameters—numerical values that can be fit to a particular dataset, changing the theories predictions on a case-by-case basis—then it can often predict a wide range of possible results. This kind of flexibility reduces the value of any particular experimental test, because the theorist can always say after the fact that the parameters were wrong but not the theory itself (Roberts and Pashler 2000).\nOne important way to remove this kind of flexibility is to make predictions in advance, holding all parameters constant. A preregistration is a great way to do this—the experimenter derives predictions and specifies in advance how they will be compared to the results of the experiment. We’ll talk much more about the process of preregistration in chapter 11.\nWe’ve been focusing mostly on testing a single theory. But the best state of affairs is if a theory can make a very specific prediction that other theories don’t make. If competing theories both predict that money increases happiness to the same extent, then data consistent with that predicted relationship don’t differentiate between the theories, no matter how specific the prediction might be. The experiment that teaches us the most is going to be the one where a very specific pattern of data is predicted according to one theory and another.9\n9 We can use this idea, which comes from Bayesian statistics, to try to figure out what the right experiment is by considering which specific experimental conditions derive differences between theories. In fact, the idea of choosing experiments based on the predictions that different theories make has a long history in statistics (Lindley 1956); it’s now called optimal experiment design (Myung, Cavagnaro, and Pitt 2013). The idea is, if you have two or more theories spelled out mathematically or computationally, you can simulate their predictions across a lot of conditions and pick the most informative conditions to run as an actual experiment.Given all of this discussion, as a researcher trying to come up with a specific research idea, what do you do? Our advice is: follow the theories. That is, for the general topic you’re interested in—whether it’s money and happiness, bilingualism, the nature of concepts, or depression—try to get a good sense of the existing theories. Not all theories will make specific, testable predictions, but hopefully some will! Then ask, what are the “risky bets” that these theories make? Do different theories make different bets about the same effect? If so, that’s the effect you want to measure!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#formalizing-theories",
    "href": "002-theories.html#formalizing-theories",
    "title": "2  Theories",
    "section": "2.4 Formalizing theories",
    "text": "2.4 Formalizing theories\nSay we have a set of constructs we want to theorize about. How do we describe our ideas about the relationships between them so that we can make precise predictions that can be compared with other theories? As one writer noted, mathematics is “unreasonably effective” as a vocabulary for the sciences (Wigner 1990). Indeed, there have been calls for greater formalization of theory in psychology for at least the last 50 years (Harris 1976).\n\n\n\n\n\n\n\ndepth\n\n\n\n\n\nA universal law of generalization?\nHow do you take what you know and apply it to a new situation? One answer is that you use the same answer that has worked in similar situations. To do this kind of extrapolation, however, you need a notion of similarity. Early learning theorists tried to measure similarity by creating an association between a stimulus—say a projected circle of light of a particular size—and a reward by repeatedly presenting them together. After this association was learned, they would test generalization by showing circles of different sizes and measuring the strength of the expectation for a reward. These experiments yielded generalization curves: the more similar the stimulus, the more people and other animals would give the same response, signaling generalization.\nShepard (1987) was interested in unifying the results of these different experiments. The first step in this process was establishing a stimulus space. He used a procedure called “multidimensional scaling” to infer how close stimuli were to each other on the basis of how strong the generalization between them was. When he plotted the strength of the generalization by the distance between stimuli within this space (their similarity), he found an incredibly consistent pattern: generalization decreased exponentially as similarity decreased.\nHe argued that this described a “universal law” that governed the relationship between similarity and generalization for almost any stimulus, whether it was the size of circles, the color of patches of light, or the similarity between speech sounds. Later work has even extended this same framework to highly abstract dimensions such as the relationships between numbers of different types (e.g., being even or being powers of 2; Tenenbaum 1999).\n\n\n\n\n\n\nFigure 2.4: The causal theory of similarity and generalization posited by Shepard (1987).\n\n\n\nThe pattern shown in Shepard’s work is an example of inductive theory building. In the vocabulary we’re developing, Shepard ran (or obtained the data from) randomized experiments in which the manipulation was stimulus dimension (e.g., circle size) and the measure was generalization strength. Then the theory that Shepard proposed was that manipulations of stimulus dimension acted to change the perceived similarity between the stimuli. His theory thus linked two constructs: stimulus similarity and generalization strength (figure 2.4). Critically the causal relationship he described was not just a qualitative relationship but instead a specific mathematical form.\nIn the conclusion of his paper, Shepard (1987, 1323) wrote: “Possibly, behind the diverse behaviors of humans and animals, as behind the various motions of planets and stars, we may discern the operation of universal laws.”. While Shepard’s dream is an ambitious one, it defines an ideal for psychological theorizing.\n\n\n\nThere is no one approach that will be right for theorizing across all areas of psychology (Oberauer and Lewandowsky 2019; Smaldino 2020). Mathematical theories (such as Shepard 1987; see the Depth box above) have long been one tool that allows for precise statements of particular relationships.\nComputational or formal artifacts are not themselves psychological theories, but they can be used to create psychological theories via the mapping of constructs onto entities in the model and the use of the principles of the formalism to instantiate psychological hypotheses or assumptions (Guest and Martin 2021).10 Yet stating such clear and general laws feels out of reach in many cases. If we had more Shepard-style theorists or theories, perhaps we’d be in a better place. Or perhaps such “universal laws” are simply out of reach for most of human behavior.\n10 This book won’t go into more details about routes to building computational theories, but if you are interested, we encourage you to explore these frameworks as a way to deepen your theoretical contributions and to sharpen your experimental choices.An alternative approach creates statistical models of data that incorporate substantive assumptions about the structure of the data. We use such models all the time for data analysis. The trouble is, we often don’t interpret them as having substantive assumptions about the structure of the data, even when they do (Fried 2020). But if we examine these assumptions explicitly, even the simplest statistical models can be productive tools for building theories.\nFor example, if we set up a simple linear regression model to estimate the relationship between money and happiness, we’d be positing a linear relationship between the two variables—that an increase in one would always lead to a proportional increase in the other.11 If we fit the model to a particular dataset, we could then look at the weights of the model. Our theory might then then be something like, “Giving people $100 causes 0.2 points of increase in happiness on a self-report scale.”\n11 Linear models are ubiquitous in the social sciences because they are convenient to fit, but as theoretical models they are deeply impoverished. There is a lot you can do with a linear regression, but in the end, most interesting processes are not linear combinations of factors!Obviously, this regression model is not a very good theory of the broader relationship between money and happiness, since it posits that everyone’s happiness would be at the maximum on the 10-point scale if you gave them (at most) $4,500. It also doesn’t tell us how this theory would generalize to other people, other measures of happiness, or other aspects of the psychological representation of money such as income or wealth.\nFrom our viewpoint, these sorts of questions are not distractions—they are the critical work of moving from experiment to theory (Smaldino 2020)! In chapter 7, we try to draw out this idea further, reconstruing common statistical tests as models that can be repurposed to express contentful scientific hypotheses while recognizing the limitations of their assumptions.\n\n\n\n\n\n\n\nFigure 2.5: A gradient of specificity in theoretical tools. Figure inspired by Guest and Martin (2021).\n\n\nOne of the strengths of modern cognitive science is that it provides a very rich set of tools for expressing more complex statistical models and linking them to data. For example, the modern Bayesian cognitive modeling tradition grew out of work like Shepard’s; in these models, a system of equations defines a probability distribution that can be used to estimate parameters, predict new data, or make other inferences (Goodman, Tenenbaum, and The ProbMods Contributors 2016). And neural network models—which are now fueling innovations in artificial intelligence—have a long history of being used as substantive models of human psychology (Elman, Bates, and Johnson 1996). One way to think about all these alternatives is as being on a gradient from the general, inspirational frameworks we described above all the way down through computational models and then to statistical models that can be fit to specific datasets (figure 2.5).\nIn our discussion, we’ve presented theories as static entities that are presented, tested, confirmed, and falsified. That’s a simplification that doesn’t take into account the ways that theories—especially when instantiated as formal models—can be flexibly adjusted to accommodate new data (Navarro 2019). Most modern computational theories are more like a combination of core principles, auxiliary assumptions, and supporting empirical assumptions. The best theories are always being enlarged and refined in response to new data.12\n12 In the thinking of the philosopher Imre Lakatos, a “productive” research program is one where the core principles are gradually supplemented with a limited set of additional assumptions to explain a growing base of observations. In contrast, a “degenerate” research program is one in which you are constantly making ad hoc tweaks to the theory to explain each new datapoint (Lakatos 1976).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#a-universal-law-of-generalization",
    "href": "002-theories.html#a-universal-law-of-generalization",
    "title": "2  Theories",
    "section": "A universal law of generalization?",
    "text": "A universal law of generalization?\nHow do you take what you know and apply it to a new situation? One answer is that you use the same answer that has worked in similar situations. To do this kind of extrapolation, however, you need a notion of similarity. Early learning theorists tried to measure similarity by creating an association between a stimulus—say a projected circle of light of a particular size—and a reward by repeatedly presenting them together. After this association was learned, they would test generalization by showing circles of different sizes and measuring the strength of the expectation for a reward. These experiments yielded generalization curves: the more similar the stimulus, the more people and other animals would give the same response, signaling generalization.\nShepard (1987) was interested in unifying the results of these different experiments. The first step in this process was establishing a stimulus space. He used a procedure called “multidimensional scaling” to infer how close stimuli were to each other on the basis of how strong the generalization between them was. When he plotted the strength of the generalization by the distance between stimuli within this space (their similarity), he found an incredibly consistent pattern: generalization decreased exponentially as similarity decreased.\nHe argued that this described a “universal law” that governed the relationship between similarity and generalization for almost any stimulus, whether it was the size of circles, the color of patches of light, or the similarity between speech sounds. Later work has even extended this same framework to highly abstract dimensions such as the relationships between numbers of different types (e.g., being even or being powers of 2; Tenenbaum 1999).\n\n\n\n\n\n\nFigure 2.4: The causal theory of similarity and generalization posited by Shepard (1987).\n\n\n\nThe pattern shown in Shepard’s work is an example of inductive theory building. In the vocabulary we’re developing, Shepard ran (or obtained the data from) randomized experiments in which the manipulation was stimulus dimension (e.g., circle size) and the measure was generalization strength. Then the theory that Shepard proposed was that manipulations of stimulus dimension acted to change the perceived similarity between the stimuli. His theory thus linked two constructs: stimulus similarity and generalization strength (figure 2.4). Critically the causal relationship he described was not just a qualitative relationship but instead a specific mathematical form.\nIn the conclusion of his paper, Shepard (1987, 1323) wrote: “Possibly, behind the diverse behaviors of humans and animals, as behind the various motions of planets and stars, we may discern the operation of universal laws.”. While Shepard’s dream is an ambitious one, it defines an ideal for psychological theorizing.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "002-theories.html#chapter-summary-theories",
    "href": "002-theories.html#chapter-summary-theories",
    "title": "2  Theories",
    "section": "2.5 Chapter summary: Theories",
    "text": "2.5 Chapter summary: Theories\nIn this chapter, we characterized psychological theories as a set of causal relationships between latent constructs. The role of experiments is to measure these causal relationships and to adjudicate between theories by identifying cases where different theories make different predictions about particular relationships.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nIdentify an influential theory in your field or subfield. Can you draw the “nomological network” for it? What are the key constructs and how are they measured? Are the links between constructs just directional links, or is there additional information about what type of relationship exists? Or does our description of a theory in this chapter not fit your example?\nCan you think of an experiment that falsified a theory in your area of psychology? To what extent is falsification possible for the kinds of theories that you are interested in studying?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA fabulous introduction to issues in the philosophy of science can be found in: Godfrey-Smith, Peter (2009). Theory and Reality: An Introduction to the Philosophy of Science. University of Chicago Press.\nBayesian modeling has been very influential in cognitive science and neuroscience. A good introduction in cognitive science comes from: Lee, Michael D. and Eric-Jan Wagenmakers (2013). Bayesian Cognitive Modeling: A Practical Course. Cambridge University Press. Much of the book is available free online at https://faculty.sites.uci.edu/mdlee/bgm.\nA recent introduction to Bayesian modeling with a neuroscience focus: Ma, Wei Ji, Konrad Paul Kording, and Daniel Goldreich (2023). Bayesian Models of Perception and Action: An Introduction. MIT Press. Free online at https://www.cns.nyu.edu/malab/bayesianbook.html.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Theories</span>"
    ]
  },
  {
    "objectID": "003-replication.html",
    "href": "003-replication.html",
    "title": "3  Replication",
    "section": "",
    "text": "3.1 Reproducibility\nScientific papers are full of numbers: sample sizes, measurements, statistical results, and visualizations. For those numbers to have meaning, and for other scientists to be able to verify them, we need to know where they came from (their provenance). The chain of actions that scientists perform on the raw data, all the way through to reporting numbers in their papers, is sometimes called the analysis pipeline. For much of history, scientific papers have only provided a verbal description of the analysis pipeline, usually with little detail.2\nMoreover, researchers typically do not share key research objects from this pipeline, such as the analysis scripts or the raw data (Hardwicke, Thibault, et al. 2021).3 Without code and data, the numbers reported in scientific papers are often not reproducible—an independent scientist cannot repeat all of the steps in the analysis pipeline and get the same results as the original scientists.\nReproducibility is desirable for a number of reasons. Without it:\nFrom this list, error detection and correction is probably the most pressing issue. But are errors common? There are plenty of individual instances of errors that are corrected in the published literature (e.g., some of us found an error in Cesana-Arlotti et al. 2018), and we ourselves have made significant analytic errors (e.g., Frank et al. 2013). But these kinds of experiences don’t tell us about the frequency of errors more generally (or the consequences of error for the conclusions that researchers draw).4\nEstimating the frequency of errors is a metascientific issue that researchers have attempted to answer over the years. If errors are frequent, that would suggest a need for changes in our policies and practices to reduce their frequency! Unfortunately, the lack of data availability creates a problem: it’s hard to figure out if calculations are wrong if you can’t check them in the first place. Here’s one clever approach to this issue. In standard American Psychological Association (APA) reporting format, inferential statistics must be reported with three pieces of information: the test statistic, the degrees of freedom for the test, and the \\(p\\)-value (e.g., \\(t(18) = -0.74\\), \\(p = 0.47\\)). Yet, these pieces of information are redundant with one another. Thus, reported statistics can be checked for consistency simply by evaluating whether they line up with one another—that is, whether the \\(p\\)-value recomputed from the \\(t\\) and degrees of freedom matches the reported value.\nBakker and Wicherts (2011) performed this kind of statistical consistency analysis on a sample of 281 papers, and found that around 18% of statistical results were incorrectly reported. Even more worrisome, around 15% of articles contained at least one decision error—that is, a case where the error changed the direction of the inference that was made (e.g., from significant to insignificant).5 Nuijten et al. (2016) used an automated method called “statcheck”6 to confirm and extend this analysis. They checked \\(p\\)-values for more than 250,000 psychology papers in the period 1985–2013 and found that around half of all papers contained at least one incorrect \\(p\\)-value!\nThese findings provide a lower bound on the number of errors in the literature and suggest that reproducibility of analyses is likely very important. However, they only address the consistency of statistical reporting. What would happen if we tried to repeat the entire analysis pipeline from start to finish? It’s rather difficult to answer this question at a large scale: firstly, it takes a long time to run a reproducibility check; and secondly, the lack of access to raw data means that for most scientific papers, checking reproducibility is impossible.\nNevertheless, a few years ago a group of us spotted an opportunity to check reproducibility by examining studies published in two journals that either required or encouraged data sharing. Hardwicke et al. (2018) and Hardwicke, Bohn, et al. (2021) first identified studies that shared data, then narrowed those down to studies that shared reusable data (the data were accessible, complete, and comprehensible). For 60 of these articles, we then attempted to reproduce numerical values related to a particular statistical result in the paper. The process was incredibly labor-intensive, with articles typically requiring 5–10 hours of work each. And the results were concerning: the targeted values in only about a third of articles were completely reproducible without help from the original authors! In many cases, after—sometimes extensive—correspondence with the original authors, they provided additional information that was not reported in the original paper. After author contact, the reproducibility success rate improved to 62% (figure 3.2). The remaining papers appeared to have some values that neither we, nor the original authors, could reproduce. Importantly, we didn’t identify any patterns of nonreproducibility that seriously undermined the conclusions drawn in the original articles; however, other reproducibility studies have found a distressingly high number of decision errors (Artner et al. 2020), albeit with a slightly higher success rate overall.\nIn sum: transparency is a critical imperative for decreasing the frequency of errors in the published literature. Reporting and computation errors are frequent in the published literature, and the identification of these errors depends on the findings being reproducible. If data are not available, then errors usually cannot be found.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#reproducibility",
    "href": "003-replication.html#reproducibility",
    "title": "3  Replication",
    "section": "The Open Science Collaboration",
    "text": "2 The situation is nicely summed up by a prescient quote from Buckheit and Donoho (1995, 5): “A scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.”.3 For many years, professional societies, like the American Psychological Association, have mandated data sharing (https://www.apa.org/ethics/code), but only for purposes of verification, and only “on request”—in other words, scientists could keep data hidden by default and it was their responsibility to share if another scientist requested access. In practice, this kind of policy does not work; data are rarely made available on request (Wicherts et al. 2006). We believe this situation is untenable. We provide a longer argument justifying data sharing in chapter 4 and discuss some of the practicalities of sharing in chapter 13.\n\n\nErrors in calculation or reporting could lead to disparities between the reported result and the actual result\nVague verbal descriptions of analytic computations could keep readers from understanding the computations that were actually performed\nThe robustness of data analyses to alternative model specifications cannot be checked\nSynthesizing evidence across studies, a key part of building a cumulative body of scientific knowledge, is much more difficult\n\n\n4 There is a very interesting discussion of the pernicious role of scientific error on theory building in Gould’s (1996) “The Mismeasure of Man.” Gould examines research on racial differences in intelligence and documents how scientific errors that supported racial differences were often overlooked. Errors are often caught asymmetrically; we are more motivated to double-check a result that contradicts our biases.\n\n5 Confirming Gould’s speculation (see note above), most of the reporting errors that led to decision errors were in line with the researchers’ own hypotheses.6 Statcheck is now available as a web app (http://statcheck.io) and an R package (Nuijten and Epskamp 2024) so that you can check your own manuscripts!\n\n\n\n\n\n\n\nFigure 3.2: Analytic reproducibility of results from open-data articles in Cognition and Psychological Science. From Hardwicke, Bohn, et al. (2021), figure 1 (licensed under CC BY 4.0).\n\n\n\n\n\n\n\n\n\n\ncase study\n\n\n\n\n\nThe Open Science Collaboration\nAround 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebecca Saxe (Frank and Saxe 2012). The idea was to introduce students to the nuts and bolts of research by having them run replications. A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies published in top psychology journals in 2008.\nIn the course that year we chose replication projects from the sample that Nosek had told us about. Four of these projects were executed very well and were nominated by the course TAs for inclusion in the broader project. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in figure 3.3.\n\n\n\n\n\n\n\n\nFigure 3.3: Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size represents estimated statistical power. The grey line represents a perfect replication.\n\n\n\n\n\nThe resulting metascience paper, which we and others refer to as the “replication project in psychology” (RPP), made a substantial impression on both psychologists and the broader research community, defining both a field of psychology metascience studies and providing a template for many-author collaborative projects (Open Science Collaboration 2015). But the most striking thing was the result: disappointingly, only around a third of the replications had similar findings to the original studies. The others yielded smaller effects that were not statistically significant in the replication sample (almost all of the original studies were significant). The RPP provided the first large-scale evidence that there were systematic issues with replicability in the psychology literature.\nThe RPP’s results—and their interpretation—were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power and low evidential value in the replications; nonrepresentative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success (Gilbert et al. 2016; Anderson et al. 2016; Etz and Vandekerckhove 2016). In our view, many of these critiques have merit, and you can’t simply interpret the results of RPP as an unbiased estimate of the replicability of results in the literature, contra the title. \nAnd yet, RPP’s results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this—they have flaws but they inspire follow-up studies that can address those problems. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work. Together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#the-open-science-collaboration",
    "href": "003-replication.html#the-open-science-collaboration",
    "title": "3  Replication",
    "section": "",
    "text": "Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebecca Saxe (Frank and Saxe 2012). The idea was to introduce students to the nuts and bolts of research by having them run replications. A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies published in top psychology journals in 2008.\nIn the course that year we chose replication projects from the sample that Nosek had told us about. Four of these projects were executed very well and were nominated by the course TAs for inclusion in the broader project. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in figure 3.3.\n\n\n\n\n\n\n\n\nFigure 3.3: Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size represents estimated statistical power. The grey line represents a perfect replication.\n\n\n\n\n\nThe resulting metascience paper, which we and others refer to as the “replication project in psychology” (RPP), made a substantial impression on both psychologists and the broader research community, defining both a field of psychology metascience studies and providing a template for many-author collaborative projects (Open Science Collaboration 2015). But the most striking thing was the result: disappointingly, only around a third of the replications had similar findings to the original studies. The others yielded smaller effects that were not statistically significant in the replication sample (almost all of the original studies were significant). The RPP provided the first large-scale evidence that there were systematic issues with replicability in the psychology literature.\nThe RPP’s results—and their interpretation—were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power and low evidential value in the replications; nonrepresentative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success (Gilbert et al. 2016; Anderson et al. 2016; Etz and Vandekerckhove 2016). In our view, many of these critiques have merit, and you can’t simply interpret the results of RPP as an unbiased estimate of the replicability of results in the literature, contra the title. \nAnd yet, RPP’s results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this—they have flaws but they inspire follow-up studies that can address those problems. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work. Together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#replication",
    "href": "003-replication.html#replication",
    "title": "3  Replication",
    "section": "3.2 Replication",
    "text": "3.2 Replication\nBeyond verifying a paper’s original analysis pipeline, we are often interested in understanding whether the study can be replicated—if we repeat the study methods and obtain new data, do we get similar results? To quote from Popper (1959, 86), “The scientifically significant … effect may be defined as that which can be regularly [replicated] by anyone who carries out the appropriate experiment in the way prescribed.”\nReplications can be conducted for many reasons (Schmidt 2009). One goal can be to verify that the results of an existing study can be obtained again if the study is conducted again in exactly the same way, to the best of our abilities. A second goal can be to gain a more precise estimate of the effect of interest by conducting a larger replication study, or combining the results of a replication study with the existing study. A third goal can be to investigate whether an effect will persist when, for example, the experimental manipulation is done in a different, but still theory-consistent, manner. Alternatively, we might want to investigate whether the effect persists in a different population. Such replications are often efforts to “replicate and extend,” and are common both when the same research team wants to conduct a sequence of experiments that each build on one another or when a new team wants to build on a result from a paper they have read (Rosenthal 1990).\nMuch of the metascience literature (and attendant debate and discussion) has focused on the first goal—simple verification. This focus has been so intense that the term “replication” has become associated with skepticism or even attacks on the foundations of the field. This dynamic is at odds with the role that replication is given in a lot of philosophy of science, where it is assumed to be a typical part of “normal science.”\n\n3.2.1 Conceptual frameworks for replication\nThe key challenge of replication is invariance—Popper’s stipulation that a replication be conducted “in the way prescribed” in the quote above. That is, what are the features of the world over which a particular observation should be relatively constant, and what are those that are specified as the key ingredients for the effect? Replication is relatively straightforward in the physical and biological sciences, in part because of presupposed theoretical background that allows us to make strong inferences about invariance. If a biologist reports an observation about a particular cell type from an organism, the color of the microscope is presumed not to matter to the observation.\nThese invariances are far harder to state in psychology, for both the procedure of an experiment and its sample. Procedurally, should the color of the experimental stimulus matter to the measured effect? In some cases yes, in some cases no.7 Yet, the task of postulating how a scientific effect should be invariant to lab procedures pales in comparison to the task of postulating how the effect should be invariant across different human populations!8\n\n7 A fascinating study by Baribault et al. (2018) proposes a method for empirically understanding psychological invariances. Treating a subliminal priming effect as their model system, they sampled thousands of “micro-experiments” in which small parameters of their experimental procedure were randomly sampled. These parameters allowed for measurement of their effect of interest, averaging across this irrelevant variation. In their case, it turned out that color did not matter.8 In some sense, the research program of some branches of the social sciences amounts to an understanding of invariances across human cognition.9 Presumably not very much if Dr. Toad gave the original instructions in English instead of in German—that’s another one of these pesky invariances that we are always worrying about!A lot is at stake in this discussion. If Dr. Frog publishes a finding with US undergraduates and Dr. Toad then “replicates” the procedure in Germany, to what extent should we be perturbed if the effect is different in magnitude or absent?9 Meta-researchers have made a number of replication taxonomies to try and quantify the degree of methodological consistency between two experiments.\nSome researchers have tried to distinguish “direct replications”10 and “conceptual replications.” Direct replications are those that attempt to reproduce all of the salient features of the prior study, up to whatever invariances the experimenters believe are present (e.g., color of the paint or gender of the experimenter). In contrast, conceptual replications are typically paradigms that attempt to test the same hypothesis via different operationalizations of the manipulation and/or the measure. We agree with Zwaan et al. (2018): labeling this second type of experiment as a “replication” is a little misleading. Rather, so-called conceptual replications are actually different tests of the same part of your theory. Such tests can be extremely valuable, but they serve a different goal than replication.\n10 These also get called exact replications sometimes. We think this term is misleading because similarity between two different experiments is always going to be on a gradient, and where you cut this continuum is always going to be a theory-laden decision. One person’s “exact” is another’s “inexact.”\n\n\n\n\n\naccident report\n\n\n\n\n\n“Small Telescopes”\nWe’ve been discussing the question of invariance with respect to procedure and sample, but we haven’t really discussed invariance with respect to the studies’ statistical results. To what extent can we consider two statistical results to be “the same”? Several obvious metrics, including those used by RPP, have important limitations (Simonsohn 2015). For example, if one finding is statistically significant and the other isn’t, they still could have effect sizes that are actually quite close to one another, in part because one might have a larger sample size than the other. Or you could have two significant findings that nevertheless have very different effect sizes.\n\n\n\n\n\n\n\n\nFigure 3.4: The original finding by Schwarz and Clore (1983) and two replications with much larger samples. All three estimates include a 95% confidence interval, but the confidence intervals are very small for the two replication studies. The blue dotted line shows the smallest effect that the original study could reasonably have detected. Based on Simonsohn (2015).\n\n\n\n\n\nIn a classic study, Schwarz and Clore (1983) reported that participants (\\(N = 28\\)) rated their life satisfaction as higher on sunny days than rainy days, suggesting that they misattributed temporary happiness about the weather to longer-term life satisfaction. However, when two more recent studies examined very large samples of survey responses, they yielded estimates of the effect that were much smaller. (All of these effects have been standardized so that they are on the same scale using a metric called Cohen’s \\(d\\) that we will introduce more formally in chapter 5.) In one survey, the effect was statistically significant but extremely small; in the other it was essentially zero (figure 3.4). Using statistical significance as the metric of replication success, you might be tempted to say that the first of these studies was a successful replication and the second was a failed replication.\nSimonsohn points out that this interpretation doesn’t make sense, using the analogy of a study’s sample size as a telescope. Following this analogy, Schwarz and Clore had a very small telescope (i.e., a small sample size), and they pointed it in a particular direction and claimed to have observed a planet (i.e., a nonzero effect). Now it might turn out that there was a planet at that location when you look with a much larger telescope (first replication), and it might turn out that there wasn’t (second replication). Regardless, however, the original small telescope was simply not powerful enough to have seen whatever was there. Both studies fail to replicate the original observation, regardless of whether their observed effect was in the same direction.\nFollowing Simonsohn’s example, numerous metrics for replication success have been proposed (Mathur and VanderWeele 2020). The best of these move away from the idea that there is a binary test of whether an individual replication was successful and toward a comparison of the two effects and whether they appear consistent with the same theory. Gelman (2018) suggests the “time reversal” heuristic—rather than thinking of a replication as a success or a failure, consider the alternative world in which the replication study had been performed first and the original study followed it.\nIf we leave behind the idea that the original study has precedence, it makes much more sense to consider the sum total of the evidence across multiple experiments. Using this approach, it seems pretty clear that the weather misattribution effect is, at best, a tiny factor in people’s overall judgments of their life satisfaction, even if a small study once found a larger effect.\n\n\n\n\n\n3.2.2 The metascience of replication\nIn RPP, replication teams reported subjectively that 39% of replications were successful, with 36% reporting a significant effect in the same direction as the original. How generalizable is this estimate—and how replicable is psychological research more broadly? Based on the discussion above, we hope we’ve made you skeptical that this is a well-posed question, at least without additional qualifiers. Any answer is going to have to provide details about the scope of this claim, the definition of replication being used, and the metric for replication success. On the other hand, versions of this question have led to a number of empirical studies that help us better understand the scope of replication issues.\n\nMany subsequent empirical studies of replication have focused on particular subfields or journals, with the goal of informing particular field-specific practices or questions. For example, Camerer et al. (2016) replicated all of the between-subject laboratory articles published in two top economics journals in the period 2011–2014. They found a replication rate of 61% of significant effects in the same direction of the original, higher than the rate in RPP but lower than the naive expectation based on their level of statistical power. Another study attempted to replicate all 21 behavioral experiments published in the journals Science and Nature in 2010–2015, finding a replication rate of 62% significant effects (Camerer et al. 2018). This study was notable because they followed a two-step procedure—after an initial round of replications, they followed up on the failures by consulting with the original authors and pursuing extremely large sample sizes. The resulting estimate thus is less subject to many of the critiques of the original RPP paper. While these types of studies do not answer all the questions that were raised about RPP, they suggest that replication rates for top experiments are not as high as we’d like them to be, even when care is taken with the sampling and individual study protocols.\nOther scientists working in the same field can often predict when an experiment will fail to replicate. Dreber et al. (2015) showed that prediction markets (where participants bet small sums of real money on replication outcomes) made fairly accurate estimates of replication success in the aggregate. This result has itself now been replicated several times (e.g., in the Camerer et al. 2018 study described earlier). Maybe even more surprisingly, there’s some evidence that machine learning models trained on the text of papers can predict replication success (Yang, Youyou, and Uzzi 2020; Youyou, Yang, and Uzzi 2023), though more work still needs to be done to validate these models and understand the features they use. More generally, these two lines of research suggest the possibility of isolating consistent factors that lead to replication success or failure. (In the next section we consider what these factors are in more depth.)\nAlthough more work still needs to be done to get generalizable estimates of replicability, taken together, the metascience literature does provide some clarity on what we should expect. Altogether, the chance of a significant finding in a (well-powered) replication study of a generic experiment in social and cognitive psychology is likely somewhere around 56%. Furthermore, the replication effect will likely be on average 53% as large (Nosek et al. 2022).\nOn the other hand, these large-scale replication studies have substantial limitations as well. With relatively few exceptions, the studies chosen for replication used short, computerized tasks that mostly would fall into the categories of social and cognitive psychology. Further, and perhaps most troubling from the perspective of theory development, they tell us only whether a particular experimental effect can be replicated. They tell us much less about whether the construct that the effect was meant to operationalize is in fact real! We’ll return to the difficult issue of how replication and theory construction relate to one another in the final section of this chapter.\nSome have called the narrative that emerges from the sum of these metascience studies the “replication crisis.” We think of it as a major tempering of expectations with respect to the published literature. Your naive expectation might reasonably be that you could read a typical journal article, select an experiment from it, and replicate that experiment in your own research. The upshot of this literature is, unfortunately, if you try selecting and replicating an exeriment, you might well be disappointed by the result.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nConsequences for the study, consequences for the person\n“Power posing” is the idea that adopting a more open and expansive physical posture might also change your confidence. Carney, Cuddy, and Yap (2010) told 42 participants that they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head); in another condition, the poses were contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more “in charge” in a survey. This result suggested that a two-minute manipulation could lead to striking physiological and psychological changes—in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers’ careers, including becoming a principal piece of evidence in a hugely popular TED talk by Amy Cuddy, one of the authors.\nFollow-up work has questioned these findings, however. A replication study with a larger number of participants (\\(N = 200\\)) failed to find evidence for physiological effects of power posing, even as it did find some effects on participants’ own beliefs (Ranehill et al. 2015). And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had \\(p\\)-values very close to the 0.05 threshold (Simmons and Simonsohn 2017). In light of this evidence, the first author of the replication study bravely made a public statement that she does not believe that “power pose” effects are real (Carney 2016).\nFrom the scientific perspective, it’s very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well known throughout the social psychology community, and overall interest from the lay public has gone down. But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record.\nThe process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tightly associated with the findings, and many critiques of the findings became critiques of the individual. Several commentators used Cuddy’s name as a stand-in for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were harmful to Cuddy personally and her career more generally. \nScientists should critique, reproduce, and replicate results—these are all parts of the progress of normal science. But it’s important to do this in a way that’s sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct:\n\nAlways communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.\nAvoid using language that assumes negative intentions, for example, “the authors misleadingly state that …”\nAsk someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to predict how someone else will experience the tone of your writing; a reader can help you make this judgement.\nConsider communicating personally before communicating publicly. As Joe Simmons, one critic in the power posing debate, said, “I wish I’d had the presence of mind to pick up the phone and call [before publishing my critique]” (Dominus 2017). Personal communication isn’t always necessary (and can be difficult due to asymmetries of power or status), but it can be helpful.\n\nAs we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low-quality science. But we also have a duty to our colleagues and communities to be good to one another.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#small-telescopes",
    "href": "003-replication.html#small-telescopes",
    "title": "3  Replication",
    "section": "“Small Telescopes”",
    "text": "“Small Telescopes”\nWe’ve been discussing the question of invariance with respect to procedure and sample, but we haven’t really discussed invariance with respect to the studies’ statistical results. To what extent can we consider two statistical results to be “the same”? Several obvious metrics, including those used by RPP, have important limitations (Simonsohn 2015). For example, if one finding is statistically significant and the other isn’t, they still could have effect sizes that are actually quite close to one another, in part because one might have a larger sample size than the other. Or you could have two significant findings that nevertheless have very different effect sizes.\n\n\n\n\n\n\n\n\nFigure 3.4: The original finding by Schwarz and Clore (1983) and two replications with much larger samples. All three estimates include a 95% confidence interval, but the confidence intervals are very small for the two replication studies. The blue dotted line shows the smallest effect that the original study could reasonably have detected. Based on Simonsohn (2015).\n\n\n\n\n\nIn a classic study, Schwarz and Clore (1983) reported that participants (\\(N = 28\\)) rated their life satisfaction as higher on sunny days than rainy days, suggesting that they misattributed temporary happiness about the weather to longer-term life satisfaction. However, when two more recent studies examined very large samples of survey responses, they yielded estimates of the effect that were much smaller. (All of these effects have been standardized so that they are on the same scale using a metric called Cohen’s \\(d\\) that we will introduce more formally in chapter 5.) In one survey, the effect was statistically significant but extremely small; in the other it was essentially zero (figure 3.4). Using statistical significance as the metric of replication success, you might be tempted to say that the first of these studies was a successful replication and the second was a failed replication.\nSimonsohn points out that this interpretation doesn’t make sense, using the analogy of a study’s sample size as a telescope. Following this analogy, Schwarz and Clore had a very small telescope (i.e., a small sample size), and they pointed it in a particular direction and claimed to have observed a planet (i.e., a nonzero effect). Now it might turn out that there was a planet at that location when you look with a much larger telescope (first replication), and it might turn out that there wasn’t (second replication). Regardless, however, the original small telescope was simply not powerful enough to have seen whatever was there. Both studies fail to replicate the original observation, regardless of whether their observed effect was in the same direction.\nFollowing Simonsohn’s example, numerous metrics for replication success have been proposed (Mathur and VanderWeele 2020). The best of these move away from the idea that there is a binary test of whether an individual replication was successful and toward a comparison of the two effects and whether they appear consistent with the same theory. Gelman (2018) suggests the “time reversal” heuristic—rather than thinking of a replication as a success or a failure, consider the alternative world in which the replication study had been performed first and the original study followed it.\nIf we leave behind the idea that the original study has precedence, it makes much more sense to consider the sum total of the evidence across multiple experiments. Using this approach, it seems pretty clear that the weather misattribution effect is, at best, a tiny factor in people’s overall judgments of their life satisfaction, even if a small study once found a larger effect.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#consequences-for-the-study-consequences-for-the-person",
    "href": "003-replication.html#consequences-for-the-study-consequences-for-the-person",
    "title": "3  Replication",
    "section": "Consequences for the study, consequences for the person",
    "text": "Consequences for the study, consequences for the person\n“Power posing” is the idea that adopting a more open and expansive physical posture might also change your confidence. Carney, Cuddy, and Yap (2010) told 42 participants that they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head); in another condition, the poses were contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more “in charge” in a survey. This result suggested that a two-minute manipulation could lead to striking physiological and psychological changes—in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers’ careers, including becoming a principal piece of evidence in a hugely popular TED talk by Amy Cuddy, one of the authors.\nFollow-up work has questioned these findings, however. A replication study with a larger number of participants (\\(N = 200\\)) failed to find evidence for physiological effects of power posing, even as it did find some effects on participants’ own beliefs (Ranehill et al. 2015). And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had \\(p\\)-values very close to the 0.05 threshold (Simmons and Simonsohn 2017). In light of this evidence, the first author of the replication study bravely made a public statement that she does not believe that “power pose” effects are real (Carney 2016).\nFrom the scientific perspective, it’s very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well known throughout the social psychology community, and overall interest from the lay public has gone down. But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record.\nThe process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tightly associated with the findings, and many critiques of the findings became critiques of the individual. Several commentators used Cuddy’s name as a stand-in for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were harmful to Cuddy personally and her career more generally. \nScientists should critique, reproduce, and replicate results—these are all parts of the progress of normal science. But it’s important to do this in a way that’s sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct:\n\nAlways communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.\nAvoid using language that assumes negative intentions, for example, “the authors misleadingly state that …”\nAsk someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to predict how someone else will experience the tone of your writing; a reader can help you make this judgement.\nConsider communicating personally before communicating publicly. As Joe Simmons, one critic in the power posing debate, said, “I wish I’d had the presence of mind to pick up the phone and call [before publishing my critique]” (Dominus 2017). Personal communication isn’t always necessary (and can be difficult due to asymmetries of power or status), but it can be helpful.\n\nAs we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low-quality science. But we also have a duty to our colleagues and communities to be good to one another.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#causes-of-replication-failure",
    "href": "003-replication.html#causes-of-replication-failure",
    "title": "3  Replication",
    "section": "3.3 Causes of replication failure",
    "text": "3.3 Causes of replication failure\n\n\n\n\n\n\ndepth\n\n\n\n\n\nContext, moderators, and expertise\nThere are many explanations for failed replications. The wonderful thing about metascience is that these explanations can be tested empirically!\nLet’s start with the idea that specific experimental operationalizations of a theory might be “context sensitive,” especially in subfields, like social psychology, whose theories inherently refer to environmental context (Van Bavel et al. 2016). Critics brought this issue up for RPP, where there were several studies in which the original experimental materials were tailored to one cultural context but then were deployed in another context, potentially leading to failure due to mismatch (Gilbert et al. 2016).\nContext sensitivity seems like a great explanation because in some sense, it must be right. If the context of an experiment includes the vast network of learned associations, practices, and beliefs that we all hold, then there’s no question that an experiment’s materials tap into this context to one degree or another. For example, if your experiment relies on the association between doctor and nurse concepts, you would expect this experiment to work differently in the past when nurse meant something more like nanny (Ramscar 2016).\nOn the other hand, as an explanation of specific replication failures, context sensitivity has not fared very well. The “Many Labs” projects were a series of replication projects in which multiple labs independently attempted to replicate several original studies. (In contrast, in RPP and similar studies, a single replication was conducted for each original study.) Some of the Many Labs projects assessed variation in replication success across different labs. In ManyLabs 2, Klein et al. (2018) replicated 28 findings, distributed across 125 different samples and more than 15,000 participants. ManyLabs 2 found almost no support for the context sensitivity hypothesis as an explanation of replication failure. In general, when effects failed to replicate, they did so when conducted in person as well as when conducted online, and these failures were consistent across many cultures and labs. \nOn the other hand, a review of several Many Labs–style replication projects indicated, on reanalysis, that population effects differed across replication labs even when the replication protocols were very similar to one another (Olsson-Collentine, Wicherts, and Assen 2020; Errington et al. 2021). So, context sensitivity is almost certainly present—and we’ll return to the broader issues of generalizability, context, and invariance in the next section—but so far we have not identified specific forms of context sensitivity that reliably affect replication success.\n\nThese observations—that (1) direct replications vary in how successful they are, but (2) we cannot identify specific contextual moderators—together suggest the possible presence of “hidden moderators.” That is, when faced with a successful original study and a failed replication, there may be some unknown factor(s) that moderates the effect.\nWe’ve personally had several experiences that corroborate the idea that there are hidden moderators. For example, in Lewis and Frank (2016), we were unsuccessful in replicating a simple categorization experiment. We then made a series of iterative changes to the stimuli and instructions, for example, changing the color and pattern of the stimuli (figure 3.5), eventually resulting in a larger (and statistically significant) effect—though still much smaller than the original. Critically, however, each alteration that we made to the procedure yielded a very small change in the effect, and it would have taken us many thousands of participants to figure exactly which alteration made the difference. (If you’re keeping score, here’s a case where stimulus color did matter to the outcome of the experiment!)\n\n\n\n\n\n\nFigure 3.5: Stimuli from Lewis and Frank (2016) (https://github.com/mllewis/xtSamp).\n\n\n\nAnother explanation for replication failure that is often cited is experimenter expertise (e.g., Schwarz and Strack 2014). On this hypothesis, replications fail because the researchers performing the replication do not have sufficient expertise to execute the study. Like context sensitivity, this explanation is almost certainly true for some replications. In our own work, we have repeatedly performed experiments that failed due to our own incompetence!\nYet as an explanation of the pattern of metascience findings, the expertise hypothesis hasn’t been supported empirically. First, team expertise was not a predictor of replication success in RPP (cf. Bench et al. 2017). More convincingly, Many Labs 5 selected ten findings from RPP with unsuccessful replications and systematically evaluated whether formal expert peer review of the protocols, including by the authors of the original study, would lead to a larger effect sizes. Despite a massive sample size and extremely thorough review process, there was little to no change in the effects for the vetted protocols relative to the original protocol used in RPP (Ebersole et al. 2020).\nContext, moderators, and expertise seem like reasonable explanations for individual replication failures. Certainly, we should expect them to be explanatory! But for these hypotheses to be operationalized in such a way that they carry weight in our evaluation of the metascientific evidence, they must be evaluated empirically rather than accepted uncritically. When such evaluations have been carried out, they have failed to support a large role for these factors.\n\n\n\nThe general argument of this chapter is that everything is not all right in experimental psychology and, hence, that we need to change our methodological practices to avoid negative outcomes like irreproducible papers and unreplicable results. Toward that goal, we have been presenting metascientific evidence on reproducibility and replicability. But this evidence has been controversial, to say the least! Do large-scale replication studies like RPP—or for that matter, smaller-scale individual replications of effects like “power posing”—really lead to the conclusion that our methods require changes? Or are there reasons why a lower replication rate is actually consistent with a cumulative, positive vision of psychology?\nOne line of argument addresses this question through the dynamics of scientific change. There are many versions, but one is given by Wilson, Harris, and Wixted (2020). The idea is that progress in psychology consists of a two-step process by which candidate ideas are “screened” by virtue of small, noisy experiments that reveal promising but tentative ideas that can then be “confirmed” by large-scale replications. On this kind of view, it’s business as usual to find that many randomly selected findings don’t hold up in large-scale replications and so we shouldn’t be distressed by results like those of RPP. The key to progress is to finding a small set that do hold up, which will lead to new areas of inquiry. We’re not sure that this view is either a good description of current practice or a good normative goal for scientific progress, but we won’t focus on that critique of Wilson et al.’s argument here. Instead, since this book is written for experimenters-in-training, we assume that you do not want your experiment to be a false positive from a noisy screening procedure, regardless of your feelings about the rest of the literature!\nIn RPP and subsequent metascience studies, original studies with lower \\(p\\)-values, larger effect sizes, and larger sample sizes were more likely to replicate successfully (Yang, Youyou, and Uzzi 2020). From a theoretical perspective, this result is to be expected, because the \\(p\\)-value literally captures the probability of the data (or any “more extreme”) under the null hypothesis of no effect. So a lower \\(p\\)-value should indicate a lower probability of a spurious result.11 In some sense, the fundamental question about the replication metascience literature is why the \\(p\\)-values aren’t better predictors of replicability! For example, Camerer et al. (2018) computes an expected number of successful replications on the basis of the effects and sample sizes—and their proportion of successful replications is substantially lower than that number.12 \n11 In chapter 6, we will have a lot more to say about \\(p &lt; 0.05\\), but for now we’ll mostly just treat it as a particular research outcome.12 This calculation, as with most other metrics of replication success, assumes that the underlying population effect is exactly the same for the replication and the original. This is a limitation because there could be unmeasured moderators that could produce genuine substantive differences between the two estimates.13 These terms basically mean the same thing and are not used very precisely in the literature. \\(p\\)-hacking is an informal term that sounds like you know you are doing something bad; sometimes people do, and sometimes they don’t. Questionable research practices is a more formal-sounding term that is in principle vague enough to encompass many ethical failings but in practice gets used to talk about \\(p\\)-hacking. Unless \\(p\\)-hacking intent is crystal clear, we favor two clunkier terms: “data-dependent decision-making” and “undisclosed analytic flexibility.” These terms describe the actual practices more precisely: trying many different things after looking at data, typically without reporting all of them.One explanation is that the statistical evidence presented in papers often dramatically overstates the true evidence from a study. That’s because of two pervasive and critical issues: analytic flexibility (also known as \\(p\\)-hacking or questionable research practices) and publication bias.13\nPublication bias refers to the relative preference (of scientists and other stakeholders, like journals) for experiments that “work” over those that do not, where “work” is typically defined as yielding a significant result at \\(p&lt;0.05\\). Because of this preference, it is typically easier to publish positive (statistically significant) results. The relative absence of negative results leads to biases in the literature. Intuitively, this bias will lead to a literature filled with papers where \\(p&lt;0.05\\). Negative findings will then remain unpublished, living in the proverbial “file drawer” (Rosenthal 1979).14 In a literature with a high degree of publication bias, many findings will be spurious because experimenters got lucky and published the study that “worked” even if that success was due to chance variation. In this situation, these spurious findings will not be replicable and so the overall rate of replicability in the literature will be lowered.15\n14 One estimate is that 96% of (not preregistered) papers report positive findings (Scheel, Schijen, and Lakens 2021). We’ll have a lot more to say about analytic flexibility and publication bias in chapters 11 and 16, respectively.15 The mathematics of the publication bias scenario strikes some observers as implausible: most psychologists don’t run dozens of studies and report only one out of each group (Nelson, Simmons, and Simonsohn 2018). Instead, a more common scenario is to conduct many different analyses and then report the most successful, creating some of the same effects as publication bias—a promotion of spurious variation—without a file drawer full of failed studies.It’s our view that publication bias and its even more pervasive cousin, analytic flexibility, are likely to be key drivers of lower replicability. We admit that the metascientific evidence for this hypothesis isn’t unambiguous, but that’s because there’s no sure-fire way to diagnose analytic flexibility in a particular paper—since we can almost never reconstruct the precise choices that were made in the data collection and analysis process! On the other hand, it is possible to analyze indicators of publication bias in specific literatures, and there are several cases where publication bias diagnostics appear to go hand in hand with replication failure. For example, in the “power posing” example described above, Simmons and Simonsohn (2017) noted strong evidence of analytic flexibility throughout the literature, leading them to conclude that there was no evidential value in the literature. And in the case of “money priming” (incidental exposures to images or text about money that were hypothesized to lead to changes in political attitudes), strong evidence of publication bias (Vadillo, Hardwicke, and Shanks 2016) was accompanied by a string of failed replications (Rohrer, Pashler, and Harris 2015).\n\n\n\n\n\n\naccident report\n\n\n\n\n\nAnalytic flexibility reveals a fountain of eternal youth\nThe way they tell it, Joseph Simmons, Leif Nelson, and Uri Simonsohn wrote their paper on “false positive psychology” (Simmons, Nelson, and Simonsohn 2011) as an attempt at catharsis (Simmons, Nelson, and Simonsohn 2018). They were fed up with work that they felt exploited flexibility in data analysis to produce findings blessed with \\(p &lt; 0.05\\) but likely did not reflect replicable effects. They called this practice \\(p\\)-hacking: trying different things to get your \\(p\\)-value to be below 0.05.\nTheir paper reported on a simple experiment: they played participants either the Beatles song, “When I’m 64,” or a control song and then asked them to report their date of birth (Simmons, Nelson, and Simonsohn 2011). This manipulation resulted in a significant one and a half year rejuvenation effect. Listening to the Beatles seemed to have made their participants younger!\nThis result is impossible, of course. But the authors produced a statistically significant difference between the groups that, by definition, was a false positive—a case where the statistical test indicated that there was a difference between groups despite no difference existing. In essence, they did so by trying many possible analyses and “cherry-picking” the one that produced a positive result. This practice of course invalidates the inference that the statistical test is supposed to help you make. Several of the practices they followed included:\n\nselectively reporting dependent measures (e.g., collecting several measures and reporting only one)\nselectively dropping manipulation conditions\nconducting their statistical test and then testing extra participants if they did not see a significant finding\nadjusting for gender as a covariate in their analysis if doing so resulted in a significant effect\n\nMany of the practices that the authors followed in their rejuvenation study were (and maybe still are!) commonplace in the research literature. John, Loewenstein, and Prelec (2012) surveyed research psychologists on the prevalence of what they called questionable research practices. Most participants admitted to following some of these practices—including exactly the same practices followed by the rejuvenation study.\nFor many in the field, “false positive psychology” was a galvanizing moment, leading them to recognize how common practices could lead to completely spurious (or even impossible) conclusions. As Simmons, Nelson, and Simonsohn wrote in their article (2018, 255), “Everyone knew [\\(p\\)-hacking] was wrong, but they thought it was wrong the way it is wrong to jaywalk. We decided to write ‘False-Positive Psychology’ when simulations revealed that it was wrong the way it is wrong to rob a bank.”",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#context-moderators-and-expertise",
    "href": "003-replication.html#context-moderators-and-expertise",
    "title": "3  Replication",
    "section": "Context, moderators, and expertise",
    "text": "Context, moderators, and expertise\nThere are many explanations for failed replications. The wonderful thing about metascience is that these explanations can be tested empirically!\nLet’s start with the idea that specific experimental operationalizations of a theory might be “context sensitive,” especially in subfields, like social psychology, whose theories inherently refer to environmental context (Van Bavel et al. 2016). Critics brought this issue up for RPP, where there were several studies in which the original experimental materials were tailored to one cultural context but then were deployed in another context, potentially leading to failure due to mismatch (Gilbert et al. 2016).\nContext sensitivity seems like a great explanation because in some sense, it must be right. If the context of an experiment includes the vast network of learned associations, practices, and beliefs that we all hold, then there’s no question that an experiment’s materials tap into this context to one degree or another. For example, if your experiment relies on the association between doctor and nurse concepts, you would expect this experiment to work differently in the past when nurse meant something more like nanny (Ramscar 2016).\nOn the other hand, as an explanation of specific replication failures, context sensitivity has not fared very well. The “Many Labs” projects were a series of replication projects in which multiple labs independently attempted to replicate several original studies. (In contrast, in RPP and similar studies, a single replication was conducted for each original study.) Some of the Many Labs projects assessed variation in replication success across different labs. In ManyLabs 2, Klein et al. (2018) replicated 28 findings, distributed across 125 different samples and more than 15,000 participants. ManyLabs 2 found almost no support for the context sensitivity hypothesis as an explanation of replication failure. In general, when effects failed to replicate, they did so when conducted in person as well as when conducted online, and these failures were consistent across many cultures and labs. \nOn the other hand, a review of several Many Labs–style replication projects indicated, on reanalysis, that population effects differed across replication labs even when the replication protocols were very similar to one another (Olsson-Collentine, Wicherts, and Assen 2020; Errington et al. 2021). So, context sensitivity is almost certainly present—and we’ll return to the broader issues of generalizability, context, and invariance in the next section—but so far we have not identified specific forms of context sensitivity that reliably affect replication success.\n\nThese observations—that (1) direct replications vary in how successful they are, but (2) we cannot identify specific contextual moderators—together suggest the possible presence of “hidden moderators.” That is, when faced with a successful original study and a failed replication, there may be some unknown factor(s) that moderates the effect.\nWe’ve personally had several experiences that corroborate the idea that there are hidden moderators. For example, in Lewis and Frank (2016), we were unsuccessful in replicating a simple categorization experiment. We then made a series of iterative changes to the stimuli and instructions, for example, changing the color and pattern of the stimuli (figure 3.5), eventually resulting in a larger (and statistically significant) effect—though still much smaller than the original. Critically, however, each alteration that we made to the procedure yielded a very small change in the effect, and it would have taken us many thousands of participants to figure exactly which alteration made the difference. (If you’re keeping score, here’s a case where stimulus color did matter to the outcome of the experiment!)\n\n\n\n\n\n\nFigure 3.5: Stimuli from Lewis and Frank (2016) (https://github.com/mllewis/xtSamp).\n\n\n\nAnother explanation for replication failure that is often cited is experimenter expertise (e.g., Schwarz and Strack 2014). On this hypothesis, replications fail because the researchers performing the replication do not have sufficient expertise to execute the study. Like context sensitivity, this explanation is almost certainly true for some replications. In our own work, we have repeatedly performed experiments that failed due to our own incompetence!\nYet as an explanation of the pattern of metascience findings, the expertise hypothesis hasn’t been supported empirically. First, team expertise was not a predictor of replication success in RPP (cf. Bench et al. 2017). More convincingly, Many Labs 5 selected ten findings from RPP with unsuccessful replications and systematically evaluated whether formal expert peer review of the protocols, including by the authors of the original study, would lead to a larger effect sizes. Despite a massive sample size and extremely thorough review process, there was little to no change in the effects for the vetted protocols relative to the original protocol used in RPP (Ebersole et al. 2020).\nContext, moderators, and expertise seem like reasonable explanations for individual replication failures. Certainly, we should expect them to be explanatory! But for these hypotheses to be operationalized in such a way that they carry weight in our evaluation of the metascientific evidence, they must be evaluated empirically rather than accepted uncritically. When such evaluations have been carried out, they have failed to support a large role for these factors.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#analytic-flexibility-reveals-a-fountain-of-eternal-youth",
    "href": "003-replication.html#analytic-flexibility-reveals-a-fountain-of-eternal-youth",
    "title": "3  Replication",
    "section": "Analytic flexibility reveals a fountain of eternal youth",
    "text": "Analytic flexibility reveals a fountain of eternal youth\nThe way they tell it, Joseph Simmons, Leif Nelson, and Uri Simonsohn wrote their paper on “false positive psychology” (Simmons, Nelson, and Simonsohn 2011) as an attempt at catharsis (Simmons, Nelson, and Simonsohn 2018). They were fed up with work that they felt exploited flexibility in data analysis to produce findings blessed with \\(p &lt; 0.05\\) but likely did not reflect replicable effects. They called this practice \\(p\\)-hacking: trying different things to get your \\(p\\)-value to be below 0.05.\nTheir paper reported on a simple experiment: they played participants either the Beatles song, “When I’m 64,” or a control song and then asked them to report their date of birth (Simmons, Nelson, and Simonsohn 2011). This manipulation resulted in a significant one and a half year rejuvenation effect. Listening to the Beatles seemed to have made their participants younger!\nThis result is impossible, of course. But the authors produced a statistically significant difference between the groups that, by definition, was a false positive—a case where the statistical test indicated that there was a difference between groups despite no difference existing. In essence, they did so by trying many possible analyses and “cherry-picking” the one that produced a positive result. This practice of course invalidates the inference that the statistical test is supposed to help you make. Several of the practices they followed included:\n\nselectively reporting dependent measures (e.g., collecting several measures and reporting only one)\nselectively dropping manipulation conditions\nconducting their statistical test and then testing extra participants if they did not see a significant finding\nadjusting for gender as a covariate in their analysis if doing so resulted in a significant effect\n\nMany of the practices that the authors followed in their rejuvenation study were (and maybe still are!) commonplace in the research literature. John, Loewenstein, and Prelec (2012) surveyed research psychologists on the prevalence of what they called questionable research practices. Most participants admitted to following some of these practices—including exactly the same practices followed by the rejuvenation study.\nFor many in the field, “false positive psychology” was a galvanizing moment, leading them to recognize how common practices could lead to completely spurious (or even impossible) conclusions. As Simmons, Nelson, and Simonsohn wrote in their article (2018, 255), “Everyone knew [\\(p\\)-hacking] was wrong, but they thought it was wrong the way it is wrong to jaywalk. We decided to write ‘False-Positive Psychology’ when simulations revealed that it was wrong the way it is wrong to rob a bank.”",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#replication-theory-building-and-open-science",
    "href": "003-replication.html#replication-theory-building-and-open-science",
    "title": "3  Replication",
    "section": "3.4 Replication, theory building, and open science",
    "text": "3.4 Replication, theory building, and open science\nEmpirical measures of reproducibility and replicability in the experimental psychology literature are low—lower than we may have naively suspected and lower than we want. How do we address these issues? And how do these issues interact with the goal of building theories? In this section, we discuss the relationship between replication and theory—and the role that open and transparent research practices can play.\n\n3.4.1 Reciprocity between replication and theory\nAnalytic reproducibility is a prerequisite for theory building because if the twin goals of theories are to explain and to predict experimental measurements, then an error-ridden literature undermines this goal. If some proportion of all numerical values reported in the literature were simple, unintentional typos, this situation would create an extra level of noise—irrelevant random variation—impeding our goal of getting precise enough measurements to distinguish between theories. But the situation is likely worse: errors are much more often in the direction that favors authors’ own hypotheses. Thus, irreproducibility not only decreases our precision; it also increases the bias in the literature, creating obstacles to the fair evaluation of theories with respect to data.\nReplicability is also foundational to theory building. Across a range of different conceptions of how science works, scientific theories are evaluated with respect to their relationship to the world. They must be supported, or at least fail to be falsified, by specific observations. It may be that some observations are by their nature unrepeatable (e.g., a particular astrophysical event might be observed once a human lifetime). But for laboratory sciences—and experimental psychology can be counted among these, to a certain extent at least—the independent and skeptical evaluation of theories requires repeatability of measurements.\nSome authors have argued (following the philosopher Heraclitus), “You Cannot Step in the Same River Twice” (McShane and Böckenholt 2014)—meaning that the circumstances and context of psychological experiments are constantly changing, and no observation will be identical to another. This is of course technically true from a philosophical perspective. But that’s where theory comes in! As we discussed above, our theories postulate the invariances that allow us to group together similar observations and generalize across them.\nIn this sense, replication is critical to theory, but theory is also critical to replication. Without a theory of “what matters” to a particular outcome, we really are stepping into an ever-changing river. But a good theory can concentrate our expectations on a much smaller set of causal relationships, allowing us to make strong predictions about what factors should and shouldn’t matter to experimental outcomes. To return to an example we discussed earlier, should stimulus color matter to the outcome of an experiment? Our theory could tell us that it shouldn’t matter for a priming experiment (Baribault et al. 2018) but that it should for a generalization experiment (Lewis and Frank 2016).\n\n\n3.4.2 Deciding when to replicate to maximize epistemic value\nAs a scientific community, how much emphasis should we place on replication? In the words of Newell (1973), “You Can’t Play 20 Questions with Nature and Win.” A series of well-replicated measurements does not itself constitute a theory. Theory construction is its own important activity. We’ve tried to make the case here that a reproducible and replicable literature is a critical foundation for theory building. That doesn’t necessarily mean you have to do replications all the time.\nMore generally, any scientific community needs to trade off between exploring new phenomena and confirming previously reported effects. In a thought-provoking analysis, Oberauer and Lewandowsky (2019) suggest that perhaps replications also aren’t the best test of theoretical hypotheses. In their analysis, if you don’t have a theory, then it makes sense to try and discover new phenomena and then to replicate them. If you do have a theory, you should expend your energy in testing new predictions rather than repeating the same test across multiple replications. Analyses such as Oberauer and Lewandowsky (2019) can provide a guide to our allocation of scientific effort.\nOur goal in this book is somewhat different than the general goal of metascientists considering how science should be conducted. Once you as a researcher decide to do a particular experiment, we think you will want to maximize its scientific value and so you will want it to be replicable. But we aren’t suggesting that you should necessarily do a replication study. There are many concerns that go into whether to replicate—including not only whether you are trying to gather evidence about a particular phenomenon but also whether you are trying to master techniques and paradigms related to it. As we said at the beginning of this chapter, not all replication is for the purpose of verification, and you as a researcher can make an informed decision about what experimental strategy is best for you.\n\n\n3.4.3 Open science\nThe open science movement is, in part, a response—really a set of responses—to the challenges of reproducibility and replicability. The open science (and now the broader open scholarship) movement is a broad umbrella (figure 3.6), but in this book we take open science to be a set of beliefs, research practices, results, and policies that are organized around the central roles of transparency and verifiability in scientific practice.16 The core of this movement is the idea of “nullius in verba” (the motto of the British Royal Society, which roughly means “take no one’s word for it”).17\n16 Another part of the open science umbrella involves a democratization of the scientific process through efforts to open access to science. This process involves both removal of barriers to access the scientific literature but also efforts to remove barriers to scientific training—especially to groups historically underrepresented in the sciences. The hope is that these processes increase both the set of people and the range of perspectives contributing to science. We view these changes as no less critical than the transparency aspects of the open science movement, though more indirectly related to the current discussion of reproducibility and replicability.17 At least that’s a reasonable paraphrase; there’s some interesting discussion about what this quote from Horace really means in a letter by Gould (1991).\n\n\n\n\n\nFigure 3.6: The broad umbrella of open science (adapted from an image created for the Stanford Lane Library Blog).\n\n\n\nTransparency initiatives are critical for ensuring reproducibility. As we discussed above, you cannot even evaluate reproducibility in the absence of data sharing. Code sharing can go even further toward helping reproducibility, as code makes the exact computations involved in data analysis much more explicit than the verbal descriptions that are the norm in papers (Hardwicke et al. 2018). Further, as we will discuss in chapter 13, the set of practices involved in preparing materials for sharing can themselves encourage reproducibility by leading to better organizational practices for research data, materials, and code.\nTransparency also plays a major role in advancing replicability. This point may not seem obvious at first—why would sharing things openly lead to more replicable experiments?—but it is one of the major theses of this book, so we’ll unpack it a bit. Here are a couple of routes by which transparent practices lead to greater replication rates.\n\nSharing of experimental materials enables replications that closely follow the original study’s methods. One critique of many replications has been that they differ in key respects from the originals. Sometimes those deviations were purposeful, but in other cases they were simply because the replicators could not use the original experimental materials. Sharing materials solves this problem.\nSharing sampling and analysis plans allows replication of key aspects of design and analysis that may not be clear in verbal descriptions, for example exclusion criteria or details of data preprocessing.\nSharing of analytic decision-making via preregistration can lead to a decrease in \\(p\\)-hacking and other practices that can introduce bias. The strength of statistical evidence in the original study is a predictor of replicability in subsequent studies. If original studies are preregistered, they are more likely to report effects that are not subject to inflation via questionable research practices.\nPreregistration can also clarify the distinction between confirmatory and exploratory findings, helping subsequent experimenters to make a more informed judgment about which effects are likely to be good targets for replication.\n\nFor all of these reasons, we believe that open science practices can play a critical role in increasing reproducibility and replicability.\n\n\n3.4.4 A crisis?\nSo, is there a “replication crisis”? The common meaning of “crisis” is “a difficult time.” The data we reviewed in this chapter suggest that there are real problems in the reproducibility and replicability of the psychology literature. But there’s no evidence that things have gotten worse. If anything, we are optimistic about the changes in practices that have happened in the last ten years. So in that sense, we are not sure that a crisis narrative is warranted.\nOn the other hand, for Kuhn (1962), the term “crisis” had a special meaning: it is a period of intense uncertainty in a scientific field brought on by the failure of a particular paradigm (chapter 2). A crisis typically heralds a shift in paradigm, in which new approaches and phenomena come to the fore.\nIn this sense, the replication crisis narrative isn’t mutually exclusive with other crisis narratives, including the “generalizability crisis” (Yarkoni 2020) and the “theory crisis” (Oberauer and Lewandowsky 2019). All of these are symptoms of discontent with the status quo. We share this discontent! We are writing this book to encourage further changes in experimental methods and practices to improve reproducibility and replicability outcomes—many of them driven by the broader set of ideas referred to as “open science.” These changes may not lead to a paradigm shift in the Kuhnian sense, but we hope that they lead to eventual improvements. In that sense, we think agree with those who say that the “replication crisis” has led to a “credibility revolution” (Vazire 2018).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "003-replication.html#chapter-summary-replication",
    "href": "003-replication.html#chapter-summary-replication",
    "title": "3  Replication",
    "section": "3.5 Chapter summary: Replication",
    "text": "3.5 Chapter summary: Replication\nIn this chapter, we introduce the notions of reproducibility—getting the same numbers from the same analysis—and replicability—getting the same conclusions from a new dataset. Both of these are critical prerequisites of a cumulative scientific literature, yet the metascience literature has suggested that the rate of both reproducibility and replicability in the published literature is quite a bit lower than we would hope. A strong candidate explanation for low reproducibility is simply that code and data are rarely shared alongside published research. Lowered replicability is more difficult to explain, but our best guess is that analytic flexibility (“\\(p\\)-hacking”) is at least partially to blame. On our account, replication is a metascientific tool for understanding the status of the scientific literature rather than an end in itself. Instead, we see the open science movement, a movement focused on the role of transparency in the scientific process, as a promising response to issues of reproducibility and replicability.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nHow would you design a measure of the context sensitivity of an experiment? Think of a measure you could apply post hoc to a description of an experiment (e.g., from reading a paper) so that you could take a group of experiments and annotate how context-sensitive they are on some scale.\nTake the measure you designed above. How would you test that this measure really captured context sensitivity in a way that was not circular? What would be an “objective measure” of context sensitivity?\nWhat proportion of reproducibility failures do you think are due to questionable practices by experimenters vs just plain errors? How would you test your hypothesis?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nStill a very readable and entertaining introduction to the idea of \\(p\\)-hacking: Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn. (2011). “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” Psychological Science 22 (11):1359–1366. https://doi.org/10.1177/0956797611417632.\nA recent review of issues of replication in psychology: Brian A. Nosek, Tom E. Hardwicke, Hannah Moshontz, Aurélien Allard,Katherine S. Corker, Anna Dreber Almenberg, Fiona Fidler, et al. (2022). “Replicability, Robustness, and Reproducibility in Psychological Science.” Annual Review of Psychology 73 (1): 719–748. https://doi.org/10.1146/annurev-psych-020821-114157.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Replication</span>"
    ]
  },
  {
    "objectID": "004-ethics.html",
    "href": "004-ethics.html",
    "title": "4  Ethics",
    "section": "",
    "text": "4.1 Ethical frameworks\nWas Milgram’s experiment (see Case Study) really ethically wrong—in the sense that it should not have been performed? You might have the intuition that is was unethical due to the harms that the participants experienced or the way they were (sometimes) deceived by the experimenter. Others might consider arguments in defense of the experiment, perhaps that what we learned from it was sufficiently valuable to justify its being conducted. Beyond simply arguing back and forth, how could we approach this issue more systematically?\nEthical frameworks offer tools for analyzing such situations. In this section, we’ll introduce three of the most commonly used frameworks and discuss how each of these could be applied to Milgram’s paradigm.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#shock-treatment",
    "href": "004-ethics.html#shock-treatment",
    "title": "4  Ethics",
    "section": "Shock treatment",
    "text": "Shock treatment\nA decade after surviving prisoners were liberated from the last concentration camp, Adolf Eichmann, one of the Holocaust’s primary masterminds, was tried for his role in the mass genocide (Baade 1961). While reflecting on his rationale for forcibly removing, torturing, and eventually murdering millions of Jews, an unrepentant Eichmann claimed that he was “merely a cog in the machinery that carried out the directives of the German Reich” and therefore was not directly responsible (Kilham and Mann 1974). This startling admission gave a young researcher an interesting idea: “Could it be that Eichmann and his million accomplices in the Holocaust were just following orders? Could we call them all accomplices?” (Milgram 1974, 123).\nStanley Milgram aimed to make a direct test of whether people would comply under the direction of an authority figure no matter how uncomfortable or harmful the outcome. He invited participants into the laboratory to serve as a teacher for an activity (Milgram 1963). Participants were told that they were to administer electric shocks of increasing voltage to another participant, the student, in a nearby room whenever the student provided an incorrect response. In reality, there were no shocks, and the student was an actor who was in on the experiment and only pretended to be in pain when the “shocks” were administered. Participants were encouraged to continue administering shocks despite clearly audible pleas from the student to stop. In one of Milgram’s studies, nearly 65% of participants administered the maximum voltage to the student.\nThis deeply unsettling result has become, as Ross and Nisbett (1991, 55) say, “part of our society’s shared intellectual legacy,” informing our scientific and popular conversation in myriad different ways. At the same time, modern reanalyses of archival materials from the study have called into question whether the deception in the study was effective, casting doubt on its central findings (Perry et al. 2020).\nRegardless of its scientific value, Milgram’s study blatantly violates modern ethical norms around the conduct of research. Among other violations, the procedure involved coercion that undermined participants’ right to withdraw from the experiment. This coercion appeared to have negative consequences: Milgram noted that a number of his participants displayed anxiety symptoms and nervousness. This observation was distressing and led to calls for this sort of research to be declared unethical (e.g., Baumrind 1964). The ethical issues surrounding Milgram’s study are complex, and some are relatively specific to the particulars of his study and moment (Miller 2009). But the controversy around the study was an important part of convincing the scientific community to adopt stricter policies that protect study participants from unnecessary harm.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#ethical-frameworks",
    "href": "004-ethics.html#ethical-frameworks",
    "title": "4  Ethics",
    "section": "Was Milgram justified?",
    "text": "4.1.1 Consequentialist theories\nEthical theories provide principles for what constitutes good actions. The simplest theory of good actions is the consequentialist theory: good actions lead to good results. The most famous consequentialist position is the utilitarian position, originally defined by the philosopher John Stuart Mill (Flinders 1992). This view emphasizes decision-making based on the “greatest happiness principle,” or the idea that an action should be considered morally good based on the degree of happiness or pleasure people experience because of it, and likewise that an action should be considered morally bad based on the degree of unhappiness or pain people experience by the same action (Mill 1859).\nA consequentialist analysis of Milgram’s study considers the study’s negative and positive effects and weighs these against one another. Did the study cause harm to its participants? On the other hand, did the study lead to knowledge that prevented harm or caused positive benefits?\nConsequentialist analysis can be a straightforward way to justify the risks and benefits of a particular action, but in the research setting it is unsatisfying. Many horrifying experiments would be licensed by a consequentialist analysis and yet feel untenable to us. Imagine that a researcher forced you to undergo a risky and undesired medical intervention because the resulting knowledge might benefit thousands of others. This experiment seems like precisely the kind of thing our ethical framework should rule out!\n\n\n4.1.2 Deontological approaches\nHarmful research performed against participants’ will or without their knowledge is repugnant; we consider the Tuskegee Syphilis Experiment, a horrifying example of such research (Case Study, below). Considering such cases, a few rules seem obvious, for example: “Researchers must ask participants’ permission before conducting research on them.” Principles like this one are now formalized in all ethical codes for research. They exemplify an approach called deontological (or duty-based) ethics.\nDeontology emphasizes the importance of taking ethically permissible actions, regardless of their outcome (Biagetti, Gedutis, and Ma 2020). In general, university ethics boards take a deontological approach to ethics (Boser 2007). In the context of research, there are four primary principles being applied:\n\nRespect for autonomy. This principle requires that people participating in research studies can make their own decisions about their participation, and that those with diminished autonomy (e.g., children) should receive equal protections (Beauchamp and Childress 2001). Respecting someone’s autonomy also means providing them with all the information they need to make an informed decision about whether to participate in a research study (giving consent) and giving them further context about the study they have participated in after it is done (debriefing).\nBeneficence. This principle means that researchers are obligated to protect the well-being of participants for the duration of the study. Beneficence has two parts. The first is to do no harm. Researchers must take steps to minimize the risks to participants and to disclose any known risks at the onset. If risks are discovered during participation, researchers must notify participants of their discovery and make reasonable efforts to mitigate these risks, even if that means stopping the study altogether. The second is to maximize potential benefits to participants.2\nNonmaleficence. This principle is similar to beneficence (in fact, beneficence and nonmaleficence were a single principle when they were first introduced in the Belmont Report, which we’ll discuss later) but differs in its emphasis on doing/causing no harm. In general, harm is bad—but deontology is about intent, not impact, so harm is sometimes warranted when the intent is morally good. For example, administering a vaccine may cause some discomfort and pain, but the intent is to protect the patient from developing a deadly virus in the future. The harm is justifiable under this framework.\nJustice. This principle means that both the benefits and risks of a study should be equally distributed among all participants. For example, participants should not be systematically assigned to one condition over another based on features of their identity such as socioeconomic status, race and ethnicity, or gender.\n\n2 In practice, this doesn’t mean compensating participants with exorbitant amounts of money or gifts, which might cause other issues, like exerting an undue influence on low-income participants to participate. Instead “maximizing benefits” is interpreted as identifying all possible benefits of participation in the research and making them available where possible.Analyzed from the perspective of these principles, Milgram’s study raises several red flags. First, Milgram’s study reduced participants’ autonomy by making it difficult for them to voluntarily end their involvement (participants were told up to four times to continue administering shocks even after they expressed clear opposition). Second, the paradigm was designed in a way that it was likely to cause harm to its participants by putting them in a very stressful situation. Further, Milgram’s study may have induced unnecessary harm on certain participants by failing to screen participants for existing mental health issues before beginning the session.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nWas Milgram justified?\nWas the harm done in Milgram’s experiment justifiable given that it informed our understanding of obedience and conformity? We can’t say for sure. What we can say is that in the 10 years following the publication of Milgram’s study, the number of papers on (any kind of) obedience increased and the nature of these papers expanded from a focus on religious conformity to a broader interest in social conformity, suggesting that Milgram changed the direction of this research area. Additionally, in a follow-up that Milgram conducted, he reported that 84% of participants in the original study said they were happy to have been involved (Milgram 1974). On the other hand, given concerns about validity in the original study, perhaps its influence on the field was not warranted (Perry et al. 2020).\nMany researchers believe there was no ethical way to conduct Milgram’s experiment while also protecting the integrity of the research goals, but some have tried. One study recreated a portion of the original experiment, with some critical changes (Burger 2009). Before enrolling in the study, participants completed both a phone screening for mental health concerns, addiction, or extreme trauma, and a formal interview with a licensed clinical psychologist, who identified signs of depression or anxiety. Those who passed these assessments were invited into the lab for a Milgram-type learning study. Experimenters clearly explained that participation was voluntary and the decision to participate could be reversed at any point, either by the participant themselves or by a trained clinical psychologist who was present for the duration of the session. Additionally, shock administration never exceeded 150 volts (compared to 450 volts in the original study) and experimenters debriefed participants extensively following the end of the session. This modified replication study found similar patterns of obedience as Milgram’s; further, one year later, no participants expressed any indication of stress or trauma associated with their involvement in the study.\n\n\n\n\n\n4.1.3 Virtue-based approaches\nA final way that we can approach ethical dilemmas is through a virtue framework. A virtue is a trait, disposition, or quality that is thought to be a moral foundation (Annas 2006). Virtue ethics suggests that people can learn to be virtuous by observing those actions in others they admire (Morris and Morris 2016). Proponents of virtue ethics say this works for two reasons: (1) people are generally good at recognizing morally good traits in others and (2) people receive some fulfillment from living virtuously. Virtue ethics differs from deontology and utilitarianism because it focuses on a person’s character rather than on the nature of a rule or the consequences of an action.\nFrom a research perspective, virtue ethics tells us that in order to behave virtuously, we must make decisions that consider the context surrounding the experiment (Dillern 2021). In other words, researchers should evaluate how their studies might influence a participant’s behaviors, especially when those behaviors deviate from typical expectations. This process is also meant to be adaptive, meaning that researchers must be vigilant about both the changing mental states of their participants during the experimental session and whether the planned procedure is no longer acceptable.\nHow can we apply this ethical framework to Milgram’s experiment? Many virtue ethicists would probably conclude that Milgram’s approach was neither appropriate (for participants) nor adaptive. Upon noticing increasing levels of participant distress, an experimenter following the virtue ethics framework should have chosen to end the session early or—even better—to have minimized participant distress from the beginning.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#was-milgram-justified",
    "href": "004-ethics.html#was-milgram-justified",
    "title": "4  Ethics",
    "section": "",
    "text": "Was the harm done in Milgram’s experiment justifiable given that it informed our understanding of obedience and conformity? We can’t say for sure. What we can say is that in the 10 years following the publication of Milgram’s study, the number of papers on (any kind of) obedience increased and the nature of these papers expanded from a focus on religious conformity to a broader interest in social conformity, suggesting that Milgram changed the direction of this research area. Additionally, in a follow-up that Milgram conducted, he reported that 84% of participants in the original study said they were happy to have been involved (Milgram 1974). On the other hand, given concerns about validity in the original study, perhaps its influence on the field was not warranted (Perry et al. 2020).\nMany researchers believe there was no ethical way to conduct Milgram’s experiment while also protecting the integrity of the research goals, but some have tried. One study recreated a portion of the original experiment, with some critical changes (Burger 2009). Before enrolling in the study, participants completed both a phone screening for mental health concerns, addiction, or extreme trauma, and a formal interview with a licensed clinical psychologist, who identified signs of depression or anxiety. Those who passed these assessments were invited into the lab for a Milgram-type learning study. Experimenters clearly explained that participation was voluntary and the decision to participate could be reversed at any point, either by the participant themselves or by a trained clinical psychologist who was present for the duration of the session. Additionally, shock administration never exceeded 150 volts (compared to 450 volts in the original study) and experimenters debriefed participants extensively following the end of the session. This modified replication study found similar patterns of obedience as Milgram’s; further, one year later, no participants expressed any indication of stress or trauma associated with their involvement in the study.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#ethical-responsibilities-to-research-participants",
    "href": "004-ethics.html#ethical-responsibilities-to-research-participants",
    "title": "4  Ethics",
    "section": "4.2 Ethical responsibilities to research participants",
    "text": "4.2 Ethical responsibilities to research participants\nMilgram’s shock experiment was just one of dozens of unethical human subjects studies that garnered the attention and anger of the public in the United States. In 1978, the US National Commission for the Protection of Human Services of Biomedical and Behavioral Research released the Belmont Report, which described protections for the rights of human subjects participating in research studies (Adashi, Walters, and Menikoff 2018). Perhaps the most important message found in the report was the notion that “investigators should not have sole responsibility for determining whether research involving human subjects fulfills ethical standards. Others, who are independent of the research, must share the responsibility.” In other words, ethical research requires both transparency and external oversight.\n\n4.2.1 Institutional review boards\nThe creation of institutional review boards (IRBs) in the United States was an important result of the Belmont Report. While regulatory frameworks and standards vary across national boundaries, ethical review of research is ubiquitous across countries. In what follows, we focus on the US regulatory framework as it has been a model for other ethical review systems, but we use the clearer label “ethics review boards” for IRBs.\nAn ethics board is a committee of people who review, evaluate, and monitor human subjects research to make sure that participants’ rights are protected when they participate in research (Oakes 2002). Ethics boards are local; every organization that conducts human subjects or animal research is required to have its own ethics board or to contract with an external one. If you are based at a university, yours likely has its own, and its members are probably a mix of scientists, doctors, professors, and community residents.3\n3 The local control of ethics boards can lead to very different practices in ethical review across institutions, which is obviously inconsistent with the idea that ethical standards should be uniform! In addition, critics have wondered about the structural issue that institutional ethics boards have an incentive to decrease liability for the institution, while private boards have an incentive to provide approvals to the researchers who pay them (Lemmens and Freedman 2000).When a group of researchers have a research question they are interested in pursuing with human subjects, they must receive approval from their local ethics board before beginning any data collection. The ethics board reviews each study to make sure:\n\nA study poses no more than minimal risk to participants. This means the anticipated harm or discomfort to the participant is not greater than what would be experienced in everyday life. It is possible to perform a study that poses greater than minimal risk, but it requires additional monitoring to detect any adverse events that may occur.\nResearchers obtain informed consent from participants before collecting any data. This requirement means experimenters must disclose all potential risks and benefits so that participants can make an informed decision about whether or not to participate in the study. Importantly, informed consent does not stop after participants sign a consent form. If researchers discover any new potential risks or benefits along the way, they must disclose these discoveries to all participants (see chapter 12).\nSensitive information remains confidential. Although regulatory frameworks vary, researchers typically have an obligation to their participants to protect all identifying information recorded during the study (see chapter 13).\nParticipants are recruited equitably and without coercion. Before ethics boards became standard, researchers often coercively recruited marginalized and vulnerable populations to test their research questions, rather than making participation in research studies voluntary and providing equitable access to the opportunity to participate.\n\n\n\n\n\n\n\ncase study\n\n\n\n\n\nThe Tuskegee Syphilis Study\nIn 1929, The United States Public Health Service (USPHS) was perplexed by the effects of syphilis in Macon County, Alabama, an area with an overwhelmingly Black population (Brandt 1978). Syphilis is a sexually transmitted bacterial infection that can either be in a visible and active stage or in a latent stage. At the time of the study’s inception, roughly 36% of Tuskegee’s adult population had developed some form of syphilis, one of the highest infection rates in America (White 2006).\nThe USPHS recruited 400 Black males from 25–60 years of age with latent syphilis and 200 Black males without the infection to serve as a control group to participate (Brandt 1978). The USPHS sought the help of the Macon County Board of Health to recruit participants with the promise that they would provide treatment for community members with syphilis. The researchers sought poor, illiterate Black people and, instead of telling them that they were being recruited for a research study, merely informed them that they would be treated for “bad blood.”\nBecause the study was interested in tracking the natural course of latent syphilis without any medical intervention, the USPHS had no intention of providing any care to its participants. To assuage participants, the USPHS distributed an ointment that had not been shown to be effective in the treatment of syphilis, and only small doses of a medication actually used to treat the infection. In addition, participants underwent a spinal tap, which was presented to them as another form of therapy and their “last chance for free treatment.”\nBy 1955, just over 30% of the original participants had died from syphilis complications. It took until the 1970s before the final report was released and (the lack of) treatment ended. In total, 128 participants died of syphilis or complications from the infection, 40 wives became infected, and 19 children were born with the infection (Katz and Warren 2011). The damage rippled through two generations, and many never actually learned what had been done to them.\nThe Tuskegee experiment violates nearly every single guideline for research described above—indeed in its many horrifying violations of research participants’ agency, it provides a blueprint for future regulation to prevent any aspect of it from being repeated: Investigators did not obtain informed consent. Participants were not made aware of all known risks and benefits involved with their participation. Instead, they were deceived by researchers who led them to believe that diagnostic and invasive exams were directly related to their treatment.\nPerhaps most shocking, participants were denied appropriate treatment following the discovery that penicillin was effective at treating syphilis (Mahoney, Arnold, and Harris 1943). The USPHS requested that medical professionals overseeing their care outside of the research study not offer treatment to participants so as to preserve the study’s methodological integrity. This intervention violated participants’ rights to equal access to care, which should have taken precedence over the results of the study.\nFinally, recruitment was both imbalanced and coercive. Not only were participants selected from the poorest of neighborhoods in the hopes of finding vulnerable populations with little agency but they were also bribed with empty promises of treatment and a monetary incentive (payment for burial fees, a financial obstacle for many sharecroppers and tenant farmers at the time).\n\n\n\n\n\n4.2.2 Risks and benefits\nImagine that you were approached about participating in a research study at your local university. You were only told you would be paid $25 in exchange for completing an hour of cognitive tasks on a computer. Now imagine that halfway through the session, the experimenter revealed they would also need to collect a blood sample, “which should only take a couple of minutes and which will really help the research study.” Would you agree to the sample? Would you feel uncomfortable in any way?\nParticipants need to understand the risks and benefits of participation in an experiment before they give consent. To do otherwise compromises their autonomy (a key deontological principle). In the case of this hypothetical experiment, a new and unexpected invasive component of an experiment is coercive: participants would have to choose to forfeit their expected compensation to opt out. They also might feel that they have been deceived by the experimenter.\nIn human subjects research, deception is a specific technical term that refers to cases when (1) experimenters withhold any information about its goals or intentions, (2) experimenters hide their true identity (such as when using actors), (3) some aspects of the research are under- or overstated to conceal information, or (4) participants receive any false or misleading information. The use of deception requires special consideration from a human subjects perspective (Kelman 2017; Baumrind 1985).\nEven assuming they are disclosed properly without coercion or deception, the risks and benefits of a study must be assessed from the perspective of the participant, not the experimenter. By doing so, we allow participants to make an informed choice. In the case of the blood sample, the risks to the participant were not disclosed and the benefits were stated in terms of the research project (and the experimenter).\nThe benefits of participation in research can either be direct or indirect, and it is important to specify which type participants may receive. While some clinical studies and interventions may offer some direct benefit due to participation, many of the benefits of basic science research are indirect. Both have their place in science, but participants must ultimately determine the degree to which each type of benefit motivates their own involvement in a study (Shatz 1986).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#the-tuskegee-syphilis-study",
    "href": "004-ethics.html#the-tuskegee-syphilis-study",
    "title": "4  Ethics",
    "section": "The Tuskegee Syphilis Study",
    "text": "The Tuskegee Syphilis Study\nIn 1929, The United States Public Health Service (USPHS) was perplexed by the effects of syphilis in Macon County, Alabama, an area with an overwhelmingly Black population (Brandt 1978). Syphilis is a sexually transmitted bacterial infection that can either be in a visible and active stage or in a latent stage. At the time of the study’s inception, roughly 36% of Tuskegee’s adult population had developed some form of syphilis, one of the highest infection rates in America (White 2006).\nThe USPHS recruited 400 Black males from 25–60 years of age with latent syphilis and 200 Black males without the infection to serve as a control group to participate (Brandt 1978). The USPHS sought the help of the Macon County Board of Health to recruit participants with the promise that they would provide treatment for community members with syphilis. The researchers sought poor, illiterate Black people and, instead of telling them that they were being recruited for a research study, merely informed them that they would be treated for “bad blood.”\nBecause the study was interested in tracking the natural course of latent syphilis without any medical intervention, the USPHS had no intention of providing any care to its participants. To assuage participants, the USPHS distributed an ointment that had not been shown to be effective in the treatment of syphilis, and only small doses of a medication actually used to treat the infection. In addition, participants underwent a spinal tap, which was presented to them as another form of therapy and their “last chance for free treatment.”\nBy 1955, just over 30% of the original participants had died from syphilis complications. It took until the 1970s before the final report was released and (the lack of) treatment ended. In total, 128 participants died of syphilis or complications from the infection, 40 wives became infected, and 19 children were born with the infection (Katz and Warren 2011). The damage rippled through two generations, and many never actually learned what had been done to them.\nThe Tuskegee experiment violates nearly every single guideline for research described above—indeed in its many horrifying violations of research participants’ agency, it provides a blueprint for future regulation to prevent any aspect of it from being repeated: Investigators did not obtain informed consent. Participants were not made aware of all known risks and benefits involved with their participation. Instead, they were deceived by researchers who led them to believe that diagnostic and invasive exams were directly related to their treatment.\nPerhaps most shocking, participants were denied appropriate treatment following the discovery that penicillin was effective at treating syphilis (Mahoney, Arnold, and Harris 1943). The USPHS requested that medical professionals overseeing their care outside of the research study not offer treatment to participants so as to preserve the study’s methodological integrity. This intervention violated participants’ rights to equal access to care, which should have taken precedence over the results of the study.\nFinally, recruitment was both imbalanced and coercive. Not only were participants selected from the poorest of neighborhoods in the hopes of finding vulnerable populations with little agency but they were also bribed with empty promises of treatment and a monetary incentive (payment for burial fees, a financial obstacle for many sharecroppers and tenant farmers at the time).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#ethical-responsibilities-in-analysis-and-reporting-of-research",
    "href": "004-ethics.html#ethical-responsibilities-in-analysis-and-reporting-of-research",
    "title": "4  Ethics",
    "section": "4.3 Ethical responsibilities in analysis and reporting of research",
    "text": "4.3 Ethical responsibilities in analysis and reporting of research\n\n\n\n\n\n\naccident report\n\n\n\n\n\nWhat data?\nDutch social psychologist Diederick Stapel contributed to more than 200 articles on social comparison, stereotype threat, and discrimination, many published in the most prestigious journals. Stapel reported that affirming positive personal qualities buffered against dangerous social comparison, that product advertisements related to a person’s attractiveness changed their sense of self, and that exposure to intelligent in-group members boosted a person’s performance on future tasks (Stapel and Linde 2012; Trampe, Stapel, and Siero 2011; Gordijn and Stapel 2006). These findings were fresh and noteworthy at the time of publication, and Stapel’s papers were cited thousands of times. The only problem? Stapel’s data were made up.\nStapel has admitted that when he first began fabricating data, he would make small tweaks to a few data points (Stapel 2012). Changing a single number here and there would turn a flat study into an impressive one. Having achieved comfortable success (and having aroused little suspicion from journal editors and others in the scientific community), Stapel eventually began creating entire data sets and passing them off as his own. Several colleagues began to grow skeptical of his overwhelming success, however, and brought their concerns to the Psychology Department at Tilburg University. By the time the investigation of his work concluded, 58 of Stapel’s papers were retracted, meaning that the publishing journal withdrew the paper after discovering that its contents were invalid.\nEveryone agrees that Stapel’s behavior was deeply unethical. But should we consider cases of falsification and fraud to be different in kind from other ethical violations in research? Or is fraud merely the endpoint in a continuum that might include other practices like \\(p\\)-hacking? Lawyers and philosophers grapple with the precise boundary between sloppiness and neglect, and it can be difficult to know which one is at play when a typo or coding mistake changes the conclusion of a scientific paper. Similarly, if a researcher engages in so-called questionable research practices, at what point should they be considered to have made an ethical violation as opposed to simply performing their research poorly?\nThe ethical frameworks above provide a framework for thinking about this topic. For the consequentialist, sloppy science can lead to good outcomes for the scientist (quicker publication) but bad outcomes for the rest of the scientific community who have to waste time and effort on papers that may not be correct. For the deontologist, the scientist’s intention plays a key role: it is not a generally acceptable principle to knowingly use substandard practices. And for the virtue ethicist, sloppiness is not a morally good trait. On all analyses, researchers have a duty to pursue their work carefully.\n\n\n\nAs scientists, we not only have a responsibility to participants; we are also responsible for what we do with our data and for the kinds of conclusions we draw. Cases like Stapel’s (see Accident Report) seem stunning, but they are part of a continuum. Codes of professional ethics for organizations like the American Psychological Association encourage researchers to take care in the management and analysis of their data so as to avoid errors and misstatements (American Psychological Association 2017).\nResearchers also have an obligation not to suppress findings based on their own beliefs about the right answer. One unfortunate way that this suppression can happen is when researchers selectively report their research, leading to publication bias, as you learned in chapter 3. Researchers’ own biases can be another (invalid) rationale for not publishing: it’s also an ethical violation to suppress findings that contradict your theoretical commitments.\nImportantly, researchers don’t have an obligation to publish everything they do. Publishing in the peer-reviewed literature is difficult and time-consuming. There are plenty of reasons not to publish an experimental finding! For example, there’s no reason to publish a result if you believe it is truly uninformative because of a confound in the experimental design. You also aren’t typically committing an ethical violation if you decide to quit your job in research and so you don’t publish a study from your dissertation.4 The primary ethical issue arises when you use the result of a study—and how it relates to your own beliefs or to a threshold like \\(p &lt; 0.05\\)—to decide whether to publish it or not.\n4 On the other hand, if your dissertation contains the cure to a terrible disease, you do have a duty to publish it!As we’ll discuss again and again in this book, the preparation of research reports must also be done with care and attention to detail (see chapter 14). Sloppiness in writing up results can lead to imprecise or over-broad claims; and if that sloppiness extends to the reporting of data, and analysis, it may lead to irreproducibility as well.\nFurther, professional ethics dictates that published contributions to the literature be original. In general, the text of a paper must not be plagiarized (copied) from the text of other reports whether by you or by another author without attribution. Copying from others outside of a direct, attributed quotation is obviously an ethical violation because it leads to credit for text being given to you rather than the true author. But self-plagiarism is also not acceptable—it is a violation to receive credit multiple times for the same product.5\n5 Standards on this issue differ from field to field. Our sense is that the rule on self-plagiarism applies primarily to duplication of content between journal papers. So, for example, barring any specific policy of the funder or journal, it is acceptable to use text from one of your own grant proposals in a journal paper. It is also typically acceptable to reuse text from a conference abstract or preregistration (that you wrote, of course) when prepare a journal paper.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#what-data",
    "href": "004-ethics.html#what-data",
    "title": "4  Ethics",
    "section": "What data?",
    "text": "What data?\nDutch social psychologist Diederick Stapel contributed to more than 200 articles on social comparison, stereotype threat, and discrimination, many published in the most prestigious journals. Stapel reported that affirming positive personal qualities buffered against dangerous social comparison, that product advertisements related to a person’s attractiveness changed their sense of self, and that exposure to intelligent in-group members boosted a person’s performance on future tasks (Stapel and Linde 2012; Trampe, Stapel, and Siero 2011; Gordijn and Stapel 2006). These findings were fresh and noteworthy at the time of publication, and Stapel’s papers were cited thousands of times. The only problem? Stapel’s data were made up.\nStapel has admitted that when he first began fabricating data, he would make small tweaks to a few data points (Stapel 2012). Changing a single number here and there would turn a flat study into an impressive one. Having achieved comfortable success (and having aroused little suspicion from journal editors and others in the scientific community), Stapel eventually began creating entire data sets and passing them off as his own. Several colleagues began to grow skeptical of his overwhelming success, however, and brought their concerns to the Psychology Department at Tilburg University. By the time the investigation of his work concluded, 58 of Stapel’s papers were retracted, meaning that the publishing journal withdrew the paper after discovering that its contents were invalid.\nEveryone agrees that Stapel’s behavior was deeply unethical. But should we consider cases of falsification and fraud to be different in kind from other ethical violations in research? Or is fraud merely the endpoint in a continuum that might include other practices like \\(p\\)-hacking? Lawyers and philosophers grapple with the precise boundary between sloppiness and neglect, and it can be difficult to know which one is at play when a typo or coding mistake changes the conclusion of a scientific paper. Similarly, if a researcher engages in so-called questionable research practices, at what point should they be considered to have made an ethical violation as opposed to simply performing their research poorly?\nThe ethical frameworks above provide a framework for thinking about this topic. For the consequentialist, sloppy science can lead to good outcomes for the scientist (quicker publication) but bad outcomes for the rest of the scientific community who have to waste time and effort on papers that may not be correct. For the deontologist, the scientist’s intention plays a key role: it is not a generally acceptable principle to knowingly use substandard practices. And for the virtue ethicist, sloppiness is not a morally good trait. On all analyses, researchers have a duty to pursue their work carefully.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#ethical-responsibilities-to-the-broader-scientific-community",
    "href": "004-ethics.html#ethical-responsibilities-to-the-broader-scientific-community",
    "title": "4  Ethics",
    "section": "4.4 Ethical responsibilities to the broader scientific community",
    "text": "4.4 Ethical responsibilities to the broader scientific community\nThe open science principles that we will describe throughout this book are not only important correctives to issues of reproducibility and replicability; they are also ethical duties.\nThe sociologist Robert Merton described a set of norms that science is assumed to follow: communism—that scientific knowledge belongs to the community; universalism—that the validity of scientific results is independent of the identity of the scientists; disinterestedness—that scientists and scientific institutions act for the benefit of the overall enterprise; and organized skepticism—that scientific findings must be critically evaluated (Merton 1979).\nIf the products of science aren’t open, it is very hard to be a scientist by Merton’s definition. To contribute to the communal good, papers need to be openly available. And to be subject to skeptical inquiry, experimental materials, research data, analytic code, and software must be all available so that analytic calculations can be verified and experiments can be reproduced. Otherwise, you have to accept arguments on authority rather than by virtue of the materials and data.\nOpenness is not only definitionally part of the scientific enterprise; it’s also good for science and individual scientists (Gorgolewski and Poldrack 2016). Publications that are open access are cited more (Eysenbach 2006; Gargouri et al. 2010). Open data also increases the potential for citation and reuse, and maximizes the chances that errors are found and corrected.\nBut these benefits mean that researchers have a responsibility to their funders to pursue open practices so as to seek the maximal return on funders’ investments. And by the same logic, if research participants contribute their time to scientific projects, the researchers also owe it to these participants to maximize the impact of their contributions (Brakewood and Poldrack 2013). For all of these reasons, individual scientists have a duty to be open—and scientific institutions have a duty to promote transparency in the science they support and publish.\nHow should these duties be balanced against researchers’ other responsibilities? For example, how should we balance the benefit of data sharing against the commitment to preserve participant privacy? And, since transparency policies also carry costs in terms of time and effort, how should researchers consider those costs against other obligations?\nFirst, open practices should be a default in cases where risks and costs are limited. For example, the vast majority of journals allow authors to post accepted manuscripts in their un-typeset form to an open repository. This route to “green” open access is easy, cost free, and—because it comes only after articles are accepted for publication—confers essentially no risks of scooping. As a second example, the vast majority of analytic code can be posted as an explicit record of exactly how analyses were conducted, even if posting data is sometimes more complicated due to privacy restrictions. These kinds of “incentive-compatible” actions toward openness can bring researchers much of the way to a fully transparent workflow, and there is no excuse not to take them.\nSecond, researchers should plan for sharing and build a workflow that decreases the costs of openness. As we discuss in chapter 13, while it can be costly and difficult to share data after the fact if they were not explicitly prepared for sharing, good project management practices can make this process far simpler (and in many cases completely trivial).\nFinally, given the ethical imperative toward openness, institutions like funders, journals, and societies need to use their role to promote open practices and to mitigate potential negatives (Nosek et al. 2015). Scholarly societies have an important role to play in educating scientists about the benefits of openness and providing resources to steer their members toward best practices for sharing their publication and other research products. Similarly, journals can set good defaults, for example by requiring data and code sharing except in cases where a strong justification is given. Funders of research can—and increasingly do—signal their interest in openness through data sharing mandates.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "004-ethics.html#chapter-summary-ethics",
    "href": "004-ethics.html#chapter-summary-ethics",
    "title": "4  Ethics",
    "section": "4.5 Chapter summary: Ethics",
    "text": "4.5 Chapter summary: Ethics\nIn this chapter, we discussed three ethical frameworks and evaluated how they can be applied to our own research through the lens of Milgram’s famous obedience experiment. Studies like Milgram’s prompted serious conversations about how best to reconcile experimenter goals with participant well-being. The publication of the Belmont Report and later creation of ethics boards in the United States standardized the way scientists approach human subjects research, and created much-needed accountability. We also addressed our ethical responsibilities to the scientific community, both in how we report our data and how we distribute it. We hope that we have convinced you that careful, open science is an ethical imperative for researchers!\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nThe COVID-19 pandemic led to an immense amount of “rapid-response” research in psychology that aimed to discover—and influence—the way people reasoned about contagion, vaccines, masking, and other aspects of the public health situation. What are the specific ethical concerns that researchers should be aware of for this type of research? Are there reasons for more caution in this kind of research than in other “run-of-the-mill” research?\nThink of an argument against open science practices—for example, that following open science practices is especially burdensome for researchers with more limited resources (you can make up another if you want!). Given our argument that researchers have an ethical duty to openness, how would you analyze this argument under the three different ethical frameworks we discussed?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nThe Belmont Report has shaped US research ethics policy from its publication to the present day. It’s also short and quite readable: https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html.\nA rich reference with several case studies on science misconduct and with strong arguments for open science: Ritchie, Stuart. (2020). Science fictions: How fraud, bias, negligence, and hype undermine the search for truth. Metropolitan Books.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "005-estimation.html",
    "href": "005-estimation.html",
    "title": "5  Estimation",
    "section": "",
    "text": "5.1 Estimating a quantity\nIf experiments are about estimating effects, how do we actually use our experimental data to make these estimates? For our example we’ll design a slightly more modern version of Fisher’s experiment, shown in figure 5.1.\nOur causal theory is that the tea quality is affected by milk-tea ordering, so we’ll test that by rating tea quality both milk-first and tea-first, represented by a DAG like the one in figure 5.2. Our intended population to generalize to is the set of all tea drinkers, and toward that goal we sample a set of tea drinkers. In practice, we might do a field trial in a cafe in which we approach patrons and ask them to participate in our experiment in exchange for a free cup of tea. Although this sample size is almost certainly too small to get precise estimates, for the purpose of this example, we’ll sample 18 tea drinkers—nine in each condition.\nAs our manipulation, we follow Fisher in randomly assigning participants (who of course should give consent to participate) into to one of our two conditions: milk-first and tea-first.1 This design is a between-participants design, so each participant gets only one cup of tea. They receive their cup of tea and taste it. Then, as our measure, we ask for a rating of the tea on a continuous scale from 1 (terrible) to 7 (delicious).2\nAn example dataset from our experiment is shown in figure 5.3. Eventually, we’ll want to estimate the effect of milk-first preparation on quality ratings (our effect of interest). But for now, our goal will be to estimate the quality of the tea when it is milk-first (which some data suggest is actually the better way, at least for British tea drinkers; Kennedy 2003). More formally, we want to use our sample of nine milk-first tea judgments to estimate a number that we can’t directly observe, namely the true perceived quality of all possible milk-first cups. We’ll call this number a population parameter for reasons that will become clear in a moment.\nWe’ll try to go easy on notation, but some amount will hopefully make things clearer. We will use \\(\\theta_{\\textrm{M}}\\) (“theta”) to denote the parameter we want to estimate (the population parameter) and \\(\\widehat{\\theta}_{\\textrm{M}}\\), its sample estimate.3",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "005-estimation.html#the-lady-tasting-tea",
    "href": "005-estimation.html#the-lady-tasting-tea",
    "title": "5  Estimation",
    "section": "The Lady Tasting Tea",
    "text": "The Lady Tasting Tea\nThe birth of modern statistical inference arose from the age-old conundrum of how to best make a cup of tea. The statistician Ronald Fisher was apparently at an afternoon tea party when a lady declared that she could tell the difference when tea was added to milk vs milk to tea. Rather than taking her at her word, Fisher devised an experimental and data analysis procedure to test her claim.\nThe lady would have to judge a set of six new cups of tea and sort them into milk-first vs tea-first sets. Her data would then be analyzed to determine whether her level of correct choice exceeded that expected by chance. While this process now sounds like a quotidian experiment that might be done on a cooking reality show, it seems unremarkable in hindsight only because it set the standard for the way science was done going forward.\nThe important and unusual element of the experiment was its treatment of potential design confounds such which cup of tea was prepared first, which cup of tea was presented first, or the material that the cups were made out of. Prior experimental practice would have been to try to equate all of the cups as closely as possible, decreasing the influence of confounds. Fisher recognized that this strategy was insufficient because of the presence of unobserved confounders. Only by randomizing all other aspects of the experiment could he make strong causal inferences about the treatment (milk then tea vs tea then milk). We discussed the causal power of random assignment in chapter 1—the Lady Tasting Tea experiment is a key touchstone in the popularization of randomized experiments!",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "005-estimation.html#estimating-a-quantity",
    "href": "005-estimation.html#estimating-a-quantity",
    "title": "5  Estimation",
    "section": "",
    "text": "Figure 5.1: The structure of our tea-tasting experiment.\n\n\n\nAn important piece of context for the work of Ronald Fisher, Karl Pearson, and other early pioneers of statistical inference is that they were all strong proponents of eugenics. Fisher was the founding chairman of the Cambridge Eugenics Society. Pearson was perhaps even worse, an avowed Social Darwinist who believed fervently in eugenic legislation. These views are repugnant and provide important context for their statistical contributions.\n\n\n\n\n\n\n\n\n\nFigure 5.2: A directed acyclic graph representing our causal theory of tea quality.\n\n\n1 Technically, randomized experiments were not invented by Fisher. Perhaps the earliest example of a (somewhat) randomized experiment was a trial of scurvy treatments in the 1700s (Dunn 1997). Peirce and Jastrow (1884) also report a strikingly modern use of randomized stimulus presentation (via shuffling cards). Nevertheless, Fisher’s statistical work popularized randomized experiments throughout the sciences, in part by integrating them with a set of analytic methods.2 Right now we’re going to assume that our ratings are just simple numerical values and not worry about the fact that they come from a rating scale that is bounded (e.g., can’t go above 7). If you’re curious about Likert scales (the name for discrete numerical rating scales; pronounced LICK-ERT), we’ll talk a bit more about them in chapter 8.\n\n\n\n\n\n\n\nFigure 5.3: Schematic data from the tea-tasting experiment.\n\n\n\n\n3 Statisticians use “hats” like this to denote estimates from a specific sample. One way to remember this is that the “person in the hat” is wearing a hat to dress up as the actual quantity.\n5.1.1 Maximum likelihood estimation\nOkay, you are probably saying, if we want our estimate of milk-first quality, shouldn’t we just take the average rating across the nine cups of milk-first tea? The answer is yes. But let’s unpack that choice: taking the sample mean as our estimate \\(\\widehat{\\theta}_{\\textrm{M}}\\) is an example of an estimation approach called maximum likelihood estimation. In general terms, maximum likelihood estimation is a two-step process.\nFirst, we assume a model for how the data were generated.4 This model is specified in terms of certain population parameters. In our example, the model is as simple as they come: we just assume there is some average level of tea quality and that the measurements vary around it. Let’s take a look at the data from the milk-first condition, shown in figure 5.4. Our observations are clustered around the mean, but they also show some variation. Some are higher and some are lower. Variation of this type is a feature of every data set. This variation can be summarized via a probability distribution, a mathematical entity that describes the properties of possible datasets.\n4 This sense of “model” is actually a formal instantiation of the type of causal model we discussed in chapter 1. As you get deeper into causal modeling, typically what you do is define a causal “story” for the statistical process that generated a dataset, using both DAGs and the kinds of probability distributions we define below.\n\n\n\n\n\nFigure 5.4: The best-fitting normal distribution for data from the milk-first condition.\n\n\n\nThe only probability distribution we’ll discuss here is the ubiquitous normal distribution (also sometimes called a “Gaussian distribution”). A normal distribution has two parameters (numbers that define its shape): a mean and a standard deviation. These two parameters define the shape of the curve. The mean (\\(\\theta_M\\)) describes where its center goes, and the standard deviation describes how wide it is. Technically, the mean is the expected value for new samples from the distribution. Our best guess about the value of these new samples is that they are at the mean. We can write this more formally by introducing \\(E[M]\\) to denote the expectation of the variable \\(M\\).\nThe standard deviation \\(\\sigma_M\\) is then a way of describing the expected variation in these samples. A bigger standard deviation means that we expect samples to be on average further from the mean. We can write this formally as \\(\\sigma_M = \\sqrt{E[(M - \\theta_M)^2]}\\): the standard deviation is the expected absolute distance between individual samples and the mean, with the square and square root being necessary to compute distance.\nUsing a probability distribution to describe our dataset gives us a way of summarizing our observations through the parameters of the distribution and encoding an assumption about what future observations might look like. How do we fit a normal distribution to our data? We try to find the values of the population parameters that make our observed data as likely as possible. Let’s start with the mean.\n{#fig-estimation-ml2 .margin-caption width=60% fig-alt=“A plot with a”substantially worse curve” and “maximum likelihood curve”, each point has dashed line to curve.”}\nFor example, if our sample mean is \\(\\widehat{\\theta}_{\\textrm{M}} = 4.5\\), what underlying value of \\(\\theta_{\\textrm{M}}\\) would make these data most likely to occur? Well, suppose the underlying parameter were \\(\\theta_{\\textrm{M}}=2.5\\). Then it would be pretty unlikely that our sample mean would be so much bigger. So, \\(\\widehat{\\theta}_{\\textrm{M}}=2.5\\) is a poor estimate of the population parameter based on these data (?fig-estimation-ml2). Conversely, if the parameter were \\(\\theta_{\\textrm{M}}=6.5\\), it would be a bit unlikely that our sample mean would be so much smaller. The value of \\(\\widehat{\\theta}_{\\textrm{M}}\\) that makes these data most likely is just 4.5 itself: the sample mean! That is why the sample mean in this case is the maximum likelihood estimate.\n\n\n\n\n5.1.2 Bayesian estimation\nThe maximum likelihood estimation example above describes a common approach to estimating parameters, where the researcher completely puts aside their prior expectations about what these values might be. This approach is an example of a frequentist statistical approach, an approach that focuses on the long-run performance of estimation procedures.\nOften this approach makes sense, especially when we have no prior expectations about the values we are estimating. But sometimes we do have relevant beliefs about the value. For example, before we perform our tea experiment, we don’t know exactly what \\(\\theta_{\\textrm{M}}\\) will be, but it seems a bit unlikely that tea would be consistently rated as either horrible (1) or perfect (7). We have what you might call weak prior expectations about the kinds of ratings we’ll receive.\nThese kind of expectations are most useful when we have a very small amount of data. Remember that our goal is to estimate a population parameter using the sample data, and small data sets can be rather noisy. Taking into account our prior expectations can help to temper the influence of noise. For example, if our very first participant in the experiment rated their tea as terrible, we wouldn’t want to jump to the conclusion that the tea was actually bad. Instead, we might speculate that the participant was having a bad day or had just brushed their teeth. On the other hand, if all of our participants gave bad ratings to their tea, the data would be more persuasive; in that case, we might want to tell the cafe that they are serving substandard tea. The extent to which our prior expectations should moderate our conclusions should vary with the amount of sample data; with only a little data, our prior expectations should have more influence, but as we gather more, we should put greater weight on the data.\n\n\n\n\n\n\n\nFigure 5.5: Bayes’s rule, annotated.\n\n\nHow do we quantify this trade-off between our prior expectations and our current observations? We can do this via Bayesian estimation of \\(\\widehat{\\theta}_{\\textrm{M}}\\). Bayesian estimation provides a principled framework for integrating prior beliefs and data. These estimation techniques can be very helpful in cases where data are sparse or prior beliefs are strong.\nIn Bayesian estimation, we observe some data \\(d\\), consisting of the set of responses in the experiment. Now we can use Bayes’s rule, a tool from basic probability theory, to estimate this number (figure 5.5). Each part of this equation has a name, and it’s worth becoming familiar with them. The thing we want to compute, \\(p(\\theta_{\\textrm{M}} |\\text{data})\\), is called the posterior probability—it tells us what we should believe about the population parameter on tea quality, given the data we observed.5\n5 We’re making the posterior purple to indicate the combination of likelihood (red) and prior (blue).6 Speaking informally, “likelihood” is just a synonym for probability, but in Bayesian estimation, “likelihood” is a technical term specifically referring to probability of the data given our hypothesis. This ambiguity can get a bit confusing.The first part of the numerator is \\(p(\\text{data}|\\theta_{\\textrm{M}})\\), the probability of the data we observed given our hypothesis about the participant’s ability. This part is called the likelihood.6 This term tells us about the relationship between our hypothesis and the data we observed—so, if we think the tea is of high quality (say \\(\\theta_{\\textrm{M}} = 6.5\\)), then the probability of observing a bunch of low-quality ratings will be fairly low.\n\n\n\n\n\n\n\nFigure 5.6: Bayesian inference about tea ratings with a strong prior on low values.\n\n\nThe second term in the numerator, \\(p(\\theta_{\\textrm{M}} )\\), is called the prior. This term encodes our beliefs about the likely distribution of tea quality. Intuitively, if we think that the tea is likely of high quality, we should require more evidence to convince us that it’s bad. In contrast, if we think it’s probably bad, a few examples of low ratings might serve to convince us.\n\n\n\n\n\n\n\nFigure 5.7: Bayesian inference about tea ratings with a weak prior on low values.\n\n\nFigure 5.6 gives an example of the combination of prior and data. In this example, we look at what difference the prior makes after observing 9 ratings. If we go in assuming that the tea is likely to be bad, the posterior mean (purple line) will be pushed downward relative to the maximum likelihood estimate (red line). This prior is operating only over on ratings—estimates of tea quality. Later on when we talk about comparing milk-first and tea-first ratings to get an estimate of the experimental effect, we could consider putting a prior on tea discrimination (e.g., the experimental effect).\n\n\n\n\n\n\n\nFigure 5.8: Bayesian inference about tea ratings with a strong prior on low values and more data.\n\n\nPriors aren’t usually as strong as the one shown above. Figure 5.7 shows how the picture shifts when we have a weaker prior reflecting a flatter, more widely spread belief about the distribution of ratings. Now the posterior mean (purple) is closer to the maximum likelihood mean (red). This situation is more common—the prior encodes a weak assumption that ratings won’t cluster around the ends of the scale.\nThe effect of the prior is also decreased when you have more data. Take a look at figure 5.8. The prior is the same as in figure 5.6, but we have more data. As a result, the posterior distribution is much more peaked and also much closer to the data—the prior makes much less difference.\nBayesian estimation is most important when you have strong beliefs and not a lot of data. That can be a case where you have just a few participants in your experiment, but it’s also good—and perhaps more common—to use Bayesian methods when you have a lot of data, but maybe not that much data about particular units that you care about. For example, you might have a large dataset about the effects of an educational intervention but not that much data about how it affects a particular subgroup. Bayesian estimates and maximum likelihood estimates will exactly coincide either under a flat prior (a prior that makes any value equally likely) or as the amount of data goes to infinity.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "005-estimation.html#estimating-and-comparing-effects",
    "href": "005-estimation.html#estimating-and-comparing-effects",
    "title": "5  Estimation",
    "section": "5.2 Estimating and comparing effects",
    "text": "5.2 Estimating and comparing effects\nWe’ve now covered estimating a single parameter (the mean for people who had milk-first tea) using both frequentist and Bayesian methods. But recall that what we really wanted to do was to estimate the causal effect we were interested in, namely the milk-first vs tea-first effect. In this section, we’ll discuss how to estimate the effect, and then how to use effect size measures to compare effects across experiments (as well as some of the pros and cons of doing so).7\n7 This method doesn’t have to be used only with a causal effect; it can be any between-group difference. In the current example, we can say with certainty that this effect is a causal one because our experiment uses random assignment.\n5.2.1 Estimating the treatment effect\nLet’s refer to the causal effect we care about as our treatment effect.8 In practice, estimating \\(\\beta\\) (a parameter describing the treatment effect) is going to be a pretty straightforward extension to what we did before.\n8 This is the effect of our manipulation—what we sometimes call an “intervention” as well. “Treatment” is a term that comes from medical statistics but is used more broadly in statistics now.\n\n\n\n\n\nFigure 5.9: Estimating the average treatment effect from the tea-tasting data.\n\n\n9 For simplicity, we’re assuming that the standard deviations in each tea group are equal.\nIn the maximum likelihood framework, we could posit that ratings in each group (milk-first and tea-first) follow a normal distribution but that these normal distributions might have different means and standard deviations. Extending the notation introduced above, let’s term the parameters for the tea-first group \\(\\theta_{\\textrm{T}}\\) (the mean) and \\(\\sigma\\) (the standard deviation). To estimate the treatment effect, we are positing a model in which the milk-first ratings are normally distributed with mean \\(\\theta_{\\textrm{M}} = \\theta_{\\textrm{T}} + \\beta\\) and with standard deviation \\(\\sigma\\).9 This equation says that milk-first ratings have the same distribution as tea-first ratings, except that their average is shifted by \\(\\beta\\). Setting our model up this way then lets us compute \\(\\hat{\\beta}\\), our estimate of the treatment effect in our sample.\nAs in the one-sample case (i.e., estimating the mean of just the milk-first group), maximum likelihood estimation would then proceed by finding the value of \\(\\beta\\) that makes the data most likely under the assumed model. As you’d probably expect, this estimate \\(\\widehat{\\beta}\\) turns out to be simply the difference in sample means, \\(\\widehat{\\theta}_{\\textrm{M}} - \\widehat{\\theta}_{\\textrm{T}}\\). You can see this difference pictured in figure 5.9.\nIn the Bayesian framework, we would again specify a prior \\(p(\\beta)\\) that encodes our prior beliefs about the size and direction of the treatment effect. If we have no prior beliefs at all, then we could specify a flat prior, \\(p(\\beta) \\propto 1\\).10. If we believe the treatment effect is likely to favor milk-first pouring (\\(\\beta&gt;0\\)), we could specify that the prior is a normal distribution centered at some positive value (e.g., \\(\\beta=0.5\\)); the standard deviation of this prior would encode how certain we are about our prior beliefs. And, if we have no prior beliefs about the direction of the treatment effect, but we think it is unlikely to be very large, we could specify a normal prior centered at 0, which has the effect of “shrinking” the estimates closer to 0.11\n10 This equation says that the probability of any value of \\(\\beta\\) is “proportional to” 1, meaning that it’s constant (“flat”) regardless of what value \\(\\beta\\) takes.11 The measures of variability that we discuss here account for statistical uncertainty reflecting the fact that we have only a finite sample size. If the sample size were infinite, there would be no uncertainty of this kind. Statistical uncertainty is only one kind of uncertainty, though. A more holistic view of the overall credibility of an estimate should also account for other things outside of the model, like study design issues and bias.As in our example above, maximum likelihood estimates and Bayesian estimates are going to be pretty similar if we have a lot of data or weak priors. They will only diverge when we have strong priors or relatively little data. The reason we are setting up these two different frameworks, however, is that they provide very different inferential tools, as we’ll see in the next chapter.\n\n\n5.2.2 Measures of effect size\nOnce we have measured something, we need to make a decision about how to describe this effect to others. Sometimes we are working with fairly intuitive relationships that are easy to describe. A researcher might say, for example, that people who received milk-first tea drank the tea, on average, five minutes quicker than people who received tea-first tea (i.e., that \\(\\widehat{\\beta} = 5\\) minutes). Time is measured in units like minutes and seconds and so we all have a shared understanding of what five minutes means.\nBut what about our participants’ ratings of tea quality, which were provided on an arbitrary 7-point rating scale that we devised? What does it mean to that participants who drank milk-first tea rated it 1 point higher than participants who drank tea-first tea (i.e., that \\(\\widehat{\\beta} = 1\\) point)? And how is this difference comparable to, for instance, a 1-point change on a scale that has similar anchors (“terrible” and “delicious”) but uses a 100-point rating system?\nTo provide a common language for describing these relationships, some researchers use standardized effect sizes. A common standardized effect size is Cohen’s d, which provides a standardized estimate of the difference between two means. There are many different ways to calculate Cohen’s d (Lakens 2013), but all approaches are usually some variant of the following formula:\n\\[\nd = \\frac{\\theta_{\\textrm{M}} - \\theta_{\\textrm{T}}}{\\sigma_{\\text{pooled}}}\n\\]\nwhere the difference between means (\\(\\theta_{\\textrm{T}}\\) and \\(\\theta_{\\textrm{M}}\\)) is divided by the pooled standard deviation \\(\\sigma_{\\text{pooled}}\\). Intuitively, what you’re doing is taking the study effect (\\(\\beta\\)) and dividing it—scaling it—by the variation we saw between individuals in the study.\n\n\n\n\n\n\nFigure 5.10: Schematic effect size computation.\n\n\n\nLet’s compute this measure for our tea-drinking study. We can just plug in the estimates we see in figure 5.9 and compute the standard deviation of our observed data:\n\\[\\widehat{d} = \\frac{\\widehat{\\theta}_{\\textrm{M}} - \\widehat{\\theta}_{\\textrm{T}}}{\\widehat{\\sigma}_{\\text{pooled}}} = \\frac{4.5- 3.5}{1.25} = \\frac{1}{1.25} = 0.80\\] In other words, the effect size of the difference between the two conditions is 0.8 standard deviations. This process is shown graphically in figure 5.10.12\n12 Cohen’s \\(d\\), also referred to as a standardized mean difference (SMD), can be tricky to apply to more complex experimental designs, such as when you have within-participant designs and multiple measurements of each participant. For some guidance on this topic, see Lakens (2013).We previously said that people who drank milk-first tea had quality ratings that were, on average, 1 point higher on a 7-point scale (\\(\\beta = 1\\) point). Cohen’s d translates the arbitrary units of our rating scale into a unit-less effect size that is measured in terms of the variation in the data. You may find yourself wondering: “Why would I ever describe things in terms of standard deviations?” The key benefit is that it allows us to compare the size of the effect between studies that use different measures.\nLet’s say that we ran a replication of our tea study with two changes: (1) we studied patrons in a US cafe instead of a UK cafe, and (2) we used a 100-point quality rating scale instead of a 7-point scale. Imagine that, just as we found that participants in the UK rated the milk-first tea 1 point higher on a 7-point quality scale, US participants rated the milk-first tea 1 point higher on a 100-point quality scale. It seems clear that these effects are different because of the difference in scale. But how different?\nIt might at first seem reasonable just to normalize by the length of the scale. So, maybe the UK experimental participants showed a 1/7 rating effect and the US participants showed a 1/100 rating effect. The trouble with this move is that it presupposes that participants from two different populations are using two different scales in exactly the same way! For example, maybe US participants made very clumpy judgments that were mostly centered around 50 (perhaps because of a lack of milk tea experience). Standardized effect sizes get around this kind of issue by scaling according to the variability of the data.\nLet’s compute the effect size for the cross-cultural replication. We’ll imagine that participants who drank milk-first tea gave an average rating of 50/100 and participants who drank tea-first tea rated it 49 on average. But if their variability was also relatively lower, perhaps the standard deviation of their ratings was only 5. Using the formula above, we find\n\\[\\widehat{d}_{US} = \\frac{\\widehat{\\theta}_{\\textrm{M}} - \\widehat{\\theta}_{\\textrm{T}}}{\\widehat{\\sigma}_{\\text{pooled}}} = \\frac{50 - 49}{5} = \\frac{1}{5}=  0.2\\]\nA Cohen’s d of 0.2 means that US cafe patrons rated their tea 0.2 standard deviations higher when it was milk-first, much smaller than the 0.8 standard deviation difference in the UK patrons.\nThere are no hard and fast rules for interpreting what makes a big effect or a small effect, but people often refer back to a standard suggested by Cohen (1992). On those standards, \\(d = 0.8\\) is a “large effect” and \\(d = 0.2\\) is a “small effect.” But these effect size interpretation norms are somewhat arbitrary. The key point here was that US and UK patrons had the same raw score change in quality ratings (\\(\\widehat{\\beta} = 1\\)), and standardizing the differences allowed us to communicate that the difference was larger among the UK patrons.\nCohen’s d is one of many standardized effect sizes that researchers can use. Just as Cohen’s d standardizes differences in group means, there are also generalizations that allow for continuous treatment variables or covariate adjustment (e.g., Pearson’s r, as we discuss below; \\(r^2\\); or \\(\\eta^2\\)). And there is a whole other set of effect-size measures for relationships between binary variables (e.g., odds ratio). We’ll be using effect sizes throughout the book, but we’ll be using Cohen’s d as our example.13\n13 If you’d like to learn more about other varieties of effect size, take a look at Fritz, Morris, and Richler (2012) and Lakens (2013).\n\n5.2.3 Pros and cons of standardizing effect sizes\nStandardizing effect size helps communicate that a 1-point change on a 7-point scale is not the same as a 1-point change on a 100-point scale. But is it any better to say that the first change represents a 0.8 standard deviation difference and the second a 0.08 standard deviation difference?\nEffect sizes allow us to compare results across studies more easily. Across studies, researchers use different measures, different study designs, and different populations. Standardization gives us a “common language” to describe estimated relationships in these varied contexts. This language is helpful when we want to aggregate and compare effects across studies via meta-analysis. And it is also helpful when planning new studies. When trying to figure out how many participants to run in a study, almost all techniques for sample size planning use standardized effect sizes to determine how much data would be needed to reliably detect an effect.\nStandardizing effect sizes has limitations, though. For example, if two interventions produce the same absolute change in the same outcome measure, but are studied in different populations in which the variability on the outcome differs substantially, the interventions would produce different standardized mean differences (Baguley 2009) (see the Depth box “Reliability paradoxes!” in chapter 8).\nImagine we conducted our tea experiment again, but this time with decaf tea and focusing on children. Maybe milk-first tea tastes the same amount better than tea-first tea for kids and for adults. But kids are, as a rule, more variable in their responding than adults. This higher level of variability would lead us to observe a smaller effect size in kids vs adults. Recall that our UK adult SD was 1.25 and our effect size was \\(d = 0.8\\). Imagine that children’s SD is 2.5. In this scenario, even if tea led to the same 1-point absolute change in ratings among adults and children, the standardized effect size for kids would look half as big:\n\\[\\widehat{d}_{kids} = \\frac{\\widehat{\\theta}_{\\textrm{M}} - \\widehat{\\theta}_{\\textrm{T}}}{\\widehat{\\sigma}_{\\text{pooled}}} = \\frac{5- 4}{2.5} = \\frac{1}{2.5} =  0.4\\]\nThis example highlights some of the challenges with standardization. If we focused on the fact that both adults and children show a 1-point change in ratings levels (\\(\\widehat{\\beta} = 1\\)), we would conclude that milk-first tea ordering is as much better for adults as kids. If we focused on the standardized effect sizes, however, we would conclude that the milk ordering effect is twice as big for adults.\nSo which is better: Describing raw measures or standardized effect sizes? In general, our response is “Why not both?” But if you wanted to pick one or the other, we recommend considering what type of measurement you are using. With measures that yield common measurement units that are likely to be reported in many studies already, use raw scores (Baguley 2009). For example, if your study uses physical units such as milliseconds (e.g., for reaction times) or counts (e.g., for a study tracking an outcome like number of words), these measurements can be quite useful to compare across studies. Reporting raw measurements also can allow you to check whether your measurements make sense—for example, a reaction time of 70 milliseconds is inhumanly fast while a reaction time of 10 seconds might be extremely slow (at least, for many speeded tasks).\nIn contrast, we recommend using standardized effect sizes for cases where the measurement is relatively unlikely to be comparable with other studies in its original form, or unlikely to be meaningful on its own. For example, reporting the effect of an intervention on raw math test scores is only meaningful if the reader knows how many items are on the test, how difficult it is, and so forth. In such a case where it is hard for a reader to be “calibrated” to the specific measurement units you are using, standardized effect sizes may be the best way to report your finding (Kelley and Preacher 2012).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "005-estimation.html#estimating-the-relationship-between-variables",
    "href": "005-estimation.html#estimating-the-relationship-between-variables",
    "title": "5  Estimation",
    "section": "5.3 Estimating the relationship between variables",
    "text": "5.3 Estimating the relationship between variables\n\n\n\n\n\n\n\nFigure 5.11: The relationship between age and milk-first tea rating.\n\n\n14 Remember, this is a correlational relationship, and there’s no causal inference possible here.Our focus up until now has been in estimating individual effects, but sometimes we also want to estimate the relationship between two different variables. Extending our example, figure 5.11 shows the relationship between the age of the tea taster and their rating of milk-first tea. It seems that younger people overall like tea less than older people.14 How could we quantify this result?\nThe first concept we need is covariance. Covariance captures the degree to which we expect two variables to deviate from their means in the same direction. We’re looking at milk-first tea ratings \\(M\\) and participant ages \\(A\\). We can write the covariance between these two variables as\n\\[Cov(M, A) = E[(M - \\theta_M) (A - \\theta_A)]\\]\nThis formula expresses the expected product of how much each observation differs from its expectation (mean) along each variable. Figure 5.12 shows these differences, which are multiplied together for each point to get the covariation.15\n15 This looks a little tricky, but it’s actually very related to the basic concepts we’ve already seen. Remember when we introduced the standard deviation, we described it as the expected distance between new samples from a distribution and the mean of that distribution. The covariance is very related: the standard deviation is just \\(\\sqrt{Cov(X,X)}\\), in other words, the square root of the covariance of a variable with itself.\n\n\n\n\n\nFigure 5.12: Estimating the covariation between age and milk-first tea rating.\n\n\n\nThis covariance number gives us an estimate of how much age and ratings covary, but its units are a bit funny: it’s hard to know what to make of an expected deviation of 1 point-year. We can do a simple trick to standardize its units and make it into a wonderful form of effect size called the correlation coefficient (denoted \\(r\\)). Remember that to create effect sizes above, we divided by the standard deviation of the variable. Here all we have to do is divide by the standard deviation of both variables.\n\\[r_{M,A} = \\frac{Cov(M,A)}{\\sigma_M \\sigma_A}\\] In other words, the correlation between two variables is the standardized covariation.\nThe correlation coefficient is the most ubiquitous measure of association between variables. It ranges between -1, where two variables covary in exactly the opposite direction, to 1, when two variables covary perfectly. A correlation means that there is no association between two variables. A correlation of -1 or 1 doesn’t mean that these two variables have the same scale, however: it just means that they “move together.”\nThis section has described one way of looking at a correlation coefficient: as standardized covariation. For a great discussion of all the different ways of thinking about correlations, see Lee Rodgers and Nicewander (1988).\nCritically, a correlation is an effect size. Correlations can be compared across different measures and different studies (including both experimental and observational studies), making it a very valuable scale-free comparison tool.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "005-estimation.html#chapter-summary-estimation",
    "href": "005-estimation.html#chapter-summary-estimation",
    "title": "5  Estimation",
    "section": "5.4 Chapter summary: Estimation",
    "text": "5.4 Chapter summary: Estimation\nIn this chapter, we introduced the idea of estimating both individual measurements and treatment effects from observed data. These ideas are simple but they lay the foundations for hypothesis testing and modeling (our next two chapters). Further, we set up the distinction between Bayesian and frequentist approaches, which we will expand in the next chapter since these traditions provide different inferential tools.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nIn this chapter you learned about estimation, and in this book more generally, we have argued that the goal of an experiment is to provide a maximally precise estimate of a causal effect. Psychology as a field has often been criticized for focusing too much on inference and too little on estimation. Find an article in the journal Psychological Science that reports on an experiment or series of experiments and read the abstract. Does it mention an estimate of any particular quantity? What might be the benefits of reporting estimates in the study abstract?\nTry the same exercise with a paper in the New England Journal of Medicine or Journal of the American Medical Association. Find a paper and check if there is a mention of any specific quantity being estimated. (We suspect there will be!) Consider this contrast between the medical article and the psychology article. What do you make of this difference between fields?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA great narrative introduction to the history and practice of statistics: Salsburg, Daniel. (2002). The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century. Macmillan.\nAn open-source statistics textbook that follows a similar approach as chapters 5—7: Poldrack, Russell A. (2024). Statistical Thinking for the 21st Century. Available free online at https://statsthinking21.org.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "006-inference.html",
    "href": "006-inference.html",
    "title": "6  Inference",
    "section": "",
    "text": "6.1 Sampling variation\nIn chapter 5, we introduced Fisher’s tea-tasting experiment and discussed how to estimate means and differences in means from our observed data. These so-called point estimates represent our best guesses about the population parameters given the data—and possibly also given our prior beliefs. We can also report how much statistical uncertainty is involved in these point estimates.3 Quantifying and reasoning about this uncertainty is an important goal: in our original study we only had nine participants in each group, which will only provide a low-precision (i.e., highly uncertain) estimate of the population. By contrast, if we repeated the experiment with 200 participants in each group, the data would be far less noisy, and we would have much less uncertainty, even if the point estimates happened to be identical.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#sampling-variation",
    "href": "006-inference.html#sampling-variation",
    "title": "6  Inference",
    "section": "",
    "text": "3 As in the previous chapter, we’re only capturing statistical uncertainty. A holistic view of a particular estimate’s credibility also include everything else you know about the study design.\n\n\n\n\n\nFigure 6.2: Sampling distribution for the treatment effect in the tea-tasting experiment, given many different repetitions of the same experiment, each with n = 9 per group. Circles represent average treatment effects from different individual experiments, while the thick line represents the form of the underlying distribution.\n\n\n\n\n6.1.1 Standard errors\nTo characterize the uncertainty in an estimate, it helps to picture its sampling distribution, which is the distribution of the estimate across different, hypothetical, samples. That is, let’s imagine that we conducted the tea experiment not just once but dozens, hundreds, or even thousands of times. This idea is often called repeated sampling as a shorthand. For each hypothetical sample, we use similar recruitment methods to recruit a new sample of participants, and we compute \\(\\widehat{\\beta}\\) for that sample. Would we get exactly the same answer each time? No, simply because the samples will have some random variability (noise). If we plotted these estimates, \\(\\widehat{\\beta}\\), we would get the sampling distribution in figure 6.2.\n\n\n\n\n\n\ncode\n\n\n\n\n\nIn this chapter and the subsequent statistics and visualization chapters of the book, we’ll try to facilitate understanding and illustrate how to use these concepts in practice by giving the R code we use in constructing our examples in these code boxes. We’ll assume that you have some knowledge of base R and the Tidyverse—to get started with these, go ahead and take a look at appendix D if you haven’t already. Although our figures are often drawn by hand, even the hand-drawn ones are based on actual simulation results!\nSince we’re going to be working with lots of data from the tea tasting example, we wrote a function called make_tea_data() that creates a tibble with some (made-up) data from our modern tea-tasting experiment. You can find the function on GitHub (https://github.com/langcog/experimentology/blob/main/helper/tea_helper.qmd) if you want to follow along.\n\ntea_data &lt;- make_tea_data(n_total = 18)\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Comparing sampling distributions for the treatment effect with smaller and larger size samples.\n\n\nNow imagine we also did thousands of repetitions of the experiment with \\(n = 200\\) per group instead of \\(n = 9\\) per group. Figure 6.3 shows what the sampling distribution might look like in that case. Notice how much narrower the sampling distribution becomes when we increase the sample size, showing our decreased uncertainty. More formally, the standard deviation of the sampling distribution itself, called the standard error, decreases as the sample size increases.\nThe sampling distribution is not the same thing as the distribution of tea ratings in a single sample. Instead, it’s a distribution of estimates across samples of a given size. In essence, it tells us what the mean of a new experiment might be, if we ran it with a particular sample size.\n\n\n\n\n\n\ncode\n\n\n\n\n\nTo do simulations where we repeat the tea-tasting experiment over and over again, we’re using a special tidyverse function from the purrr library: map(). map() is an extremely powerful function that allows us to run another function (in this case, the make_tea_data() function that we introduced last chapter) many times with different inputs. Here we create a tibble made up of a set of 1,000 runs of the make_tea_data() function.\n\nsamps &lt;- tibble(sim = 1:1000) |&gt;\n  mutate(data = map(sim, \\(i) make_tea_data(n_total = 18))) |&gt;\n  unnest(cols = data)\n\nNext, we just use the group_by() and summarize() workflow from appendix D to get the estimated treatment effect for each of these simulations.\n\ntea_summary &lt;- samps |&gt;\n  group_by(sim, condition) |&gt;\n  summarize(mean_rating = mean(rating)) |&gt;\n  group_by(sim) |&gt;\n  summarize(delta = mean_rating[condition == \"milk first\"] -\n              mean_rating[condition == \"tea first\"])\n\nThis tibble gives us what we would need to plot the sampling distributions above in figure 6.2 and figure 6.3.\n\n\n\n\n\n6.1.2 The central limit theorem\nWe talked in the last chapter about the normal distribution, a convenient and ubiquitous tool for quantifying the distribution of measurements. A shocking thing about sampling distributions for many kinds of estimates—and for all maximum likelihood estimates—is that they become normally distributed as the sample size gets larger and larger. This result holds even for estimates that are not even remotely normally distributed in small samples!\n\n\n\n\n\n\n\nFigure 6.4: Sampling distribution of samples from a biased coin (n = 2 flips per sample). Bar height is the proportion of flips resulting in a particular mean.\n\n\n\n\n\n\n\n\nFigure 6.5: Sampling distribution for 2, 8, 32, and 128 flips.\n\n\n\nFor example, say we are flipping a coin and we want to estimate the probability that it lands heads (\\(p_H\\)). If we draw samples each consisting of only \\(n = 2\\) coin flips, figure 6.4 is the sampling distribution of the estimates (\\(\\widehat{p}_H\\)). This sampling distribution doesn’t look normally distributed at all—it doesn’t have the characteristic “bell curve” shape! In a sample of only two coin flips, \\(\\widehat{p}_H\\) can only take on the values 0, 0.5, or 1.\nBut look what happens as we draw increasingly larger samples in figure 6.5: we get a normal distribution! This tendency of sampling distributions to become normal as \\(n\\) becomes very large reflects a deep and elegant mathematical law called the central limit theorem.\nThe practical upshot is that the central limit theorem directly helps us characterize the uncertainty of sample estimates. For example, when the sample size is reasonably large (approximately \\(n&gt;30\\) in the case of sample means) the standard error (i.e., the standard deviation of the sampling distribution) of a sample mean is approximately \\(\\widehat{SE} = \\sigma/\\sqrt{n}\\). The sampling distribution becomes narrower as the sample size increases because we are dividing by the square root of the number of observations.\n\n\n\n\n\n\ncode\n\n\n\n\n\nEven though our figures are hand-drawn, they’re based on real simulations. For our central limit theorem simulations, we again use the map() function. We set up a tibble with the different values we want to to try (which we call n_flips). Then we make use of the map() function to run rbinom() (random binomial samples) for each value of n_flips.\nOne trick we make use of here is that rbinom() takes an extra argument that says how many of these random values you want to generate. Here we generate nsamps = 1000 samples, giving us 1,000 independent replicates at each n. But returning an array of 1,000 values for a single value of n_flips results in something odd: the value for each element of flips is an array. To deal with that, we use the unnest() function, which expands the array back into a normal tibble.\n\nn_samps &lt;- 1000\nn_flips_list &lt;- c(2, 8, 32, 128)\n\nsample_p &lt;- tibble(n_flips = n_flips_list) |&gt;\n  mutate(flips = map(n_flips, \\(f) rbinom(n = n_samps, size = f, prob = .7))) |&gt;\n  unnest(cols = flips) |&gt;\n  mutate(p = flips / n_flips)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#from-variation-to-inference",
    "href": "006-inference.html#from-variation-to-inference",
    "title": "6  Inference",
    "section": "6.2 From variation to inference",
    "text": "6.2 From variation to inference\nLet’s go back to Fisher’s tea-tasting experiment. The first innovation of that experiment was the use of randomization to recover an estimate of the causal effect of milk ordering. But there was more to Fisher’s analysis than we described.\nThe second innovation of the tea-tasting experiment was the idea of creating a model of what might happen during the experiment. Specifically, Fisher described a hypothetical null model that would arise if the lady had chosen cups by chance rather than because of some tea sensitivity. In our tea-rating experiment, the null model describes what happens when there is no difference in ratings between tea-first and milk-first cups. Under the null model, the true treatment effect (\\(\\beta\\)) is zero.\nEven with an actual treatment effect of zero, across repeated sampling, we should see some variation in \\(\\widehat{\\beta}\\), our estimate of the treatment effect. Sometimes we’ll get a small positive effect, sometimes a small negative one. Occasionally just by chance we’ll get a big effect. This is just sampling variation as we described above.\nFisher’s innovation was to quantify the probability of observing various values of \\(\\hat{\\beta}\\), given the null model. Then, if the observed data that were very low probability under the null model, we could declare that the null was rejected. How unlikely must the observed data be in order to reject the null? Fisher declared that it is “usual and convenient for experimenters to take 5 percent as a standard level of convenience,” establishing the 0.05 cutoff that has become gospel throughout the sciences.4\n4 Actually, right after establishing 0.05 as a cutoff, Fisher then writes that “in the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon … in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.” In other words, Fisher was all for replication!Let’s take a look at what the null model might look like. We already tried out repeating our tea-tasting experiment thousands of times in our discussion of sampling above. Now in figure 6.6, we do the same thing but we assume that the null hypothesis of no treatment effect is true. The plot shows the distribution of treatment effects \\(\\hat{\\beta}\\) we observe: some a little negative, some a little positive, and a few substantially positive or negative, but mostly zero.\nLet’s apply the \\(p&lt;0.05\\) standard. If our observation has less than a 5% probability under the null model, then the null model is likely wrong. The red dashed lines on figure 6.6 show the point below which only 2.5% of the data are found and the point above which only 2.5% of the data are found. These are called the tails of the distribution. Because we’d be equally willing to accept milk-first tea or tea-first tea being better, we consider both positive and negative observations as possible.5\n5 Because we’re looking at both tails of the distribution, this is called a “two-tailed” test.\n\n\n\n\n\ncode\n\n\n\n\n\nTo simulate our null model, we can do the same kind of thing we did before, just specifying to our make_tea_data() function that the true difference in effects is zero!\n\nn_sims &lt;- 1000\nnull_model &lt;- tibble(sim = 1:n_sims, n = 18) |&gt;\n  mutate(data = map(sim, \\(i) make_tea_data(n_total = n, delta = 0))) |&gt;\n  unnest(cols = data)\n\nAgain we use group_by() and summarize() to get the distribution of treatment effects under the null hypothesis.\n\nnull_model_summary &lt;- null_model |&gt;\n  group_by(sim, condition) |&gt;\n  summarize(mean_rating = mean(rating)) |&gt;\n  group_by(sim) |&gt;\n  summarize(delta = mean_rating[condition == \"milk first\"] -\n              mean_rating[condition == \"tea first\"])\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: One example of the distribution of treatment effects under the null model (with n = 9 per group). The red regions indicate the part of the distribution in which less than 5% of observations should fall.\n\n\n\nFigure 6.6 captures the logic of NHST: if the observed data fall in the region that has a probability of less than 0.05 under the null model, then we reject the null. So, then when we observe some particular treatment effect \\(\\hat{\\beta}\\) in a single (real) instance of our experiment, we can compute the probability of these data or any data more extreme than ours under the null model.6 This probability is our \\(p\\)-value, and if it is small, it gives us license to conclude that the null is false.\n6 The “more extreme” part deserves a little explanation. Any individual outcome is relatively unlikely by itself, just because it’s surprising that the estimate is that exact value (we’re simplifying here, it gets a bit trickier when you are talking about real numbers). What we care about instead is a group of values. The ones that are in the middle of the distribution are, considered as a group, quite likely; the ones on the tails are, as a group, less likely. We want to know if the probability of the group of datapoints that includes our observation and anything even further out on the tails is collectively less than 0.05.As we saw before, the larger the sample size, the smaller the standard error. That’s true for the null model too! figure 6.7 shows the expected null distribution for a bigger experiment.\n\n\n\n\n\n\n\nFigure 6.7: Example distribution of treatment effects under the null model for a larger experiment.\n\n\nThe more participants in the experiment, the tighter the null distribution becomes, and hence the smaller the region in which we should expect a null treatment effect to fall. Because our expectation based on the null becomes more precise, we will be able to reject the null based on smaller treatment effects. In this type of hypothesis testing, as with estimation, our goals matter. If we’re merely testing a hypothesis out of curiosity, perhaps we don’t want to measure too many cups of tea. But if we were designing the tea strategy for a major cafe chain, the stakes would be higher; in that case, maybe we’d want to do a more extensive experiment!\n\n\n\n\n\n\ncode\n\n\n\n\n\nWe can do a more systematic simulation of the null regions for different sample sizes by simply adding a parameter to our simulation.\n\nn_sims &lt;- 10000\nnull_model_multi_n &lt;- expand_grid(sim = 1:n_sims, n = c(12, 24, 48, 96)) |&gt;\n  mutate(sim_data = map(n, \\(n_i) make_tea_data(n_total = n_i, delta = 0))) |&gt;\n  unnest(cols = sim_data)\n\nnull_model_summary_multi_n &lt;- null_model_multi_n |&gt;\n  group_by(n, sim, condition) |&gt;\n  summarize(mean_rating = mean(rating)) |&gt;\n  group_by(n, sim) |&gt;\n  summarize(delta = mean_rating[condition == \"milk first\"] -\n              mean_rating[condition == \"tea first\"])\n\nnull_model_quantiles_multi_n &lt;- null_model_summary_multi_n |&gt;\n  group_by(n) |&gt;\n  summarize(q_025 = quantile(delta, .025),\n            q_975 = quantile(delta, .975)) \n\nHere is the plotting code to produce a comparable figure to our illustration:\n\nggplot(null_model_summary_multi_n, aes(x = delta)) + \n  facet_wrap(vars(n), nrow = 1, labeller = label_both) +\n  geom_histogram(binwidth = .25) +\n  geom_vline(xintercept = 0, color = pal$grey, linetype = \"dotted\") + \n  geom_vline(data = null_model_quantiles_multi_n, \n             aes(xintercept = q_025), color = pal$red, linetype = \"dotted\") + \n  geom_vline(data = null_model_quantiles_multi_n, \n             aes(xintercept = q_975), color = pal$red, linetype = \"dotted\") + \n  xlim(-2.5, 2.5) + \n  labs(x = \"Difference in rating\", y = \"Frequency\")\n\n\n\n\nOne last note: You might notice an interesting parallel between the NHST paradigm and Popper’s falsificationist philosophy (introduced in chapter 2). In both cases, you never get to accept the actual hypothesis of interest. The only thing you can do is observe evidence that is inconsistent with the null hypothesis. The added limitation of NHST is that the only hypothesis you can falsify is the null!7\n7 A historical note: what we describe here as NHST is not what either Fisher’s method or the Neyman-Pearson method that we introduce below. It’s what Gigerenzer (1989) called “the silent hybrid solution,” in which the more continuous approach to \\(p\\)-values that Fisher advocated for got rolled into the hypothesis testing approach of Neyman and Pearson. This hybrid—which neither Fisher nor Neyman and Pearson would have liked—is what we now mostly take for granted as the received NHST approach.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#making-inferences",
    "href": "006-inference.html#making-inferences",
    "title": "6  Inference",
    "section": "6.3 Making inferences",
    "text": "6.3 Making inferences\nIn the tea-tasting example we were just considering, we were trying to make an inference from our sample to the broader population. In particular, we were trying to test whether milk-first tea was rated as better than tea-first tea. Our inferential goal was a clear, binary answer: Is milk-first tea better?\nBy defining a \\(p\\)-value, we got one procedure for giving this answer. If \\(p &lt; 0.05\\), we reject the null. Then we can look at the direction of the difference and, if it’s positive, declare that milk-first tea is “significantly” better. Let’s compare this procedure to a different process that builds on the Bayesian estimation ideas we described in the previous chapter. We can then come back to examine NHST in light of that framework.\n\n6.3.1 Bayes Factors\nBayes Factors are a method for quantifying the support for one hypothesis over another, based on an observed dataset. They don’t tell you the probability that a particular hypothesis is right, but they let you compare two different ones.\n\n\n\n\n\n\n\nFigure 6.8: The Bayes Factor (BF).\n\n\n8 Sometimes people refer to the BF in favor of \\(H_1\\) as the \\(BF_{10}\\) and the BF in favor of \\(H_0\\) as the \\(BF_{01}\\). This notation is a bit confusing because the first of these looks like the number 10.Informally, we’ve now discussed two different distinct hypotheses about the tea situation: our participants could have no tea discrimination ability—leading to chance performance. We call this \\(H_0\\). Or they could have some nonzero ability—leading to greater than chance performance. We call this \\(H_1\\). The Bayes Factor is simply the likelihood of the data (in the technical sense used above) under \\(H_1\\) vs. under \\(H_0\\) (figure 6.8). The Bayes Factor is a ratio, so if it is greater than 1, the data are more likely under \\(H_1\\) than they are under \\(H_0\\)—and vice versa for values between 1 and 0. A BF of 3 means there is three times as much evidence for \\(H_1\\) than \\(H_0\\), or equivalently 1/3 as much evidence for \\(H_0\\) as \\(H_1\\).8\n\n\n\n\n\n\ncode\n\n\n\n\n\nBayes Factors are delightfully easy to compute using the BayesFactor R package (Morey and Rouder 2023). All we do is feed in the two sets of ratings to the ttestBF() function!\n\nlibrary(BayesFactor)\n\ntea_bf &lt;- ttestBF(x = filter(tea_data, condition == \"milk first\")$rating,\n                  y = filter(tea_data, condition == \"tea first\")$rating,\n                  paired = FALSE)\n\n\n\n\nThere are a couple of things to notice about the Bayes Factor. The first is that, like a \\(p\\)-value, it is inherently a continuous measure. You can artificially dichotomize decisions based on the Bayes Factor by declaring a cutoff (say, BF &gt; 3 or BF &gt; 10), but there is no intrinsic threshold at which you would say the evidence is “significant.” Some guidelines for interpretation (from S. N. Goodman 1999) are shown in table 6.1.9 On the other hand, cutoffs like BF &gt; 5 or \\(p &lt; 0.05\\) are not very informative. So, although we provide this table to guide interpretation, we caution that you should always report and interpret the actual Bayes Factor, not whether it is above or below some cutoff.\n9 Some like the guidelines provided by Jeffreys (1961), which include categories such as “barely worth mentioning” (1 &gt; BF &gt; 3).\n\n\n\nTable 6.1: S. N. Goodman (1999) interpretation guidelines for Bayes Factors.\n\n\n\n\n\nBF range\nInterpretation\n\n\n\n\n&lt; 1\nNegative (supports \\(H_0\\))\n\n\n1–5\nWeak\n\n\n5–10\nModerate\n\n\n10–20\nModerate to strong\n\n\n20–100\nStrong to very strong\n\n\n\n\n\n\n\nThe second thing to notice about the Bayes Factor is that it doesn’t depend on our prior probability of \\(H_1\\) vs. \\(H_0\\). We might think of \\(H_1\\) as very implausible. But the BF is independent of that prior belief. So that means it’s a measure of how much the evidence should shift our beliefs away from our prior. One nice way to think about this is that the Bayes Factor computes how much our beliefs—whatever they are—should be changed by the data (Morey and Rouder 2011).\nIn practice, the thing that is both tricky and good about Bayes Factors is that you need to define an actual model of what \\(H_0\\) and \\(H_1\\) are. That process involves making some assumptions explicit. We won’t go into how to make these models here—this is a big topic that is covered extensively in books on Bayesian data analysis.10 The goal here is just to give a general sense of what Bayes Factors are.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Two good ones beyond the McElreath book mentioned above are Gelman et al. (1995), which is a bit more statistical, and Kruschke (2014), which is a bit more focused on psychological data analysis. An in-prep web-book by Nicenboim et al. (https://vasishth.github.io/bayescogsci/book) also looks great.\n\n6.3.2 \\(p\\)-values\nNow let’s turn back to NHST and the \\(p\\)-value. We already have a working definition of what a \\(p\\)-value is from our discussion above: it’s the probability of the data (or any data that would be more extreme) under the null hypothesis. How is this quantity related to either our Bayesian estimate or the BF? Well, the first thing to notice is that the \\(p\\)-value is very close (but not identical) to the likelihood itself.11\n11 The likelihood—for both Bayesians and frequentists—is the probability of the data, just like the \\(p\\)-value. But unlike the \\(p\\)-value, it doesn’t include the probability of more extreme data as well.12 \\(t\\)-tests can also be used in cases where one sample is being compared to some baseline.Next we can use a simple statistical test, a \\(t\\)-test, to compute \\(p\\)-values for our experiment. In case you haven’t encountered one, a \\(t\\)-test is a procedure for computing a \\(p\\)-value by comparing the distribution of two variables using the null hypothesis that there is no difference between them.12 The \\(t\\)-test uses the data to compute a test statistic whose distribution under the null hypothesis is known. Then the value of this statistic can be converted to \\(p\\)-values for making an inference.\n\n\n\n\n\n\ncode\n\n\n\n\n\nThe standard t.test() function is built into R via the default stats package. Here we simply make sure to specify the variety of test we want by using the flags paired = FALSE and var.equal = TRUE (denoting the assumption of equal variances).\n\ntea_t &lt;- t.test(x = filter(tea_data, condition == \"milk first\")$rating,\n                y = filter(tea_data, condition == \"tea first\")$rating,\n                paired = FALSE, var.equal = TRUE)\n\n\n\n\nImagine we conduct a tea-tasting experiment with \\(n = 48\\) and perform a \\(t\\)-test on our experimental results. In this case, we see that the difference between the two groups is significant at \\(p&lt;0.05\\): \\(t(46) = 2.86\\), \\(p = 0.006\\).\n\n\n\n\n\n\nTable 6.2: Comparison of \\(p\\)-value and BF for several different (randomly-generated) tea-tasting scenarios.\n\n\n\n\n\n\nN\nEffect size\np-value\nBF\n\n\n\n\n12\n0.5\n&gt; .999\n0.5\n\n\n12\n1.0\n.076\n1.4\n\n\n12\n1.5\n.002\n18.7\n\n\n24\n0.5\n.858\n0.4\n\n\n24\n1.0\n.061\n1.5\n\n\n24\n1.5\n.009\n5.6\n\n\n48\n0.5\n.002\n17.7\n\n\n48\n1.0\n.033\n2.0\n\n\n48\n1.5\n&lt; .001\n133.6\n\n\n96\n0.5\n.038\n1.5\n\n\n96\n1.0\n&lt; .001\n12218.2\n\n\n96\n1.5\n&lt; .001\n3081.4\n\n\n\n\n\n\n\n\nThe expression \\(t(46) = 2.86\\), \\(p = 0.006\\) is the standard way to report a \\(t\\)-test according to the American Psychological Association. The first part of this report gives the \\(t\\) value, qualified by the degrees of freedom for the test in parentheses. We won’t focus much on the idea of degrees of freedom here, but for now it’s enough to know that this number quantifies the amount of information given by the data, in this case 48 datapoints minus the two means (one for each of the samples).\nLet’s compare \\(p\\) values and Bayes Factors (computed using the default setup in the BayesFactor R package). In table 6.2, the rows represent simulated experiments with varying total numbers of participants (N) and varying average treatment effects. Both \\(p\\) and BF go up with more participants and larger effects. In general, BFs tend to be a bit more conservative than \\(p\\)-values, such that \\(p&lt;0.05\\) can sometimes translate to a BF of less than 3 (Benjamin et al. 2018). For example, take a look at the row with 48 participants and an effect size of 1: the \\(p\\) value is less than 0.05, but the Bayes Factor is only 2.0.\nThe critical thing about \\(p\\)-values, though, is not just that they are a kind of data likelihoods. It is that they are used in a specific inferential procedure. The logic of NHST is that we make a binary decision about the presence of an effect. If \\(p &lt; 0.05\\), the null hypothesis is rejected; otherwise not. As Fisher (1949, 19) wrote,\n\nIt should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis.\n\nThe main problem with \\(p\\)-values from a scientific perspective is that researchers are usually interested in not just rejecting the null hypothesis but also in the evidence for the alternative (the one we are interested in). The Bayes Factor is one approach to quantifying positive evidence for the alternative hypothesis in a Bayesian framework. This issue with the Fisher approach to \\(p\\)-values has been known for a long time, though, and so there is an alternative frequentist approach as well.\n\n\n\n\n\n\n\nFigure 6.9: Standard decision matrix for the Neyman-Pearson approach to statistical inference.\n\n\n\n\n6.3.3 The Neyman-Pearson approach\nOne way to “patch” NHST is to introduce a decision-theoretic view, shown in figure 6.9.13 On this view, called the Neyman-Pearson view, there is a real \\(H_1\\), albeit one that is not specified. Then the true state of the world could be that \\(H_0\\) is true or \\(H_1\\) is true. The \\(p&lt;0.05\\) criterion is the threshold at which we are willing to reject the null, and so this constitutes our false positive rate \\(\\alpha\\). But we also need to define a false negative rate, which is conventionally called \\(\\beta\\).14\n13 A little bit of useful history here is given in Cohen (1990), and we also recommend Gigerenzer (1989) for a broader perspective.14 Unfortunately, \\(\\beta\\) is very commonly used for regression coefficients—and for that reason we’ve used it as our symbol for causal effects. We’ll be using these \\(\\beta\\)s in the next chapter as well. Those \\(\\beta\\)s are not to be confused with false negative rates. Sorry, this is just a place where statisticians have used the same Greek letter for two different things.15 To make really rational decisions, you could couple this chart to some kind of utility function that assessed the costs of different outcomes. For example, you might think it’s worse to proceed with an intervention that doesn’t work than to stay with business as usual. In that case, you’d assign a higher cost to a false positive and accordingly try to adopt a more conservative criterion. We won’t cover this kind of decision analysis here, but Pratt et al. (1995) is a classic textbook on statistical decision theory if you’re interested.Setting these rates is a decision problem: If you are too conservative in your criteria for the intervention having an effect, then you risk a false negative, where you incorrectly conclude that it doesn’t work. And if you’re too liberal in your assessment of the evidence, then you risk a false positive.15 In practice, however, people usually leave \\(\\alpha\\) at 0.05 and try to control the false negative rate by increasing their sample size.\nAs we saw in figure 6.6, the larger the sample, the better your chance of rejecting the null for any given non-null effect. But these chances will depend also on the effect size you are estimating. This formulation gives rise to the idea of classical power analysis, which we cover in chapter 10. Most folks who defend binary inference are interested in using the Neyman-Pearson approach. In our view, this approach has its place (it’s especially useful for power analysis) but it still suffers from the substantial issues that plague all binary inference techniques.\n\n\n\n\n\n\n\ndepth\n\n\n\n\n\nNonparametric resampling under the null\nHypothesis testing requires knowing the null distribution. In the examples above, it was easy to use statistical theory to work out the null distribution using knowledge of the binomial or normal distribution. But sometimes we don’t know what the null distribution would look like. What if the ratings data from our tea-tasting experiment were very skewed, such that there were many low ratings and a few very high ratings (as in figure 6.10)?\n\n\n\n\n\n\nFigure 6.10: A small tea-tasting experiment with a skewed distribution of ratings.\n\n\n\nWith skewed data like these, we couldn’t proceed with a \\(t\\)-test in good conscience because, with only \\(n = 18\\), we can’t necessarily trust that the central limit theorem has “kicked in” sufficiently for the test to work despite the skewness. Put another way, we can’t be sure that the null distribution is normal (Gaussian) in this case.\nAn alternative way to approximate a null distribution is through nonparametric resampling. Resampling means that we’re going to draw new samples from our existing sample, and nonparametric means that we will do this in a way that obviates assumptions about the shape of the null distribution—in contrast to parametric approaches that do rely on such assumptions). These techniques are sometimes called “bootstrapping” techniques.\nThe idea is, if the treatment truly had no effect on the outcome, then the observations would be exchangeable between the treatment and control groups. That is, there would not be systematic differences between the treatment and control groups. This property may or may not be true in our observed sample (after all, that’s why we’re doing a hypothesis test in the first place), but we can draw new samples from our existing sample in a manner that forces exchangeability.\nTo perform this kind of test with our tea-tasting data, we would randomly shuffle the ratings in our dataset while leaving the condition assignments fixed. If we did this thousands of times and computed the treatment effect in each case, the result would be a null distribution: what we might expect the treatment effect to look like if there was no condition effect. In essence, we’re using a simulated version of “random assignment” here to break the dependency between the condition manipulation and the observed data.\nWe can then compare our actual treatment effect to this nonparametric null distribution. If the actual treatment was smaller than the 2.5th percentile or larger than the 97.5th percentile in the null distribution, we would reject the null with \\(p &lt; 0.05\\), just the same as if we had used a \\(t\\)-test.\nResampling-based tests are extremely useful in a wide variety of cases. They can sometimes be less powerful than parametric approaches and they almost always require more computation, but their versatility makes them a great generic tool for data analysis.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#nonparametric-resampling-under-the-null",
    "href": "006-inference.html#nonparametric-resampling-under-the-null",
    "title": "6  Inference",
    "section": "Nonparametric resampling under the null",
    "text": "Nonparametric resampling under the null\nHypothesis testing requires knowing the null distribution. In the examples above, it was easy to use statistical theory to work out the null distribution using knowledge of the binomial or normal distribution. But sometimes we don’t know what the null distribution would look like. What if the ratings data from our tea-tasting experiment were very skewed, such that there were many low ratings and a few very high ratings (as in figure 6.10)?\n\n\n\n\n\n\nFigure 6.10: A small tea-tasting experiment with a skewed distribution of ratings.\n\n\n\nWith skewed data like these, we couldn’t proceed with a \\(t\\)-test in good conscience because, with only \\(n = 18\\), we can’t necessarily trust that the central limit theorem has “kicked in” sufficiently for the test to work despite the skewness. Put another way, we can’t be sure that the null distribution is normal (Gaussian) in this case.\nAn alternative way to approximate a null distribution is through nonparametric resampling. Resampling means that we’re going to draw new samples from our existing sample, and nonparametric means that we will do this in a way that obviates assumptions about the shape of the null distribution—in contrast to parametric approaches that do rely on such assumptions). These techniques are sometimes called “bootstrapping” techniques.\nThe idea is, if the treatment truly had no effect on the outcome, then the observations would be exchangeable between the treatment and control groups. That is, there would not be systematic differences between the treatment and control groups. This property may or may not be true in our observed sample (after all, that’s why we’re doing a hypothesis test in the first place), but we can draw new samples from our existing sample in a manner that forces exchangeability.\nTo perform this kind of test with our tea-tasting data, we would randomly shuffle the ratings in our dataset while leaving the condition assignments fixed. If we did this thousands of times and computed the treatment effect in each case, the result would be a null distribution: what we might expect the treatment effect to look like if there was no condition effect. In essence, we’re using a simulated version of “random assignment” here to break the dependency between the condition manipulation and the observed data.\nWe can then compare our actual treatment effect to this nonparametric null distribution. If the actual treatment was smaller than the 2.5th percentile or larger than the 97.5th percentile in the null distribution, we would reject the null with \\(p &lt; 0.05\\), just the same as if we had used a \\(t\\)-test.\nResampling-based tests are extremely useful in a wide variety of cases. They can sometimes be less powerful than parametric approaches and they almost always require more computation, but their versatility makes them a great generic tool for data analysis.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#inference-and-its-discontents",
    "href": "006-inference.html#inference-and-its-discontents",
    "title": "6  Inference",
    "section": "6.4 Inference and its discontents",
    "text": "6.4 Inference and its discontents\nIn earlier sections of this chapter, we reviewed NHST and Bayesian approaches to inference. Now it’s time to step back and think about some of the ways that inference practices—especially those related to NHST—have been problematic for psychology research. We’ll begin with some issues surrounding \\(p\\)-values and then give a specific accident report related to the process of “\\(p\\)-hacking” and some general philosophical discussion of how statistical testing relates to human reasoning.\n\n6.4.1 Problems with the interpretation of \\(p\\)-values\n\\(p\\)-values are basically likelihoods, in the sense we introduced in the previous chapter.16 They are the likelihood of the data under the null hypothesis! This likelihood is a critical number to know—for computing the Bayes Factor among other reasons. But it doesn’t tell us a lot of things that we might like to know!\n16 The only thing that is different is the idea that they are the likelihood of the observed data or any more extreme.For example, \\(p\\)-values don’t tell us the probability of the data under a specific alternative hypothesis that we might be interested in—that’s the posterior probability \\(p(H_1 | \\text{data})\\). When our tea-tasting \\(t\\)-test yielded \\(t(46) = 2.86\\), \\(p = 0.006\\), that \\(p\\) is not the probability of the null hypothesis being true! And it’s definitely not the probability of milk-first tea being better.\nWhat can you conclude when \\(p&gt;0.05\\)? According to the classical logic of NHST, the answer is “nothing”! A failure to reject the null hypothesis doesn’t give you any additional evidence for the null. Even if the probability of the data (or some more extreme data) under \\(H_0\\) is high, their probability might be just as high or higher under \\(H_1\\).17 But many practicing researchers make this mistake. Aczel et al. (2018) coded a sample of articles from 2015 and found that 72% of negative statements were inconsistent with the logic of their statistical paradigm of choice—most were cases where researchers said that an effect was not present when they had simply failed to reject the null.\n17 Of course, weighing these two against each other brings you back to the Bayes Factor.These are not the only issues with \\(p\\)-values. In fact, people have so much trouble understanding what \\(p\\)-values do say that there are whole articles written about these misconceptions. Table 6.3 shows a set of misconceptions documented and refuted by S. N. Goodman (2008).\nLet’s take a look at just a few. Misconception 1 is that, if \\(p=0.05\\), the null has a 5% chance of being true. This misconception is a result of confusing \\(p(H_0 | \\text{data})\\) (the posterior) and \\(p(\\text{data} | H_0)\\) (the likelihood—also known as the \\(p\\)-value). Misconception 2—that \\(p &gt; 0.05\\) allows us to accept the null—also stems from this reversal of posterior and likelihood. And misconception 3 is a misinterpretation of the \\(p\\)-value as an effect size (which we learned about in the last chapter): a large effect is likely to be clinically important, but with a large enough sample size, you can get a small \\(p\\)-value even for a very small effect. We won’t go through all the misconceptions here, but we encourage you to challenge yourself to work through them (as in the exercise below).\n\n\n\nTable 6.3: A “dirty dozen” \\(p\\)-value misconceptions. Adapted from S. N. Goodman (2008).\n\n\n\n\n\n\n\n\n\n\nMisconception\n\n\n\n\n1\n“If \\(p = 0.05\\), the null hypothesis has only a 5% chance of being true.”\n\n\n2\n“A nonsignificant difference (e.g., \\(p \\geq 0.05\\)) means there is no difference between groups.”\n\n\n3\n“A statistically significant finding is clinically important.”\n\n\n4\n“Studies with \\(p\\)-values on opposite sides of 0.05 are conflicting.”\n\n\n5\n“Studies with the same \\(p\\)-value provide the same evidence against the null hypothesis.”\n\n\n6\n“\\(p\\) = 0.05 means that we have observed data that would occur only 5% of the time under the null hypothesis.”\n\n\n7\n“\\(p\\) = 0.05 and \\(p \\leq 0.05\\) mean the same thing.”\n\n\n8\n“\\(p\\)-values are properly written as inequalities (e.g., ‘\\(p \\leq 0.02\\)’ when \\(p = .015\\)).”\n\n\n9\n“\\(p\\) = 0.05 means that if you reject the null hypothesis, the probability of a false positive error is only 5%.”\n\n\n10\n“With a \\(p\\) = 0.05 threshold for significance, the chance of a false positive error will be 5%.”\n\n\n11\n“You should use a one-sided \\(p\\)-value when you don’t care about a result in one direction, or a difference in that direction is impossible.”\n\n\n12\n“A scientific conclusion or treatment policy should be based on whether or not the \\(p\\) value is significant.”\n\n\n\n\n\n\nBeyond these misconceptions, there’s another problem. The \\(p\\)-value is a probability of a certain set of events happening (corresponding to the observed data or any “more extreme” data—that is to say, data further from the null). Since \\(p\\)-values are probabilities, we can combine them together across different events. If we run a “null experiment”—an experiment where the true effect is zero—the probability of a dataset with \\(p &lt; 0.05\\) is of course 0.05. But if we run two such experiments, we can get \\(p &lt; 0.05\\) with probability 0.1. By the time we run 20 experiments, we have an 0.64 chance of getting a positive result.\nIt would obviously be a major mistake to run 20 experiments and then report only the positive ones (which, by design, are false positives) as though these still were “statistically significant.” The same thing applies to doing 20 different statistical tests within a single experiment. There are many statistical corrections that can be made to adjust for this problem, which is known as the problem of multiple comparisons.18 But the the broader issue is one of transparency: unless you know what the appropriate set of experiments or tests is, it’s not possible to implement one of these corrections!19\n18 The simplest and most versatile one, the Bonferroni correction, just divides 0.05 (or technically, whatever your threshold is) by the number of comparisons you are making. Using that correction, if you do 20 null experiments, you would have a 3% chance of a false positive.19 This issue is especially problematic with \\(p\\)-values because they are so often presented as an independent set of tests, but the problem of multiple comparisons comes up when you compute a lot of independent Bayes Factors as well. “Posterior hacking” via selective reporting of Bayes Factors is perfectly possible (Simonsohn 2014).\n\n\n\n\n\naccident report\n\n\n\n\n\nDo extraordinary claims require extraordinary evidence?\nIn a blockbuster paper that may have inadvertently kicked off the replication crisis, Bem (2011) presented nine experiments he claimed provided evidence for precognition—that participants somehow had foreknowledge of the future. In the first of these experiments, Bem showed each of a group of 100 undergraduates 36 two-alternative forced choice trials in which they had to guess which of two locations on a screen would reveal a picture immediately before the picture was revealed. By chance, participants should choose the correct side 50% of the time of course. Bem found that, specifically for erotic pictures, participants’ guesses were 53.1% correct. This rate of guessing was unexpected under the null hypothesis of chance guessing (\\(p = 0.01\\)). Eight other studies with a total of more than 1,000 participants yielded apparently supportive evidence, with participants appearing to show a variety of psychological effects even before the stimuli were shown!\nBased on this evidence, should we conclude that precognition exists? Probably not. Wagenmakers et al. (2011) presented a critique of Bem’s findings, arguing that (1) Bem’s experiments were exploratory (not preregistered) in nature, (2) that Bem’s conclusions were a priori unlikely, and (3) that the level of statistical evidence from his experiments was quite low. We find each of these arguments alone compelling; together they present a knockdown case against Bem’s interpretation.\nFirst, we’ve already discussed the need to be skeptical about situations where experimenters have the opportunity for analytic flexibility in their choice of measures, manipulations, samples, and analyses. Flexibility leads to the possibility of cherry-picking those set of decisions from the “garden of forking paths” that lead to a positive outcome for the researcher’s favored hypothesis (for more details, see chapter 11). And there is plenty of flexibility on display even in experiment 1 of Bem’s paper. Although there were 100 participants in the study, they may have been combined post hoc from two distinct samples of 40 and 60, each of which saw different conditions. The 40 made guesses about the location of erotic, negative, and neutral pictures; the 60 saw erotic, positive non-romantic, and positive romantic pictures. The means of each of these conditions were presumably tested against chance (at least six comparisons, for a false positive rate of 0.26). Had positive romantic pictures been found significant, Bem certainly could have interpreted this finding the same way he interpreted the erotic ones.\nSecond, as we discussed, a \\(p\\)-value close to 0.05 does not necessarily provide strong evidence against the null hypothesis. Wagenmakers et al. computed the Bayes Factor for each of the experiments in Bem’s paper and found that, in many cases, the amount of evidence for \\(H_1\\) was quite modest under a default Bayesian \\(t\\)-test. Experiment 1 was no exception: the BF was 1.64, giving only “anecdotal” support for the hypothesis of some nonzero effect, even before the multiple-comparisons problem mentioned above.\nFinally, since precognition is not supported by any prior compelling scientific evidence (despite many attempts to obtain such evidence) and defies well-established physical laws, perhaps we should assign a low prior probability to Bem’s \\(H_1\\), a nonzero precognition effect. Taking a strong Bayesian position, Wagenmakers et al. suggest that we might do well to adopt a prior reflecting how unlikely precognition is, say \\(p(H_1) = 10^{-20}\\). And if we adopt this prior, even a very well-designed, highly informative experiment (with a Bayes factor conveying substantial or even decisive evidence) would still lead to a very low posterior probability of precognition.\nWagenmakers et al. concluded that, rather than supporting precognition, the conclusion from Bem’s paper should be psychologists should revise how they think about analyzing their data (and avoid \\(p\\)-hacking)!\n\n\n\n\n\n6.4.2 Philosophical (and empirical) views of probability\nUp until now, we’ve presented Bayesian and frequentist tools as two different sets of computations. But, in fact, these different tools derive from fundamentally different philosophical perspectives on what a probability even is. Very roughly, frequentist approaches tend to believe that probabilities quantify the long-run frequencies of certain events. So, if we say that some outcome of an event has probability 0.5, we’re saying that if that event happened thousands of times, the long-run frequency of the outcome would be 50% of the total events. In contrast, the Bayesian viewpoint doesn’t depend on this sense that events could be exactly repeated. Instead, the subjective Bayesian interpretation of probability is that it quantifies a person’s degree of belief in a particular outcome.20\n20 This is really a very rough description. If you’re interested in learning more about this philosophical background, we recommend the Stanford Encyclopedia of Philosophy entry, “Interpretations of Probability” (https://plato.stanford.edu/entries/probability-interpret).You don’t have to take sides in this deep philosophical debate about what probability is. But it’s helpful to know that people actually seem to reason about the world in ways that are well described by the subjective Bayesian view of probability. Recent cognitive science research has made a lot of headway in describing reasoning as a process of Bayesian inference where probabilities describe degrees of belief in different hypotheses (for a textbook review of this approach, see N. D. Goodman, Tenenbaum, and The ProbMods Contributors 2016). These hypotheses in turn are a lot like the theories we described in chapter 2: they describe the relationships between different abstract entities (Tenenbaum et al. 2011). You might think that scientists are different from laypeople in this regard, but one of the striking findings from research on probabilistic reasoning and judgment is that expertise doesn’t matter that much. Statistically trained scientists—and even statisticians—make many of the same reasoning mistakes as their untrained students (Kahneman and Tversky 1979). Even children seem to reason intuitively in a way that looks a bit like Bayesian inference (Gopnik 2012).\nThese cognitive science findings help to explain some of the problems that people (scientists included) have in reasoning about \\(p\\)-values. If you are an intuitively Bayesian reasoner, the quantity that you’re probably tracking is how much you believe in your hypothesis (its posterior probability). So, many people treat the \\(p\\)-value as the posterior probability of the null hypothesis.21 That’s exactly what fallacy 1 in table 6.3 states—“If \\(p = 0.05\\), the null hypothesis has only a 5% chance of being true.” But this equivalence is incorrect! Written in math, \\(p(\\text{data} | H_0)\\) (the likelihood that lets us compute the p-value) is not the same thing as \\(p(H_0 | \\text{data})\\) (the posterior that we want). Pulling from our accident report above, even if the probability of the observed ESP data given the null hypothesis is low, that doesn’t mean that the probability of ESP is high.\n21 Cohen (1994) is a great treatment of this issue.\n\n6.4.3 What framework to use?\nThe problem with binary inferences is that they enable behaviors that can introduce bias into the scientific ecosystem. By the logic of statistical significance, either an experiment “worked” or it didn’t. Because everyone would usually rather have an experiment that worked than one that didn’t, inference criteria like \\(p\\)-values often become a target for selection, as we discussed in chapter 3.22\n22 More generally, this pattern is probably an example of Goodhart’s law, which states that when a measure becomes a target, it ceases to be a good measure (Strathern 1997). Once the outcomes of statistical inference procedures become targets for publication, they are subject to selection biases—\\(p\\)-hacking, for example—that make them less meaningful.If you want to quantify evidence for or against a hypothesis, it’s worth considering whether Bayes Factors address your question better than \\(p\\)-values. In practice, \\(p\\)-values are hard to understand and many people misuse them—though to be fair, BFs are misused plenty too. These issues may be rooted in basic facts about how human beings reason about probability.\nDespite the reasons to be worried about \\(p\\)-values, for many practicing scientists (at least at time of writing), there is no one right answer about whether to use them or not. Even if we’d like to be Bayesian all the time, there are a number of obstacles. First, though new computational tools make fitting Bayesian models and extracting Bayes Factors much easier than before, it’s still on average quite a bit harder to fit a Bayesian model than it is a frequentist one. Second, because Bayesian analyses are less familiar, it may be an uphill battle to convince advisors, reviewers, and funders to use them.\nAs a group of authors, some of us are more Bayesian than frequentist, while others are more frequentist than Bayesian—but all of us recognize the need to move between statistical paradigms depending on the problem we’re working on. Furthermore, a lot of the time we’re not so worried about which paradigm we’re using. The paradigms are at their most divergent when making binary inferences, and they often look much more similar when they are used in the context of quantifying measurement precision.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#do-extraordinary-claims-require-extraordinary-evidence",
    "href": "006-inference.html#do-extraordinary-claims-require-extraordinary-evidence",
    "title": "6  Inference",
    "section": "Do extraordinary claims require extraordinary evidence?",
    "text": "Do extraordinary claims require extraordinary evidence?\nIn a blockbuster paper that may have inadvertently kicked off the replication crisis, Bem (2011) presented nine experiments he claimed provided evidence for precognition—that participants somehow had foreknowledge of the future. In the first of these experiments, Bem showed each of a group of 100 undergraduates 36 two-alternative forced choice trials in which they had to guess which of two locations on a screen would reveal a picture immediately before the picture was revealed. By chance, participants should choose the correct side 50% of the time of course. Bem found that, specifically for erotic pictures, participants’ guesses were 53.1% correct. This rate of guessing was unexpected under the null hypothesis of chance guessing (\\(p = 0.01\\)). Eight other studies with a total of more than 1,000 participants yielded apparently supportive evidence, with participants appearing to show a variety of psychological effects even before the stimuli were shown!\nBased on this evidence, should we conclude that precognition exists? Probably not. Wagenmakers et al. (2011) presented a critique of Bem’s findings, arguing that (1) Bem’s experiments were exploratory (not preregistered) in nature, (2) that Bem’s conclusions were a priori unlikely, and (3) that the level of statistical evidence from his experiments was quite low. We find each of these arguments alone compelling; together they present a knockdown case against Bem’s interpretation.\nFirst, we’ve already discussed the need to be skeptical about situations where experimenters have the opportunity for analytic flexibility in their choice of measures, manipulations, samples, and analyses. Flexibility leads to the possibility of cherry-picking those set of decisions from the “garden of forking paths” that lead to a positive outcome for the researcher’s favored hypothesis (for more details, see chapter 11). And there is plenty of flexibility on display even in experiment 1 of Bem’s paper. Although there were 100 participants in the study, they may have been combined post hoc from two distinct samples of 40 and 60, each of which saw different conditions. The 40 made guesses about the location of erotic, negative, and neutral pictures; the 60 saw erotic, positive non-romantic, and positive romantic pictures. The means of each of these conditions were presumably tested against chance (at least six comparisons, for a false positive rate of 0.26). Had positive romantic pictures been found significant, Bem certainly could have interpreted this finding the same way he interpreted the erotic ones.\nSecond, as we discussed, a \\(p\\)-value close to 0.05 does not necessarily provide strong evidence against the null hypothesis. Wagenmakers et al. computed the Bayes Factor for each of the experiments in Bem’s paper and found that, in many cases, the amount of evidence for \\(H_1\\) was quite modest under a default Bayesian \\(t\\)-test. Experiment 1 was no exception: the BF was 1.64, giving only “anecdotal” support for the hypothesis of some nonzero effect, even before the multiple-comparisons problem mentioned above.\nFinally, since precognition is not supported by any prior compelling scientific evidence (despite many attempts to obtain such evidence) and defies well-established physical laws, perhaps we should assign a low prior probability to Bem’s \\(H_1\\), a nonzero precognition effect. Taking a strong Bayesian position, Wagenmakers et al. suggest that we might do well to adopt a prior reflecting how unlikely precognition is, say \\(p(H_1) = 10^{-20}\\). And if we adopt this prior, even a very well-designed, highly informative experiment (with a Bayes factor conveying substantial or even decisive evidence) would still lead to a very low posterior probability of precognition.\nWagenmakers et al. concluded that, rather than supporting precognition, the conclusion from Bem’s paper should be psychologists should revise how they think about analyzing their data (and avoid \\(p\\)-hacking)!",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#computing-precision",
    "href": "006-inference.html#computing-precision",
    "title": "6  Inference",
    "section": "6.5 Computing precision",
    "text": "6.5 Computing precision\nOur last section presented an argument against using \\(p\\)-values for making dichotomous inferences. But we still want to move from what we know about our own limited sample to some inference about the population. How should we do this?\n\n6.5.1 Confidence intervals\nOne alternative to binary hypothesis testing is to ask about the precision of our estiamates, in particular how similar an estimate from a particular sample is to the population parameter of interest. For example, how close is our tea-tasting effect estimate to the true effect in the population? We don’t know what the true effect is, but our knowledge of sampling distributions lets us make some guesses about how precise our estimate is.\nThe confidence interval is a convenient frequentist way to summarize the variability of the sampling distribution—and hence how precise our point estimate is. The confidence interval represents the range of possible values for the parameter of interest that are plausible given the data. More formally, a 95% confidence interval for some estimate (call it \\(\\widehat{\\beta}\\), as in our example) is defined as a range of possible values for \\(\\beta\\) such that, if we did repeated sampling, 95% of the intervals generated by those samples would contain the true parameter, \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals (CIs) are constructed by estimating the middle 95% of the sampling distribution of \\(\\widehat{\\beta}\\). Because of our hero, the central limit theorem, we can treat the sampling distribution as normal for reasonably large samples. Given this, it’s common to construct a 95% confidence interval \\(\\widehat{\\beta} \\pm 1.96 \\; \\widehat{SE}\\).23 If we were to conduct the experiment 100 times and calculate a confidence interval each time, we should expect 95 of the intervals to contain the true \\(\\beta\\), whereas we would expect the remaining 5 to not contain \\(\\beta\\).24\n23 This type of CI is called a “Wald” confidence interval.24 In case you don’t have enough tea to do the experiment 100 times to confirm this, you can do it virtually using this nice simulation tool: https://istats.shinyapps.io/ExploreCoverage.Confidence intervals are like betting on the inferences drawn from your sample. The sample you drew is like one pull of a slot machine that will pay off (i.e., have the confidence interval contain the true parameter) 95% of the time. Put more concisely: 95% of 95% confidence intervals contain the true value of the population parameter.\n\n\n\n\n\n\nFigure 6.11: Confidence intervals on each of the two condition estimates, as well as on the difference between conditions.\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nComputing confidence intervals analytically is pretty easy. Here we first compute the standard error for the difference between conditions. The only tricky bit here is that we need to compute a pooled standard deviation.\n\ntea_ratings &lt;- filter(tea_data, condition == \"tea first\")$rating\nmilk_ratings &lt;- filter(tea_data, condition == \"milk first\")$rating\n\nn_tea &lt;- length(tea_ratings)\nn_milk &lt;- length(milk_ratings)\nsd_tea &lt;- sd(tea_ratings)\nsd_milk &lt;- sd(milk_ratings)\n\ntea_sd_pooled &lt;- sqrt(((n_tea - 1) * sd_tea ^ 2 + (n_milk - 1) * sd_milk ^ 2) / \n                        (n_tea + n_milk - 2))\n\ntea_se &lt;- tea_sd_pooled * sqrt((1 / n_tea) + (1 / n_milk))\n\nOnce we have the standard error, we can get the estimated difference between conditions and compute the confidence intervals by multiplying the standard error by 1.96.\n\ndelta_hat &lt;- mean(milk_ratings) - mean(tea_ratings)\ntea_ci_lower &lt;- delta_hat - tea_se * qnorm(0.975)\ntea_ci_upper &lt;- delta_hat + tea_se * qnorm(0.975)\n\n\n\n\nFor visualization purposes, we can show the confidence intervals on individual estimates (left side of figure 6.11). These tell us about the precision of our estimates of each quantity relative to the population estimate. But we’ve been talking primarily about the CI on the treatment effect \\(\\widehat{\\beta}\\) (right side of figure 6.11). This CI allows us to make an inference about whether or not it overlaps with zero—which is actually equivalent in this case to whether or not the \\(t\\)-test is statistically significant.\n\n\n6.5.2 Confidence in confidence intervals?\nConfidence intervals are often misinterpreted by students and researchers alike (Hoekstra et al. 2014). Imagine a researcher conducts an experiment and reports that “the 95% confidence interval for the mean ranges from 0.1 to 0.4.” All of the statements in table 6.4, though tempting to make about this situation, are technically false.\n\n\n\nTable 6.4: Confidence interval misconceptions for a confidence interval [0.1,0.4]. Adapted from Hoekstra et al. (2014).\n\n\n\n\n\n\n\n\n\n\nMisconception\n\n\n\n\n1\n“The probability that the true mean is greater than 0 is at least 95%.”\n\n\n2\n“The probability that the true mean equals 0 is smaller than 5%.”\n\n\n3\n“The ‘null hypothesis’ that the true mean equals 0 is likely to be incorrect.”\n\n\n4\n“There is a 95% probability that the true mean lies between 0.1 and 0.4.”\n\n\n5\n“We can be 95% confident that the true mean lies between 0.1 and 0.4.”\n\n\n6\n“If we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.4.”\n\n\n\n\n\n\nThe problem with all of these statements is that, in the frequentist framework, there is only one true value of the population parameter, and the variability captured in confidence intervals is about the samples, not the parameter itself.25 For this reason, we can’t make any statements about the probability of the value of the parameter or of our confidence in specific numbers. To reiterate, what we can say is: if we were to repeat the procedure of conducting the experiment and calculating a confidence interval many times, in the long run, 95% of those confidence intervals would contain the true parameter.\n25 In contrast, Bayesians think of parameters themselves as variable rather than fixed.The Bayesian analog to a confidence interval is a credible interval. Recall that for Bayesians, parameters themselves are considered probabilistic (i.e., subject to random variation), not fixed. A 95% credible interval for an estimate, \\(\\widehat{\\beta}\\), represents a range of possible values for \\(\\beta\\) such that there is a 95% probability that \\(\\beta\\) falls inside the interval. Because we are now wearing our Bayesian hats, we are “allowed” to talk about \\(\\beta\\) as if it were probabilistic rather than fixed. In practice, credible intervals are constructed by finding the posterior distribution of \\(\\beta\\), as in chapter 5, and then taking the middle 95%, for example.\nCredible intervals are nice because they don’t give rise to many of the inference fallacies surrounding confidence intervals. They actually do represent our beliefs about where \\(\\beta\\) is likely to be, for example. Despite the technical differences between credible intervals and confidence intervals, in practice—with larger sample sizes and weaker priors—they turn out to be quite similar to each other in many cases.26\n26 They can diverge sharply in cases with less data or stronger priors (Morey et al. 2016), but in our experience this is relatively rare.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "006-inference.html#chapter-summary-inference",
    "href": "006-inference.html#chapter-summary-inference",
    "title": "6  Inference",
    "section": "6.6 Chapter summary: Inference",
    "text": "6.6 Chapter summary: Inference\nInference tools help you move from characteristics of the sample to characteristics of the population. This move is a critical part of generalization from research data. But we hope we’ve convinced you that inference doesn’t have to mean making a binary decision about the presence or absence of an effect. A strategy that seeks to estimate an effect and its associated precision is often much more helpful as a building block for theory. As we move toward estimating causal effects in more complex experimental designs, the process will require more sophisticated models. Toward that goal, the next chapter provides some guidance for how to build such models.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nCan you write the definition of a \\(p\\)-value and a Bayes Factor without looking them up? Try this out—what parts of the definitions did you get wrong?\nTake three of Goodman’s (2008) “dirty dozen” in table 6.3) and write a description of why each is a misconception. (These can be checked against the original article, which gives a nice discussion of each.)\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nMany of the concepts described here are illustrated beautifully via interactive visualizations. We recommend https://seeing-theory.brown.edu for a broad overview of statistical concepts and https://rpsychologist.com/viz for a number of interactives that specifically illustrate concepts discussed in this chapter and the previous one, including \\(p\\)-values, effect sizes, maximum likelihood estimation, confidence intervals, and Bayesian inference.\nA fun, polemical critique of NHST: Cohen, Jacob (1994). “The Earth is Round (\\(p\\) &lt; .05).” American Psychologist 49 (12):997–1003. https://doi.org/10.1037/0003-066X.49.12.997.\nA nice introduction to Bayesian data analysis: Kruschke, John K., and Torrin M. Liddell (2018a). “Bayesian Data Analysis for Newcomers.” Psychonomic Bulletin & Review 25 (1): 155–-77. https://doi.org/10.3758/s13423-017-1272-1.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "007-models.html",
    "href": "007-models.html",
    "title": "7  Models",
    "section": "",
    "text": "7.1 Regression models\nThere are many types of statistical models, but this chapter will focus primarily on regression, a broad and extremely flexible class of models. A regression model relates a dependent variable to one or more independent variables. Dependent variables are sometimes called outcome variables, and independent variables are sometimes called predictor variables, covariates, or features.3 We will see that many common statistical estimators (like the sample mean) and methods of inference (like the \\(t\\)-test) are actually simple regression models. Understanding this point will help you see many statistical methods as special cases of the same underlying framework, rather than as unrelated, ad hoc tests.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#regression-models",
    "href": "007-models.html#regression-models",
    "title": "7  Models",
    "section": "",
    "text": "3 The reverse is not true—not every predictor or covariate is an independent variable! One of the tricky things about relating regression models to causal hypotheses is that just because something is on the right side of a regression equation doesn’t mean it’s a causal manipulation. And of course, just because you’ve got an estimate of some predictor in a regression, that doesn’t mean the estimate tells you about the magnitude of the causal effect. It could, but it also might not!\n7.1.1 Regression for estimating a simple treatment effect\nLet’s start with one of these special cases, namely estimating a treatment effect, \\(\\beta\\), in a two-group design. In chapter 5, we solved this exact challenge for the tea-tasting experiment. We applied a classical model in which the milk-first ratings are assumed to be normally distributed with mean \\(\\theta_{\\text{M}} = \\theta_{\\text{T}} + \\beta\\) and standard deviation \\(\\sigma\\).4\n4 Here’s a quick reminder that “model” here is a way of saying “set of assumptions about the data generating procedure.” So saying that some equation is a “model” is the same as saying that we think this is where the data came from. We can “turn the crank”—that is, generate data through the process that’s specified in those equations, such as by pulling numbers from a normal distribution with mean \\(\\theta_{\\text{T}} + \\beta\\) and standard deviation \\(\\sigma\\). In essence, we’re committing to the idea that this process will give us data that are substantively similar to the ones we have already.5 Using 0 and 1 is known as dummy coding and allows us to interpret the parameter as the difference of the treatment group (tea-first) from the baseline (milk-first). There are many other ways to code categorical variables, with other interpretations. As a practical tip, be careful to check how your variables are coded before reporting anything!Let’s now write that model as a regression model—that is, as a model that predicts each participant’s tea rating, \\(Y_i\\), given that participant’s treatment assignment, \\(X_i\\). \\(X_i=0\\) represents the control (milk-first) group and \\(X_i=1\\) represents the treatment (tea-first) group.5 Here, \\(Y_i\\) is the dependent variable and \\(X_i\\) is the independent variable. The subscripts \\(i\\) index the participants. To make this concrete, you can see some sample tea-tasting data (the first three observations from each condition) below (table 7.1), with the index \\(i\\), the condition and its predictor \\(X_i\\), and the rating \\(Y\\).\nLet’s write this model more formally as a linear regression of Y on X. Conventionally, regression models are written with \\(\\beta\\) symbols for all parameters, so we’ll now use \\(\\beta_0 = \\theta_M\\) for the mean in the milk-first group and \\(\\beta_1 = \\theta_T - \\theta_M\\) as the average difference between the tea-first and milk-first groups. This \\(\\beta\\) is a generalization of the one we were using to denote the causal effect above and in the previous two chapters.\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\n\n\n\n\n\n\nTable 7.1: Example tea tasting data.\n\n\n\n\n\n\n\nid\ncondition\nX\nrating (Y)\n\n\n\n\n1\nmilk first\n0\n6\n\n\n2\nmilk first\n0\n4\n\n\n3\nmilk first\n0\n5\n\n\n4\ntea first\n1\n1\n\n\n5\ntea first\n1\n3\n\n\n6\ntea first\n1\n5\n\n\n\n\n\n\n\n\n\nThe term \\(\\beta_0 + \\beta_1 X_i\\) is called the linear predictor, and it describes the expected value of an individual’s tea rating, \\(Y_i\\), given that participant’s treatment group \\(X_i\\) (the single independent variable in this model). That is, for a participant in the control group (\\(X_i=0\\)), the linear predictor is just equal to \\(\\beta_0\\), which is indeed the mean for the control group that we specified above. On the other hand, for a participant in the treatment group, the linear predictor is equal to \\(\\beta_0 + \\beta_1\\), which is the mean for the treatment group that we specified. In regression jargon, \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients, where \\(\\beta_1\\) represents the association of the independent variable \\(X\\) with the outcome \\(Y\\).\nThe term \\(\\epsilon_i\\) is the error term, referring to random variation of participants’ ratings around the group mean.6 Note that this is a very specific kind of “error”; it does not include “error” due to bias, for example. Instead, you can think of the error terms as capturing the “error” that would be associated with predicting any given participant’s rating based on just the linear predictor. If you predicted a control group participant’s rating as \\(\\beta_0\\), that would be a good guess—but you still expect the participant’s rating to deviate somewhat from \\(\\beta_0\\) (i.e., due to variability across participants beyond what is captured by their treatment groups). In our regression model, the linear predictor and error terms together say that participants’ ratings scatter randomly (in fact, normally) around their group means with standard deviation \\(\\sigma\\). And that is exactly the same model we posited in chapter 5.7\n6 Formally, we’d write \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\). The tilde means “is distributed as,” and what follows is a normal distribution with mean 0 and variance \\(\\sigma^2\\).7 You may be wondering why so much effort was put into building boutique solutions for these special cases when a unified framework was available the whole time. A partial answer is that the classical infrastructure of statistics was developed before computers were widespread, and these special cases were chosen because they were easy to work with “analytically” (meaning to work out all the math by hand, using values from big numerical tables). Now that we have computers with more flexible algorithms, the model-based perspective is more practical and accessible than it used to be.Now we have the model. But how do we estimate the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\)? The usual method is called ordinary least squares (OLS). Here’s the basic idea. For any given regression coefficient estimates \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\), we would obtain different predicted values, \\(\\widehat{Y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_i\\), for each participant. Some regression coefficient estimates will yield better predictions than others. Ordinary least squares estimation is designed to find the values of the regression coefficients that optimize these predictions, meaning that the predictions are as close as possible to participants’ true outcomes, \\(Y_i\\).\n\n\n\n\n\n\n\n\nFigure 7.1: (left) Best-fitting regression coefficients for the tea-tasting experiment. (right) Much worse coefficients for the same data. Dotted lines: residuals. Circles: data points for individual participants.\n\n\n\n\n\nFigure 7.1 illustrates the tea-tasting data for each condition (the dots) along with the model predictions for each condition \\(\\beta_0\\) and \\(\\beta_0 + \\beta_1\\) (blue lines). The gap between each data point and the corresponding predictions (the thing that OLS wants to minimize) is shown by the dotted lines.8 These distances are sample estimates, called residuals, of the true errors (\\(\\epsilon_i\\). The left-hand plot shows the OLS coefficient values—the ones that move the model’s predictions as close as possible to the data points, in the sense of minimizing the total squared length of the dashed lines. The right-hand plot shows a substantially worse set of coefficient values.\n8 Ordinary least squares minimizes squared error loss, in the sense that it will choose the regression coefficient estimates whose predictions minimize \\(\\sum_{i=1}^n \\left( Y_i - \\widehat{Y}_i\\right)^2\\), where \\(n\\) is the sample size. A wonderful thing about OLS is that those optimal regression coefficients (generically termed \\(\\widehat{\\mathbf{\\beta}}\\)) turn out to have a very simple closed-form solution: \\(\\widehat{\\mathbf{\\beta}} = \\left( \\mathbf{X}'\\mathbf{X} \\right)^{-1} \\mathbf{X}'\\mathbf{y}\\). We are using more general notation here that supports multiple independent variables: \\(\\widehat{\\mathbf{\\beta}}\\) is a vector, \\(\\mathbf{X}\\) is a matrix of independent variables for each subject, and \\(\\mathbf{y}\\) is a vector of participants’ outcomes. As more good news, the standard error for \\(\\widehat{\\mathbf{\\beta}}\\) has a similarly simple closed form!You’ll notice that we aren’t talking much about \\(p\\)-values in this chapter. Regression models can be used to produce \\(p\\)-values for specific coefficients, representing inferences about the likelihood of the observed data under some null hypothesis regarding the coefficients. You can also compute Bayes Factors for specific regression coefficients, or use Bayesian inference to fit these coefficients under some prior expectation about their distribution. We won’t talk much about this, or more generally how to fit the models we describe. As we said, we’re not going to give a full treatment of all the relevant statistical topics. Instead we want to help you begin thinking about making models of your data.\n\n\n\n\n\n\ncode\n\n\n\n\n\nAs it turns out, fitting an OLS regression model in R is extremely easy. The underlying function is lm(), which stands for linear model. You can fit the model with a single call to this function with a “formula” as its argument. Here’s the call:\n\nmod &lt;- lm(rating ~ condition, data = tea_data)\n\nFormulas in R are a special kind of terse notation for regression equations where you write the outcome, ~ (distributed as), and the predictors. R assumes that you want an intercept by default, and there are also a number of other handy defaults that make R formulas a nice easy way to specify relatively complex regression models, as we’ll see below.\nOnce you’ve fit the model and assigned it to a variable, you can call summary() to see a summary of the parameters of the model:\n\nsummary(mod)\n\nYou can also extract the coefficient values using coef(mod) and put them in a handy dataframe using tidy(mod) from the broom package (Robinson, Hayes, and Couch 2023).\n\n\n\n\n\n7.1.2 Adding predictors\nThe regression model we just wrote down is the same model that underlies the \\(t\\)-test from chapter 6. But the beauty of regression modeling is that much more complex estimation problems can also be written as regression models that extend the model we made above. For example, we might want to add another predictor variable, such as the age of the participant.9\n9 The ability to estimate multiple coefficients at once is a huge strength of regression modeling, so much so that sometimes people use the label multiple regression to denote that there is more than one predictor + coefficient pair.Let’s add this new independent variable and a corresponding regression coefficient to our model: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}  + \\epsilon_i\n\\] Now that we have multiple independent variables, we’ve labeled them \\(X_{1}\\) (treatment group) and \\(X_{2}\\) (age) for clarity.\nTo illustrate how to interpret the regression coefficients in this model, let’s use the linear predictor to compare the model’s predicted tea ratings for two hypothetical participants who are both in the treatment group: 20-year-old Alice and 21-year-old Bob. Alice’s linear predictor tells us that her expected rating is \\(\\beta_0 + \\beta_1 + \\beta_2 \\cdot 20\\). In contrast, Bob’s linear predictor is \\(\\beta_0 + \\beta_1 + \\beta_2 \\cdot 21\\). We could therefore calculate the expected difference in ratings for 21-year-olds versus 20-year-olds by subtracting Alice’s linear predictor from Bob’s, yielding just \\(\\beta_2\\).\nWe would get the same result if Alice and Bob were instead 50 and 51 years old, respectively. This equivalence illustrates a key point about linear regression models in general:\n\nThe regression coefficient represents the expected difference in outcome when comparing any two participants who differ by 1 unit of the relevant independent variable, and who do not differ on any other independent variables in the model.\n\nHere, the coefficient compares participants who differ by one year of age. In “Practical modeling considerations” below, we discuss whether and when to “control for” additional variables (i.e., when to add them to your model).\n\n\n7.1.3 Interactions\nIn our running example, we now have two predictors: condition and age. But what if the effect of condition varies depending on the age of the participant? This situation would correspond to a case where (say) older people were more sensitive to tea ordering, perhaps because of their greater tea experience. We call this an interaction effect: the effect of one predictor depends on the state of another.\nInteraction effects are easily accommodated in our modeling framework. We simply add a term to our model that is the product of condition (\\(X_1\\)) and age (\\(X_2\\)), and weight this product by another beta, which represents the strength of this interaction: \\[\nY_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2}  + \\beta_3 X_{i1} X_{i2}  + \\epsilon_i\n\\] Statistical interactions are a very powerful modeling tool that can help us understand the relationship between different experimental manipulations or between manipulationes and covariates (such as age). We discuss their role in experimental design—as well as some of the interpretive challenges that they pose—in much more detail in chapter 9.10\n10 We won’t go into this topic here, but we do want to provide a pointer to one of the most persistent challenges that comes up when you specify regression models with categorical predictors—and especially their interactions: how you “code” these categorical predictors. Above we created a “dummy” variable \\(X\\) that encoded milk-first tea as 0 and tea-first tea as 1. Dummy variables are very easy to think about, but in models with interactions, they can cause some problems. Because the interaction in our example model is a product of the dummy-coded condition variable and age, the interaction term \\(\\beta_3\\) is interpreted as the effect of age for the tea-first condition (\\(X=1\\)) and hence the effect of age \\(\\beta_2\\) is actually the effect of age for the milk-first condition. The way to deal with this issue is to use a different coding system, such as contrast coding. Davis (2010) gives a good tutorial on this tricky topic.\n\n7.1.4 When does linear regression work?\nLinear regression modeling with OLS is an incredibly powerful technique for creating models to estimate the influence of multiple predictors on a single dependent variable. In fact, OLS is in a mathematical sense the best way to fit a linear model!11 But OLS only “works”—in the sense of yielding good estimates—if three big conditions are met.\n11 There is a precise sense in which OLS gives the very best predictions we could ever get from any model that posits linear relationships between the independent variables and the outcome. That is, you can come up with any other linear, unbiased model you want, and yet if the assumptions of OLS are fulfilled, predictions from OLS will always be less noisy than those of your model. This is because of an elegant mathematical result called the Gauss-Markov Theorem.\nThe relationship between the predictor and outcome must be linear. In our comparison of Alice’s and Bob’s expected outcomes based on their one-year age difference, we were able to interpret the coefficient \\(\\beta_2\\) as the average difference in \\(Y_i\\) when comparing participants who differ by one year of age, regardless of whether those ages are 20 vs 21 or 50 vs 51. If we believed this relationship was nonlinear, then we could transform our predictor—for example, including a quadratic effect of age by adding a \\(\\beta_3 * X_2^2\\) term. The relationship between this new predictor and the outcome would still be linear, however. It is always a good idea to use visualizations like scatter plots to look for possible problems with assuming a linear relationship between a predictor and your outcome.\n\n\n\nErrors must be independent. In our example, observations in the regression model (i.e., rows in the dataset) were sampled independently: each participant was recruited independently to the study and each performed a single trial. On the other hand, suppose we have repeated-measures data in which we sample participants and then obtain multiple measurements for each participant. Within each participant, measurements would likely be correlated (perhaps because participants differ on their general level of tea enjoyment). This correlation can invalidate inferences from a model that does not accommodate the correlation. We’ll discuss this problem in detail below.\n\n\n\n\n\nErrors must be normally distributed and unrelated to the predictor. Imagine older people have very consistent tea-ordering preferences while younger people do not. In that case, the models’ error term would be less variable for older participants than younger ones. This issue is called heteroskedasticity. It is a good idea to plot each independent variable versus the residuals to see if the residuals are more variable for certain values of the independent variable than for others.\n\n\n\nIf any of these three conditions are violated, it can undermine the estimates and inferences you draw from your model.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#generalized-linear-models",
    "href": "007-models.html#generalized-linear-models",
    "title": "7  Models",
    "section": "7.2 Generalized linear models",
    "text": "7.2 Generalized linear models\nSo far we have considered continuous outcome measures, like tea ratings. What if we instead had a binary outcome, such as whether a participant liked or didn’t like the tea, or a count outcome, such as the number of cups a participant chose to drink? These and other noncontinuous outcomes often violate the assumptions of OLS, in particular because they often induce heteroskedastic errors.\nBinary outcomes inherently produce heteroskedastic errors because the variance of a binary variable depends directly on the outcome probability. Errors will be more variable when the outcome probability is closer to 0.50, and much less variable for when the probability is closer to 0 or 1.12 This heteroskedasticity in turn means that inferences from the model (e.g., \\(p\\)-values) can be incorrect, sometimes just a little bit off but sometimes dramatically incorrect.13\n12 Specifically, the variance of a binary variable with probability \\(p\\) is simply \\(p(1-p)\\), which is largest when \\(p=0.50\\).13 Ordinary least squares can also be used with binary outcomes, in which case the coefficients represent differences in probabilities. However, the usual model-based standard errors will be incorrect.Happily, generalized linear models (GLMs) are regression models closely related to OLS that can handle noncontinuous outcomes. These models are called “generalized” because OLS is one of many members of this large class of models. To see the connection, let’s first write an OLS model more generally in terms of what it says about the expected value of the outcome, which we notate as \\(E[Y_i]\\): \\[\nE[Y_i] = \\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij}\n\\] where \\(p\\) is the number of independent variables, \\(\\beta_0\\) is the intercept, and \\(\\beta_j\\) is the regression coefficient for the \\(j^{th}\\) independent variable. This equation is just a math-y way of saying that you predict from a regression model by adding up each of the predictors’ contributions to the expected outcome (\\(\\beta_j X_{ij}\\)).\nThe linear predictor of a GLM (i.e., \\(\\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij}\\)) looks exactly the same as for OLS, but instead of modeling \\(E[Y_i]\\), a GLM models some transformation, \\(g(.)\\), of the expectation: \\[\ng( E[Y_i] ) = \\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij}\n\\] GLMs involve transforming the expectation of the outcome, not the outcome itself! That is, in GLMs, we are not just taking the outcome variable in our dataset and transforming it before fitting an OLS model, but rather we are fitting a different model entirely, one that posits a fundamentally different relationship between the predictors and the expected outcomes. This transformation is called the link function. In other words, to fit different kinds of outcomes, all we need to do is construct a standard linear model and then just transform its output via the appropriate link function.\nPerhaps the most common link function is the logit link, which is suitable for binary data. This link function looks like this, where \\(w\\) is any probability that is strictly between 0 and 1: \\[g(w) = \\log \\left( \\frac{w}{1 - w} \\right)\\] The term \\(w / (1 - w)\\) is called the odds and represents the probability of an event occurring divided by the probability of its not occurring. The resulting model is called logistic regression and looks like: \\[\n\\text{logit}( E[Y_{it}] ) = \\log \\left( \\frac{ E[Y_i] }{1 - E[Y_i] } \\right) = \\beta_0 + \\sum_{j=1}^p \\beta_j X_{ij}\n\\] Exponentiating the coefficients (i.e., \\(e^{\\beta}\\)) would yield odds ratios, which are the multiplicative increase in the odds of \\(Y_i=1\\) that is associated with a one-unit increase in the relevant predictor variable.\n\n\n\n\n\n\n\n\n\nFigure 7.2: An example of how logistic regression transforms a change in the mean-centered predictor X into a change in the expected outcome Y. The same absolute change in X is associated in a large difference in the probability of the outcome when X is near its mean (blue) vs a small change in the outcome when X is large (red) or small.\n\n\n\n\nFigure 7.2 shows the way that a logistic regression model transforms a predictor (\\(X\\)) into an outcome probability that is bounded at 0 and 1. Critically, although the predictor is still linear, the logit link means that the same change in \\(X\\) can result in a different change in the absolute probability of \\(Y\\) depending on where you are on the \\(X\\) scale. In this example, if you are in the middle of the predictor range, a one-unit change in \\(X\\) results in a 0.24 change in probability (blue). At a higher value, the change is much smaller (0.02). Notice how this is different from the linear regression model above, where the same change in age always resulted in the same change in preference!\n\n\n\n\n\n\ncode\n\n\n\n\n\nGLMs are as easy to fit in R as standard LMs. You simply need to call the glm() function—and to specify the link function. For our example above of a binary “liking” judgment, the call would be:\n\nglm(liked_tea ~ condition, data = tea_data, family = \"binomial\")\n\nThe family argument specifies the type of distribution being used, where binomial is the logistic link function.\n\n\n\nWe have only scratched the surface of GLMs here. First, there are many different link functions that are suitable for different outcome types. And second, GLMs differ from OLS not only in their link functions but also in how they handle the error terms. Our broader goal in this chapter is to show you how regression models are models of data. In that context, GLMs use link functions as a way to make models that generate many different types of outcome data.14\n14 We sometimes think of linear models as a set of tinker toys you can snap together to stack up a set of predictors. In that context, link functions are an extra “attachment” that you can snap onto your linear model to make it generate a different response type.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#linear-mixed-effects-models",
    "href": "007-models.html#linear-mixed-effects-models",
    "title": "7  Models",
    "section": "7.3 Linear mixed effects models",
    "text": "7.3 Linear mixed effects models\nExperimental data often contain multiple measurements for each participant (so-called repeated measures). In addition, these measurements are often based on a sample of stimulus items (which then each have multiple measures as well). This clustering is problematic for OLS models because the error terms for each datapoint are not independent.\nNon-independence of datapoints may seem at first glance like a small issue, but it can present a deep problem for making inferences. Take the tea-tasting data we looked at above, where we had 24 observations in each condition. If we fit an OLS model, we observe a highly significant tea-first effect. Here is the estimate and confidence interval for that coefficient: \\(b = -2.42\\), 95% CI \\([-3.50, -1.33]\\). Based on what we talked about in the previous chapter, it seems like we’d be licensed in rejecting the null hypothesis that this effect is due to sampling variation and interpret this instead as evidence for a generalizable difference in tea preference in our sampled population.\nBut suppose we told you that all of those 48 total observations (24 in each condition) were from one individual named George. That would change the picture considerably. Now we’d have no idea whether the big effect we observed reflected a difference in the population, but we would have a very good sense of what George’s preference is!15 The confidence intervals and p-values from our OLS model would be wrong now because all of the error terms would be highly correlated—they would all reflect George’s preferences.\n15 We discuss the strengths and weaknesses of repeated-measures designs like this in chapter 9 and the statistical trade-offs of having many people with a small number of observations per person vs a small number of people with many observations per person in chapter 10.How can we make models that deal with clustered data? There are a number of widely used approaches for solving this problem including linear mixed effects models, generalized estimating equations, and clustered standard errors (often used in economics). Here we will illustrate how the problem gets solved in linear mixed models, which are an extension of OLS models that are fast becoming a standard in many areas of psychology (Bates et al. 2015).\n\n7.3.1 Modeling random variation in clusters\nIn linear mixed effects models, we modify the linear predictor itself to model differences across clusters. Instead of just measuring George’s preferences, suppose we modified the original tea-tasting experiment (without the age covariate) to collect ten ratings from each participant: five milk-first and five tea-first. We define the model the same way as we did before, with some minor differences: \\[\nY_{it} = \\beta_0 + \\beta_1 X_{it} + \\gamma_i + \\epsilon_{it}\n\\] where \\(Y_{it}\\) is participant \\(i\\)’s rating in trial \\(t\\) and \\(X_{it}\\) is the participant’s assigned treatment in trial \\(t\\) (i.e., milk-first or tea-first).\nIf you compare this equation to the OLS equation above, you will notice that we added two things. First, we’ve added subscripts that distinguish trials from participants. But the big one is that we added \\(\\gamma_i\\), a separate intercept value for each participant. We call this a random intercept because it varies across participants (who are randomly selected from the population).16\n16 Formally, we’d notate this random variation by saying that \\(\\gamma_i \\sim N(0, \\tau^2)\\)—in other words, that participants’ random intercepts are sampled from a normal distribution around the shared intercept \\(\\beta_0\\) with standard deviation \\(\\tau\\).17 Of course, this would be a terrible experiment! Ideally, we would address this problem upstream in our experiment design; see chapter 9.The random intercept means that we have assumed that each participant has their own typical “baseline” tea rating—some participants overall just like tea more than others—and these baseline ratings are normally distributed across participants. Thus, ratings are correlated within participants because ratings cluster around each participant’s distinct baseline tea rating. This model is better able to block misleading inferences. For example, suppose we only had one participant in each condition (say, George provided 24 milk-first ratings and Alice provided 24 tea-first ratings). If we found higher ratings in one condition, we would be able to attribute this difference to participant-level variation rather than to the treatment.17\nFollowing the same logic, we could fit random intercepts for different stimulus items (for example, if we used different types of tea for different trials). We modeled participants as having normally distributed variation, and we can model stimulus variation the same way. Each stimulus item is assumed to produce a particular average outcome (i.e., some teas are tastier than others), with these average outcomes sampled from a normally distributed population.\n\n\n\n\n\n\ncode\n\n\n\n\n\nRemarkably, GLMMs are not much harder to specify in R than standard LMs. One very popular package is lme4 (bates2014?), which provides the lmer() and glmer() functions (the latter for generalized linear mixed effect models). For our example here, we’d write:\n\nlibrary(lme4)\nlmer(rating ~ condition + (1 | id), data = tea_data)\n\nIn this model, the syntax (1 | id) specifies that we want a random intercept for each level of id.\n\n\n\n\n\n7.3.2 Random slopes and the challenges of mixed effects models\nLinear mixed effects models can be further extended to model clustering of the independent variables’ effects within subjects, not just clustering of average outcomes within subjects. To do so, we can introduce random slopes (\\(\\delta_i\\)) to the model, which are multiplied by the condition variable \\(X\\) and represent differences across participants in the effect of tea tasting: \\[\nY_i = \\beta_0 + \\beta_1 X_{it} + \\gamma_i + \\delta_{i} X_{it} + \\epsilon_{it}\n\\] Just like the random intercepts, these random slopes will be assumed to vary across participants, following a normal distribution.18\n18 These random slopes and intercepts can be assumed to be independent or correlated with one another, depending on the modeler’s preference.19 There’s lots of debate in the literature about the best random effect structure for mixed effects models. This is a very tricky and technical subject. In brief, some folks argue for so-called maximal models, in which you include every random effect that is justified by the design (Barr et al. 2013). Here that would mean including random slopes for each participant. The problem is that these models can get very complex and can be very hard to fit using standard software. We won’t weigh in on this topic, but as you start to use these models on more complex experimental designs, it might be worth reading up on.This model now describes random variation in both overall how much someone likes tea and how strong their ordering preference is. Both of these likely do vary in the population, and so it seems like a good thing to put these in your model. Indeed, under some circumstances, adding random slopes is argued to be very important for making appropriate inferences.19\n\n\n\n\n\n\ncode\n\n\n\n\n\nSpecifying random slopes in the lme4 package is also relatively straightforward:\n\nlmer(rating ~ condition + (condition | id), data = tea_data)\n\nHere, (condition | id) means “a separate random slope for condition should be fit for each level of id.” Of course, specifying such a model is easier than fitting it correctly.\n\n\n\nOn the other hand, the model is much more complicated. When we had a simple OLS model above, we had only two parameters to fit (\\(\\beta_0\\) and \\(\\beta_1\\)), but now we have those two plus two more, representing the standard deviations of the individual participant intercepts and slopes, plus parameters for each participant and for the condition effect for each participant. So we went from two parameters to 24!20 This complexity can lead to problems in fitting the models, especially with very small datasets (where these parameters are not very well constrained by the data) or very large datasets (where computing all these parameters can be tricky).21\n20 Though we should note that these parameters aren’t technically all independent from one another due to the structure of the mixed effect model.21 Many R users may be familiar with the widely used lme4 package for fitting mixed effects models using frequentist tools related to maximum likelihood. Such models can also be fit using Bayesian inference with the brms package (Bürkner 2021), which provides many powerful methods for specifying complex models.22 One particularly problematic situation is when the correlation structure of the errors is mis-specified, for example if observations within a participant are more correlated for participants in the treatment group than in the control group; in such cases, mixed model estimates can be substantially biased (Bie et al. 2021).More generally, linear mixed effects models are very flexible, and they have become quite common in psychology. But they do have significant limitations. As we discussed, they can be tricky to fit in standard software packages. Further, the accuracy of these models relies on our ability to specify the structure of the random effects correctly.22 If we specify an incorrect model, our inferences will be wrong! But it is sometimes difficult to know how to check whether your model is reasonable, especially with a small number of clusters or observations.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#how-do-you-use-models-to-analyze-data",
    "href": "007-models.html#how-do-you-use-models-to-analyze-data",
    "title": "7  Models",
    "section": "7.4 How do you use models to analyze data?",
    "text": "7.4 How do you use models to analyze data?\nIn the prior parts of this chapter, we’ve described a suite of regression-based techniques—standard OLS, the generalized linear model, and linear mixed effects models—that can be used to model the data resulting from randomized experiments (as well as many other kinds of data). The advantage of regression models over the simpler estimation and inference methods we described in the prior two chapters is that these models can more effectively take into account a range of different kinds of variation, including covariates, multiple manipulations, and clustered structure. Further, when used appropriately to analyze a well-designed randomized experiment, regression models can give an unbiased estimate of a causal effect of interest, our main goal in doing experiments.\nBut—practically speaking—how should go you about building a model for your experiment? What covariates should you include, and what should you leave out? There are many ways to use models to explore datasets, but in this section we will try to sketch a default approach for the use of models to estimate causal effects in experiments in the most straightforward way. Think of this as a starting point. We’ll begin this section by giving a set of rules of thumb, then discuss a worked example. Our final subsections will deal with the issues of when you should include covariates in your model and how to check if your result is robust across multiple different model specifications.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nAn alternative approach: Generalized estimating equations\nA second class of methods that helps resolve issues of clustering is generalized estimating equations (GEE). In this approach, we leave the linear predictor alone. We do not add random intercepts or slopes, nor do we assume anything about the distribution of the errors (i.e., we no longer assume that they are normal, independent, and homoskedastic).\nIn GEE, we instead provide the model with an initial “guess” about how we think the errors might be related to one another; for example, in a repeated-measures experiment, we might guess that the errors are exchangeable, meaning that they are correlated to the same degree within each participant but are uncorrelated across participants. Instead of assuming that our guess is correct, as do linear mixed models (LMM), GEE estimates the correlation structure of the errors empirically, using our guess as a starting point, and it uses this correlation structure to arrive at point estimates and inference for the regression coefficients. Remarkably, as the number of clusters and observations become very large, GEE will always provide unbiased point estimates and valid inference, even if our guess about the correlation structure was bad. Additionally, with simple finite-sample corrections (Mancl and DeRouen 2001), GEE seems to provide valid inference at smaller numbers of clusters than does LMM.\nThe price paid for these nice safeguards against model misspecification is that, in principle, GEE will typically have less statistical power than LMM if the LMM is in fact correctly specified, but the difference may be surprisingly slight in practice (Bie et al. 2021). For these reasons, some of this book’s authors actually favor GEE with finite-sample corrections over LMM as the default model for clustered data, though they are much less common in psychology. \n\n\n\n\n7.4.1 Modeling rules of thumb\nOur approach to statistical modeling is to start with a “default model” that is known in the literature as a saturated model. The saturated model of an experiment includes the full design of the experiment—all main effects and interactions—and nothing else. If you are manipulating a variable, include it in your model. If you are manipulating two, include them both and their interaction. If your design includes repeated measurements for participants, include a random effect of participant; if it includes experimental items for which repeated measurements are made, include random effect of stimulus.23\n23 As discussed above, you can also include the “maximal” random effect structure (Barr et al. 2013), which involves random slopes as well as intercepts—but recognize that you cannot always fit such models.24 One corollary to having this kind of default perspective on data analysis: When you see an analysis that deviates substantially from the default, these deviations should provoke some questions. If someone drops a manipulation from their analysis, adds a covariate or two, or fails to control for some clustering in the data, did they deviate because of different norms in their subfield, or was there some other rationale? This line of reasoning sometimes leads to questions about the extent to which particular analytic decisions are post hoc and driven by the data (in other words, \\(p\\)-hacked). For an example, see the case study in chapter 11.Don’t include lots of other stuff in your default model. You are doing a randomized experiment, and the strength of randomized experiments is that you don’t have to worry about confounding based on the population (see chapter 1). So don’t put a lot of covariates in your default model—usually don’t put in any!24\nThis default saturated model then represents a simple summary of your experimental results. Its coefficients can be interpreted as estimates of the effects of interest, and it can be used as the basis for inferences about the relation of the experimental effect to the population using either frequentist or Bayesian tools.\nHere’s a bit more guidance about this modeling strategy.\n\nPreregister your model. If you change your analysis approach after you see your data, you risk \\(p\\)-hacking—choosing an analysis that biases the estimate of your effect of interest. As we discussed in chapter 3 and as we will discuss in more detail in chapter 11, one important strategy for minimizing this problem is to preregister your analysis.25\nVisualize the model predictions against the observed data. As we’ll discuss in chapter 15, the “default model” for an experiment should go alongside a “default visualization” known as the design plot that similarly reflects the full design structure of the experiment and any primary clusters. One way to check whether a model fits your data is then to plot it on top of those data. Sometimes this combination of model and data can be as simple as a scatter plot with a regression line. But seeing the model plotted alongside the data can often reveal a mismatch between the two. A model that does not describe the data very well is not a good source of generalizable inferences!\nInterpret the model predictions. Once you have a model, don’t just read off the \\(p\\)-values for your coefficients of interest. Walk through each coefficient, considering how it relates to your outcome variable. For a simple two-group design like we’ve been considering, the condition coefficient is the estimate of the causal effect that you intended to measure! Consider its sign, its magnitude, and its precision (standard error or confidence interval).\n\n25 A side benefit of preregistration is it makes you think through whether your experimental design is appropriate—that is, is there actually an analysis capable of estimating the effect you want from the data you intend to collect?That said, there are some contexts in which it does make sense to depart from the default saturated model. For example, there may be insufficient statistical power to estimate multiple interaction terms, or covariates might be included in the model to help handle certain forms of missing data. The default model simply represents a very good starting point.\n\n\n7.4.2 A worked example\n\n\n\n\n\n\n\nFigure 7.3: Example stimulus materials analogous to those used in Stiller, Goodman, and Frank (2015).\n\n\nAll this advice may seem abstract, so let’s put it into practice on a simple example. For a change, let’s look at an experiment that’s not about tea tasting. Here we’ll consider data from an experiment testing preschool children’s language comprehension (Stiller, Goodman, and Frank 2015). In this experiment, two- to five-year-old children saw displays like the one in figure 7.3. In the experimental condition, a puppet might say, for example, “My friend has glasses! Which one is my friend?” The goal was to measure how many children made the “pragmatic inference” that the puppet’s friend was the face with glasses and no hat.\nTo estimate the effect, participants were randomly assigned to either the experimental condition or to a control condition in which the puppet had eaten too much peanut butter and couldn’t talk, but they still had to guess which face was his friend. There were also three other types of experimental stimuli (houses, beds, and plates of pasta). Data from this experiment consisted of 588 total observations from 147 children, with all four stimuli presented to each child. The primary hypothesis of this experiment was that that preschool children could make pragmatic inferences by correctly inferring which of the three faces (for example) the puppet was describing.\n\n\n\n\n\n\ncode\n\n\n\n\n\nIf you want to follow along with this example, you’ll have to load the example data and do a little bit of preprocessing (also covered in appendix D):\n\nrepo &lt;- \"https://raw.githubusercontent.com/langcog/experimentology/main\"\nsgf &lt;- read_csv(file.path(repo, \"data/tidyverse/stiller_scales_data.csv\")) |&gt;\n  mutate(age_group = cut(age, 2:5, include.lowest = TRUE), \n         condition = condition |&gt;\n           fct_recode(\"Experimental\" = \"Label\", \"Control\" = \"No Label\"))\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Data for Stiller, Goodman, and Frank (2015). Each point shows a single participant’s proportion correct trials (out of four experimental stimuli) plotted by age group, jittered slightly to avoid overplotting. Larger points and associated confidence intervals show mean and 95% confidence intervals for each condition.\n\n\n\n\n\nThis experimental design looks a lot like some versions of our tea-tasting experiment. We have one primary condition manipulation (the puppet provides information versus does not), presented between participants so that some participants are in the experimental condition and others are in the control condition. Our measurements are repeated within participants across different experimental stimuli. Finally, we have one important, preplanned covariate: children’s age. The experimental data are plotted in figure 7.4.26\n26 Our sampling plan for this experiment was actually stratified across age, meaning that we intentionally recruited the same number of participants for each one-year age group—because we anticipated that age was highly correlated with children’s ability to succeed in this task. We’ll describe this kind of sampling in more detail in chapter 10.27 This experiment was not preregistered, but the paper includes a separate replication dataset with the same analysis.28 As discussed above, this is a tricky decision point; we could very reasonably have added random slopes as well.How should we go about making our default model for this dataset?27 We simply include each of these design factors in a mixed effects model; we use a logistic link function for our mixed effects model (a generalized linear mixed effects model) because we would like to predict correct performance on each trial, which is a binary variable. So that gives us an effect of condition and age as a covariate. We further add an interaction between condition and age in case the condition effect varies meaningfully across groups. Finally, we add random effects of participant, \\(\\gamma_i\\), and experimental item, \\(\\gamma_t\\).28\nThe resulting model looks like this: \\[\n\\text{logit}( E[Y_{it}] ) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i1} X_{i2} + \\gamma_i + \\delta_t\n\\]\nLet’s break this complex equation down from left to right:\n\n\\(\\text{logit}( E[Y_{it}] )\\) says that we are predicting a logistic function of \\(E[Y_{it}]\\) (where \\(Y_{it}\\) indicates whether child \\(i\\) was correct on trial \\(t\\)).\n\\(\\beta_0\\) is the intercept, our estimate of the average log-odds (i.e., the log of the odds ratio) of correct responses for participants in the control condition.\n\\(\\beta_1 X_{i1}\\) is the condition predictor. \\(\\beta_1\\) represents the change in log-odds associated with being in the experimental condition (the causal effect of interest!), and \\(X_{i1}\\) is an indicator variable that is 1 if child \\(i\\) is in the experimental condition and 0 for the control condition. Multiplying \\(\\beta_1\\) by this indicator means that the predictor has the value 0 for participants in the control condition and \\(\\beta_1\\) for those in the experimental condition.\n\\(\\beta_2 X_{i2}\\) is the age predictor. \\(\\beta_2\\) represents the difference in log-odds associated with one additional year of age for participants in the control condition29 and \\(X_{i2}\\) is the age for each participant.30\n\\(\\beta_3 X_{i1} * X_{i2}\\) is the interaction between experimental condition and age. \\(\\beta_3\\) represents the difference in log odds (i.e., the log of the odds ratio) that is associated with being one year older and in the experimental condition versus the control condition. This term is multiplied by both each child’s age and the condition indicator \\(X_i\\).\n\\(\\gamma_i\\) is the random intercept for participant \\(i\\), capturing individual variation in the odds of success across trials.\n\\(\\gamma_t\\) is the random intercept for stimulus \\(t\\), capturing variation in the odds of success across the four different stimuli.\n\n29 The age coefficient is a simple effect, meaning it is the effect of age in the control condition only. That’s because we have dummy coded the condition predictor. If we wanted the average age effect (the main effect) then we would need to use contrast coding, per the note in the “Interactions” section above.30 We have centered our age predictor in this example so that all estimates from our model are for the average age of our participants. Centering is a good practice for modeling continuous predictors because it increases the interpretability of other parts of the model. For example, because age is centered in this model, the intercept \\(\\beta_0\\) can be interpreted as the predicted odds of a correct trial for a participant in the control condition at the average age.\n\n\n\nTable 7.2: Estimated effects for our generalized linear mixed effects model on data from Stiller, Goodman, and Frank (2015).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nconf.int\nstatistic\np.value\n\n\n\n\nControl condition\n0.80\n[0.42, 1.18]\n4.16\n&lt; .001\n\n\nAge (years)\n0.55\n[0.21, 0.88]\n3.19\n.001\n\n\nExpt condition\n-2.26\n[-2.70, -1.82]\n-10.07\n&lt; .001\n\n\nAge (years) * Expt condition\n-0.92\n[-1.43, -0.42]\n-3.60\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nTo fit the model described above, the first step is to prepare your predictors. In this case, we center the age predictor.\n\nsgf$age_centered &lt;- scale(sgf$age, center = TRUE, scale = FALSE)\n\nAgain we use the lme4 package, this time with the glmer() function. Again we have to specify our link function, just like in a standard GLM, by choosing the distribution family.\n\nmod &lt;- glmer(correct ~ age_centered * condition + (1|subid) + (1|item), \n             family = \"binomial\", data = sgf)\n\nYou can see a summary of the fitted model using summary(mod) as before. The only big difference from lm() is that here you can extract both fixed and random effects (with fixef(mod) and ranef(mod), respectively).\n\n\n\nLet’s estimate this model and see how it looks. We’ll focus here on interpretation of the so-called fixed effects (the main predictors), as opposed to the participant and item random effects.31 Table 7.2 shows the coefficients. Again, let’s walk through each:\n31 Participant means are estimated to have a standard deviation of 0.23 (in log-odds), while items have a standard deviation of 0.27. These indicate that both of our random effects capture meaningful variation.\nThe intercept (control condition estimate) is \\(\\hat{\\beta} = 0.80\\), 95% CI \\([0.42, 1.18]\\), \\(z = 4.16\\), \\(p &lt; 0.001\\). This estimate reflects that the log-odds of a correct response for an average-age participant in the control condition is 0.8, which corresponds to a probability of 0.69. If we look at figure 7.4, that estimate makes sense: 0.69 seems close to the average for the control condition.\nThe age effect estimate is \\(\\hat{\\beta} = 0.55\\), 95% CI \\([0.21, 0.88]\\), \\(z = 3.19\\), \\(p = 0.001\\). This means there is a slight decrease in the log-odds of a correct response for older children in the control condition. Again, looking at figure 7.4, this estimate is interpretable: we see a small decline in the probability of a correct response for the oldest age group.\nThe key experimental condition estimate then is \\(\\hat{\\beta} = -2.26\\), 95% CI \\([-2.70, -1.82]\\), \\(z = -10.07\\), \\(p &lt; 0.001\\). This estimate means that the log-odds of a correct response for an average-age participant in the experimental condition is the sum of the estimates for the control (intercept) and the experimental conditions: 0.8 + -2.26, which corresponds to a probability of 0.19. Grounding our interpretation in figure 7.4, this estimate corresponds to the average value for the experimental condition.\nFinally, the interaction of age and condition is \\(\\hat{\\beta} = -0.92\\), 95% CI \\([-1.43, -0.42]\\), \\(z = -3.60\\), \\(p &lt; 0.001\\). This positive coefficient reflects that with every year of age, the difference between control and experimental conditions grows.\n\nIn sum, this model suggests that there was a substantial difference in performance between experimental and control conditions, in turn supporting the hypothesis that children in the sampled age group can perform pragmatic inferences above chance.\nThis example illustrates the “default saturated model” framework that we recommend—the idea that a single regression model corresponding to the design of the experiment can yield an interpretable estimate of the causal effect of interest, even in the presence of other sources of variation.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nWhen does it makes sense to include covariates in a model?\nLet’s come back to one piece of advice that we gave above about making a “default” model of an experiment: not including covariates. This advice can seem surprising. Many demographic factors are of interest to psychologists and other behavioral scientists, and in observational studies these factors will almost always be related to important life outcomes. So, why not put them into our experimental models? After all, we did include age in our worked example above!\nWell, if you have one or at most a small handful of covariates that you believe are meaningfully related to the outcome, you can plan in advance to put them in your model. If you think that your effect is likely to be moderated a specific demographic characteristic—as we did with age in our developmental example above—then this inclusion can be quite useful.\nFurther, including covariates can increase the precision of your estimates by reducing “noise” in your outcome, if you hypothesize that they interact. What’s surprising though is how little this adjustment does to increase your overall precision unless the correlation between covariate and outcome is very strong.\n\n\n\n\n\n\nFigure 7.5: Decreases in estimation error due to adjusting for covariates, plotted by the N participants in each group and the correlation between the outcome (X) and the covariate (Z).\n\n\n\nFigure 7.5 shows the results of a simple simulation investigating the relationship between estimation error and the inclusion of covariates. Only when the correlation between covariate and outcome (e.g., age and tea rating) is greater than \\(r=0.6\\) to \\(r=0.8\\) does this adjustment really help.\nThat said, there are quite a few reasons not to include covariates. These motivate our recommendation to skip them in your default model unless you have very strong theory-based expectations for either (A) a correlation with the outcome or (B) a strong moderation relationship.\nThe first reason not to include covariates is simply because we don’t need to. Because randomization cuts causal links, our experimental estimate is an unbiased estimate of the causal effect of interest (at least for large samples). We are guaranteed that, in the limit of many different experiments, even though people with different ages will be in the different tea-tasting conditions, this source of variation will be averaged out. Actually, including unnecessary covariates into models (slightly) decreases the probability that the model can detect a true effect (that is, it decreases statistical precision and power). Just by chance, covariates can “soak up” variation in the outcome, leaving less to be accounted for by the true effect!\nThe second reason is that you can actually compromise your causal inference by including some covariates, particularly those that are collected after randomization. The logic of randomization is that you cut all causal links between features of the sample and the condition manipulation. But you can “uncut” these links by accident by adding variables into your model that are related to group status. This problem is generically called conditioning on post-treatment variables, and a full discussion of is out of the scope of this book, but it’s something to avoid (and read up on if you’re worried about it, see Montgomery, Nyhan, and Torres 2018).\nFinally, one of the standard justifications for adding covariates—because your groups are unbalanced—is actually ill-founded as well. People often talk about “unhappy randomization”: you randomize to the different tea-tasting groups, for example, but then it turns out the mean age is a bit different between groups. Then you do a \\(t\\)-test or some other statistical test and find out that you actually have a significant age difference. This practice makes no sense! Because you randomized, you know that the difference in ages occurred by chance. Further, incidental demographic differences between groups are unlikely to be important unless that characteristic is highly correlated with the outcome (see above). Instead, if the sample size is small enough that meaningfully large incidental differences could arise in important confounders, then it is preferable to stratify on that confounder at the outset—we’ll have lot more to say about this issue in chapter 10.\nSo these are our options: if a covariate is known to be very strongly related to our outcome, we can include it in our default model. Otherwise, we avoid a lot of trouble by leaving covariates out.\n\n\n\n\n\n7.4.3 Robustness checks and the multiverse\nUsing the NHST statistical testing approach that has been common in the psychology literature, even a simple two-factor experimental design affords a host of different \\(t\\)-tests and ANOVAs,32 offering many opportunities for \\(p\\)-hacking and selective reporting. We’ve been advocating here instead for a “default model” approach in which you preplan and preregister a single regression model that captures the planned features of your experimental design, including manipulations and sources of clustering. This approach can help you to navigate some of the complexity of data analysis by having a standard approach that you take in almost every case.\n32 Although we don’t cover this point here, ANOVAs are also a special case of regression.33 To be fair, often the analytic questions being investigated in “Many Analysts” projects are more complex than the simple experiments we recommend doing, and there is debate about how much true variability these investigations reveal (Breznau et al. 2022; Mathur, Covington, and VanderWeele 2023).Not every dataset will be amenable to this approach, however. For complex experimental designs or unusual measures, sometimes it can be hard to figure out how to specify or fit the default saturated model. And especially, in these cases, the choice of model can make a big difference to the magnitude of the reported effect. To quantify variability in effect size due to model choice, “Many Analysts” projects have asked a set of teams to approach a dataset using different analysis methods. The result from these projects has been that there is substantial variability in outcomes depending on what approach is taken (Silberzahn et al. 2018; Botvinik-Nezer et al. 2020).33\nRobustness analysis (also sometimes called “sensitivity analysis” or “multiverse analysis,” which sounds cooler) is a technique for addressing the possibility that an individual analysis overestimates or underestimates a particular effect by chance (Steegen et al. 2016). The general idea is that analysts explore a space of different possible analyses. In its simplest form, alternative model specifications can be reported in a supplement; more sophisticated versions of the idea call for averaging estimates across a range of possible specifications and reporting this average as the primary effect estimate.\nThe details of this kind of analysis will vary depending on what you are worried about your model being sensitive to. One analyst might be concerned about the effects of adding different covariates; another might be using a Bayesian framework and be concerned about sensitivity to particular prior values. If you get similar results across many different specifications, you can sleep better at night. The primary principle to take home is a bit of humility about our models. Any given model is likely wrong in some of its details. Investigating the sensitivity of your estimates to the details of your model specification is a good idea.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#an-alternative-approach-generalized-estimating-equations",
    "href": "007-models.html#an-alternative-approach-generalized-estimating-equations",
    "title": "7  Models",
    "section": "An alternative approach: Generalized estimating equations",
    "text": "An alternative approach: Generalized estimating equations\nA second class of methods that helps resolve issues of clustering is generalized estimating equations (GEE). In this approach, we leave the linear predictor alone. We do not add random intercepts or slopes, nor do we assume anything about the distribution of the errors (i.e., we no longer assume that they are normal, independent, and homoskedastic).\nIn GEE, we instead provide the model with an initial “guess” about how we think the errors might be related to one another; for example, in a repeated-measures experiment, we might guess that the errors are exchangeable, meaning that they are correlated to the same degree within each participant but are uncorrelated across participants. Instead of assuming that our guess is correct, as do linear mixed models (LMM), GEE estimates the correlation structure of the errors empirically, using our guess as a starting point, and it uses this correlation structure to arrive at point estimates and inference for the regression coefficients. Remarkably, as the number of clusters and observations become very large, GEE will always provide unbiased point estimates and valid inference, even if our guess about the correlation structure was bad. Additionally, with simple finite-sample corrections (Mancl and DeRouen 2001), GEE seems to provide valid inference at smaller numbers of clusters than does LMM.\nThe price paid for these nice safeguards against model misspecification is that, in principle, GEE will typically have less statistical power than LMM if the LMM is in fact correctly specified, but the difference may be surprisingly slight in practice (Bie et al. 2021). For these reasons, some of this book’s authors actually favor GEE with finite-sample corrections over LMM as the default model for clustered data, though they are much less common in psychology.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#when-does-it-makes-sense-to-include-covariates-in-a-model",
    "href": "007-models.html#when-does-it-makes-sense-to-include-covariates-in-a-model",
    "title": "7  Models",
    "section": "When does it makes sense to include covariates in a model?",
    "text": "When does it makes sense to include covariates in a model?\nLet’s come back to one piece of advice that we gave above about making a “default” model of an experiment: not including covariates. This advice can seem surprising. Many demographic factors are of interest to psychologists and other behavioral scientists, and in observational studies these factors will almost always be related to important life outcomes. So, why not put them into our experimental models? After all, we did include age in our worked example above!\nWell, if you have one or at most a small handful of covariates that you believe are meaningfully related to the outcome, you can plan in advance to put them in your model. If you think that your effect is likely to be moderated a specific demographic characteristic—as we did with age in our developmental example above—then this inclusion can be quite useful.\nFurther, including covariates can increase the precision of your estimates by reducing “noise” in your outcome, if you hypothesize that they interact. What’s surprising though is how little this adjustment does to increase your overall precision unless the correlation between covariate and outcome is very strong.\n\n\n\n\n\n\nFigure 7.5: Decreases in estimation error due to adjusting for covariates, plotted by the N participants in each group and the correlation between the outcome (X) and the covariate (Z).\n\n\n\nFigure 7.5 shows the results of a simple simulation investigating the relationship between estimation error and the inclusion of covariates. Only when the correlation between covariate and outcome (e.g., age and tea rating) is greater than \\(r=0.6\\) to \\(r=0.8\\) does this adjustment really help.\nThat said, there are quite a few reasons not to include covariates. These motivate our recommendation to skip them in your default model unless you have very strong theory-based expectations for either (A) a correlation with the outcome or (B) a strong moderation relationship.\nThe first reason not to include covariates is simply because we don’t need to. Because randomization cuts causal links, our experimental estimate is an unbiased estimate of the causal effect of interest (at least for large samples). We are guaranteed that, in the limit of many different experiments, even though people with different ages will be in the different tea-tasting conditions, this source of variation will be averaged out. Actually, including unnecessary covariates into models (slightly) decreases the probability that the model can detect a true effect (that is, it decreases statistical precision and power). Just by chance, covariates can “soak up” variation in the outcome, leaving less to be accounted for by the true effect!\nThe second reason is that you can actually compromise your causal inference by including some covariates, particularly those that are collected after randomization. The logic of randomization is that you cut all causal links between features of the sample and the condition manipulation. But you can “uncut” these links by accident by adding variables into your model that are related to group status. This problem is generically called conditioning on post-treatment variables, and a full discussion of is out of the scope of this book, but it’s something to avoid (and read up on if you’re worried about it, see Montgomery, Nyhan, and Torres 2018).\nFinally, one of the standard justifications for adding covariates—because your groups are unbalanced—is actually ill-founded as well. People often talk about “unhappy randomization”: you randomize to the different tea-tasting groups, for example, but then it turns out the mean age is a bit different between groups. Then you do a \\(t\\)-test or some other statistical test and find out that you actually have a significant age difference. This practice makes no sense! Because you randomized, you know that the difference in ages occurred by chance. Further, incidental demographic differences between groups are unlikely to be important unless that characteristic is highly correlated with the outcome (see above). Instead, if the sample size is small enough that meaningfully large incidental differences could arise in important confounders, then it is preferable to stratify on that confounder at the outset—we’ll have lot more to say about this issue in chapter 10.\nSo these are our options: if a covariate is known to be very strongly related to our outcome, we can include it in our default model. Otherwise, we avoid a lot of trouble by leaving covariates out.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "007-models.html#chapter-summary-models",
    "href": "007-models.html#chapter-summary-models",
    "title": "7  Models",
    "section": "7.5 Chapter summary: Models",
    "text": "7.5 Chapter summary: Models\nIn the last three chapters, we have spelled out a framework for data analysis that focuses on our key experimental goal: a measurement of a particular causal effect. We began with basic techniques for estimating effects and making inferences about how these effects estimated from a sample can be generalized to a population. This chapter showed how these ideas naturally give rise to the idea of making models of data, which allow estimation of effects in more complex designs. Simple regression models, which are formally identical to other inference methods in the most basic case, can be extended with the generalized linear model as well as with mixed effects models. Finally, we ended with some guidance on how to build a “default model”—an (often preregistered) regression model that maps onto your experimental design and provides the primary estimate of your key causal effect.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nChoose a paper that you have read for your research and take a look at the statistical analysis. Does the reporting focus more on hypothesis testing or on estimating effect sizes?\nWe focused here on the linear model as a tool for building models, contrasting this perspective with the common “statistical testing” mindset. But—here’s the mind-blowing thing—most of those statistical tests are special cases of the linear model anyway. Take a look at this extended meditation on the equivalences between tests and models: https://lindeloev.github.io/tests-as-linear/#9_teaching_materials_and_a_course_outline. If the paper you chose for question 1 used tests, could their tests be easily translated to models? How would the use of a model-based perspective change the results section of the paper?\nTake a look at this cool visualization of hierarchical (mixed effect) models: http://mfviz.com/hierarchical-models. In your own research, what are the most common units that group together your observations?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nAn opinionated practical guide to regression modeling and data description: Gelman, Andrew, Jennifer Hill, and Aki Vehtari (2020). Regression and Other Stories. Cambridge University Press. Free online at https://avehtari.github.io/ROS-Examples.\nA more in-depth introduction to the process of developing Bayesian models of data that allow for estimation and inference in complex datasets: McElreath, Richard (2018). Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman and Hall/CRC. Free materials available at https://xcelab.net/rm/statistical-rethinking.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "008-measurement.html",
    "href": "008-measurement.html",
    "title": "8  Measurement",
    "section": "",
    "text": "8.1 Reliability\nReliability is a way of describing the extent to which a measure yields signal relative to noise. Intuitively, if there’s less noise, then there will be more similarity between different measurements of the same quantity, illustrated in figure 8.1 as a tighter grouping of points on the bulls-eye. But how do we measure signal and noise?",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#reliability",
    "href": "008-measurement.html#reliability",
    "title": "8  Measurement",
    "section": "A reliable and valid measure of children’s vocabulary",
    "text": "Figure 8.1: Reliability and validity visualized. The reliability of an instrument is its expected precision. The bias of measurements from an instrument also provide a metaphor for its validity.\n\n\n\n\n\n\n\n\n\ncase study\n\n\n\n\n\nA reliable and valid measure of children’s vocabulary\nAnyone who has worked with little children, or had children of their own, can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes (Marchman and Fernald 2008). Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.\nBecause bringing children into a lab can be expensive, one popular alternative option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form that asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words. But is parent report a reliable or valid measure of children’s early language?\n\n\n\n\n\n\n\n\nFigure 8.2: Longitudinal (test-retest) correlations between a child’s score on one administration of the CDI and another one several months later. Based on Frank et al. (2021).\n\n\n\n\n\nAs we’ll see below, one way to measure the reliability of the CDI is to compute the correlation between two different administrations of the form for the same child. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! figure 8.2 displays these correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases (Frank et al. 2021).\n\nGiven that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? Bornstein and Haynes (1998) collected many different measures of children’s language—including the Early Language Inventory (ELI; an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. CDI scores were highly correlated with all the different measures, suggesting that the CDI was a valid measure of the construct.\nThe combination of reliability and validity evidence suggests that CDIs are a useful (and relatively inexpensive) source of data about children’s early language, and indeed they have become one of the most common assessments for this age group!\n\n\n\n\n\n8.1.1 Measurement scales\n\n\n\n\n\n\n\nFigure 8.3: Computing the coefficient of variation (CV).\n\n\nIn the physical sciences, it’s common to measure the precision of an instrument using its coefficient of variation (Brandmaier et al. 2018): \\[CV = \\frac{\\sigma_w}{\\mu_w}\\] where \\(\\sigma_w\\) is the standard deviation of the measurements within an individual and \\(\\mu_w\\) is the mean of those measurements (figure 8.3).\nImagine we measure the height of a person five times, resulting in measurements of 171 cm, 172cm, 171 cm, 173 cm, and 172 cm. These are the combination of the person’s true height (we assume they have one!) and some measurement error. Now we can use these measurements to compute the coefficient of variation, which is 0.005, suggesting very limited variability relative to the overall quantity being measured. Why can’t we just do this same thing with psychological measurements?\nThinking about this question takes us on a detour through the different kinds of measurement scales used in psychological research (Stevens 1946). The height measurements in our example are on what is known as a ratio scale: a scale in which numerical measurements are equally spaced and on which there is a true zero point. These scales are common for physical quantities but somewhat less frequent in psychology (with reaction times as a notable exception). More common are interval scales, in which there is no true zero point. For example, IQ (and other standardized scores) are intended to capture interval variation on some dimension but zero is meaningless—an IQ of zero does not correspond to any particular interpretation.4\n4 It can actually be shown in a suitably rigorous sense that ratio and interval scales (and another lying in between) are the only scales possible for the real numbers (Narens and Luce 1986).Ordinal scales are also often used. These are scales that are ordered but are not necessarily spaced equally. For example, levels of educational achievement (“elementary,” “high school,” “some college,” “college,” “graduate school”) are ordered, but there is no sense in which “high school” is as far from “elementary” as “graduate school” is from “college.” The last type in Stevens’s hierarchy is nominal scales, in which no ordering is possible either. For example, race is an unordered scale in which multiple categories are present but there is no inherent ordering of these categories. The hierarchy is shown in table 8.1.\n\n\n\nTable 8.1: Scale types and their associated operations and statistics (Stevens 1946).\n\n\n\n\n\n\n\n\n\n\n\nScale\nDefinition\nOperations\nStatistics\n\n\n\n\nNominal\nUnordered list\nEquality\nMode\n\n\nOrdinal\nOrdered list\nGreater than or less than\nMedian\n\n\nInterval\nNumerical\nEquality of intervals\nMean, SD\n\n\nRatio\nNumerical & zero\nEquality of ratios\nCoefficient of variation\n\n\n\n\n\n\nCritically, different summary measures work for each scale type. If you have an unordered list like a list of options for a question about race on a survey, you can present the modal response (the most likely one). It doesn’t even make sense to think about what the median was—there’s no ordering! For ordered levels of education, a median is possible but you can’t compute a mean. And for interval variables like “number of correct answers on a math test,” you can compute a mean and a standard deviation.5\n5 You might be tempted to think that “number of correct answers” is a ratio variable—but is zero really meaningful? Does it truly correspond to “no math knowledge” or is it just a stand-in for “less math knowledge than this test requires”?Now we’re ready to answer our initial question about why we can’t quantify reliability using the coefficient of variation. Unless you have a ratio scale with a true zero, you can’t compute a coefficient of variation. Think about it for IQ scores: currently, by convention, standardized IQ scores are set to have a mean of 100. If we tested someone multiple times and found the standard deviation of their test scores was four points, then we could estimate the precision of their measurements as “CV” of 4/100 = 0.04. But since IQ of zero isn’t meaningful, we could just set the mean IQ for the population to 200. Our test would be the same, and so the CV would be 4/200 = 0.02. On that logic, we just doubled the precision of our measurements by rescaling the test! That doesn’t make any sense.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nEarly controversies over psychological measurement\n\nPsychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of … measurement.\n—Cattel (1890, 373)\n\nIt is no coincidence that the founders of experimental psychology were obsessed with measurement (Heidelberger 2004). It was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science. For example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor) was a 1887 philosophical treatise titled “Zahlen und Messen” (“Counting and Measuring,” see Darrigol 2003). In the same year, Fechner (1987 [1887]) explicitly grappled with the foundations of measurement in “Über die psychischen Massprincipien” (“On psychic measurement principles”).\nMany of the early debates over measurement revolved around the emerging area of psychophysics, the problem of relating objective, physical stimuli (e.g., light, sound, pressure) to the subjective sensations they produce in the mind. For example, Fechner (1860) was interested in a quantity called the “just noticeable difference”—the smallest change in a stimulus that can be discriminated by our senses. He argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the intensity people reported (up to some constant). In other words, sensation was measurable via instruments like just noticeable difference.\nIt may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated. But this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative “measurements” of anything at all (Moscati 2018). It was unable to reach a conclusion, with physicists and psychologists deadlocked:\n\nHaving found that individual sensations have an order, they [some psychologists] assume that they are measurable. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws … which, if they mean anything, are certainly false.\n—Ferguson et al. (1940, 347)\n\nThe heart of the disagreement was rooted in the classical definition of quantity requiring a strictly additive structure. An attribute was only considered measurable in light of a meaningful concatenation operation. For example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rocks on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones). Norman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined fundamental measurement in this way (e.g., Campbell 1928), contrasting it with derived measurement, which involved computing some function based on one or more fundamental measures. According to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any fundamental scale with this kind of additive operation. It just didn’t make sense to break up people’s sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined (Cattell 1962). Even the intuitive additive logic of Donders’s (1969 [1868]) “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g., in an early textbook, Woodworth (1938, 83) claimed, “We cannot break up the reaction into successive acts and obtain the time for each act”).\nThe primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments. Exiled from classical frameworks of measurement, he went about developing an alternative “operational” framework (Stevens 1946), where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see table 8.1 above). Stevens’s framework quickly spread, leading to an explosion of proposed measures. However, operationalism remains controversial outside psychology (Michell 1999). The most extreme version of Stevens’s (1946, 677) stance (“Measurement … is defined as the assignment of numerals to objects or events according to rules”) permits researchers to define constructs operationally in terms of a measure (Hardcastle 1995). For example, one may say that the construct of intelligence is simply whatever it is that IQ measures. It is then left up to the researcher to decide which scale type their proposed measure should belong to.\nIn chapter 2, we outlined a somewhat different view, closer to a kind of constructive realism (Giere 2004; Putnam 2000). Psychological constructs like happiness are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct. In other words, we are not free to assign numbers however we like. Whether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.\nThe next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations. This effort culminated in the three-volume Foundations of Measurement series (Krantz et al. 1971; Suppes et al. 1989; Luce et al. 1990), which has become the canonical text for every psychology student seeking to understand measurement in the nonphysical sciences.  One of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) effects of constructs in conjunction with one another:\n\nWhen no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive.\n—Luce and Tukey (1964, 4)\n\nThis modern viewpoint broadly informs the view we describe here.\n\n\n\n\n\n8.1.2 Measuring reliability\nSo then, how do we measure signal and noise when we don’t have a true zero? We can still look at the variation between repeated measurement, but rather than comparing that variation between measurements to the mean, we can compare it to some other kind of variation—for example, variation between people. In what follows, we’ll discuss reliability on interval scales, but many of the same tools have been developed for ordinal and nominal scales.\nImagine that you are developing an instrument to measure some cognitive ability. We assume that every participant has a true ability, \\(t\\), just the same way that they have a true height in the example above. Every time we measure this true ability with our instrument, however, it gets messed up by some measurement error. Let’s specify that error is normally distributed with a mean of zero—so it doesn’t bias the measurements, it just adds noise. The result is our observed score, \\(o\\).6\n6 The approach we use to introduce this set of ideas is called classical test theory. There are other—more modern—alternative approaches, but CTT (as it’s called) is a good starting point for thinking through the concepts.Taking this approach, we could define a relative version of the coefficient of variation. The idea is that the reliability of a measurement is the amount of variance attributable to the true score variance (signal), rather than the observed score variance (which includes noise). If \\(\\sigma^2_t\\) is the variance of the true scores and \\(\\sigma^2_o\\) is the variance of the observed scores, then this ratio is: \\[\nR = \\frac{\\sigma^2_t}{\\sigma^2_o}.\n\\] When noise is high, then the denominator is going to be big and \\(R\\) will go down to 0; when noise is low, the numerator and the denominator will be almost the same and \\(R\\) will approach 1.\nThis all sounds great, except for one problem: we can’t compute reliability using this formula without knowing the true scores and their variance. But if we did, we wouldn’t need to measure anything at all!\nThere are two main approaches to computing reliability from data. Each of them makes an assumption that lets you circumvent the fundamental issue that we only have access to observed scores and not true scores. Let’s think these through in the context of a math test.\nTest-retest reliability. Imagine you have two parallel versions of your math test that are the same difficulty. Hence, you think a student’s score on either one will reflect the same true score, modulo some noise. In that case, you can use these two sets of observed scores (\\(o_1\\) and \\(o_2\\)) to compute the reliability of the instrument by simply computing the correlation between them (\\(\\rho_{o_1, o_2}\\)). The logic is that, if both variants reflect the same true score, then the shared variance (covariance in the sense of chapter 5) between them is just \\(\\sigma^2_t\\), the true score variance, which is the variable that we wanted but didn’t have. Test-retest reliability is thus a very convenient way to measure reliability (figure 8.4).\n\n\n\n\n\n\n\nFigure 8.4: Computing test-retest reliability.\n\n\n7 The problem is that each half is … half as long as the original instrument. To get around this, there is a correction called the Spearman-Brown correction that can be applied to estimate the expected correlation for the full-length instrument. You also want to make sure that the test doesn’t get harder from the beginning to the end. If it does, you may want to use the even-numbered and odd-numbered questions as the two parallel versions.Internal reliability. If you don’t have two parallel versions of the test, or you can’t give the test twice for whatever reason, then you have another option. Assuming you have multiple questions on your math test (which is a good idea!), then you can split the test in pieces and treat the scores from each of these subparts as parallel versions. The simplest way to do this is to split the instrument in half and compute the correlation between participants’ scores on the two halves—this quantity is called split half reliability.7\nAnother method for computing the internal reliability (the consistency of a test) is to treat each test item as a subinstrument and compute the average split-half correlation over all splits. This method yields the statistic Cronbach’s \\(\\alpha\\) (“alpha”). \\(\\alpha\\) is a widely reported statistic, but it is also widely misinterpreted (Sijtsma 2009). First, it is actually a lower bound on reliability rather than a good estimate of reliability itself. And second, it is often misinterpreted as evidence that an instrument yields scores that are “internally consistent,” which it does not; it’s not an accurate summary of dimensionality. \\(\\alpha\\) is a standard statistic, but it should be used with caution.\n\nOne final note: these tools often get used for observers’ ratings of the same stimulus (inter-rater or inter-annotator reliability), say for example when you have two coders rate how aggressive a person seems in a video. The most common measure of inter-annotator agreement is a categorical measure called Cohen’s \\(\\kappa\\) (“kappa”), for categorical agreement, but you can use intra-class correlation coefficients (see the Depth box below) for continuous data as well as many other measures.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nReliability paradoxes!\nThere’s a major issue with calculating reliabilities using the approaches we described here: because reliability is defined as a ratio of two measures of variation, it will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!\nOne way to define reliability formally is by using the intra-class correlation coefficient (ICC): \\[ICC = \\frac{\\sigma^2_b}{\\sigma^2_w + \\sigma^2_b}\\] where \\(\\sigma^2_w\\) is the within-subject variance in measurements and \\(\\sigma^2_b\\) is the between-subject variance in the measurements. (The denominator of the ICC comes from partitioning the total observed variance \\(\\sigma^2_o\\) in the reliability formula above.)\nSo now, instead of comparing variation to the mean, we’re comparing variation on one dimension (between participants) to total variation (within and between participants). ICCs are tricky, and there are several different flavors available depending on the structure of your data and what you’re trying to do with them. McGraw and Wong (1996) and Gwet (2014) provide extensive guidance on how to compute and interpret this statistic in different situations.\nLet’s think about the CDI data in our case study, which showed high reliability. Now imagine we restricted our sample to only change scores between 16–18-month-olds (our prior sample had 16–30-month-olds). Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change within a child (\\(\\sigma_w\\)) would be larger relative to the differences between children (\\(\\sigma_b\\)). That would make our reliability go down, even though we would be computing it on a subset of the exact same data.\nThat doesn’t sound so bad. But we can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually increase our estimate of split-half reliability (by increasing \\(\\sigma_b\\)). While the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox”—sloppier data collection can actually lead to higher reliabilities. \nWe need to be sensitive to the sources of variability we’re quantifying reliability over—both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs across individuals (from some population). Both of these sampling decisions affect reliability—if the population is more variable or the questions are less variable, we’ll get higher reliability. In sum, reliability is relative: reliability measures depend on the circumstances in which they are computed.\n\n\n\n\n\n8.1.3 Practical advice for computing reliability\nIf you don’t know the reliability of your measures for an experiment, you risk wasting your and your participants’ time. Ignorance is not bliss. A higher reliability measure will lead to more precise measurements of a causal effect of interest and, hence, smaller required sample sizes.\nTest-retest reliability is generally the most conservative practical measure of reliability. Test-retest estimates include not only measurement error but also participants’ state variation across different testing sessions and variance due to differences between versions of your instrument. These real-world quantities are absent from internal reliability estimates, which may make you erroneously think that there is more signal present in your instrument than there is.8 It’s hard work to measure test-retest reliability estimates, in part because you need two different versions of a test (to avoid memory effects). If you plan on using an instrument more than once or twice, though, it will likely be worthwhile!\n8 Even though \\(\\alpha\\) is a theoretical lower bound on reliability, in practice, test-retest accuracy often ends up lower than \\(\\alpha\\) because it incorporates all these other sources of variation.Finally, if you have multiple measurement items as part of your instrument, make sure you evaluate how they contribute to the reliability of the instrument. Perhaps you have several questions in a survey that you’d like to use to measure the same construct; perhaps multiple experimental vignettes that vary in content or difficulty. Some of these items may not contribute to your instrument’s reliability—and some may even detract. At a bare minimum, you should always visualize the distribution of responses across items to scan for floor and ceiling effects—when items always yield responses bunched at the bottom or top of the scale, limiting their usefulness—and take a look at whether there are particular items on which items do not relate to the others.\nIf you are thinking about developing an instrument that you use repeatedly, it may be useful to use more sophisticated psychometric models to estimate the dimensionality of responses on your instrument as well as the properties of the individual items. If your items have binary answers, like test questions, then item response theory is a good place to start (Embretson and Reise 2013). If your items are more like ratings on a continuous (interval or ratio) scale, then you may want to look at factor analysis and related methods (Furr 2021).\n\n\n\n\n\n\naccident report\n\n\n\n\n\nWasted effort\nLow-reliability measures limit your ability to detect correlations between measurements. Mike spent several fruitless months in graduate school running dozens of participants through batteries of language processing tasks and correlating the results across tasks. Every time data collection finished, one or the other (spurious) correlation would show up in the data analysis. Something was always correlated with something else. Thankfully, he would always attempt to replicate the correlation in a new sample—and in that next dataset, the correlation we were trying to replicate would be null but another (again likely spurious) correlation would show up.\nThis exercise was a waste of time because most of the tasks were of such low reliability that, even had they been highly correlated with one another, relationship would have been almost impossible to detect without a huge sample size. (It also would have been helpful if someone had mentioned multiplicity corrections [chapter 6] to him.)\nOne rule of thumb that’s helpful for individual difference designs of this sort is that the maximal correlation that can be observed between two variables \\(x\\) and \\(y\\) is the square root of the product of their reliabilities: \\(\\sqrt{r_x r_y}\\) . So if you have two measures that are reliable at 0.25, the maximal measured correlation between them is 0.25 as well! This kind of method is now frequently used in cognitive neuroscience (and other fields as well) to compute the so-called noise ceiling for a measure: the maximum amount of signal that in principle could be predicted (Lage-Castellanos et al. 2019). If your sample size is too small to detect correlations at the noise ceiling (see chapter 10), then the study is not worth doing.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#a-reliable-and-valid-measure-of-childrens-vocabulary",
    "href": "008-measurement.html#a-reliable-and-valid-measure-of-childrens-vocabulary",
    "title": "8  Measurement",
    "section": "",
    "text": "Anyone who has worked with little children, or had children of their own, can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes (Marchman and Fernald 2008). Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.\nBecause bringing children into a lab can be expensive, one popular alternative option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form that asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words. But is parent report a reliable or valid measure of children’s early language?\n\n\n\n\n\n\n\n\nFigure 8.2: Longitudinal (test-retest) correlations between a child’s score on one administration of the CDI and another one several months later. Based on Frank et al. (2021).\n\n\n\n\n\nAs we’ll see below, one way to measure the reliability of the CDI is to compute the correlation between two different administrations of the form for the same child. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! figure 8.2 displays these correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases (Frank et al. 2021).\n\nGiven that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? Bornstein and Haynes (1998) collected many different measures of children’s language—including the Early Language Inventory (ELI; an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. CDI scores were highly correlated with all the different measures, suggesting that the CDI was a valid measure of the construct.\nThe combination of reliability and validity evidence suggests that CDIs are a useful (and relatively inexpensive) source of data about children’s early language, and indeed they have become one of the most common assessments for this age group!",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#early-controversies-over-psychological-measurement",
    "href": "008-measurement.html#early-controversies-over-psychological-measurement",
    "title": "8  Measurement",
    "section": "Early controversies over psychological measurement",
    "text": "Early controversies over psychological measurement\n\nPsychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of … measurement.\n—Cattel (1890, 373)\n\nIt is no coincidence that the founders of experimental psychology were obsessed with measurement (Heidelberger 2004). It was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science. For example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor) was a 1887 philosophical treatise titled “Zahlen und Messen” (“Counting and Measuring,” see Darrigol 2003). In the same year, Fechner (1987 [1887]) explicitly grappled with the foundations of measurement in “Über die psychischen Massprincipien” (“On psychic measurement principles”).\nMany of the early debates over measurement revolved around the emerging area of psychophysics, the problem of relating objective, physical stimuli (e.g., light, sound, pressure) to the subjective sensations they produce in the mind. For example, Fechner (1860) was interested in a quantity called the “just noticeable difference”—the smallest change in a stimulus that can be discriminated by our senses. He argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the intensity people reported (up to some constant). In other words, sensation was measurable via instruments like just noticeable difference.\nIt may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated. But this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative “measurements” of anything at all (Moscati 2018). It was unable to reach a conclusion, with physicists and psychologists deadlocked:\n\nHaving found that individual sensations have an order, they [some psychologists] assume that they are measurable. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws … which, if they mean anything, are certainly false.\n—Ferguson et al. (1940, 347)\n\nThe heart of the disagreement was rooted in the classical definition of quantity requiring a strictly additive structure. An attribute was only considered measurable in light of a meaningful concatenation operation. For example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rocks on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones). Norman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined fundamental measurement in this way (e.g., Campbell 1928), contrasting it with derived measurement, which involved computing some function based on one or more fundamental measures. According to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any fundamental scale with this kind of additive operation. It just didn’t make sense to break up people’s sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined (Cattell 1962). Even the intuitive additive logic of Donders’s (1969 [1868]) “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g., in an early textbook, Woodworth (1938, 83) claimed, “We cannot break up the reaction into successive acts and obtain the time for each act”).\nThe primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments. Exiled from classical frameworks of measurement, he went about developing an alternative “operational” framework (Stevens 1946), where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see table 8.1 above). Stevens’s framework quickly spread, leading to an explosion of proposed measures. However, operationalism remains controversial outside psychology (Michell 1999). The most extreme version of Stevens’s (1946, 677) stance (“Measurement … is defined as the assignment of numerals to objects or events according to rules”) permits researchers to define constructs operationally in terms of a measure (Hardcastle 1995). For example, one may say that the construct of intelligence is simply whatever it is that IQ measures. It is then left up to the researcher to decide which scale type their proposed measure should belong to.\nIn chapter 2, we outlined a somewhat different view, closer to a kind of constructive realism (Giere 2004; Putnam 2000). Psychological constructs like happiness are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct. In other words, we are not free to assign numbers however we like. Whether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.\nThe next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations. This effort culminated in the three-volume Foundations of Measurement series (Krantz et al. 1971; Suppes et al. 1989; Luce et al. 1990), which has become the canonical text for every psychology student seeking to understand measurement in the nonphysical sciences.  One of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) effects of constructs in conjunction with one another:\n\nWhen no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive.\n—Luce and Tukey (1964, 4)\n\nThis modern viewpoint broadly informs the view we describe here.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#reliability-paradoxes",
    "href": "008-measurement.html#reliability-paradoxes",
    "title": "8  Measurement",
    "section": "Reliability paradoxes!",
    "text": "Reliability paradoxes!\nThere’s a major issue with calculating reliabilities using the approaches we described here: because reliability is defined as a ratio of two measures of variation, it will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!\nOne way to define reliability formally is by using the intra-class correlation coefficient (ICC): \\[ICC = \\frac{\\sigma^2_b}{\\sigma^2_w + \\sigma^2_b}\\] where \\(\\sigma^2_w\\) is the within-subject variance in measurements and \\(\\sigma^2_b\\) is the between-subject variance in the measurements. (The denominator of the ICC comes from partitioning the total observed variance \\(\\sigma^2_o\\) in the reliability formula above.)\nSo now, instead of comparing variation to the mean, we’re comparing variation on one dimension (between participants) to total variation (within and between participants). ICCs are tricky, and there are several different flavors available depending on the structure of your data and what you’re trying to do with them. McGraw and Wong (1996) and Gwet (2014) provide extensive guidance on how to compute and interpret this statistic in different situations.\nLet’s think about the CDI data in our case study, which showed high reliability. Now imagine we restricted our sample to only change scores between 16–18-month-olds (our prior sample had 16–30-month-olds). Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change within a child (\\(\\sigma_w\\)) would be larger relative to the differences between children (\\(\\sigma_b\\)). That would make our reliability go down, even though we would be computing it on a subset of the exact same data.\nThat doesn’t sound so bad. But we can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually increase our estimate of split-half reliability (by increasing \\(\\sigma_b\\)). While the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox”—sloppier data collection can actually lead to higher reliabilities. \nWe need to be sensitive to the sources of variability we’re quantifying reliability over—both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs across individuals (from some population). Both of these sampling decisions affect reliability—if the population is more variable or the questions are less variable, we’ll get higher reliability. In sum, reliability is relative: reliability measures depend on the circumstances in which they are computed.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#wasted-effort",
    "href": "008-measurement.html#wasted-effort",
    "title": "8  Measurement",
    "section": "Wasted effort",
    "text": "Wasted effort\nLow-reliability measures limit your ability to detect correlations between measurements. Mike spent several fruitless months in graduate school running dozens of participants through batteries of language processing tasks and correlating the results across tasks. Every time data collection finished, one or the other (spurious) correlation would show up in the data analysis. Something was always correlated with something else. Thankfully, he would always attempt to replicate the correlation in a new sample—and in that next dataset, the correlation we were trying to replicate would be null but another (again likely spurious) correlation would show up.\nThis exercise was a waste of time because most of the tasks were of such low reliability that, even had they been highly correlated with one another, relationship would have been almost impossible to detect without a huge sample size. (It also would have been helpful if someone had mentioned multiplicity corrections [chapter 6] to him.)\nOne rule of thumb that’s helpful for individual difference designs of this sort is that the maximal correlation that can be observed between two variables \\(x\\) and \\(y\\) is the square root of the product of their reliabilities: \\(\\sqrt{r_x r_y}\\) . So if you have two measures that are reliable at 0.25, the maximal measured correlation between them is 0.25 as well! This kind of method is now frequently used in cognitive neuroscience (and other fields as well) to compute the so-called noise ceiling for a measure: the maximum amount of signal that in principle could be predicted (Lage-Castellanos et al. 2019). If your sample size is too small to detect correlations at the noise ceiling (see chapter 10), then the study is not worth doing.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#validity",
    "href": "008-measurement.html#validity",
    "title": "8  Measurement",
    "section": "8.2 Validity",
    "text": "8.2 Validity\nIn chapter 2, we talked about the process of theory building as a process of describing the relationships between constructs. But for the theory to be tested, the constructs must be measured so that you can test the relationships between them! Measurement and measure construction is therefore intimately related to theory construction, and the notion of validity is central.9\n9 Some authors have treated “validity” as a broader notion that can include, for example, statistical issues (Shadish, Cook, and Campbell 2002). The sense of validity that we are interested in here is a bit more specific. We focus on construct validity, the relationship between the measure and the construct.10 This metaphor is a good rough guide but it doesn’t distinguish an instrument that is systematically biased (for example, by estimating scores too low for one group) and one that is invalid (because it measures the wrong construct).A valid instrument measures the construct of interest. In figure 8.1, invalidity is pictured as bias—the holes in the target are tightly grouped but in the wrong place.10 How can you tell if a measure is valid, given that the construct of interest is unobserved? There is no single test of the validity of a measure (Cronbach and Meehl 1955). Rather, the measure is valid if there is evidence that fits into the broader theory as it relates to the specific construct it is supposed to be measuring. For example, it should be strongly related to other measures of the construct, but not as related to measures of different constructs.\nHow do you establish that a measure fits into the broader theory? Validity of a measure is typically established via an argument that calls on different sources of support (Kane 1992). Here are some of the ways that you might support the relationship between a measure and a construct:\n\nFace validity: The measure looks like the construct, such that intuitively it is reasonable that it measures the construct. Face validity is a relatively weak source of evidence for validity, since it relies primarily on pretheoretic intuitions rather than any quantitative assessment. For example, reaction time is typically correlated with intelligence test results (e.g., Jensen and Munro 1979) but does not appear to be a face-valid measure of intelligence in that simply being fast doesn’t accord with our intuition about what it means to be intelligent!\nEcological validity: The measure relates to the context of people’s lives. For example, a rating of a child’s behavioral self-control in the classroom is a more ecologically valid measure of executive function than a reaction-time task administered in a lab context. Ecological validity arguments can be made on the basis of the experimental task, the stimuli, and the general setting of the experiment (Schmuckler 2001). Researchers differ in how much weight they assign to ecological validity based on their goals and their theoretical orientation.\nInternal validity: Usually used negatively. A “challenge to internal validity” is a description of a case where the measure is administered in such a way as to weaken the relationship between measure and construct. For example, if later items on a math test showed lower performance due to test-taker’s fatigue rather than lower knowledge of the concepts, the test might have an internal validity issue.11\nConvergent validity: The classic strategy for showing validity is to show that a measure relates (usually, correlates) with other putative measures of the same construct. When these relationships are measured concurrently, this is sometimes called concurrent validity. As we mentioned in chapter 2, self-reports of happiness relate to independent ratings by friends and family, suggesting that both measure the same underlying construct (Sandvik, Diener, and Seidlitz 1993).12\nPredictive validity. If the measure predicts other later measures of the construct, or related outcomes that might be of broader significance. Predictive validity is often used in lifespan and developmental studies where it is particularly prized for a measure to be able to predict meaningful life outcomes such as educational success in the future. For example, classroom self-control ratings (among other measures) appear strongly predictive of later life health and wealth outcomes (Moffitt et al. 2011).\nDivergent validity. If the measure can be shown to be distinct from measure(s) of a different construct, this evidence can help establish that the measure is specifically linked to the target construct. For example, measures of happiness (specifically, life satisfaction) can be distinguished from measures of optimism as well as both positive and negative affect, suggesting that these are distinct constructs (Lucas, Diener, and Suh 1996).\n\n11 Often this concept is described as being relevant to the validity of a manipulation also, for example, when the manipulation of the construct is confounded and some other psychological variable is manipulated as well. We discuss internal validity further in chapter 9.12 This idea of convergent validity relates to the idea of holism we described in chapter 2. A measure is valid if it relates to other valid measures, which themselves are only valid if the first one is! The measures are valid because the theory works, and the theory works because the measures are valid. This circularity is a difficult but perhaps unavoidable part of constructing psychological theories (see the Depth box above on the history of measurement). We don’t ever have an objective starting point for the study of the human mind.\n8.2.1 Validity arguments in practice\nLet’s take a look at how we might make an argument about the validity of the CDI, the vocabulary instrument from our case study.\nFirst, the CDI is face valid—it is clearly about early language ability. In contrast, even though a child’s height would likely be correlated with their early language ability, we should be skeptical of this measure due to its lack of face validity. In addition, the CDI shows good convergent and predictive validity. Concurrently, the CDI correlates well with evidence from transcripts of children’s actual speech and from standardized language assessments (as discussed in the case study above). And predictively, CDI scores at age two relate to reading scores during elementary school (Marchman and Fernald 2008).\nOn the other hand, users of the CDI must avoid challenges to the internal validity of the data they collect. For example, some CDI data are compromised by confusing instructions or poor data collection processes (Frank et al. 2021). Further, advocates and critics of the CDI argue about its ecological validity. There is something quite ecologically valid about asking parents and caregivers—who are experts on their own child—to report on their child’s abilities. On the other hand, the actual experience of filling out a structured form estimating language ability might be more familiar to some families from higher education backgrounds than for others from lower education backgrounds. Thus, a critic could reasonably say that comparisons of CDI scores across socioeconomic strata would be an invalid usage (Feldman et al. 2000).\n\n\n8.2.2 Avoid questionable measurement practices!\nExperimentalists sometimes have a tendency to make up ad hoc measures on the fly. It’s fine to invent new measures, but the next step is to think about what evidence there is that it’s valid! Table 8.2 gives a set of questions to guide thoughtful reporting of measurement practices (adapted from Flake and Fried 2020).\n\n\n\nTable 8.2: Questions about measurement that every reseacher should answer in their paper. Adapted from Flake and Fried (2020).\n\n\n\n\n\n\n\n\n\nQuestion\nInformation to Report\n\n\n\n\nWhat is your construct?\nDefine construct, describe theory and research.\n\n\nWhat measure did you use to operationalize your construct?\nDescribe measure and justify operationalization.\n\n\nDid you select your measure from the literature or create it from scratch?\nJustify measure selection and review evidence on reliability and validity (or disclose the lack of such evidence).\n\n\nDid you modify your measure during the process?\nDescribe and justify any modifications; note whether they occurred before or after data collection.\n\n\nHow did you quantify your measure?\nDescribe decisions underlying the calculation of scores on the measure; note whether these were established before or after data collection and whether they are based on standards from previous literature.\n\n\n\n\n\n\nOne big issue to be careful about is that researchers have been known to modify their scales and their scale scoring practices (say, omitting items from a survey or rescaling responses) after data collection. This kind of post hoc alteration of the measurement instrument can sometimes be justified by features of the data, but it can also look a lot like \\(p\\)-hacking! If researchers modify their measurement strategy after seeing their data, this decision needs to be disclosed, and it may undermine their statistical inferences.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nTalk about flexible measurement!\nThe competitive reaction time task (CRTT) is a lab-based measure of aggression. Participants are told that they are playing a reaction-time game against another player and are asked to set the parameters of a noise blast that will be played to their opponent. Unfortunately, in an analysis of the literature using CRTT, Elson et al. (2014) found that different papers using the CRTT use dramatically different methods for scoring the task. Sometimes the analysis focused on the volume of the noise blast and sometimes it focused on the duration. Sometimes these scores were transformed (via logarithms) or thresholded. Sometimes they were combined into a single score. Elson was so worried by this flexibility that he created a website, https://flexiblemeasures.com, to document the variation he observed.\n\nAs of 2016, Elson had found 130 papers using the CRTT. And across these papers, he documented an astonishing 157 quantification strategies. One paper reported ten different strategies for extracting numbers from this measure! More worrisome still, Elson and colleagues found that when they tried out some of these strategies on their own data, different strategies led to very different effect sizes and levels of statistical significance. They could effectively make a finding appear bigger or smaller depending on which scoring they chose.\nTriangulating a construct through multiple prespecified measurements can be a good thing. But the issue with the CRTT analysis was that changes in the measurement strategy appeared to be made in a post hoc, data-driven way so as to maximize the significance of the experimental manipulation (just like the \\(p\\)-hacking we discussed in chapters 3 and 6).\nThis examination of the use of the CRTT measure has several implications. First, and most troublingly, there may have been undisclosed flexibility in the analysis of CRTT data across the literature, with investigators taking advantage of the lack of standardization to try many different analysis variants and report the one most favorable to their own hypothesis. Second, it is unknown which quantification of CRTT behavior is in fact most reliable and valid. Since some of these variants are presumably better than others, researchers are effectively “leaving money on the table” by using suboptimal quantifications. As a consequence, if researchers adopt the CRTT, they find much less guidance from the literature on what quantification to adopt.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#talk-about-flexible-measurement",
    "href": "008-measurement.html#talk-about-flexible-measurement",
    "title": "8  Measurement",
    "section": "Talk about flexible measurement!",
    "text": "Talk about flexible measurement!\nThe competitive reaction time task (CRTT) is a lab-based measure of aggression. Participants are told that they are playing a reaction-time game against another player and are asked to set the parameters of a noise blast that will be played to their opponent. Unfortunately, in an analysis of the literature using CRTT, Elson et al. (2014) found that different papers using the CRTT use dramatically different methods for scoring the task. Sometimes the analysis focused on the volume of the noise blast and sometimes it focused on the duration. Sometimes these scores were transformed (via logarithms) or thresholded. Sometimes they were combined into a single score. Elson was so worried by this flexibility that he created a website, https://flexiblemeasures.com, to document the variation he observed.\n\nAs of 2016, Elson had found 130 papers using the CRTT. And across these papers, he documented an astonishing 157 quantification strategies. One paper reported ten different strategies for extracting numbers from this measure! More worrisome still, Elson and colleagues found that when they tried out some of these strategies on their own data, different strategies led to very different effect sizes and levels of statistical significance. They could effectively make a finding appear bigger or smaller depending on which scoring they chose.\nTriangulating a construct through multiple prespecified measurements can be a good thing. But the issue with the CRTT analysis was that changes in the measurement strategy appeared to be made in a post hoc, data-driven way so as to maximize the significance of the experimental manipulation (just like the \\(p\\)-hacking we discussed in chapters 3 and 6).\nThis examination of the use of the CRTT measure has several implications. First, and most troublingly, there may have been undisclosed flexibility in the analysis of CRTT data across the literature, with investigators taking advantage of the lack of standardization to try many different analysis variants and report the one most favorable to their own hypothesis. Second, it is unknown which quantification of CRTT behavior is in fact most reliable and valid. Since some of these variants are presumably better than others, researchers are effectively “leaving money on the table” by using suboptimal quantifications. As a consequence, if researchers adopt the CRTT, they find much less guidance from the literature on what quantification to adopt.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#how-to-select-a-good-measure",
    "href": "008-measurement.html#how-to-select-a-good-measure",
    "title": "8  Measurement",
    "section": "8.3 How to select a good measure?",
    "text": "8.3 How to select a good measure?\nIdeally you want a measure that is reliable and valid. How do you get one? An important first principle is to use a preexisting measure. Perhaps someone else has done the hard work of compiling evidence on reliability and validity, and in that case, you will most likely want to piggyback on that work. Standardized measures are typically broad in their application, and so the tendency can be to discard these because they are not tailored for our studies specifically. But the benefits of a standardized measure are substantial. Not only can you justify the measure using the prior literature, but you also have an important index of population variability by comparing absolute scores to other reports.13\n13 Comparing absolute measurements is a really important trick for “sanity-checking” your data. If your measurements are very different than the ones in the paper you’re following up (for example, if reaction times are much longer or shorter, or if accuracies on a test are much higher or lower), that may be a signal that something has gone wrong.If you don’t use someone else’s measure, you’ll need to make one up yourself. Most experimenters go down this route at some point, but if you do, remember that you will need to figure out how to estimate its reliability and also how to make an argument for its validity!\nWe can assign numbers to almost anything people do. We could run an experiment on children’s exploratory play and count the number of times they interact with another child (Ross and Lollis 1989), or run an experiment on aggression where we quantify the amount of hot sauce participants serve (Lieberman et al. 1999). Yet, most of the time we choose from a relatively small set of operational variables: asking survey questions, collecting choices and reaction times, and measuring physiological variables like eye movements. Besides following these conventions, how do we choose the right measurement type for a particular experiment?\nThere’s no hard and fast rule about what aspect of behavior to measure, but here we will focus on two dimensions that can help us organize the broad space of possible measure targets.14 The first of these is the continuum between simple and complex behaviors. The second is the focus on explicit, voluntary behaviors vs implicit or involuntary behaviors.\n14 Some authors differentiate between “self-report” and “observational” measures. This distinction seems simple on its face but actually gets kind of complicated. Is a facial expression a “self-report”? Language is not the only way that people communicate with one another—many actions are intended to be communicative (Shafto, Goodman, and Frank 2012).\n8.3.1 Simple vs complex behaviors\nFigure 8.5 shows a continuum between simple and complex behaviors. The simplest measurable behaviors tend to be button presses, for example:\n\npressing a key to advance to the next word in a word-by-word self-paced reading study;\nselecting “yes” or “no” in a lexical decision task; and\nmaking a forced choice between different alternatives to indicate which has been seen before.\n\n\n\n\n\n\n\nFigure 8.5: Often choosing a measure can be consolidated into a choice along a continuum from simple measures that provide a small amount of information but are quick and easy to repeat and those that provide much richer information but require more time.\n\n\n\nThese specific measures—and many more like them—are the bread and butter of many cognitive psychology studies. Because they are quick and easy to explain, these tasks can be repeated over many trials. They can also be executed with a wider variety of populations including with young children and sometimes even with nonhuman animals with appropriate adaptation. (A further benefit of these paradigms is that they can yield useful reaction time data, which we discuss further below.)\nIn contrast, a huge range of complex behaviors have been studied by psychologists, including:\n\nopen-ended verbal interviews;\nwritten expression, for example, via handwriting or writing style;\nbody movements, including gestures, walking, or dance; and\ndrawing or artifact building.\n\nThere are many reasons to study these kinds of behaviors. First, the behaviors themselves may be examples of tasks of interest (e.g., studies of drawing that seek to understand the origins of artistic expression). Or, the behavior may stand in for other even more complex behaviors of interest, as in studies of typing that use this behavior as a proxy for lexical knowledge (Rumelhart and Norman 1982).\nComplex behaviors typically afford a huge variety of different measurement strategies. So any experiment that uses a particular measurement of a complex behavior will typically need to do significant work up front to justify the choice of that measurement strategy—for example, how to quantify dances or gestures or typing errors—and provide some assurance about its reliability. Further, it is often much more difficult to have a participant repeat a complex behavior many times under the same conditions. Imagine asking someone to draw hundreds of sketches as opposed to pressing a key hundreds of times! Thus, the choice of a complex behavior is often a choice to forego a large number of simple trials for a small number of more complex trials.\nComplex behaviors can be especially useful to study either at the beginning or the end of a set of experiments. At the beginning of a set of experiments, they can provide inspiration about the richness of the target behavior and insight into the many factors that influence it. And at the end, they can provide an ecologically valid measure to complement a reliable but more artificial lab-based behavior.\nThe more complex the behavior, however, the more it will vary across individuals and the more environmental and situational factors will affect it. These can be important parts of the phenomenon, but they can also be nuisances that are difficult to get under experimental control.15 Simple measures are typically easier to use and, hence, easier to deploy repeatedly in a set of experiments where you iterate your manipulation to test a causal theory.\n15 When they are not designed with care, complex, open-ended behaviors such as verbal interviews can be especially affected by the experimental biases that we will describe in chapter 9, including, for example, demand characteristics, in which participants say what they think experimenters want to hear. Qualitative interview methods can be incredibly powerful as a method in their own right, but they should be deployed with care as measures for an experimental intervention.\n\n8.3.2 Implicit vs explicit behaviors\nA second important dimension of organization for measures is the difference between implicit and explicit measures. An explicit measure provides a measurement of a behavior that a participant has conscious awareness of—for example, the answer to a question. In contrast, implicit measures provide measurements of psychological processes that participants are unable to report (or occasionally, unwilling to).16 Implicit measures, especially reaction time, have long been argued to reflect internal psychological processes (Donders 1969 [1868]). They also have been proposed as measures of qualities such as racial bias that participants may have motivation not to disclose (Greenwald, McGhee, and Schwartz 1998). There are also of course a host of physiological measurements available. Some of these measure eye movements, heart rate, or skin conductance, which can be linked to aspects of cognitive process. Others reflect underlying brain activity via the signals associated with MRI (magnetic resonance imaging), MEG (magnetoencephalography), NIRS (near-infrared spectroscopy), and EEG (electroencephalogram) measurements. These methods are outside the scope of this book, though we note that the measurement concerns we discuss here definitely apply (e.g., Zuo, Xu, and Milham 2019).\n16 Implicit/explicit is likely more of a continuum, but one cut-point is whether the participants’ behavior is considered intentional: that is, participants intend to press a key to register a decision, but they likely do not intend to react in 300 as opposed to 350 milliseconds due to having seen a prime.17 One way of describing the information processing underlying this trade-off is given by drift diffusion models (DDMs), which allow joint analysis of accuracy and reaction time (Voss, Nagler, and Lerche 2013). Used appropriately, DDMs can provide a way to remove speed-accuracy trade-offs and extract more reliable signals from tasks where accuracy and reaction time are both measured (see Johnson et al. 2017 for an example of a DDM on a weapon-decision task).Many tasks produce both accuracy and reaction time data. Often these trade off with one another in a classic speed-accuracy trade-off: the faster participants respond, the less accurate they are. For example, to investigate racial bias in policing, Payne (2001) showed US college students a series of pictures of tools and guns, proceeded by a prime of either a White face or a Black face. In a first study, participants were faster to identify weapons when primed by a Black face but had similar accuracies. A second study added a response deadline to speed up judgments: this manipulation resulted in equal reaction times across conditions but greater errors in weapon identification after Black faces. These studies likely revealed the same phenomenon—some sort of bias to associate Black faces with weapons—but the design of the task moved participants along a speed-accuracy trade-off, yielding effects on different measures.17\nSimple, explicit behaviors are often a good starting point. Work using these measures—often the least ecologically valid—can be enriched with implicit measures or measurements of more complex behaviors.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#the-temptation-to-measure-lots-of-things",
    "href": "008-measurement.html#the-temptation-to-measure-lots-of-things",
    "title": "8  Measurement",
    "section": "8.4 The temptation to measure lots of things",
    "text": "8.4 The temptation to measure lots of things\nIf one measure is good, shouldn’t two be better? Many experimenters add multiple measurements to their experiments, reasoning that more data is better than less. But that’s not always true!\nDeciding whether to include multiple measures is an aesthetic and practical issue as well as a scientific one. Throughout this book, we have been advocating for a viewpoint in which experiments should be as simple as possible. For us, the best experiment is one that shows that a simple and valid manipulation affects a single measure that is reliable and valid.18 If you are tempted to include more than one measure, see if we can talk you out of it.19\n18 In an entertaining article called “Things I Have Learned (So Far),” Cohen (1990) quips that he leans so far in the direction of large numbers of observations and small numbers of measures that some students think his perfect study has 10,000 participants and no measures.19 As usual, we want to qualify that we are only talking about randomized experiments here! In observational studies, often the point is to measure the associations between multiple measures, so you typically have to include more than one. Additionally, some of the authors of this book have advocated for measuring multiple outcomes in longitudinal observational studies, which could reduce investigator bias, encourage reporting null effects, enable comparison of effect sizes, and improve research efficiency (VanderWeele, Mathur, and Chen 2020). We’ve also done plenty of descriptive studies—these can be very valuable. In a descriptive context, often the goal is to include as many measures as possible so as to have a holistic picture of the phenomenon of interest.First, make sure that including more measures doesn’t compromise each individual measure. This can happen via fatigue or carryover effects. For example, if a brief attitude manipulation is followed by multiple questionnaire measures, it is a good bet that there is likely to be “fade-out” of the effect over time, so it won’t have the same effect on the first questionnaire as the last one. Further, even if a manipulation has a long duration effect on participants, survey fatigue may lead to less meaningful responses to later questions (Herzog and Bachman 1981).\nSecond, consider whether you have a strong prediction for each measure, or whether you’re just looking for more ways to see an effect of your manipulation. As discussed in chapter 2, we think of an experiment as a “bet.” The more measures you add, the more bets you are making but the less value you are putting on each. In essence, you are “hedging your bets,” and so the success of any one bet is less convincing.\nThird, if you include multiple measures in your experiment, you need to think about how you will interpret inconsistent results. Imagine you have experimental participants engage in a brief written reflection that is hypothesized to affect a construct (vs a control writing exercise, say listing meals). If you include two measures of the construct of interest and one shows a larger effect, what will you conclude? It may be tempting to assume that the one that shows a larger effect is the “better measure,” but the logic is circular—it’s only better if the manipulation affected the construct of interest, which is what you were testing in the first place! Including multiple measures because you’re uncertain which one is more related to the construct indulges in this circular logic, since the experiment often can’t resolve the situation. A much better move in this case is to do a preliminary study of the reliability and validity of the two measures so as to be able to select one as the experiment’s primary endpoint.20\n20 One caveat to this argument is that it can sometimes be useful to examine the effects of a manipulation on different measures because the measures are important. For example, you might be interested in whether an educational intervention increased grades and decreased dropout rates. Both outcome measures are important, and so it is useful to include both in your study.Finally, if you do include multiple measures, selective reporting of significant or hypothesis-aligned measures becomes a real risk. For this reason, preregistration and transparent reporting of all measures becomes even more important.\nThere are some cases where more measures are better. The more expensive the experiment, the less likely it is to be repeated to gather a new measurement of the effects of the same manipulation. Thus, larger studies present a stronger rationale for including multiple measures. Clinical trials often involve interventions that can have effects on many different measures; imagine a cancer treatment that might affect mortality rates, quality of life, tumor growth rates, and other measures. Further, such trials are extremely expensive and difficult to repeat. Thus, there is a good reason for including more measures in such studies.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nSurvey measures\nSometimes the easiest way to elicit information from participants is simply to ask. Surveys are an important part of experimental measurement, so we’ll share a few best practices, primarily derived from Krosnick and Presser (2010).\nTreat survey questions as a conversation. The easier your items are to understand, the better. Don’t repeat variations on the same question unless you want different answers! Try to make the order reasonable, for example, by grouping together questions about the same topic and moving from more general to more specific questions. The more you include “tricky” items, the more you invite tricky answers to straightforward questions. One specific kind of tricky questions are “check” questions that evaluate participant compliance. We’ll talk more in chapter 12 about various ways of evaluating compliance and their strengths and weaknesses.\nOpen-ended survey questions can be quite rich and informative, especially when an appropriate coding (classification) scheme is developed in advance and responses are categorized into a relatively small number of types. On the other hand, they present practical obstacles because they require coding (often by multiple coders to ensure reliability of the coding). Further, they tend to yield nominal data, which are often less useful for quantitative theorizing. Open-ended questions are a useful tool to add nuance and color to the interpretation of an experiment.\nOne common mistake that survey developers make is trying to put too much into one question. Imagine asking a restaurant-goer for a numerical ranking on the question, “How do you like our food and service?” What if they loved the food but hated the service, or vice versa? Would they choose an intermediate option? Items that ask about more than one thing at once are known as double-barreled questions. They can confuse and frustrate participants as well as lead to uninterpretable data.\n\n\n\n\n\n\nFigure 8.6: Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled.\n\n\n\nEspecially given their ubiquity in commercial survey research, Likert scales—scales with a fixed number of ordered, numerical response options—are a simple and conventional way of gathering data on attitude and judgment questions (figure 8.6). Bipolar scales are those in which the end points represent opposites: for example the continuum between “strongly dislike” and “strongly like.” Unipolar scales have one neutral endpoint, like the continuum between “no pain” and “very intense pain.” Survey methods research suggests that reliability is maximized when bipolar scales have seven points and unipolar scales have five. Labeling every point on the scale with verbal labels is preferable to labeling only the endpoints.\nOne important question is whether to treat data from Likert scales as ordinal or interval. It’s extremely common (and convenient) to make the assumption that Likert ratings are interval, allowing the use of standard statistical tools like means, standard deviations, and linear regression. The risk in this practice comes from the possibility that scale items are not evenly spaced—for example, on a scale labeled “never,” “seldom,” “occasionally,” “often,” and “always,” the distance from “often” to “always” may be larger than the distance from “seldom” to “occasionally.”\nIn practice, you can choose to use regression variants that are appropriate, for example, ordinal logistic regression and its variants, or they can attempt to assess and mitigate the risks of treating the data as interval. If you choose the second option, it’s definitely a good idea to look carefully at the raw distributions for individual items to see if their distribution appears approximately normal (see chapter 15).\nRecently some researchers have begun to use “visual analog scales” (or sliders) as a solution. We don’t recommend these—the distribution of the resulting data is often anchored at the starting point or endpoints (Matejka et al. 2016), and a meta-analysis shows they’re a lot lower than Likert scales in reliability (Krosnick and Presser 2010).\nIt rarely helps matters to add an “I don’t know” or “other” option to survey questions. These are some of a variety of practices that encourage satisficing, where survey takers give answers that are good enough but don’t reflect substantial thought about the question. Another behavior that results from satisficing is “straight-lining”—that is, picking the same option for every question. In general, the best way to prevent straight-lining is to make surveys relatively short, engaging, and well-compensated. The practice of “reverse coding” to make the expected answers to some questions more negative can block straight-lining, but at the cost of making items more confusing. Some obvious formatting options can reduce straight-lining as well: for example, placing scales further apart or on subsequent (web) pages.\nIn sum, survey questions can be a helpful tool for eliciting graded judgments about explicit questions. The best way to execute them well is to try and make them as clear and easy to answer as possible.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#survey-measures",
    "href": "008-measurement.html#survey-measures",
    "title": "8  Measurement",
    "section": "Survey measures",
    "text": "Survey measures\nSometimes the easiest way to elicit information from participants is simply to ask. Surveys are an important part of experimental measurement, so we’ll share a few best practices, primarily derived from Krosnick and Presser (2010).\nTreat survey questions as a conversation. The easier your items are to understand, the better. Don’t repeat variations on the same question unless you want different answers! Try to make the order reasonable, for example, by grouping together questions about the same topic and moving from more general to more specific questions. The more you include “tricky” items, the more you invite tricky answers to straightforward questions. One specific kind of tricky questions are “check” questions that evaluate participant compliance. We’ll talk more in chapter 12 about various ways of evaluating compliance and their strengths and weaknesses.\nOpen-ended survey questions can be quite rich and informative, especially when an appropriate coding (classification) scheme is developed in advance and responses are categorized into a relatively small number of types. On the other hand, they present practical obstacles because they require coding (often by multiple coders to ensure reliability of the coding). Further, they tend to yield nominal data, which are often less useful for quantitative theorizing. Open-ended questions are a useful tool to add nuance and color to the interpretation of an experiment.\nOne common mistake that survey developers make is trying to put too much into one question. Imagine asking a restaurant-goer for a numerical ranking on the question, “How do you like our food and service?” What if they loved the food but hated the service, or vice versa? Would they choose an intermediate option? Items that ask about more than one thing at once are known as double-barreled questions. They can confuse and frustrate participants as well as lead to uninterpretable data.\n\n\n\n\n\n\nFigure 8.6: Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled.\n\n\n\nEspecially given their ubiquity in commercial survey research, Likert scales—scales with a fixed number of ordered, numerical response options—are a simple and conventional way of gathering data on attitude and judgment questions (figure 8.6). Bipolar scales are those in which the end points represent opposites: for example the continuum between “strongly dislike” and “strongly like.” Unipolar scales have one neutral endpoint, like the continuum between “no pain” and “very intense pain.” Survey methods research suggests that reliability is maximized when bipolar scales have seven points and unipolar scales have five. Labeling every point on the scale with verbal labels is preferable to labeling only the endpoints.\nOne important question is whether to treat data from Likert scales as ordinal or interval. It’s extremely common (and convenient) to make the assumption that Likert ratings are interval, allowing the use of standard statistical tools like means, standard deviations, and linear regression. The risk in this practice comes from the possibility that scale items are not evenly spaced—for example, on a scale labeled “never,” “seldom,” “occasionally,” “often,” and “always,” the distance from “often” to “always” may be larger than the distance from “seldom” to “occasionally.”\nIn practice, you can choose to use regression variants that are appropriate, for example, ordinal logistic regression and its variants, or they can attempt to assess and mitigate the risks of treating the data as interval. If you choose the second option, it’s definitely a good idea to look carefully at the raw distributions for individual items to see if their distribution appears approximately normal (see chapter 15).\nRecently some researchers have begun to use “visual analog scales” (or sliders) as a solution. We don’t recommend these—the distribution of the resulting data is often anchored at the starting point or endpoints (Matejka et al. 2016), and a meta-analysis shows they’re a lot lower than Likert scales in reliability (Krosnick and Presser 2010).\nIt rarely helps matters to add an “I don’t know” or “other” option to survey questions. These are some of a variety of practices that encourage satisficing, where survey takers give answers that are good enough but don’t reflect substantial thought about the question. Another behavior that results from satisficing is “straight-lining”—that is, picking the same option for every question. In general, the best way to prevent straight-lining is to make surveys relatively short, engaging, and well-compensated. The practice of “reverse coding” to make the expected answers to some questions more negative can block straight-lining, but at the cost of making items more confusing. Some obvious formatting options can reduce straight-lining as well: for example, placing scales further apart or on subsequent (web) pages.\nIn sum, survey questions can be a helpful tool for eliciting graded judgments about explicit questions. The best way to execute them well is to try and make them as clear and easy to answer as possible.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "008-measurement.html#chapter-summary-measurement",
    "href": "008-measurement.html#chapter-summary-measurement",
    "title": "8  Measurement",
    "section": "8.5 Chapter summary: Measurement",
    "text": "8.5 Chapter summary: Measurement\nIn olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians thought a lot about how different problems on tests had different measurement properties. They began exploring how to select good and bad items, and how to figure out people’s ability abstracted away from specific items. This research led to a profusion of interesting ideas about measurement, but these ideas rarely percolated into day-to-day practice in other areas of psychology. For example, cognitive psychologists collected lots of trials and measured quantities of interest with high precision, but they worried less about measurement validity. Social psychologists spent more time worrying about issues of ecological validity in their experiments, but they often used ad hoc scales with poor psychometric properties.\nThese sociological differences between fields have led to an unfortunate divergence, where experimentalists often don’t recognize the value of the conceptual tools developed to aid measurement, and so fail to reason about the reliability and validity of their measures in ways that can help them make better inferences. As we said in our discussion of reliability, ignorance is not bliss. Much better to think these choices through!\n\n\n\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nLet’s go back to our example on the relationship between money and happiness from chapter 1. How many different kinds of measures of happiness can you come up with? Make a list with at least five.\nChoose one of your measures of happiness and come up with a validation strategy for it, making reference to at least three different types of validity. What data collection would this validation effort require?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA classic textbook on psychometrics that introduces the concepts of reliability and validity in a simple and readable way: Furr, R. Michael (2021). Psychometrics: An Introduction. SAGE publications.\nA great primer on questionnaire design: Krosnick, Jon A. (2018). “Improving Question Design to Maximize Reliability and Validity.” In The Palgrave Handbook of Survey Research, edited by David L. Vannette and Jon A. Krosnick, 95–101. Springer. https://doi.org/10.1007/978-3-319-54395-6_13.\nIntroduction to general issues in measurement and why they shouldn’t be ignored: Flake, Jessica Kay, and Eiko I. Fried (2020). “Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them.” Advances in Methods and Practices in Psychological Science 3 (4): 456–465. https://doi.org/10.1177/2515245920952393.\nAn accessible popular book on scientific measurement: Vincent, James (2022). Beyond Measure: The Hidden History of Measurement from Cubits to Quantum Constants. Faber & Faber.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Measurement</span>"
    ]
  },
  {
    "objectID": "009-design.html",
    "href": "009-design.html",
    "title": "9  Design",
    "section": "",
    "text": "9.1 Experimental designs\nExperimental designs are fundamental to many fields; unfortunately, the terminology used to describe them can vary, which can get quite confusing! Here we will mostly describe an experiment as a relationship between some manipulation(s), in which participants are randomly assigned to experimental conditions to estimate effects on some measure. Factors are the dimensions along which manipulations vary. For example, in our case study above, the two factors were participant belief and agent belief. Another terminology it’s good to be familiar with is the terms used in chapters 5–7, which are often used in econometrics and statistics: treatment (manipulation) and outcome (measure).2\nIn this section, we’ll discuss key dimensions on which experiments vary: (1) how many factors they incorporate and how these factors are crossed; (2) how many conditions and measures are given to each participant; and (3) if manipulations have discrete levels or fall on a continuous scale.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#automatic-theory-of-mind",
    "href": "009-design.html#automatic-theory-of-mind",
    "title": "9  Design",
    "section": "Automatic theory of mind?",
    "text": "Automatic theory of mind?\nIn an early version of our course, student Desmond Ong set out to replicate a thought-provoking finding: both infants and adults seemed to show evidence of tracking other agents’ belief state, even when it was irrelevant to the task at hand (Kovács, Téglás, and Endress 2010). In the paradigm, an animated Smurf character would watch as a self-propelled ball came in and out from behind a screen. At the end of the video, the screen would swing down and the participant had to respond whether the ball was present or absent. Reaction time for this decision was the key dependent variable.\nThe experimental design investigated two factors: whether the participant believed the ball was present or absent (P+/P-) and whether the animated agent would have believed the ball was present or absent (A+/A-) based on what it saw. The result was four conditions: P+/A+, P+/A-, P-/A+, and P-/A-. (We could call this a fully-crossed design because each level of one factor was presented with each level of the other.)\n\n\n\n\n\n\n\n\nFigure 9.1: Original data from Kovács, Téglás, and Endress (2010). Error bars show 95% confidence intervals. Based on Phillips et al. (2015).\n\n\n\n\n\nBoth the original experiments and the replication that Desmond ran showed a significant effect of the agent’s beliefs on participants’ reaction times, suggesting that what the—totally irrelevant—agent thought about the ball was leading them to react more or less quickly to the presence of the ball. Figure 9.1 shows the original data (\\(N = 24\\)). But, although both studies showed an effect of agent belief, the replication and several variations also showed a crossover interaction of participant and agent belief. The participants were slower when the agents and the participants believed that the ball was behind the screen (figure 9.2). That finding wasn’t consistent with the theory that tracking inconsistent beliefs slowed down reaction times. If participants were tracking their own beliefs about the ball and the agent’s, they should have been fastest in the P+/A+ condition, not slower.\n\n\n\n\n\n\n\n\nFigure 9.2: Data from a series of replications of Kovács, Téglás, and Endress (2010), including versions on the web (experiments 1a and 1b) and in lab (experiment 1c), as well as several variations on the format of responding (experiments 2 and 3; 2AFC = two alternative forced choice) and an experiment where a large wall kept the agent from seeing the ball at all (experiment 4). “Hits” and “CRs” panels refer to different subsets of trials where participants responded “present” when the ball was present and “absent” when the ball was absent. Error bars are 95% confidence intervals. Based on Phillips et al. (2015).\n\n\n\n\n\nA collaborative team working on this paradigm identified a key issue (Phillips et al. 2015). There was a confound in the experimental design—another factor that varied across conditions besides the target factors. In other words, something was changing between conditions other than the agent’s and participant’s belief states. The confound was an attention check (discussed further in chapter 12): participants had to press a key when the agent left the scene to show that they were paying attention. This attention check appeared a few seconds later in the videos for the P+/A+ and P-/A- trials—the ones that yielded the slow reaction times—than it did for the other two. When the attention check was removed or when its timing was equalized across conditions, reaction time effects were eliminated, suggesting that the original pattern of findings may have been due to the confound.\nIf the standard for replication is significance of particular statistical tests at \\(p &lt; 0.05\\), then this experiment replicated successfully. But the effect estimates were inconsistent with the proposed theoretical explanation. A finding can be replicable without providing support for the underlying theory!\nThere’s an important caveat to this story. The followup work only revealed that there was a confound in one particular experimental operationalization and did not provide evidence against automatic theory of mind in general. Indeed, others have suggested that different versions of this paradigm do reveal evidence for theory of mind processing once the confound is eliminated (El Kaddouri et al. 2020).",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#experimental-designs",
    "href": "009-design.html#experimental-designs",
    "title": "9  Design",
    "section": "Estimation strategies for generalized factorial designs",
    "text": "2 Terminology here is hard. In psychology people sometimes say there’s an independent variable (the manipulation, which is causally prior and hence “independent” of other causal influences) and a dependent variable (the measure, which causally depends on the manipulation, or so we hypothesize). We find this terminology to be hard to remember because the terms are so different from the actual concepts being described.\n\n9.1.1 A two-factor experiment\nThe classical “design of experiments” framework has as its goal to separate observed variability in the dependent measure into (1) variability due to the manipulation(s) and (2) other variability, including measurement error and participant-level variation. This framework maps nicely onto the statistical framework described in chapters 5–7. In essence, this framework models the distribution of the measure using the condition structure of our experiment as the predictor.\nDifferent experimental designs will allow us to estimate specific effects more and less effectively. Recall in chapter 5, we estimated the effect of our tea/milk order manipulation by a simple subtraction: \\(\\beta = \\theta_{T} - \\theta_{C}\\) (where \\(\\beta\\) is the effect estimate, and \\(\\theta\\)s indicate the estimates for each condition, treatment \\(T\\) and control \\(C\\); we called them \\(\\theta_T\\) and \\(\\theta_M\\) in that chapter to denote tea- and milk-first conditions). This logic works just fine also if there are two distinct treatments in a three-condition experiment: each treatment can be compared to control separately. For treatment 1, \\(\\beta_{T_1} = \\theta_{T_1} - \\theta_{C}\\) and \\(\\beta_{T_2} = \\theta_{T_2} - \\theta_{C}\\).\n\n\n\n\n\n\n\nFigure 9.3: The 2 x 2 crossed design used in Young et al. (2007)\n\n\nThis logic is going to get more complicated if we have more than one distinct factor of interest, though. Let’s look at an example.\nYoung et al. (2007) were interested in how moral judgments depend on both the beliefs of actors and the outcomes of their actions. They presented participants with vignettes in which they learned, for example, that Grace visits a chemical factory with her friend and goes to the coffee break room, where she sees a white powder that she puts in her friend’s coffee. They then manipulated both Grace’s beliefs and the outcomes of her action following the schema in figure 9.3. Participants (\\(N = 10\\)) used a four-point Likert scale to rate whether the actions were morally forbidden (1) or permissible (4). Figure 9.4 shows the data.\n\n\n\n\n\n\nFigure 9.4: Moral permissibility as a function of belief and outcome. Results from Young et al. (2007), annotated with the estimated effects. Simple effects measure differences between the individual conditions and the neutral belief, neutral outcome condition. The interaction measures the difference between the predicted sum of the two simple effects and the actual observed data for the negative belief, negative outcome condition.\n\n\n\nYoung et al.’s design has two factors—belief and outcome—each with two levels (neutral and negative, noted as \\(B\\) and \\(-B\\) for belief and \\(O\\) and \\(-O\\) for outcome).3 These factors are fully crossed: each level of each factor is combined with each level of each other.\n\n3 Neither of these is necessarily a “control” condition: the goal is simply to compare these two levels of the factor—negative and neutral—to estimate the effect due to the factor.This fully-crossed design makes it easy for us to estimate quantities of interest. Let’s say that our reference group (equivalent to the control group for now) is neutral belief, neutral outcome. Now it’s easy to use the same kind of subtraction we did before to estimate particular effects we care about. For example, we can look at the effect of negative belief in the case of a neutral outcome: \\(\\beta_{-B,O} = \\theta_{-B,O} - \\theta_{B,O}\\). This effect is shown on the left side of figure 9.4. \nBut now there is a complexity: these two simple effects (effects of one variable at a particular level of another variable) together suggest that the combined effect \\(\\beta_{-B,-O}\\) in the negative belief, negative outcome condition should be equal to the sum of \\(\\beta_{-B,O}\\) and \\(\\beta_{B,-O}\\).4 As we can see from figure 9.4, that’s not right. If it were, the negative belief, negative outcome condition would be below the minimum possible rating!\n4 If you’re interested, you can also compute the average or main effect of a particular factor via the same subtractive logic. For example, the average effect of negative belief (\\(-B\\)) vs a neutral belief (\\(B\\)) can be computed as \\(\\beta_{-B} = \\frac{(\\theta_{-O, -B} + \\theta_{O, -B}) - (\\theta_{-O, B} + \\theta_{O, B})}{2}\\).5 If you’re reading carefully, you might be thinking that this all sounds like we’re talking about the analysis of variance (ANOVA), not about experimental design per se. These two topics are actually the same topic! The question is how to design an experiment so that these statistical models can be used to estimate particular effects—and combinations of effects—that we care about. In case you missed it, we discuss modeling interactions in a regression framework in chapter 7.Instead, we observe an interaction effect (sometimes called a two-way interaction when there are two factors): the effect when both factors are present is different than the sum of the two simple effects. To capture this effect, we need an interaction term: \\(\\beta_{-B,-O}\\).5 In other words, the effect of negative beliefs (intent) on subjective moral permissibility depends on whether the action caused harm. Critically, without a fully-crossed design, we can’t estimate this interaction and we would have made an incorrect prediction about one condition.\n\n\n9.1.2 Generalized factorial designs\nYoung et al.’s design, in which there are two factors with two levels each, is called a 2 x 2 design (pronounced “two by two”). These 2 x 2 designs are incredibly common and useful, but they are only one of an infinite variety of such designs that can be constructed.\nSay we added a third factor to Young et al.’s design such that Grace either feels neutral toward her friend or is angry on that day. If we fully crossed this third affective factor with the other two (belief and outcome), we’d have a 2 x 2 x 2 design. This design would have eight conditions: \\((A, B, O)\\), \\((A, B, -O)\\), \\((A, -B, O)\\), \\((A, -B, -O)\\), \\((-A, B, O)\\), \\((-A, B, -O)\\), \\((-A, -B, O)\\), \\((-A, -B, -O)\\). These conditions would in turn allow us to estimate both two-way and three-way interactions, enumerated in table 9.1.\n\n\n\nTable 9.1: Effects in a 2 x 2 x 2 design with affect, belief, and outcome as factors.\n\n\n\n\n\nEffect\nTerm Type\n\n\n\n\nAffect\nMain effect\n\n\nBelief\nMain effect\n\n\nOutcome\nMain effect\n\n\nAffect X Belief\n2-way interaction\n\n\nAffect X Outcome\n2-way interaction\n\n\nBelief X Outcome\n2-way interaction\n\n\nAffect X Belief X Outcome\n3-way interaction\n\n\n\n\n\n\nThree-way interactions are hard to think about! The affect X belief X outcome interaction tells you about the difference in moral permissibility that’s due to all three factors being present as opposed to what you’d predict on the basis of your estimates of the two-way interactions. In addition to being hard to think about, higher-order interactions tend to be hard to estimate, because estimating them accurately requires you to have a stable estimate of all of the lower-order interactions (McClelland and Judd 1993). For this reason, we recommend against experimental designs that rely on higher-order interactions unless you are in a situation where you both have strong predictions about these interactions and are confident in your ability to estimate them appropriately.\nThings can get even more complicated. If you have three factors with two levels each, as in the example above (table 9.1), you can estimate seven total effects of interest. But if you have four factors with two levels each, you get 15. Four factors with three levels each gets you a horrifying 80 different effects!6 This way lies madness, at least from the perspective of estimating and interpreting individual effects in a reasonable sample size. Again, we suggest starting with one- and two-factor designs. There is a lot to be learned from simple designs that follow good measurement and sampling practices.\n6 The general formula for \\(N\\) factors with \\(M\\) levels each is \\(M^N-1\\).\n\n\n\n\n\ndepth\n\n\n\n\n\nEstimation strategies for generalized factorial designs\nSo, what should you do if you really do care about four or more factors—in the sense that you want to estimate their effects and include them in your theory? The simplest strategy is to start your research off by measuring them independently in a series of single-factor experiments. This kind of setup is natural when there is a single reference level for each factor of interest, and such experiments can provide a basis for judging which factors are most important for your outcome and, hence, which should be prioritized for experiments to estimate interactions.\nOn the other hand, sometimes there is no reference level for a factor. For example, in the Kovács, Téglás, and Endress (2010) paradigm, it’s not clear whether a positive or negative belief is the reference level. That’s not a problem in a fully-crossed design like theirs, but this situation can pose a problem if you have more than two such factors. Ideally you would want to run independent experiments, but you have to choose some level for all of the other variables—you can’t just assume that one level is “neutral.”\nOne solution that lets you compute main effects but not interactions is called a Latin square. Latin squares are a good solution for three-factor designs, which is the level at which a fully-crossed design typically gets overwhelming.  A Latin square is an \\(n x n\\) matrix in which each number occurs exactly once in each row and column, for example: \\[\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    2 & 3 & 1\\\\\n    3 & 1 & 2 \\\\\n    \\end{bmatrix}\\] This Latin square for \\(n = 3\\) gives the solution for how to balance factors across a 3 x 3 x 3 experiment. The row number is one factor, the column number is the second factor, and the number in the cell is the third factor. So one condition would be (1,1,1), the first level of all factors, shown in the upper left cell. Another would be (3,3,2), the lower right cell. Although a fully-crossed design would require 27 cells to be run, the Latin square has only nine. Critically, the combinations of factors are balanced across the nine cells so that the average effect of each level of the three factors can be estimated. \nThere are also fancier methods available. For example, the literature on optimal experiment design contains methods for choosing the most informative sequence of experiments to run in order to estimate the parameters in a model that can include many factors and their interactions (Myung and Pitt 2009). Going down this road typically means having an implemented computational theory of your domain, but it can be a very productive strategy for exploring a complex experimental space with many factors.\n\n\n\n\n\n9.1.3 Between- vs within-participant designs\n\n\n\n\n\n\n\nFigure 9.5: A between-participants design.\n\n\nOnce you know what factor(s) you would like to manipulate in your experiment, the next step is to consider how these will be presented to participants, and how that presentation will interact with your measurements. The biggest decision to be made is whether each participant will experience one level of a factor—a between-participants design—or whether they will experience multiple levels—a within-participants design. Figure 9.5 shows a simple example of between-participants design with four participants (two assigned to each condition), while figure 9.6 shows a within-participants version of the same design.\n\n\n\n\n\n\nFigure 9.6: A within-participants design, counterbalanced for order (discussion of counterbalancing below).\n\n\n\nBecause people are very variable, the decision whether to measure a particular factor between- or within-participants is consequential. Imagine we’re estimating our treatment effect as before, simply by computing \\(\\widehat{\\beta} = \\widehat{\\theta}_{T} - \\widehat{\\theta}_{C}\\) with each of these estimates from different populations of participants. In this scenario, our estimate \\(\\widehat{\\beta}\\) contains three components: (1) the true differences between \\(\\theta_{T}\\) and \\(\\theta_{C}\\), (2) sampling-related variation in which participants from the population ended up in the samples for the two conditions, and (3) measurement error. Component 2 is present because any two samples of participants from a population will differ in their average on a measure—this is precisely the kind of sampling variation we saw in the null distributions in chapter 6.\nWhen our experimental design is within participants, component 2 is not present because participants in both conditions are sampled from the same population. If we get unlucky and all of our participants are lower than the population mean on our measure, then that unluckiness affects our conditions equally. The consequences for choosing an appropriate sample size are fairly extreme: between-participants designs typically require between two and eight times as many participants as within-participants designs!7\n7 If you want to estimate how big an advantage you get from within-participants data collection, you need to know how correlated (reliable) your observations are. One analysis of this issue (Lakens 2016) suggests that the key relationship is that \\(N_{within} = N_{between} (1-\\rho) /2\\) where \\(\\rho\\) is the correlation between the measurement of the two conditions within individuals. The more correlated they are, the smaller your within-participants \\(N\\).Given these advantages, why would you consider using a between-participants design? A within-participants design is simply not possible for all experiments. For example, consider a medical intervention—say, a new surgical procedure that is being compared to an established one. Patients cannot receive two different procedures, and so no within-participant comparison is possible.\nMost manipulations in the behavioral sciences are not so extreme, but it still may be impractical or inadvisable to deliver multiple conditions. Greenwald (1976) distinguishes three types of undesirable effects:8\n8 We tend to think of all of these as being forms of carryover effect, and sometimes use this label as a catch-all description. Some people also use the picturesque description “poisoning the well” (Gelman 2017)—earlier conditions “ruin” the data for later conditions.\nPractice effects occur when administering the measure or the treatment will lead to change. Imagine a curriculum intervention for teaching a math concept—it would be hard to convince a school to teach the same topic to students twice, and the effect of the second round of teaching would likely be quite different than the first!\nSensitization effects occur when seeing two versions of an intervention mean that you might respond differently to the second than the first because you have compared them and noticed the contrast. Consider a study on room lighting—if the experimenters are constantly changing the lighting, participants may become aware that lighting is the focus of the study!\nCarryover effects refer to the case where one treatment might have a longer-lasting effect than the measurement period. For example, imagine a study in which one treatment was to make participants frustrated with an impossible puzzle; if a second condition were given after this first one, participants might still be frustrated, leading to spillover of effects between conditions.\n\nAll of these issues can lead to real concerns with respect to within-participant designs. But the desire for effect estimates that are completely unbiased by these concerns may lead to the overuse of between-participant designs (Gelman 2017). As we mentioned above, between-participant designs come at a major cost in terms of power and precision.\nAn alternative approach is to acknowledge the possibility of carryover type effects and seek to mitigate them. First, you can make sure that the order of condition is randomized or balanced (see below); and second, you can analyze these carryover effects within your statistical model (for example by estimating the interaction of condition and order).9\n9 Even when one factor must be varied between participants, it is often still possible to vary others within subjects, leading to a mixed design in which some factors are between and others within.10 Caveat: this study used an observational design, so no causal inference is possible.We summarize the state of affairs from our perspective in figure 9.7. We think that within-participant designs should be preferred whenever possible. This conclusion is also consistent with meta-research we’ve done on replications from our course: across 176 student replications, the use of a within-subjects design was the strongest correlate of a successful replication (Boyce, Mathur, and Frank 2023).10\n\n\n\n\n\n\nFigure 9.7: Pros and cons of between- vs within-participant designs. We recommend within-participant designs when possible.\n\n\n\n\n\n9.1.4 Repeated measures and experimental items\nWe just discussed decision-making about whether to administer multiple manipulations to a single participant. An exactly analogous decision comes up for measures! And our take-home will be similar: unless there are specific difficulties that come up, it’s usually a very good idea to make multiple measurements (via multiple experimental trials) for each participant in each condition.\nYou can create a between-participants design where you administer your manipulation and then measure multiple times. This scenario is pictured in figure 9.8). Sometimes this works quite well. For example, imagine a transcranial magnetic stimulation (TMS) experiment: participants receive neural stimulation for a period of time, targeted at a particular region. Then they perform some measurement task repeatedly until it wears off. The more times they perform the measurement task, the better the estimate of whatever effect (when compared to a control of TMS to another region, say).\n\n\n\n\n\n\nFigure 9.8: A between-participants, repeated-measures design.\n\n\n\nSometimes this design is called a repeated measures design, but terminology here is tricky again. The term “repeated measures” refers to any experiment where each participant is measured more than once, including both between-participants and within-participants designs.11 Our advice is both to use within-participants designs and to get multiple measurements from each participant.\n11 We’re talking about multiple trials with the same measure, not multiple distinct measures. As we discussed in chapter 8, we tend to be against measuring lots of different things in a single experiment—in part because of the concerns that we’re articulating in this chapter: if you have time, it’s better to make more precise measures of what you care about most. Measuring one thing well is hard enough. Much better to measure one thing well than many things badly.Why? In the last subsection, we described how variability in our estimates in a between-participants design depends on three components: (1) true condition differences; (2) sampling variation between conditions; and (3) measurement error.\nWithin-participants designs are good because they don’t include (2). Repeated measures reduce (3): the more times you measure, the lower your measurement error, leading to greater measure reliability!\nThere are problems with repeating the same measure many times, however. Some measures can’t be repeated without altering the response. To take an obvious example, we can’t give the exact same math problem twice and get two useful measurements of mathematical ability! The typical solution to this problem is to create multiple items. In the case of a math assessment, you create multiple problems that you believe test the same concept but have different numbers or other superficial characteristics.\nUsing multiple items for measurement is good for two reasons. First, it reduces measurement error by allowing responses to be combined across items. But second, it increases the generalizability of the measurement. An effect that is consistent across many different items is more likely to be an effect that can be generalized to a whole class of stimuli—in precisely the same way that the use of multiple participants can license generalizations across a population of people (Clark 1973).\n\n\n\n\n\n\nFigure 9.9: A between-participants, pre-post design.\n\n\n\nOne variation on the repeated measures, between-participants design is a specific version where the measure is administered both before (pre-) and after (post-) intervention, as in figure 9.9. This design is sometimes known as a pre-post design. It is extremely common in cases where the intervention is larger scale and harder to give within-participants, such as in a field experiment where a policy or curriculum is given to one sample and not to another. The premeasurements can be used to subtract out participant-level variability and recover a more precise estimate of the treatment effect. Recall that our treatment effect in a pure between-participants design is \\(\\beta = \\theta_{T} - \\theta_{C}\\). In a pre-post design, we can do better by computing \\(\\beta = (\\theta_{T_{post}} - \\theta_{T_{pre}}) - (\\theta_{C_{post}} - \\theta_{C_{pre}})\\). This equation says, “How much more did the treatment group go up than the control group?12\n12 This estimate is sometimes called a “difference in differences.” The basic idea is widely used in the field of econometrics, both in experimental and quasi-experimental cases (Cunningham 2021). In practice, though, we recommend using the pre-treatment measurements as a covariate in a model-based analysis, not just doing the simple subtraction.In sum, within-participants, repeated-measurement designs are the bread and butter of most research in perception, psychophysics, and cognitive psychology. When both manipulations and measures can be repeated, these designs afford high measurement precision even with small sample sizes; they are recommended whenever possible.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nStimulus-specific effects\nImagine you’re a psycholinguist who has the hypothesis that nouns are processed faster than verbs. You run an experiment where you pick out ten verbs and ten nouns, then measure a large sample of participants’ reading time for each of these. You find strong evidence for the predicted effect and publish a paper on your claim. The only problem is that, at the same time, someone else has done exactly the same study—with different nouns and verbs—and published a paper making the opposite claim. When this happens, it is possible that each effect is driven by the specific experimental items that were chosen, rather than a generalization that is true of nouns and verbs in general (Clark 1973).\nThe problem of generalization from sample to population is not new—as we discussed in chapter 6, we are constantly making this kind of inference with the samples of people that participate in our experiments. Our classic statistical techniques are designed to quantify our ability to generalize from a sample of participants to a population, so we recognize that a very small sample size leads to a weak generalization. The exact same issue comes up with items: a very small sample of experimental items leads to a weak generalization to the population of items.\nItem effects are kind of like accidentally finding a group of ten people whose left toes are longer than their right ones. If you continued to measure the same group’s toes, you could continue to replicate the difference in length. But that doesn’t mean it’s true of the population as a whole.\nThis kind of stimulus generalizability problem comes up across many different areas of psychology. In one example, hundreds of papers were written about a phenomenon called the “risky shift”—in which groups deliberating about a decision would produce riskier decisions than individuals. Unfortunately, this phenomenon appeared to be completely driven by the specific choice of vignettes that groups deliberated about, with some stories producing a risky shift and others producing a more conservative shift (Westfall, Judd, and Kenny 2015).\nAnother example comes from the memory literature, where in a classic paper, Baddeley, Thomson, and Buchanan (1975) suggested that words that take longer to pronounce (“tycoon” or “morphine”) would be remembered worse than words that took a shorter amount of time (“ember” or “wicket”) even when they had the same number of syllables. This effect also appears to be driven by the specific sets of words chosen in the original paper. It’s very replicable with that particular stimulus set but not generalizable across other sets (Lovatt, Avons, and Masterson 2000).\nThe implication of these examples is clear: experimenters need to take care in both their experimental design and analysis to avoid overgeneralizing from their stimuli to a broader construct. Three primary steps can help experimenters avoid this pitfall:\n\nTo maximize generality, use samples of experimental items—words, pictures, or vignettes—that are comparable in size to your samples of participants.\nWhen replicating an experiment, consider taking a new sample of items as well as a new sample of participants. It’s more work to draft new items, but it will lead to more robust conclusions.\nWhen experimental items are sampled at random from a broader population, use a statistical model that includes this sampling process (e.g., mixed effects models with random intercepts for items from chapter 7).\n\n\n\n\n\n\n9.1.5 Discrete and continuous experimental manipulations\nMost experimental designs in psychology use discrete condition manipulations: treatment vs control. In our view, this decision often leads to a lost opportunity relative to a more continuous manipulation of the strength of the treatment. The goal of an experiment is to estimate a causal effect; ideally, this estimate can be generalized to other contexts and used as a basis for theory. Measuring not just one effect but instead a dose-response relationship—how the measure changes as the strength of the manipulation is changed—has a number of benefits in helping to achieve this goal.\n\n\n\n\n\n\nFigure 9.10: Three schematic designs. (left) Control and treatment are two levels of a nominal variable. (middle) Control is compared to ordered levels of a treatment. (right) Treatment level is an interval or ratio variable such that points can be connected and a parametric curve can be extrapolated.\n\n\n\nMany manipulations can be titrated—that is, their strength can be varied continuously—with a little creativity on the part of an experimenter. A curriculum intervention can be applied at different levels of intensity, perhaps by changing the number of sessions in which it is taught. For a priming manipulation, the frequency or duration of prime stimuli can be varied. Two stimuli can be morphed continuously so that categorization boundaries can be examined.13\n13 These methods are extremely common in perception and psychophysics research, in part because the dimensions being studied are often continuous in nature. It would be basically impossible to estimate a participant’s visual contrast sensitivity without continuously manipulating the contrast of the stimulus!14 These assumptions are theory-laden, of course—the choice of a linear function or a sigmoid is not necessary: nothing guarantees that simple, smooth, or monotonic functions are the right ones. The important point is that choosing a function makes explicit your assumptions about the nature of the treatment-effect relationship.Dose-response designs are useful because they provide insight into the shape of the function mapping your manipulation to your measure. Knowing this shape can inform your theoretical understanding! Consider the examples given in figure 9.10. If you only have two conditions in your experiment, then the most you can say about the relationship between your manipulation and your measure is that it produces an effect of a particular magnitude; in essence, you are assuming that condition is a nominal variable. If you have multiple ordered levels of treatment, you can start to speculate about the nature of the relationship between treatment and effect magnitude. But if you can measure the strength of your treatment, then you can begin to describe the nature of the relationship between the strength of treatment and strength of effect via a parametric function (e.g., a linear regression, a sigmoid, or other function).14 These parametric functions can in turn allow you to generalize from your experiment, making predictions about what would happen under intervention conditions that you didn’t measure directly!\n\n\n\n\n\n\ndepth\n\n\n\n\n\nTrade-offs associated with titrated designs\nLike adults, babies like to look at more interesting, complex stimuli. But do they uniformly prefer complex stimuli, or do they search for stimuli at an appropriate level of complexity for their processing abilities? To test this hypothesis, Brennan, Ames, and Moore (1966) exposed infants in three different age groups (3, 8, and 14 weeks, \\(N = 30\\)) to black and white checkerboard stimuli with three different levels of complexity (2 x 2, 8 x 8, and 24 x 24).\nTheir findings are plotted in figure 9.11: the youngest infants preferred the simplest stimuli, while infants at an intermediate age preferred stimuli of intermediate complexity, and the oldest infants preferred the most complex stimuli. These findings help to motivate the theory that infants attend preferentially to stimuli that provide appropriate learning input for their processing ability (Kidd, Piantadosi, and Aslin 2012).\n\n\n\n\n\n\n\n\nFigure 9.11: Infants’ looking time, plotted by stimulus complexity and infant age. Data from Brennan, Ames, and Moore (1966).\n\n\n\n\n\nIf your goal is simply to detect whether an effect is zero or nonzero, then dose-response designs do not achieve the maximum statistical power. For example, if Brennan, Ames, and Moore (1966) simply wanted to achieve maximal statistical power, they probably should have only tested two age groups and two levels of complexity (say, 3 and 14 week infants and 2 x 2 and 24 x 24 checkerboards). That would have been enough to show an interaction of complexity and age, and their greater resources devoted to these four (as opposed to nine) conditions would mean more precise estimates of each. But their findings would be less clearly supportive of the view that infants prefer stimuli that are appropriate to their processing ability, because no group would have preferred an intermediate level of complexity (as the nine-week-olds apparently did). By seeking to measure intermediate conditions, they provided a stronger test of their theory.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#estimation-strategies-for-generalized-factorial-designs",
    "href": "009-design.html#estimation-strategies-for-generalized-factorial-designs",
    "title": "9  Design",
    "section": "",
    "text": "So, what should you do if you really do care about four or more factors—in the sense that you want to estimate their effects and include them in your theory? The simplest strategy is to start your research off by measuring them independently in a series of single-factor experiments. This kind of setup is natural when there is a single reference level for each factor of interest, and such experiments can provide a basis for judging which factors are most important for your outcome and, hence, which should be prioritized for experiments to estimate interactions.\nOn the other hand, sometimes there is no reference level for a factor. For example, in the Kovács, Téglás, and Endress (2010) paradigm, it’s not clear whether a positive or negative belief is the reference level. That’s not a problem in a fully-crossed design like theirs, but this situation can pose a problem if you have more than two such factors. Ideally you would want to run independent experiments, but you have to choose some level for all of the other variables—you can’t just assume that one level is “neutral.”\nOne solution that lets you compute main effects but not interactions is called a Latin square. Latin squares are a good solution for three-factor designs, which is the level at which a fully-crossed design typically gets overwhelming.  A Latin square is an \\(n x n\\) matrix in which each number occurs exactly once in each row and column, for example: \\[\\begin{bmatrix}\n    1 & 2 & 3 \\\\\n    2 & 3 & 1\\\\\n    3 & 1 & 2 \\\\\n    \\end{bmatrix}\\] This Latin square for \\(n = 3\\) gives the solution for how to balance factors across a 3 x 3 x 3 experiment. The row number is one factor, the column number is the second factor, and the number in the cell is the third factor. So one condition would be (1,1,1), the first level of all factors, shown in the upper left cell. Another would be (3,3,2), the lower right cell. Although a fully-crossed design would require 27 cells to be run, the Latin square has only nine. Critically, the combinations of factors are balanced across the nine cells so that the average effect of each level of the three factors can be estimated. \nThere are also fancier methods available. For example, the literature on optimal experiment design contains methods for choosing the most informative sequence of experiments to run in order to estimate the parameters in a model that can include many factors and their interactions (Myung and Pitt 2009). Going down this road typically means having an implemented computational theory of your domain, but it can be a very productive strategy for exploring a complex experimental space with many factors.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#stimulus-specific-effects",
    "href": "009-design.html#stimulus-specific-effects",
    "title": "9  Design",
    "section": "Stimulus-specific effects",
    "text": "Stimulus-specific effects\nImagine you’re a psycholinguist who has the hypothesis that nouns are processed faster than verbs. You run an experiment where you pick out ten verbs and ten nouns, then measure a large sample of participants’ reading time for each of these. You find strong evidence for the predicted effect and publish a paper on your claim. The only problem is that, at the same time, someone else has done exactly the same study—with different nouns and verbs—and published a paper making the opposite claim. When this happens, it is possible that each effect is driven by the specific experimental items that were chosen, rather than a generalization that is true of nouns and verbs in general (Clark 1973).\nThe problem of generalization from sample to population is not new—as we discussed in chapter 6, we are constantly making this kind of inference with the samples of people that participate in our experiments. Our classic statistical techniques are designed to quantify our ability to generalize from a sample of participants to a population, so we recognize that a very small sample size leads to a weak generalization. The exact same issue comes up with items: a very small sample of experimental items leads to a weak generalization to the population of items.\nItem effects are kind of like accidentally finding a group of ten people whose left toes are longer than their right ones. If you continued to measure the same group’s toes, you could continue to replicate the difference in length. But that doesn’t mean it’s true of the population as a whole.\nThis kind of stimulus generalizability problem comes up across many different areas of psychology. In one example, hundreds of papers were written about a phenomenon called the “risky shift”—in which groups deliberating about a decision would produce riskier decisions than individuals. Unfortunately, this phenomenon appeared to be completely driven by the specific choice of vignettes that groups deliberated about, with some stories producing a risky shift and others producing a more conservative shift (Westfall, Judd, and Kenny 2015).\nAnother example comes from the memory literature, where in a classic paper, Baddeley, Thomson, and Buchanan (1975) suggested that words that take longer to pronounce (“tycoon” or “morphine”) would be remembered worse than words that took a shorter amount of time (“ember” or “wicket”) even when they had the same number of syllables. This effect also appears to be driven by the specific sets of words chosen in the original paper. It’s very replicable with that particular stimulus set but not generalizable across other sets (Lovatt, Avons, and Masterson 2000).\nThe implication of these examples is clear: experimenters need to take care in both their experimental design and analysis to avoid overgeneralizing from their stimuli to a broader construct. Three primary steps can help experimenters avoid this pitfall:\n\nTo maximize generality, use samples of experimental items—words, pictures, or vignettes—that are comparable in size to your samples of participants.\nWhen replicating an experiment, consider taking a new sample of items as well as a new sample of participants. It’s more work to draft new items, but it will lead to more robust conclusions.\nWhen experimental items are sampled at random from a broader population, use a statistical model that includes this sampling process (e.g., mixed effects models with random intercepts for items from chapter 7).",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#trade-offs-associated-with-titrated-designs",
    "href": "009-design.html#trade-offs-associated-with-titrated-designs",
    "title": "9  Design",
    "section": "Trade-offs associated with titrated designs",
    "text": "Trade-offs associated with titrated designs\nLike adults, babies like to look at more interesting, complex stimuli. But do they uniformly prefer complex stimuli, or do they search for stimuli at an appropriate level of complexity for their processing abilities? To test this hypothesis, Brennan, Ames, and Moore (1966) exposed infants in three different age groups (3, 8, and 14 weeks, \\(N = 30\\)) to black and white checkerboard stimuli with three different levels of complexity (2 x 2, 8 x 8, and 24 x 24).\nTheir findings are plotted in figure 9.11: the youngest infants preferred the simplest stimuli, while infants at an intermediate age preferred stimuli of intermediate complexity, and the oldest infants preferred the most complex stimuli. These findings help to motivate the theory that infants attend preferentially to stimuli that provide appropriate learning input for their processing ability (Kidd, Piantadosi, and Aslin 2012).\n\n\n\n\n\n\n\n\nFigure 9.11: Infants’ looking time, plotted by stimulus complexity and infant age. Data from Brennan, Ames, and Moore (1966).\n\n\n\n\n\nIf your goal is simply to detect whether an effect is zero or nonzero, then dose-response designs do not achieve the maximum statistical power. For example, if Brennan, Ames, and Moore (1966) simply wanted to achieve maximal statistical power, they probably should have only tested two age groups and two levels of complexity (say, 3 and 14 week infants and 2 x 2 and 24 x 24 checkerboards). That would have been enough to show an interaction of complexity and age, and their greater resources devoted to these four (as opposed to nine) conditions would mean more precise estimates of each. But their findings would be less clearly supportive of the view that infants prefer stimuli that are appropriate to their processing ability, because no group would have preferred an intermediate level of complexity (as the nine-week-olds apparently did). By seeking to measure intermediate conditions, they provided a stronger test of their theory.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#choosing-your-manipulation",
    "href": "009-design.html#choosing-your-manipulation",
    "title": "9  Design",
    "section": "9.2 Choosing your manipulation",
    "text": "9.2 Choosing your manipulation\nIn the previous section, we reviewed a host of common experimental designs. These designs provide a palette of common options for combining manipulations and measures. But your choice must be predicated on the specific manipulation you are interested in! In this section, we discuss considerations for experimenters as they design manipulations.\nIn chapter 8, we talked about measurement validity, but the idea of validity concept can be applied to manipulations as well as measures. In particular, a manipulation is valid if it corresponds to the construct that the experimenter intends to intervene on. In this context, internal validity threats to manipulations tend to refer to cases where factors in the experimental design keep the intended manipulation from actually intervening on the construct of interest. In contrast, external validity threats to manipulations tend to be cases where the manipulation simply doesn’t line up well with the construct of interest.\n\n9.2.1 Internal validity threats: Confounding\nFirst and foremost, manipulations must actually manipulate the construct whose causal effect is being estimated. If they actually manipulate something else instead, they are confounded. This term is used widely in psychology, but it’s worth revisiting what it means. An experimental confound is a variable that is created in the course of the experimental design that is both causally related to the predictor and potentially also related to the outcome. As such, it is a threat to internal validity.\nLet’s go back to our discussion of causal inference in chapter 1. Our goal was to use a randomized experiment to estimate the causal effect of money on happiness. But just giving people money is a big intervention that involves contact with researchers—contact alone can lead to an experimental effect even if your manipulation fails. For that reason, many studies that provide money to participants either give a small amount of money or a large amount of money. This design keeps researcher contact consistent in both conditions, implying that the difference in outcomes between these two conditions should be due to the amount of money received (unless there are other confounds!).\nSuppose you were designing an experiment of this sort and you wanted to follow our advice and use a within-participants design. You could measure happiness, give participants $100, wait a month and measure happiness again, give participants $1,000, wait a month, and then measure happiness for the third time. The trouble is, this design has an obvious experimental confound (figure 9.12): the order of the monetary gifts. Maybe happiness just went up more over time, irrespective of getting the second gift.\n\n\n\n\n\n\n\nFigure 9.12: Confounding order and condition assignment means that you can’t make an inference about the link between money and happiness.\n\n\nIf you think your experimental design might have a confound, you should think about ways to remove it. A first option is elimination, which we described above: basically, matching a particular variable across different conditions. This should be our first option for most confounds. Unfortunately, in our within-participants money-happiness study, order is confounded with condition so if we match orders we have eliminated our condition manipulation entirely.\nA second option is counterbalancing, in which we vary a confounding factor systematically across participants so its average effect is zero across the whole experiment. In the case of our example, counterbalancing order across participants is a very safe choice. Some participants get $100 first and others get $1,000 first. That way, you are guaranteed that the order of conditions will have no effect of the confound on your average effect. The effect of this counterbalancing is that it “snips” the causal dependency between condition assignment and later time. We notate this on our causal diagram with a scissors icon (figure 9.13).15 Time can still have an effect on happiness, but the effect is independent from the effect of condition and, hence, your experiment can still yield an unbiased estimate of the condition effect.\n15 In practice, counterbalancing is like adding an additional factor to your factorial design! But because the factor is a nuisance factor—basically, one we don’t care about—we don’t discuss it as a true condition manipulation. Despite that, it’s a good practice to check for effects of these sorts of nuisance factors in your preliminary analysis. Even though your average effect won’t be biased by it, it introduces variation that you might want to understand to interpret other effects and plan new studies.\n\n\n\n\n\nFigure 9.13: Confounding between a specific condition and the time at which it’s administered can be removed by counterbalancing or randomization of order.\n\n\n\nCounterbalancing gets trickier when you have too many levels on a variable or multiple confounding variables. In that case, it may not be possible to do a full counterbalance so that all combinations of these factors are seen by equal numbers of participants. You may have to rely on partial counterbalancing schemes or Latin square designs (see the Depth box above; in this case, the Latin squares are used to create orderings of stimuli such that the position of each treatment in the order is controlled across two other confounding variables).\nA final option, especially useful for such tricky cases, is randomization—that is, choosing which level of a nuisance variable to administer to the participant via a random choice. Randomization is increasingly common now that many experimental interventions are delivered by software. If you can randomize experimental confounds, you probably should. The only time you really get in trouble with randomization is when you have a large number of options, a small number of participants, or some combination of the two. Then you can end up with unbalanced levels of the randomized factors. Averaging across many experiments, a lack of balance will come out in the wash, but in a single experiment, it can lead to unfortunate bias in numbers.\nA good approach to thinking through your experimental design is to walk through the experiment step by step and think about potential confounds. For each of these confounds, consider how it might be removed via counterbalancing or randomization. As our case study shows, confounds are not always obvious, especially in complex paradigms. There is no sure-fire way to ensure that you have spotted every one—sometimes the best way to avoid them is simply to present your candidate design to a skeptical friend.\n\n\n9.2.2 Internal validity threats: Placebo, demand, and expectancy\nA second class of important threats to internal validity comes from cases where the research design is confounded by factors related to how the manipulation is administered, or even that a manipulation is administered. In some cases, these create confounds that can be controlled; in others they must simply be understood and guarded against. Rosnow and Rosenthal (1997) called these “artifacts”: systematic errors related to research on people, conducted by people.\nA placebo effect is a positive effect on the measure that comes as a result of participants’ expectations about a treatment in the context of a research study. The classic example of a placebo is medical: giving an inactive sugar pill as a “treatment” leads some patients to report a reduction in whatever symptom they are being treated for. Placebo effects are a major concern in medical research as well as a fixture in experimental designs in medicine (Benedetti 2020). The key insight is that treatments must not simply be compared to a baseline of no treatment but rather to a baseline in which the psychological aspects of treatment are present but the “active ingredient” is not. In the terms we have been using, the experience of receiving a treatment (independent of the content of the treatment) is a confounding factor when you simply compare treatment to no treatment conditions.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nBrain training?\nCan doing challenging cognitive tasks make you smarter? In the late 2000s and early 2010s, a large industry for “brain training” emerged. Companies like Lumos Labs, CogMed, BrainHQ, and CogniFit offered games, often modeled on cognitive psychology tasks, that claimed to lead to gains in memory, attention, and problem-solving.\nThese companies were basing their claims in part on a scientific literature reporting that concerted training on difficult cognitive tasks could lead to benefits that transferred to other cognitive domains. Among the most influential of these was a study by Jaeggi et al. (2008). They conducted four experiments in which participants (\\(N = 70\\) across the studies) were assigned to either working memory training via a difficult working memory task (the “dual N-back”) or a no-training control, with training varying from eight days all the way to 19 days.\nThe finding from this study excited a tremendous amount of interest because they reported not only gains in performance on the specific training task but also on a general intelligence task that the participants had trained on. While the control group’s scores on these tasks improved, presumably just from being tested twice, there was a condition by time (pre-test vs post-test) interaction such that the scores of the trained groups (consolidated across all four training experiments) grew significantly more over the training period (figure 9.14). These results were interpreted as supporting transfer—whereby training on one task leads to broader gains—a key goal for “brain training.”\n\n\n\n\n\n\n\n\nFigure 9.14: The primary outcome graph for data from Jaeggi et al. (2008).\n\n\n\n\n\nCareful readers of the original paper noticed signs of analytic flexibility (as discussed in chapters 3 and 6), however. For example, the key statistical model was fit to dataset created by post hoc consolidation of experiments, which yielded \\(p = 0.025\\) on the key interaction (Redick et al. 2013). When data were disaggregated, it was clear that the measures and effects had differed in each of the different subexperiments (figure 9.15).\n\n\n\n\n\n\n\n\nFigure 9.15: The four subexperiments of Jaeggi et al. (2008), now disaggregated. Panels show 8-, 12-, 17-, and 19-session studies. Note the different measures: RAPM = Raven’s Advanced Progressive Matrices; BOMAT = Bochumer Matrizentest. Based on Redick et al. (2013).\n\n\n\n\n\nSeveral replications by the same group addressed some of these issues, but they still failed to show convincing evidence of transfer. In particular, there was no comparison to an active control group in which participants did some kind of alternative activity for the same amount of time (Simons et al. 2016). Such a comparison is critical because a comparison to a passive control group (a group that does no intervention) confounds participants’ general effort and involvement in the study with the specific training being used. Successful transfer compared to passive control could be the result of participants’ involvement, expectations, or motivation rather than brain training per se.\nA careful replication of the training study (\\(N = 74\\)) with an active control group and a wide range of outcome measures failed to find any transfer effects from working-memory training (Redick et al. 2013). A meta-analysis of 23 studies concluded that their findings cast doubt on working memory training for increasing cognitive functioning (Melby-Lervåg and Hulme 2013). In one convincing test of the cognitive transfer theory, a BBC show (“Bang Goes the Theory”) encouraged its listeners to participate in a six-week online brain-training study. More than 11,000 listeners completed the pre- and post-tests and at least two training sessions. Neither focused training of planning and reasoning nor broader training on memory, attention, and mathematics led to transfer to untrained tasks.\nPlacebo effects are one plausible explanation for some positive findings in the brain-training literature. Foroughi et al. (2016) recruited participants to participate via two different advertisements. The first advertised that “numerous studies have shown working memory training can increase fluid intelligence” (“placebo treatment” group), while the second simply offered experimental credits (control group). After a single training session, the placebo treatment group showed significant improvements to their matrix reasoning abilities. Participants in the placebo treatment group realized gains from training out of proportion with any they could have realized through training. Further, those participants who responded to the placebo treatment ad tended to endorse statements about the malleability of intelligence, suggesting that they might have been especially likely to self-select into the intervention.\nSummarizing the voluminous literature on brain training, Simons et al. (2016) wrote: “Despite marketing claims from brain-training companies of ‘proven benefits’ … we find the evidence of benefits from cognitive brain training to be ‘inadequate.’”\n\n\n\nIf placebo effects reflect what participants expect from a treatment, then demand characteristics reflect what participants think experimenters want and their desire to help the experimenters achieve that goal (Orne 1962). Demand characteristics are often raised as a reason for avoiding within-participants designs—if participants become alert to the presence of an intervention, they may then respond in a way that they believe is helpful to the experimenter. Typical tools for controlling or identifying demand characteristics include using a cover story to mask the purpose of an experiment, using a debriefing procedure to probe whether participants typically guessed the purpose of an experiment, and (perhaps most effectively) creating a control condition with similar demand characteristics but missing a key component of the experimental intervention. Note that if you use a cover story to mask the purpose of your experiment, it’s worth thinking about whether you are using deception, which can raise ethical issues (see chapter 4). Certainly you should be sure to debrief participants about the true function of the experiment!\nThe final entry into this list of internal validity threats is experimenter expectancy effects, where the experimenter’s behavior biases participants in a way that results in the appearance of condition differences where no true difference exists. The classic example of such effects is from the animal learning literature and the story of Clever Hans. Clever Hans was a horse who appeared able to do arithmetic by tapping out solutions with his hoof. On deeper investigation, it became apparent he was being cued by his trainer’s posture (apparently without the trainer’s knowledge) to stop tapping when the desired answer was reached. The horse knew nothing about math, but the experimenter’s expectations were altering the horse’s behavior across different conditions.\nIn any experiment delivered by human experimenters who know what condition they are delivering, condition differences can result from experimenters imparting their expectations. Table 9.2 shows the results of a meta-analysis estimating sizes of expectancy effects in a range of domains—the magnitudes are shocking. There’s no question that experimenter expectancy is sufficient to “create” many interesting phenomena artifactually. The mechanisms of expectancy are an interesting research topic in their own right; in many cases expectancies appear to be communicated nonverbally in much the same way that Clever Hans learned (Rosnow and Rosenthal 1997).\n\n\n\nTable 9.2: Magnitudes of expectancy effects. Based on R. Rosenthal (1994).\n\n\n\n\n\n\n\n\n\n\n\nDomain\nd\nr\nExample of type of study\n\n\n\n\nLaboratory interviews\n0.14\n0.07\nEffects of sensory restriction on reports of hallucinatory experiences\n\n\nReaction time\n0.17\n0.08\nLatency of word associations to certain stimulus words\n\n\nLearning and ability\n0.54\n0.26\nIQ test scores, verbal conditioning (learning)\n\n\nPerson perception\n0.55\n0.27\nPerception of other people’s success\n\n\nInkblot tests\n0.84\n0.39\nRatio of animal to human Rorschach responses\n\n\nEveryday situations\n0.88\n0.40\nSymbol learning, athletic performance\n\n\nPsychophysical judgments\n1.05\n0.46\nAbility to discriminate tones\n\n\nAnimal learning\n1.73\n0.65\nLearning in mazes and Skinner boxes\n\n\nWeighted mean\n0.70\n0.33\n\n\n\nUnweighted mean\n0.74\n0.35\n\n\n\nMedian\n0.70\n0.33\n\n\n\n\n\n\n\nIn medical research, the gold standard is an experimental design where neither patients nor experimenters know which condition the patients are in.16 Results from other designs are treated with suspicion because of their vulnerability to demand and expectancy effects. In psychology, the most common modern protection against experimenter expectancy is the delivery of interventions by a computer platform that can give instructions in a coherent and uniform way across conditions.\n16 These are commonly referred to as double-blind designs (though the term masked is now often preferred).In the case of interventions that must be delivered by experimenters, ideally experimenters should be unaware of which condition they are delivering. On the other hand, the logistics of maintaining experimenter ignorance can be quite complicated in psychology. For this reason, many researchers opt for lesser degrees of control: for example, choosing to standardize delivery of an intervention via a script. These designs are sometimes necessary for practical reasons but should be scrutinized closely. “How can you rule out experimenter expectancy effects?” is an uncomfortable question that should be asked more frequently in seminars and paper reviews.\n\n\n9.2.3 External validity of manipulations\nThe goal of a specific experimental manipulation is to operationalize a particular causal relationship of interest. Just as the relationship between measure and construct can be more or less valid, so too can the relationship between manipulation and construct. How can you tell? Just like in the case of measures, there’s no one royal road to validity. You need to make a validity argument (Kane 1992).17\n\n17 One caveat is that the validity of a manipulation incorporates the validity of the manipulation and the measure. You can’t really have a good estimate of a causal effect if the measurement is invalid.For testing the effect of money on happiness, our manipulation was to give participants $1,000. This manipulation is clearly face valid. But how often do people just receive a windfall of cash, versus getting a raise at work or inheriting money from a relative? Is the effect caused by having the money, or receiving the money with no strings attached? We might have to do more experiments to figure out what aspect of the money manipulation was most important. Even in straightforward cases like this one, we need to be careful about the breadth of the claims we make.\nSometimes validity arguments are made based on the success of the manipulation in producing some change in the measurement. In the implicit theory of mind case study we began with, the stimulus contained an animated Smurf character, and the argument was that participants took the Smurf’s beliefs into account in making their judgments. This stimulus choice seems surprising—not only would participants have to track the implicit beliefs of other people, but they would also have to be tracking the beliefs of depictions of nonhuman, animated characters. On the other hand, based on the success of the manipulation, the authors made an a fortiori argument: if people track even an animated Smurf’s beliefs, then they must be tracking the beliefs of real humans.\nLet’s look at one last example to think more about manipulation validity. Walton and Cohen (2011) conducted a short intervention in which college students (\\(N = 92\\)) read about social belonging and the challenges of the transition to college and then reframed their own experiences using these ideas. This intervention led to long-lasting changes in grades and well-being. While the intervention undoubtedly had a basis in theory, part of our understanding of the validity of the intervention comes from its efficacy: sense of belonging must be a powerful factor if intervening on it causes such big changes in the outcome measures.18 The only danger is when the argument becomes circular—a theory is correct because the intervention yielded a success, and the intervention is presumed to be valid because of the theory. The way out of this circle is through replication and generalization of the intervention. If the intervention repeatably produces the outcome, as has been shown in replications of the sense of belonging intervention (Walton, Brady, and Crum 2020), then the manipulation becomes an intriguing target for future theories. The next step in such a research program is to understand the limitations of such interventions (sometimes called boundary conditions).\n18 On the other hand, if the manipulation doesn’t produce a change in your measure, maybe the manipulation is invalid, but the construct still exists. Sense of belonging could still be important even if my particular intervention failed to alter it!",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#brain-training",
    "href": "009-design.html#brain-training",
    "title": "9  Design",
    "section": "Brain training?",
    "text": "Brain training?\nCan doing challenging cognitive tasks make you smarter? In the late 2000s and early 2010s, a large industry for “brain training” emerged. Companies like Lumos Labs, CogMed, BrainHQ, and CogniFit offered games, often modeled on cognitive psychology tasks, that claimed to lead to gains in memory, attention, and problem-solving.\nThese companies were basing their claims in part on a scientific literature reporting that concerted training on difficult cognitive tasks could lead to benefits that transferred to other cognitive domains. Among the most influential of these was a study by Jaeggi et al. (2008). They conducted four experiments in which participants (\\(N = 70\\) across the studies) were assigned to either working memory training via a difficult working memory task (the “dual N-back”) or a no-training control, with training varying from eight days all the way to 19 days.\nThe finding from this study excited a tremendous amount of interest because they reported not only gains in performance on the specific training task but also on a general intelligence task that the participants had trained on. While the control group’s scores on these tasks improved, presumably just from being tested twice, there was a condition by time (pre-test vs post-test) interaction such that the scores of the trained groups (consolidated across all four training experiments) grew significantly more over the training period (figure 9.14). These results were interpreted as supporting transfer—whereby training on one task leads to broader gains—a key goal for “brain training.”\n\n\n\n\n\n\n\n\nFigure 9.14: The primary outcome graph for data from Jaeggi et al. (2008).\n\n\n\n\n\nCareful readers of the original paper noticed signs of analytic flexibility (as discussed in chapters 3 and 6), however. For example, the key statistical model was fit to dataset created by post hoc consolidation of experiments, which yielded \\(p = 0.025\\) on the key interaction (Redick et al. 2013). When data were disaggregated, it was clear that the measures and effects had differed in each of the different subexperiments (figure 9.15).\n\n\n\n\n\n\n\n\nFigure 9.15: The four subexperiments of Jaeggi et al. (2008), now disaggregated. Panels show 8-, 12-, 17-, and 19-session studies. Note the different measures: RAPM = Raven’s Advanced Progressive Matrices; BOMAT = Bochumer Matrizentest. Based on Redick et al. (2013).\n\n\n\n\n\nSeveral replications by the same group addressed some of these issues, but they still failed to show convincing evidence of transfer. In particular, there was no comparison to an active control group in which participants did some kind of alternative activity for the same amount of time (Simons et al. 2016). Such a comparison is critical because a comparison to a passive control group (a group that does no intervention) confounds participants’ general effort and involvement in the study with the specific training being used. Successful transfer compared to passive control could be the result of participants’ involvement, expectations, or motivation rather than brain training per se.\nA careful replication of the training study (\\(N = 74\\)) with an active control group and a wide range of outcome measures failed to find any transfer effects from working-memory training (Redick et al. 2013). A meta-analysis of 23 studies concluded that their findings cast doubt on working memory training for increasing cognitive functioning (Melby-Lervåg and Hulme 2013). In one convincing test of the cognitive transfer theory, a BBC show (“Bang Goes the Theory”) encouraged its listeners to participate in a six-week online brain-training study. More than 11,000 listeners completed the pre- and post-tests and at least two training sessions. Neither focused training of planning and reasoning nor broader training on memory, attention, and mathematics led to transfer to untrained tasks.\nPlacebo effects are one plausible explanation for some positive findings in the brain-training literature. Foroughi et al. (2016) recruited participants to participate via two different advertisements. The first advertised that “numerous studies have shown working memory training can increase fluid intelligence” (“placebo treatment” group), while the second simply offered experimental credits (control group). After a single training session, the placebo treatment group showed significant improvements to their matrix reasoning abilities. Participants in the placebo treatment group realized gains from training out of proportion with any they could have realized through training. Further, those participants who responded to the placebo treatment ad tended to endorse statements about the malleability of intelligence, suggesting that they might have been especially likely to self-select into the intervention.\nSummarizing the voluminous literature on brain training, Simons et al. (2016) wrote: “Despite marketing claims from brain-training companies of ‘proven benefits’ … we find the evidence of benefits from cognitive brain training to be ‘inadequate.’”",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "009-design.html#summary-experimental-design",
    "href": "009-design.html#summary-experimental-design",
    "title": "9  Design",
    "section": "9.3 Summary: Experimental design",
    "text": "9.3 Summary: Experimental design\nIn this chapter, we started by examining some common experimental designs that allow us to measure effects associated with one or more manipulations. Our advice, in brief, was: “Keep it simple!” The failure mode of many experiments is that they contain too many manipulations, and these manipulations are measured with too little precision.\nStart with just a single manipulation and measure it carefully. Ideally this measurement should be done via a within-participants design unless the manipulation is completely incompatible with this design. And if this design can incorporate a dose-response manipulation, it is more likely to provide a basis for quantitative theorizing.\nHow do you ensure that your manipulation is valid? A careful experimenter needs to consider possible confounds and ensure that these are controlled or randomized. They must also consider other artifacts including placebo, demand, and expectancy effects. Finally, they must begin thinking about the relation of their manipulation to the broader theoretical construct whose causal role they hope to test.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nChoose a classic study in your area of psychology. Analyze the design choices: How many factors were manipulated? How many measures were taken? Did it use a within-participants or between-participants design? Were measures repeated? Can you justify these choices with respect to trade-offs (e.g., carryover effects, fatigue, or others)?\nConsider the same study. Design an alternative version that varies one of these design parameters (e.g., drops a manipulation or measure or changes within- to between-participants). What are the pros and cons of this change? Do you think your design improves on the original?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nMuch of this material is covered in more depth in the classic text on research methods: Rosenthal, Robert, and Ralph L. Rosnow (2008). Essentials of Behavioral Research: Methods and Data Analysis. Third Edition. McGraw-Hill. http://dx.doi.org/10.34944/dspace/66.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Design</span>"
    ]
  },
  {
    "objectID": "010-sampling.html",
    "href": "010-sampling.html",
    "title": "10  Sampling",
    "section": "",
    "text": "10.1 Sampling theory\nThe basic idea of sampling is simple: you want to estimate some measurement for a large or infinite population by measuring a sample from that population.3 Sampling strategies are split into two categories. Probability sampling strategies are those in which each member of the population has some known, prespecified probability of being selected to be in the sample—think “generalizing to Japanese people by picking randomly from a list of everyone in Japan.” Non-probability sampling covers strategies in which probabilities are unknown or shifting, or in which some members of the population could never be included in the sample—think “generalizing to Germans by sending a survey to a German email list and asking people to forward the email to their family.”",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#sampling-theory",
    "href": "010-sampling.html#sampling-theory",
    "title": "10  Sampling",
    "section": "Is everyone bad at describing smells?",
    "text": "3 There are some tools for dealing with estimation in smaller populations where your sample is a substantial fraction of the population (e.g., a survey of your department where you get responses from half of the students). We won’t discuss those here; our focus is on generalizing to large populations of humans.\n\n\n\n\n\ncase study\n\n\n\n\n\nIs everyone bad at describing smells?\nSince Darwin, scientists have assumed that smell is a vestigial sense in humans—one that we don’t even bother to encode in language. In English we don’t even have consistent words for odors. We can say something is “stinky,” “fragrant,” or maybe “musty,” but beyond these, most of our words for smells are about the source of the smell, not the qualities of it. Bananas, roses, and skunks all have distinctive smells, but we don’t have any vocabulary for naming what is common or uncommon about them. And when we make up ad hoc vocabulary, it’s typically quite inconsistent (Majid and Burenhult 2014). The same situation applies across many languages.\nSo, would it be a good generalization about human beings—all people—that olfaction as a sense is de-emphasized relative to, say, vision? This inference has a classic sample-to-population structure. Within several samples of participants using widely spoken languages, we observe limited and inconsistent vocabulary for smells, as well as poor discrimination. We use these samples to license an inference to the population—in this case, the entire human population.\n\n\n\n\n\n\n\n\nFigure 10.1: Data from Majid and Burenhult (2014) on the consistency of color and odor naming in English and Jahai speakers. Higher values indicate more consistent descriptions. Error bars show standard deviation.\n\n\n\n\n\nBut these inferences about the universal lack of olfactory vocabulary are likely based on choosing nonrepresentative samples! Multiple hunter-gatherer groups appear to have large vocabularies for consistent smell description. For example, the Jahai, a hunter-gatherer group on the Malay Peninsula, have a vocabulary that includes at least twelve words for distinct odors, for example /cŋ\\(\\symup{\\varepsilon}\\)s/, which names odors with a “stinging smell” like gasoline, smoke, or bat droppings. When Jahai speakers are asked to name odors, they produce shorter and much more consistent descriptions than English speakers—in fact, their smell descriptions were as consistent as their color descriptions (figure 10.1). Further studies implicate the hunter-gatherer lifestyle as a factor: while several hunter-gatherer groups show good odor naming, nearby horticulturalist groups don’t (Majid and Kruspe 2018).\nGeneralizations about humans are tricky. If you want to estimate the average odor naming ability, you could take a random sample of humans and evaluate their odor naming. Most of the individuals in the sample would likely speak English, Mandarin, Hindi, or Spanish. Almost certainly, none of them would speak Jahai, which is spoken by only a little more than a thousand people and is listed as “endangered” by Ethnologue (https://www.ethnologue.com/language/jhi). Your estimate of low odor naming stability might be a good guess for the majority of the world’s population, but would tell you little about the Jahai.\nOn the other hand, it’s more complicated to jump from a statistical generalization about average ability to a richer claim like, “Humans have low olfactory naming ability.” Such claims about universal aspects of the human experience require much more care and much stronger evidence (Piantadosi and Gibson 2014). From a sampling perspective, human behavior and cognition show immense and complex heterogeneity—variability of individuals and variability across clusters. Put simply, if we want to know what people in general are like, we have to think carefully about which people we include in our studies.\n\n\n\n\n10.1.1 Classical probability sampling\nIn classical sampling theory, there is some sampling frame containing every member of the population—think of a giant list with every adult human’s name in it. Then we use some kind of sampling strategy, maybe at the simplest just a completely random choice, to select \\(N\\) humans from that sample frame, and then we include them in our experiment. This scenario is the one that informs all of our statistical results about how sample means converge to the population mean (as in chapter 6).\nUnfortunately, we very rarely do sampling of this sort in psychological research. Gathering true probability samples from the large populations that we’d like to generalize to is far too difficult and expensive. Consider the problems involved in doing some experiment with a sample of all adult humans, or even adult English-speaking humans who are located in the United States. As soon as you start to think about what it would take to collect a probability sample of this kind of population, the complexities get overwhelming. How will you find their names—what if they aren’t in the phone book? How will you contact them—what if they don’t have email? How will they do your experiment—what if they don’t have an up-to-date web browser? What if they don’t want to participate at all?\nInstead, the vast majority of psychology research has been conducted with convenience samples: non-probability samples that feature individuals who can be recruited easily, such as college undergraduates or workers on crowdsourcing platforms like Amazon Mechanical Turk or Prolific Academic (see chapter 12). We’ll turn to these below.\nFor survey research, on the other hand—think of election polling—there are many sophisticated techniques for dealing with sampling; although this field is still imperfect, it has advanced considerably in trying to predict complex and dynamic behaviors. One of the basic ideas is the construction of representative samples: samples that resemble the population in their representation of one or several sociodemographic characteristics like gender, income, race and ethnicity, age, or political orientation.\nRepresentative samples can be constructed by probability sampling, but they can also be constructed through non-probability methods like recruiting quotas of individuals from different groups via various different convenience methods. These methods are critical for much social science research, but they have been used less frequently in experimental psychology research and aren’t necessarily a critical part of the beginning experimentalist’s toolkit.4\n4 Readers can come up with counter-examples of recent studies that focus on representative sampling, but our guess is that they will prove the rule more generally. For example, a recent study tested the generality of growth mindset interventions for US high school students using a national sample (Yeager et al. 2019). This large-scale study sampled more than 100 high schools from a sampling frame of all registered high schools in the US, then randomly assigned students within schools that agreed to participate. They then checked that the schools that agreed to participate were representative of the broader population of schools. This study is great stuff, but we hope you agree that if you find yourself in this kind of situation—planning a multi-investigator five-year consortium study on a national sample—you might want to consult with a statistician and not use an introductory book like this one.\n\n\n\n\n\ndepth\n\n\n\n\n\nRepresentative samples and stratified sampling\nStratified sampling is a cool method that can help you get more precise estimates of your experimental effect, if you think it varies across some grouping in your sample. Imagine you’re interested in a particular measure in a population—say, attitudes toward tea drinking across US adults—but you think that this measure will vary with one or more characteristics such as whether the adults are frequent, infrequent, or non-coffee drinkers. Even worse, your measure might be more variable within one group: perhaps most frequent and infrequent coffee drinkers feel okay about tea, but as a group, non-coffee drinkers tend to hate it (most don’t drink any caffeinated beverages).\nA simple random sample from this heterogeneous population will yield statistical estimates that converge asymptotically to the correct population average for tea-drinking attitudes. But it will do so more slowly than ideal because any given sample may over- or under-sample nondrinkers just by chance. In a small sample, if you happen to get too many non-coffee drinkers, your estimate of attitudes will be biased downward; if you happen to get too few, you will be biased upward. All of this will come out in the wash eventually, but any individual sample (especially a small one) will be noisier than ideal.\n\n\n\n\n\n\nFigure 10.2: An illustration of stratified sampling. The left panel shows the sampling frame. The upper frames show the sampling frame stratified by a participant characteristic and a stratified sample. The lower frame shows a simple random sample, which happens to omit one group completely by chance.\n\n\n\nBut, if you know the proportion of frequent, infrequent, or non-coffee drinkers in the population, you can perform stratified sampling within those subpopulations to ensure that your sample is representative along this dimension (Neyman 1992). This situation is pictured in figure 10.2, which shows how a particular sampling frame can be broken up into groups for stratified sampling. The result is a sample that matches the population proportions on a particular characteristic. In contrast, a simple random sample can over- or under-sample the subgroups by chance.\nStratified sampling can lead to substantial gains in the precision of your estimate. These gains are most prominent when either the groups differ a lot in their mean or when they differ a lot in their variance. There are several important refinements of stratified sampling in case you think these methods are important for your problem. In particular, optimal sampling can help you figure out how to over-sample groups with higher variance. On the other hand, if the characteristic on which you stratify participants doesn’t relate to your outcome at all, then estimates from stratified sampling converge just as fast as random sampling (though it’s a bit more of a pain to implement).\nFigure 10.3 shows a simulation of the scenario in figure 10.2, in which each coffee preference group has a different tea attitude mean and the smallest group has the biggest variance. Although the numbers here are invented, it’s clear that estimation error is much smaller in the stratified group and estimation error declines much more quickly as samples get larger.\n\n\n\n\n\n\n\n\nFigure 10.3: A simulation showing the potential benefits of stratification. Each dot is an estimated mean for a sample of a particular size, sampled randomly or with stratification. Red points show the mean and standard deviation of sample estimates.\n\n\n\n\n\nStratification is everywhere, and it’s useful even in convenience samples. For example, researchers who are interested in development typically stratify their samples across ages (e.g., recruiting equal numbers of two- and three-year-olds for a study of preschoolers). You can estimate developmental change in a pure random sample, but you are guaranteed good coverage of the range of interest when you stratify.\nIf you have an outcome that you think varies with a particular characteristic, it’s not a bad idea to consider stratification. But don’t go overboard—you can drive yourself to distraction finding the last left-handed nonbinary coffee drinker to complete your sample. Focus on stratifying when you know the measure varies with the characteristic of interest.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#is-everyone-bad-at-describing-smells",
    "href": "010-sampling.html#is-everyone-bad-at-describing-smells",
    "title": "10  Sampling",
    "section": "",
    "text": "Since Darwin, scientists have assumed that smell is a vestigial sense in humans—one that we don’t even bother to encode in language. In English we don’t even have consistent words for odors. We can say something is “stinky,” “fragrant,” or maybe “musty,” but beyond these, most of our words for smells are about the source of the smell, not the qualities of it. Bananas, roses, and skunks all have distinctive smells, but we don’t have any vocabulary for naming what is common or uncommon about them. And when we make up ad hoc vocabulary, it’s typically quite inconsistent (Majid and Burenhult 2014). The same situation applies across many languages.\nSo, would it be a good generalization about human beings—all people—that olfaction as a sense is de-emphasized relative to, say, vision? This inference has a classic sample-to-population structure. Within several samples of participants using widely spoken languages, we observe limited and inconsistent vocabulary for smells, as well as poor discrimination. We use these samples to license an inference to the population—in this case, the entire human population.\n\n\n\n\n\n\n\n\nFigure 10.1: Data from Majid and Burenhult (2014) on the consistency of color and odor naming in English and Jahai speakers. Higher values indicate more consistent descriptions. Error bars show standard deviation.\n\n\n\n\n\nBut these inferences about the universal lack of olfactory vocabulary are likely based on choosing nonrepresentative samples! Multiple hunter-gatherer groups appear to have large vocabularies for consistent smell description. For example, the Jahai, a hunter-gatherer group on the Malay Peninsula, have a vocabulary that includes at least twelve words for distinct odors, for example /cŋ\\(\\symup{\\varepsilon}\\)s/, which names odors with a “stinging smell” like gasoline, smoke, or bat droppings. When Jahai speakers are asked to name odors, they produce shorter and much more consistent descriptions than English speakers—in fact, their smell descriptions were as consistent as their color descriptions (figure 10.1). Further studies implicate the hunter-gatherer lifestyle as a factor: while several hunter-gatherer groups show good odor naming, nearby horticulturalist groups don’t (Majid and Kruspe 2018).\nGeneralizations about humans are tricky. If you want to estimate the average odor naming ability, you could take a random sample of humans and evaluate their odor naming. Most of the individuals in the sample would likely speak English, Mandarin, Hindi, or Spanish. Almost certainly, none of them would speak Jahai, which is spoken by only a little more than a thousand people and is listed as “endangered” by Ethnologue (https://www.ethnologue.com/language/jhi). Your estimate of low odor naming stability might be a good guess for the majority of the world’s population, but would tell you little about the Jahai.\nOn the other hand, it’s more complicated to jump from a statistical generalization about average ability to a richer claim like, “Humans have low olfactory naming ability.” Such claims about universal aspects of the human experience require much more care and much stronger evidence (Piantadosi and Gibson 2014). From a sampling perspective, human behavior and cognition show immense and complex heterogeneity—variability of individuals and variability across clusters. Put simply, if we want to know what people in general are like, we have to think carefully about which people we include in our studies.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#representative-samples-and-stratified-sampling",
    "href": "010-sampling.html#representative-samples-and-stratified-sampling",
    "title": "10  Sampling",
    "section": "Representative samples and stratified sampling",
    "text": "Representative samples and stratified sampling\nStratified sampling is a cool method that can help you get more precise estimates of your experimental effect, if you think it varies across some grouping in your sample. Imagine you’re interested in a particular measure in a population—say, attitudes toward tea drinking across US adults—but you think that this measure will vary with one or more characteristics such as whether the adults are frequent, infrequent, or non-coffee drinkers. Even worse, your measure might be more variable within one group: perhaps most frequent and infrequent coffee drinkers feel okay about tea, but as a group, non-coffee drinkers tend to hate it (most don’t drink any caffeinated beverages).\nA simple random sample from this heterogeneous population will yield statistical estimates that converge asymptotically to the correct population average for tea-drinking attitudes. But it will do so more slowly than ideal because any given sample may over- or under-sample nondrinkers just by chance. In a small sample, if you happen to get too many non-coffee drinkers, your estimate of attitudes will be biased downward; if you happen to get too few, you will be biased upward. All of this will come out in the wash eventually, but any individual sample (especially a small one) will be noisier than ideal.\n\n\n\n\n\n\nFigure 10.2: An illustration of stratified sampling. The left panel shows the sampling frame. The upper frames show the sampling frame stratified by a participant characteristic and a stratified sample. The lower frame shows a simple random sample, which happens to omit one group completely by chance.\n\n\n\nBut, if you know the proportion of frequent, infrequent, or non-coffee drinkers in the population, you can perform stratified sampling within those subpopulations to ensure that your sample is representative along this dimension (Neyman 1992). This situation is pictured in figure 10.2, which shows how a particular sampling frame can be broken up into groups for stratified sampling. The result is a sample that matches the population proportions on a particular characteristic. In contrast, a simple random sample can over- or under-sample the subgroups by chance.\nStratified sampling can lead to substantial gains in the precision of your estimate. These gains are most prominent when either the groups differ a lot in their mean or when they differ a lot in their variance. There are several important refinements of stratified sampling in case you think these methods are important for your problem. In particular, optimal sampling can help you figure out how to over-sample groups with higher variance. On the other hand, if the characteristic on which you stratify participants doesn’t relate to your outcome at all, then estimates from stratified sampling converge just as fast as random sampling (though it’s a bit more of a pain to implement).\nFigure 10.3 shows a simulation of the scenario in figure 10.2, in which each coffee preference group has a different tea attitude mean and the smallest group has the biggest variance. Although the numbers here are invented, it’s clear that estimation error is much smaller in the stratified group and estimation error declines much more quickly as samples get larger.\n\n\n\n\n\n\n\n\nFigure 10.3: A simulation showing the potential benefits of stratification. Each dot is an estimated mean for a sample of a particular size, sampled randomly or with stratification. Red points show the mean and standard deviation of sample estimates.\n\n\n\n\n\nStratification is everywhere, and it’s useful even in convenience samples. For example, researchers who are interested in development typically stratify their samples across ages (e.g., recruiting equal numbers of two- and three-year-olds for a study of preschoolers). You can estimate developmental change in a pure random sample, but you are guaranteed good coverage of the range of interest when you stratify.\nIf you have an outcome that you think varies with a particular characteristic, it’s not a bad idea to consider stratification. But don’t go overboard—you can drive yourself to distraction finding the last left-handed nonbinary coffee drinker to complete your sample. Focus on stratifying when you know the measure varies with the characteristic of interest.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#convenience-samples-generalizability-and-the-weird-problem",
    "href": "010-sampling.html#convenience-samples-generalizability-and-the-weird-problem",
    "title": "10  Sampling",
    "section": "10.2 Convenience samples, generalizability, and the WEIRD problem",
    "text": "10.2 Convenience samples, generalizability, and the WEIRD problem\nNow let’s go back to the question of generalizability. How generalizable are the experimental effect estimates that we obtain in experiments that are conducted only with convenience samples? We’ll start by laying out the worst version of the problem of generalizability in experimental psychology. We’ll then try to pull back from the brink and discuss some reasons why we might not want to be in despair despite some of the generalizability issues that plague the psychology literature.\n\n10.2.1 The worst version of the problem\nPsychology is the study of the human mind. But from a sampling theory standpoint, not a single estimate in the published literature is based on a simple random sample from the human population. And the situation is worse than that. Here are three of the most severe issues that have been raised regarding the generalizability of psychology research.\n\nConvenience samples. Almost all research in experimental psychology is performed with convenience samples. This issue has led to the remark that “the existing science of human behavior is largely the science of the behavior of sophomores” (McNemar 1946, 333; quoted in Rosenthal and Rosnow 1984, 261). The samples we have easy access to just don’t represent the populations we want to describe! At some point there was a social media account devoted to finding biology papers that made big claims about curing diseases and appending the qualifier “in mice” to them. We might consider whether we need to do the same to psychology papers. Would “Doing fonzy improves smoodling in sophomore college undergraduates in the Western US” make it into a top journal?\nThe WEIRD problem. Not only are the convenience samples that we study not representative of the local or national contexts in which they are recruited but those local and national contexts are also unrepresentative of the broad range of human experiences. Henrich, Heine, and Norenzayan (2010) coined the term WEIRD (Western, educated, industrialized, rich, and democratic) to sum up some of the ways that typical participants in psychology experiments differ from other humans. The vast over-representation of WEIRD participants in the literature has led some researchers to suggest that published results simply reflect “WEIRD psychology”—a small and idiosyncratic part of a much broader universe of human psychology.5\nThe item sampling issue. As we discussed in chapter 7 and 9, we’re typically not just trying to generalize to new people; we’re also trying to generalize to new stimuli (Westfall, Judd, and Kenny 2015). The problem is that our experiments often use a very small set of items, constructed by experimenters in an ad hoc way rather than sampled as representatives of a broader population of stimuli that we hope to generalize to with our effect size estimate. What’s more, our statistical analyses sometimes fail to take stimulus variation into account. Unless we know about the relationship of our items to the broader population of stimuli, our estimates may be based on unrepresentative samples in yet another way.\n\n5 The term WEIRD has been very useful in drawing attention to the lack of representation of the breadth of human experiences in experimental psychology. But one negative consequence of this idea has been the response that what we need to do as a field is to sample more “non-WEIRD” people. It is not helpful to suggest that every culture outside the WEIRD moniker is the same (Syed and Kathawalla 2020). A better starting point is to consider the way that cultural variation might guide our choices about sampling.In sum, experiments in the psychology literature primarily measure effects from WEIRD convenience samples of people and unsystematic samples of experimental stimuli. Should we throw up our hands and resign ourselves to an ungeneralizable “science” of sample-specific anecdotes (Yarkoni 2020)?\n\n\n10.2.2 Reasons for hope and ways forward\nWe think the situation isn’t as bleak as the arguments above might have suggested. Underlying each of the arguments above is the notion of heterogeneity, the idea that particular effects vary in the population.\nLet’s think through a very simple version of this argument. Say we have an experiment that measures the smoodling effect, and it turns out that smoodling is completely universal and invariant throughout the human population. Now, if we want to get a precise estimate of smoodling, we can take any sample we want because everyone will show the same pattern. Because smoodling is homogeneous, a nonrepresentative sample will not cause problems. There are some phenomena like this! For example, the Stroop task produces a consistent and similar interference effect for almost everyone (Hedge, Powell, and Sumner 2018).\n\n\n\n\n\n\nFigure 10.4: An illustration of the interaction of heterogeneity and convenience samples. Colors indicate arbitrary population subgroups. The left-hand panels show sample composition. Individual plots show the distribution of effect sizes in each subgroup.\n\n\n\nFigure 10.4 illustrates this argument more broadly. If you have a representative sample (top), then your sample mean and your population mean will converge to the same value, regardless of whether the effect is homogeneous (right) or heterogeneous (right). That’s the beauty of sampling theory. If you have a convenience sample, one part of the population is overrepresented in the sample. The convenience sample doesn’t cause problems if the size of your effect is homogeneous in the population—as with the case of smoodling or Stroop. The trouble comes when you have an effect that is heterogeneous. Because one group is overrepresented, you get systematic bias in the sample mean relative to the population mean.\nSo the problems listed above—convenience samples, WEIRD samples, and narrow stimulus samples—only cause issues if effects are heterogeneous. Are they? The short answer is, we don’t know. Convenience samples are fine in the presence of homogeneous effects, but we only use convenience samples, so we may not know which effects are homogeneous! Our metaphorical heads are in the sand.\nWe can’t do better than this circularity without a theory of what should be variable and what should be consistent between individuals.6 As naive observers of human behavior, differences between people often loom large. We are keen observers of social characteristics like age, gender, race, class, and education. For this reason, our intuitive theories of psychology often foreground these characteristics as the primary locus for variation between people. Certainly these characteristics are important, but they fail to explain many of the invariances of human psychology as well. An alternative line of theorizing starts with the idea that “lower-level” parts of psychology—like perception—should be less variable than “higher-level” faculties like social cognition. This kind of theory sounds like a useful place to start, but there are also counter-examples in the literature, including cases of cultural variation in perception (Henrich, Heine, and Norenzayan 2010).\n6 Many people have theorized about the ways that culture and language in general might moderate psychological processes (e.g., Markus and Kitayama 1991). What we’re talking about is related but slightly different—a theory not of what’s different but of when there should be any difference and when there shouldn’t be. As an example, Tsai’s (2007) “ideal affect” theory predicts that there should be more similarities in the distribution of actual affect across cultures, but that cultural differences should emerge in ideal affect (what people want to feel like) across cultures. This is a theory of when you should see homogeneity and when you should see heterogeneity.Multi-lab, multi-nation studies can help to address questions about heterogeneity, breaking the circularity we described above. For example, ManyLabs 2 systematically investigated the replicability of a set of phenomena across cultures (Klein et al. 2018), finding limited variation in effects between WEIRD sites and other sites. And in a study comparing a set of convenience and probability samples, Coppock, Leeper, and Mullinix (2018) found limited demographic heterogeneity in another sample of experimental effects from across the social sciences. So there are at least some cases where we don’t have to worry as much about heterogeneity. More generally, such large-scale studies offer the possibility of measuring and characterizing demographic and cultural variation—as well as how variation itself varies between phenomena.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#biases-in-the-sampling-process",
    "href": "010-sampling.html#biases-in-the-sampling-process",
    "title": "10  Sampling",
    "section": "10.3 Biases in the sampling process",
    "text": "10.3 Biases in the sampling process\nIn fields like econometrics or epidemiology that use observational methods to estimate causal effects, reasoning about sampling biases is a critical part of estimating generalizable effects. If your sample does not represent the population of interest, then your effect estimates will be biased.7 In the kind of experimental work we are discussing, many of these issues are addressed by random assignment, including the first issue we treat: collider bias. Not so for the second one, attrition bias, which is an issue even in randomized experiments.\n7 There is a deep literature on correcting these biases using causal inference frameworks. These techniques are well outside of the scope of this book, but if you’re interested, you might look at some of the textbooks we recommended earlier, such as Cunningham (2021).\n10.3.1 Collider bias\nImagine you want to measure the association between money and happiness through a (nonexperimental) survey. As we discussed in chapter 1, there are plenty of causal processes that could lead to this association. Figure 10.5 shows several of these scenarios. Money could truly cause happiness (1); happiness could cause you to make more money (2); or some third factor—say having lots of friends—could cause people to be happier and richer (3).\n\n\n\n\n\n\n\nFigure 10.5: Four reasons why money and happiness can be correlated in a particular sample: (1) causal relationship, (2) reverse causality, (3) confounding with friendship, and (4) collider bias. For this last scenario, we have to assume that our measurement is conditioned on being in this sample, meaning we only look at the association of money and happiness within the social services sample.\n\n\nBut we can also create spurious associations if we are careless in our sampling. One prominent problem that we can induce is called collider bias. Suppose we recruited our sample from the clients of a social services agency. Unfortunately, both of our variables might affect presence in a social service agency (figure 10.5, 4): people might be interacting with the agency for financial or benefits assistance, or else for psychological services (perhaps due to depression).\nBeing in a social services sample is called a collider variable because the two causal arrows collide into it (they both point to it). If we look just within the social services sample, we might see a negative association between wealth and happiness—on average the people coming for financial assistance would have less wealth and more happiness than the people coming for psychological services. The take-home here is that in observational research, you need to think carefully about the causal structure of your sampling process (Rohrer 2018).\nIf you are doing experimental research, you are mostly protected from this kind of bias: Random assignment still “works” even in subselected samples. If you run a money intervention within a social-services population using random assignment, you can still make an unbiased estimate of the effect of money on happiness. But that estimate will only be valid for members of that subselected population.\n\n\n10.3.2 Attrition bias\nAttrition is when people drop out of your study. You should do everything you can to improve participants’ experiences (see chapter 12), but sometimes—especially when a manipulation is onerous for participants or your experiment is longitudinal and requires tracking participants for some time—you will still have participants withdraw from the study.\nAttrition on its own can be a threat to the generalizability of an experimental estimate. Imagine you do an experiment comparing a new very intense after-school math curriculum to a control curriculum in a sample of elementary school children over the course of a year. By the end of the year, suppose many of your participants have dropped out. The families who have stayed in the study are likely those who care most about math. Even if you see an effect of the curriculum intervention, this effect may generalize only to children in families who love math.\n\n\n\n\n\n\n\nFigure 10.6: Selective attrition can lead to a bias even in the presence of random assignment. The dashed line indicates a causal relationship that is unobserved by the researcher.\n\n\n8 If you get deeper into drawing DAGs like we are doing here, you will want to picture attrition as its own node in the graph, but that’s beyond the scope of this book.But there is a further problem with attrition, known as selective attrition. If attrition is related to the outcome specifically within the treatment group (or for that matter, specifically within the control group), you can end up with a biased estimate, even in the presence of random assignment (Nunan, Aronson, and Bankhead 2018). Imagine that students in the control condition of your math intervention experiment stayed in the sample, but the math intervention itself was so tough that most families dropped out except those who were very interested in math. Now, when you compare math scores at the end of the experiment, your estimate will be biased (figure 10.6): scores in the math condition could be higher simply because of differences in who stuck around to the end.8\nUnfortunately, it turns out that attrition bias can be pretty common even in short studies, especially when they are conducted online when a participant can drop out simply by closing a browser window. This bias can be serious enough to lead to false conclusions. For example, Zhou and Fishbach (2016) ran an experiment in which they asked online participants to write about either four happy events (low difficulty) or 12 happy events (high difficulty) from the last year and then asked the participants to rate the difficulty of the task. Surprisingly, the high-difficulty task was rated as easier than the low-difficulty task! Selective attrition was the culprit for this counterintuitive conclusion: while only 26% of participants dropped out of the low-difficulty condition, a full 69% dropped out of the high-difficulty task. The 31% that were left found it quite easy for them to generate 12 happy events, and so they rated the objectively harder task as less difficult.\nAlways try to track and report attrition information. That lets you—and others—understand whether attrition is leading to bias in your estimates or threats to the generalizability of your findings.9\n9 If you get interested, there is a whole field of statistics that focuses on missing data and provides models for reasoning about and dealing with cases where data might not be missing completely at random (Little and Rubin 2019 is the classic reference for these tools). The causal inference frameworks referenced above also have very useful ways of thinking about this sort of bias.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#sample-size-planning",
    "href": "010-sampling.html#sample-size-planning",
    "title": "10  Sampling",
    "section": "10.4 Sample size planning",
    "text": "10.4 Sample size planning\nNow that you have spent some time considering your sample and what population it represents, how many people will your sample contain? Continuing to collect data until you observe a \\(p &lt; 0.05\\) in an inferential test is a good way to get a false positive. This practice, known as “optional stopping,” is a good example of a practice that invalidates \\(p\\)-values, much like the cases of analytic flexibility discussed in chapter 3 and chapter 6.\nDecisions about when to stop collecting data should not be data-dependent. Instead you should transparently declare your data collection stopping rule in your study preregistration (see chapter 11). This step will reassure readers that there is no risk of bias from optional stopping. The simplest stopping rule is, “I’ll collect data until I get to a target \\(N\\)”—all that’s needed in this case is a value for \\(N\\).\nBut how do you decide \\(N\\)? It’s going to be dependent on the effect that you want to measure and how it varies in the population. Smaller effects will require larger sample sizes. Classically, \\(N\\) was computed using power analysis, which can provide a sample size for which you have a good chance of rejecting the null hypothesis (given a particular expected effect size). We’ll introduce this computation below.\nClassical power analysis is not the only way to plan your sample size. There are a number of other useful strategies, some of which rely on the same kinds of computations as power analysis (table 10.1). Each of these can provide a valid justification for a particular sample size, but they are useful in different situations.\n\n\n\nTable 10.1: Types of data collection stopping rules.\n\n\n\n\n\n\n\n\n\n\nMethod\nStopping rule\nExample\n\n\n\n\nPower analysis\nStop at N for known probability of rejecting the null given known effect size\nRandomized trial with strong expectations about effect size\n\n\nResource constraint\nStop collecting data after a certain amount of time or after a certain amount of resources are used\nTime-limited field work\n\n\nSmallest effect size of interest\nStop at N for known probability of rejecting the null for effects greater than some minimum\nMeasurement of a theoretically important effect with unknown magnitude\n\n\nPrecision analysis\nStop at N that provides some known degree of precision in measure\nExperimental measurement to compare with predictions of cognitive models\n\n\nSequential analysis\nStop when a known inferential criterion is reached\nIntervention trial designed to accept or reject null with maximal efficiency\n\n\n\n\n\n\n\n10.4.1 Power analysis\n\n\n\n\n\n\n\nFigure 10.7: A standard decision matrix for null-hypothesis significance testing.\n\n\nLet’s start by reviewing the null-hypothesis significance testing paradigm that we introduced in chapter 6. Recall that we introduced the Neyman-Pearson decision-theoretic view of testing in chapter 6, shown again in figure 10.7. The idea was that we’ve got some null hypothesis \\(H_0\\) and some alternative \\(H_1\\)—something like “No effect” and “Yes, there is some effect with known size”—and we want to use data to decide which state we’re in. \\(\\alpha\\) is our criterion for rejecting the null, conventionally set to \\(\\alpha=0.05\\).\nBut what if \\(H_0\\) is actually false and the alternative \\(H_1\\) is true? Not all experiments are equally well set up to reject the null in those cases. Imagine doing an experiment with \\(N = 3\\). In that case, we’d almost always fail to reject the null, even if it were false. Our sample would almost certainly be too small to rule out sampling variation as the source of our observed data.\nLet’s try to quantify our willingness to miss the effect—the false negative rate. We’ll denote this probability with \\(\\beta\\). If \\(\\beta\\) is the probability of missing an effect (failing to reject the null when it’s really false), then \\(1-\\beta\\) is the probability that we correctly reject the null when it is false. That’s what we call the statistical power of the experiment.\nWe can only compute power if we know the effect size for the alternative hypothesis. If the alternative hypothesis is a small effect, then the probability of rejecting the null will typically be low (unless the sample size is very large). In contrast, if the alternative hypothesis is a large effect, then the probability of rejecting the null will be higher.\n\n\n\n\n\n\nFigure 10.8: An illustration of how larger sample sizes lead to greater power.\n\n\n\nThe same dynamic holds with sample size: the same effect size will be easier to detect with a larger sample size than with a small one. Figure 10.8 shows how this relationship works. A large sample size creates a tighter null distribution (right side) by reducing sampling error. A tighter null distribution means you can reject the null more of the time based on the variation in a true effect. If your sample size is too small to detect your effect much of the time, we call this being under-powered.10\n10 You can also refer to a design as over-powered, though we object slightly to this characterization, since the value of large datasets is typically not just to reject the null but also to measure an effect with high precision and to investigate how it is moderated by other characteristics of the sample.11 Our focus here is on giving you a conceptual introduction to power analysis, but we refer you to Cohen (1992) for a more detailed introduction.Classical power analysis involves computing the sample size \\(N\\) that’s necessary in order to achieve some level of power, given \\(\\alpha\\) and a known effect size.11 The mathematics of the relationship between \\(\\alpha\\), \\(\\beta\\), \\(N\\), and effect size have been worked out for a variety of different statistical tests (Cohen 2013) and codified in software like G*Power (Faul et al. 2007) and the pwr package for R (Champely 2020). For other cases (including mixed effects models), you may have to conduct a simulation in which you generate many simulated experimental runs under known assumptions and compute how many of these lead to a significant effect; luckily, R packages exist for this purpose as well, including the simr package (Green and MacLeod 2016).\n\n\n10.4.2 Power analysis in practice\nLet’s do a power analysis for our hypothetical money and happiness experiment. Imagine the experiment is a simple two-group design in which participants from a convenience population are randomly assigned either to receive $1,000 and some advice on saving money (experimental condition) vs just receiving the advice and no money (control condition). We then follow up a month later and collect self-reported happiness ratings. How many people should we have in our study in order to be able to reject the null? The answer to this question depends on our desired values of \\(\\alpha\\) and \\(\\beta\\) as well as our expected effect size for the intervention.\nFor \\(\\alpha\\) we will just set a conventional significance threshold of \\(\\alpha = 0.05\\). But what should be our desired level of power? The usual standard in the social sciences is to aim for power above 80% (e.g., \\(\\beta &lt; 0.20\\)); this gives you four out of five chances to observe a significant effect. But just like \\(\\alpha = 0.05\\), this is a conventional value that is perhaps a little bit too loose for modern standards—a strong test of a particular effect should probably have 90% or 95% power.12\n12 Really, researchers interested in using power analysis in their work should give some thought to what sort of chance of a false negative they are willing to accept. In exploratory research, perhaps a higher chance of missing an effect is reasonable; in contrast, in confirmatory research it might make sense to aim for a higher level of power.These choices are relatively easy, compared to the fundamental issue: our power analysis requires some expectation about our effect size. This is the first fundamental problem of power analysis: if you knew the effect size, you might not need to do the experiment!\nSo how are you supposed to get an estimate of effect size? Here are a few possibilities:\n\nMeta-analysis. If there is a good meta-analysis of the effect that you are trying to measure (or something closely related), then you are in luck. A strong meta-analysis will have not only a precise effect size estimate but also some diagnostics detecting and correcting potential publication bias in the literature (see chapter 16). While these diagnostics are imperfect, they still can give you a sense for whether you can use the meta-analytic effect size estimate as the basis for a power analysis.\nSpecific prior study. A more complicated scenario is when you have only one or a handful of prior studies that you would like to use as a guide. The trouble is that any individual effect in the literature is likely to be inflated by publication and other selective reporting biases (see chapter 3). Thus, using this estimate likely means your study will be underpowered—you might not get as lucky as a previous study did!\nPilot testing. Many people (including us) at some point learned that one way to do a power analysis is to conduct a pilot study, estimate the effect size from the pilot, and then use this effect estimate for power analysis in the main study. We don’t recommend this practice. The trouble is that your pilot study will have a small sample size, leading to a very imprecise estimate of effect size (Browne 1995). If you overestimate the effect size, your main study will be very underpowered. If you underestimate, the opposite will be true. Using a pilot for power analysis is a recipe for problems.\nGeneral expectations about an effect of interest. In our view, perhaps the best way you can use power analysis (in the absence of a really strong meta-analysis, at least) is to start with a general idea about the size of effect you expect and would like to be able to detect. It is totally reasonable to say, “I don’t know how big my effect is going to be, but let’s see what my power would be if it were medium-sized (say \\(d=0.5\\)), since that’s the kind of thing we’re hoping for with our money intervention.” This kind of power analysis can help you set your expectations about what range of effects you might be able to detect with a given sample size.\n\nFor our money study, using our general expectation of a medium-size effect, we can compute power for \\(d=0.5\\). In this case, we’ll simply use the two-sample \\(t\\)-test introduced in chapter 6, for which 80% power at \\(\\alpha = 0.05\\) and \\(d=0.5\\) is achieved by having \\(N = 64\\) in each group.\n\n\n\n\n\n\ncode\n\n\n\n\n\nClassic power analysis in R is quite simple using the pwr package. The package offers a set of test-specific functions like pwr.t.test(). For each, you supply three of the four parameters specifying effect size (d), number of observations (n), significance level (sig.level), and power (power); the function computes the fourth. For classic power analysis, we leave out n:\n\npwr.t.test(d = .5, \n           power = .8, \n           sig.level = .05,\n           type = \"two.sample\", \n           alternative = \"two.sided\")\n\nBut it is also possible to use this same function to compute the power achieved at a combination of \\(n\\) and \\(d\\), for example.\n\n\n\nThere’s a second issue, however. The second fundamental problem of power analysis is that the real effect size for an experiment may be zero. And in that case, no sample size will let you correctly reject the null. Going back to our discussion in chapter 6, the null hypothesis significance testing framework is just not set up to let you accept the null hypothesis. If you are interested in a bidirectional approach to hypothesis testing in which you can accept and reject the null, you may need to consider Bayes Factor or equivalence testing approaches (Lakens, Scheel, and Isager 2018), which don’t fit the assumptions of classical power analysis.\n\n\n10.4.3 Alternative approaches to sample size planning\nLet’s now consider some alternatives to classic power analysis that can still yield reasonable sample size justifications.\n\nResource constraint. In some cases, there are fundamental resource constraints that limit data collection. For example, if you are doing fieldwork, sometimes the right stopping criterion for data collection is “when the field visit is over,” since every additional datapoint is valuable. When prespecified, these kinds of sample size justifications can be quite reasonable, although they do not preclude being underpowered to test a particular hypothesis.\nSmallest effect size of interest (SESOI). SESOI analysis is a variant on power analysis that includes some resource constraint planning. Instead of trying to intuit how big your target effect is, you instead choose a level below which you might not be interested in detecting the effect. This choice can be informed by theory (what is predicted), applied concerns (what sort of effect might be useful in a particular context), or resource constraints (how expensive or time-consuming it might be to run an experiment). In practice, SESOI analysis is simply a classic power analysis with a particular small effect as the target.\nPrecision-based sample planning. As we discussed in chapter 6, the goal of research is not always to reject the null hypothesis! Sometimes—we’d argue that it should be most of the time—the goal is to estimate a particular causal effect of interest with a high level of precision, since these estimates are a prerequisite for building theories. If what you want is an estimate with known precision (say, a confidence interval of a particular width), you can compute the sample size necessary to achieve that precision (Bland 2009; Rothman and Greenland 2018).13\nSequential analysis. Your stopping rule need not be a hard cutoff at a specific \\(N\\). Instead, it’s possible to plan a sequential analysis using either frequentist or Bayesian methods, in which you plan to stop collecting data once a particular inferential threshold is reached. For the frequentist version, the key thing that keeps sequential analysis from being \\(p\\)-hacking is that you prespecify particular values of \\(N\\) at which you will conduct tests and then correct your \\(p\\)-values for having tested multiple times (Lakens 2014). For Bayesian sequential analysis, you can actually compute a running Bayes factor as you collect data and stop when you reach a prespecified level of evidence (Schönbrodt et al. 2017). This latter alternative has the advantage of allowing you to collect evidence for the null as well as against it.14\n\n13 In our experience, this kind of planning is most useful when you are attempting to gather measurements with sufficient precision to compare between computational models. Since the models can make quantitative predictions that differ by some known amount, then it’s clear how tight your confidence intervals need to be.14 Another interesting variant is sequential parameter estimation, in which you collect data until a desired level of precision is achieved (Kelley, Darku, and Chattopadhyay 2018); this approach combines some of the benefits of both precision-based analysis and sequential analysis.In sum, there are many different ways of justifying your sample size or your stopping rule. The most important things are (1) to prespecify your strategy and (2) to give a clear justification for your choice. Table 10.2 gives an example sample size justification that draws on several different concepts discussed here, using classical power computations as one part of the justification. A reviewer could easily follow the logic of this discussion and form their own conclusion about whether this study had an adequate sample size and whether it should have been conducted given the researchers’ constraints.\n\n\n\nTable 10.2: Example sample size justification, referencing elements of SESOI, resource limitation, and power-based reasoning.\n\n\n\n\n\n\n\n\n\nElement\nJustification text\n\n\n\n\nBackground\nWe did not have strong prior information about the likely effect size, so we could not compute a classical power analysis.\n\n\nSmallest effect of interest\nBecause of our interest in meaningful factors affecting word learning, we were interested in effect sizes as small as \\(d=0.5\\).\n\n\nResource limitation\nWe were also limited by our ability to collect data only at our on-campus preschool.\n\n\nPower computation\nWe calculated that based on our maximal possible sample size of \\(N = 120\\) (60 per group), we would achieve at least 80% power to reject the null for effects as small as \\(d = 0.52\\).\n\n\n\n\n\n\n\n\n\n\n\n\ndepth\n\n\n\n\n\nSample sizes for replication studies\nSetting the sample size for a replication study has been a persistent issue in the metascience literature. Naïvely speaking, it seems like you should be able to compute the effect size for the original study and then simply use that as the basis for a classical power analysis.\nThis naive approach has several flaws, however. First, the effect size from the original published paper is likely an overestimate of the true effect size due to publication bias (Nosek et al. 2022).  Second, the power analysis will only yield the sample size at which the replication will have a particular chance of rejecting the null at some criterion. But it’s quite possible that the original experiment could be \\(p&lt;0.05\\), the replication could be \\(p&gt;0.05\\), and the original experiment and the replication results are not significantly different from each other. So a statistically significant replication of the original effect size is not necessarily what you want to aim for.\nFaced with these issues, a replication sample size can be planned in several other ways. First, replicators can use the standard strategies mentioned above such as SESOI or resource-based planning to rule out large effects, either with high probability or within a known amount of time or money. If the SESOI is high or limited resources are allocated, these strategies can produce an inconclusive result, however. A conclusive answer can require a very substantial commitment of resources.\nSecond, Simonsohn (2015) recommends the “small telescopes” approach. The idea is not to test whether there is an effect, but rather where there is an effect large enough that the original study could have detected it. The analogy is to astronomy. If a birdwatcher points their binoculars at the sky and claims to have discovered a new planet, we want to ask not just whether there is a planet at that location but also whether there is any possibility that they could have seen it using binoculars—if not, perhaps they are right but for the wrong reasons! Simonsohn shows that, if a replicator collects 2.5 times as large a sample as the original, they have 80% power to detect any effect that was reasonably detectable by the original. This simple rule of thumb provides one good starting place for conservative replication studies.\nFinally, replicators can make use of sequential Bayesian analysis, in which they attempt to gather substantial evidence relative to the support for \\(H_1\\) or \\(H_0\\). Sequential bayes is an appealing option because it allows for efficient collection of data that reflects whether an effect is likely to be present in a particular sample, especially in the face of the sometimes prohibitively large samples necessary for SESOI or “small telescopes” analyses.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#sample-sizes-for-replication-studies",
    "href": "010-sampling.html#sample-sizes-for-replication-studies",
    "title": "10  Sampling",
    "section": "Sample sizes for replication studies",
    "text": "Sample sizes for replication studies\nSetting the sample size for a replication study has been a persistent issue in the metascience literature. Naïvely speaking, it seems like you should be able to compute the effect size for the original study and then simply use that as the basis for a classical power analysis.\nThis naive approach has several flaws, however. First, the effect size from the original published paper is likely an overestimate of the true effect size due to publication bias (Nosek et al. 2022).  Second, the power analysis will only yield the sample size at which the replication will have a particular chance of rejecting the null at some criterion. But it’s quite possible that the original experiment could be \\(p&lt;0.05\\), the replication could be \\(p&gt;0.05\\), and the original experiment and the replication results are not significantly different from each other. So a statistically significant replication of the original effect size is not necessarily what you want to aim for.\nFaced with these issues, a replication sample size can be planned in several other ways. First, replicators can use the standard strategies mentioned above such as SESOI or resource-based planning to rule out large effects, either with high probability or within a known amount of time or money. If the SESOI is high or limited resources are allocated, these strategies can produce an inconclusive result, however. A conclusive answer can require a very substantial commitment of resources.\nSecond, Simonsohn (2015) recommends the “small telescopes” approach. The idea is not to test whether there is an effect, but rather where there is an effect large enough that the original study could have detected it. The analogy is to astronomy. If a birdwatcher points their binoculars at the sky and claims to have discovered a new planet, we want to ask not just whether there is a planet at that location but also whether there is any possibility that they could have seen it using binoculars—if not, perhaps they are right but for the wrong reasons! Simonsohn shows that, if a replicator collects 2.5 times as large a sample as the original, they have 80% power to detect any effect that was reasonably detectable by the original. This simple rule of thumb provides one good starting place for conservative replication studies.\nFinally, replicators can make use of sequential Bayesian analysis, in which they attempt to gather substantial evidence relative to the support for \\(H_1\\) or \\(H_0\\). Sequential bayes is an appealing option because it allows for efficient collection of data that reflects whether an effect is likely to be present in a particular sample, especially in the face of the sometimes prohibitively large samples necessary for SESOI or “small telescopes” analyses.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "010-sampling.html#chapter-summary-sampling",
    "href": "010-sampling.html#chapter-summary-sampling",
    "title": "10  Sampling",
    "section": "10.5 Chapter summary: Sampling",
    "text": "10.5 Chapter summary: Sampling\nYour goal as an experimenter is to estimate a causal effect.  But the effect for whom?  This chapter has tried to help you think about how you generalize from your experimental sample to some target population. It’s very rare to be conducting an experiment based on a probability sample in which every member of the population has an equal chance of being selected. In the case that you are using a convenience sample, you will need to consider how bias introduced by the sample could relate to the effect estimate you observed. Do you think this effect is likely to be very heterogeneous in the population? Are there theories that suggest that it might be larger or smaller for the convenience sample you recruited?\nQuestions about generalizability and sampling depend on the precise construct you are studying, and there is no mechanistic procedure for answering them. Instead, you simply have to ask yourself: How does my sampling procedure qualify the inference I want to make based on my data? Being transparent about your reasoning can be very helpful—both to you and to readers of your work who want to contextualize the generality of your findings.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nWe want to understand human cognition generally, but do you think it is a more efficient research strategy to start by studying certain features of cognition (perception, for example) in WEIRD convenience populations and then later check our generalizations in non-WEIRD groups? What are the arguments against this efficiency-based strategy?\nOne alternative position regarding sampling is that the most influential experiments aren’t generalizations of some number to a population; they are demonstration experiments that show that some particular effect is possible under some circumstances (think Milgram’s conformity studies, see chapter 4). On this argument, the specifics of population sampling are often secondary. Do you think this position makes sense?\n\n\n\nOne line of argument says that we can’t ever make generalizations about the human mind because so much of the historical human population is simply inaccessible to us (we can’t do experiments on ancient Greek psychology). In other words, sampling from a particular population is also sampling a particular moment in time. How should we qualify our research interpretations to deal with this issue?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nThe original polemic article on the WEIRD problem: Henrich, Joseph, Steven J. Heine, and Ara Norenzayan (2010). “The WEIRDest People in the World?” Behavioral and Brain Sciences 33 (2–3): 61–83.\nA very accessible introduction to power analysis from its originator: Cohen, Jacob (1992). “A Power Primer.” Psychological Bulletin 112 (1): 155.\nA thoughtful and in-depth discussion of generalizability issues: Yarkoni, Tal (2020). “The Generalizability Crisis.” Behavioral and Brain Sciences 45:1–37.",
    "crumbs": [
      "Planning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling</span>"
    ]
  },
  {
    "objectID": "011-prereg.html",
    "href": "011-prereg.html",
    "title": "11  Preregistration",
    "section": "",
    "text": "11.1 Lost in a garden of forking paths\nOne way to visualize researcher degrees of freedom is as a vast decision tree or “garden of forking paths” (figure 11.3). Each node represents a decision point, and each branch represents a justifiable choice. Each unique pathway through the garden terminates in an individual research outcome.\nBecause scientific observations typically consist of both noise (random variation unique to this sample) and signal (regularities that will reoccur in other samples), some of these pathways will inevitably lead to results that are misleading (e.g., inflated effect sizes, exaggerated evidence, or false positives). The more potential paths in the garden that you might explore, the higher the chance of encountering misleading results.\nStatisticians refer to this issue as a multiplicity (multiple comparisons) problem. As we talked about in chapter 6, multiplicity can be addressed to some extent with statistical countermeasures, like the Bonferroni correction; however, these adjustment methods need to account for every path that you could have taken (Gelman and Loken 2014; de Groot 2014 [1956]). When you navigate the garden of forking paths while working with the data, it is easy to forget—or even be unaware of—every path that you could have taken, so these methods can no longer be used effectively.\nThe signal-to-noise ratio is worse in particular situations (as common in psychology) with small effect sizes, high variation, and large measurement errors (Ioannidis 2005). Researcher degrees of freedom may be constrained to some extent by strong theory (Oberauer and Lewandowsky 2019), community methodological norms, or replication studies, though these constraints may be more implicit than explicit, and can still leave plenty of room for flexible decision-making.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#undisclosed-analytic-flexibility",
    "href": "011-prereg.html#undisclosed-analytic-flexibility",
    "title": "11  Preregistration",
    "section": "Undisclosed analytic flexibility?",
    "text": "Undisclosed analytic flexibility?\nEducational apps for children are a huge market, but relatively few randomized trials have been done to see whether or when they produce educational gains. Filling this important gap, Berkowitz et al. (2015) reported a high-quality field experiment of a free educational app, “Bedtime Math at Home,” with participants randomly assigned to either math or reading conditions over the course of a full school year. Critically, along with random assignment, the study also included standardized measures of math and reading achievement. These measures allowed the authors to compute effects in grade-level equivalents, a meaningful unit from a policy perspective.\n\n\n\n\n\n\n\n\nFigure 11.1: Model fits reported in figure 1 of Berkowitz et al. (2015). Estimated years of math achievement gained over the school year across groups, as a function of app usage level.\n\n\n\n\n\nThe key result is shown in figure 11.1. Families who used the math app frequently showed greater gains in math than the control group. Although this finding appeared striking, the figure didn’t directly visualize the primary causal effect of interest, namely the size of the effect of study condition on math scores. Instead the data were presented as estimated effects for specific levels of app usage.\nBecause the authors made their data openly available, it was possible for Frank (2016) to do a simple analysis to examine the causal effect of interest. When not splitting the data by usage and adjusting by covariates, there was no significant main effect of the intervention on math performance figure 11.2. Since this analysis was not favorable to the primary intervention—and because it was not reported in the paper—it could have been the case that the authors had analyzed the data several ways and chosen to present an analysis that was more favorable to their hypotheses of interest. \n\n\n\n\n\n\n\n\nFigure 11.2: Estimated years of math achievement gained over the school year across groups in the Berkowitz et al. (2016) math app trial. Error bars show bootstrapped 95% confidence intervals. Based on Frank (2016).\n\n\n\n\n\nAs is true for many papers prior to the rise of preregistration, it’s not possible to know definitively whether the reported analysis in Berkowitz et al. (2015) was influenced by the authors’ desired result. As we’ll see below, such data-dependent analyses can lead to substantial bias in reported effects. This uncertainty about a paper’s analytic strategy can be avoided by the use of preregistration. In this case, preregistration would have convinced readers that the analyses decisions were not influenced by the data, thereby increasing the value of this otherwise high-quality study.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#lost-in-a-garden-of-forking-paths",
    "href": "011-prereg.html#lost-in-a-garden-of-forking-paths",
    "title": "11  Preregistration",
    "section": "Only human: Cognitive biases and skewed incentives",
    "text": "Figure 11.3: The garden of forking paths: many justifiable but different analytic choices are possible for an individual dataset.\n\n\n\n\n\n\n11.1.1 Data-dependent analysis\nWhen a researcher navigates the garden of forking paths during data analysis, their choices might be influenced by the data (data-dependent decision-making), which can introduce bias. If a researcher is seeking a particular kind of result (see the Depth box below), then they are more likely to follow the branches that steer them in that direction.\nYou could think of this a bit like playing a game of “hot () or cold (),” where  indicates that the choice will move the researcher closer to a desirable overall result and  indicates that the choice will move them further away. Each time the researcher reaches a decision point, they try one of the branches and get feedback on how that choice affects the results. If the feedback is  then they take that branch. If the answer is , they try a different branch. If they reach the end of a complete pathway, and the result is , maybe they even retrace their steps and try some different branches earlier in the pathway. This strategy creates a risk of bias because it systematically skews results toward the researcher’s preferences (Hardwicke and Wagenmakers 2023).1\n1 We say “risk of bias” rather than just “bias” because in most scientific contexts, we do not have a known ground truth to compare the results to. So in any specific situation, we do not know the extent to which data-dependent decisions have actually biased the results.\n\n\n\n\n\ndepth\n\n\n\n\n\nOnly human: Cognitive biases and skewed incentives\nThere’s a storybook image of the scientist as an objective, rational, and dispassionate arbiter of truth (Veldkamp et al. 2017). But in reality, scientists are only human: they have egos, career ambitions, and rent to pay! So even if we do want to live up to the storybook image, it’s important to acknowledge that our decisions and behavior are also influenced by a range of cognitive biases and external incentives that can steer us away from that goal. Let’s first look at some relevant cognitive biases that might lead scientists astray:\n\nConfirmation bias: Preferentially seeking out, recalling, or evaluating information in a manner that reinforces one’s existing beliefs (Nickerson 1998).\nHindsight bias: Believing that past events were always more likely to occur relative to our actual belief in their likelihood before they happened (“I knew it all along!”) (Slovic and Fischhoff 1977).\nMotivated reasoning: Rationalizing prior decisions so they are framed in a favorable light, even if they were irrational (Kunda 1990).\n\n\n\nApophenia: Detecting seemingly meaningful patterns in noise (Gilovich, Vallone, and Tversky 1985).\n\nTo make matters worse, the incentive structure of the scientific ecosystem often adds additional motivation to get things wrong. The allocation of funding, awards, and publication prestige is often based on the nature of research results rather than research quality (Smaldino and McElreath 2016; Nosek, Spies, and Motyl 2012). For example, many academic journals, especially those that are widely considered to be the most prestigious, appear to have a preference for novel, positive, and statistically significant results over incremental, negative, or null results (Bakker, Dijk, and Wicherts 2012). There is also pressure to write articles with concise, coherent, and compelling narratives (Giner-Sorolla 2012). This set of forces incentivizes scientists to be “impressive” over being right and encourages questionable research practices. The process of iteratively \\(p\\)-hacking and HARKing one’s way to a “beautiful” scientific paper has been dubbed “The Chrysalis Effect” (O’Boyle, Banks, and Gonzalez-Mulé 2017), illustrated in figure 11.4.\n\n\n\n\n\n\nFigure 11.4: The Chrysalis Effect, when ugly truth becomes a beautiful fiction.\n\n\n\nIn sum, scientists’ human flaws—and the scientific ecosystem’s flawed incentives—highlight the need for transparency and intellectual humility when reporting the findings of our research (Hoekstra and Vazire 2021).\n\n\n\nIn the most egregious cases, a researcher may try multiple pathways until they obtain a desirable result and then selectively report that result, neglecting to mention that they have tried several other analysis strategies (also known as \\(p\\)-hacking, a practice we’ve discussed throughout the book).2 You may remember an example of this practice in chapter 3, where participants apparently became younger when they listened to “When I’m 64” by The Beatles. Another example of how damaging the garden of forking paths can be comes from the “discovery” of brain activity in a dead Atlantic Salmon (Bennett, Miller, and Wolford 2009)! Researchers deliberately exploited flexibility in the fMRI analysis pipeline and avoided multiple comparisons corrections, allowing them to find brain activity where there was only dead fish (figure 11.5).\n2 “If you torture the data long enough, it will confess” (Good 1972).\n\n\n\n\n\nFigure 11.5: By deliberately exploiting analytic flexibility in the processing pipeline of fMRI data, Bennett, Miller, and Wolford (2009) were able to identify “brain activity” in a dead Atlantic Salmon. From Bennett, Miller, and Wolford (2009, licensed under CC BY).\n\n\n\n\n\n11.1.2 Hypothesizing after results are known\nIn addition to degrees of freedom in experimental design and analysis, there is additional flexibility in how researchers interpret research results. As we discussed in chapter 2, theories can accommodate even conflicting results in many different ways—for example, by positing auxiliary hypotheses that explain why a particular datapoint is special.\nThe practice of selecting or developing your hypothesis after observing the data has been called “hypothesizing after the results are known,” or “HARKing” (Kerr 1998). HARKing is potentially problematic because it expands the garden of forking paths and helps to justify the use of various additional design and analysis decisions (?fig-grid). For example, you may come up with an explanation for why an intervention is effective in men but not in women in order to justify a post hoc subgroup analysis based on sex (see the Case study box). The extent to which HARKing is problematic is contested (for discussion see Hardwicke and Wagenmakers 2023). But at the very least, it’s important to be honest about whether hypotheses were developed before or after observing the data.\n{#fig-grid .margin-caption width=70% fig-alt=“A diagram of a grid with axes degrees of freedom to fit”evidence to hypotheses” and “hypotheses to evidence”.”}\nBut hang on a minute! Isn’t it a good thing to seek out interesting results if they are there in the data? Shouldn’t we “let the data speak”? The answer is yes! But it’s crucial to understand the distinction between exploratory and confirmatory modes of research.3 Confirmation involves making research decisions before you’ve seen the data whereas exploration involves making research decisions after you’ve seen data.\n3 In practice, an individual study may contain both exploratory and confirmatory aspects, which is why we describe them as different “modes.”The key things to remember about exploratory research are that you need to (1) be aware of the increased risk of bias arising from data-dependent decision-making and calibrate your confidence in the results accordingly; and (2) be honest with other researchers about your analysis strategy so they are also aware of the risk of bias and can calibrate their confidence in the outcomes accordingly. In the next section, we will learn about how preregistration helps us to make this important distinction between exploratory and confirmation research.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#only-human-cognitive-biases-and-skewed-incentives",
    "href": "011-prereg.html#only-human-cognitive-biases-and-skewed-incentives",
    "title": "11  Preregistration",
    "section": "",
    "text": "There’s a storybook image of the scientist as an objective, rational, and dispassionate arbiter of truth (Veldkamp et al. 2017). But in reality, scientists are only human: they have egos, career ambitions, and rent to pay! So even if we do want to live up to the storybook image, it’s important to acknowledge that our decisions and behavior are also influenced by a range of cognitive biases and external incentives that can steer us away from that goal. Let’s first look at some relevant cognitive biases that might lead scientists astray:\n\nConfirmation bias: Preferentially seeking out, recalling, or evaluating information in a manner that reinforces one’s existing beliefs (Nickerson 1998).\nHindsight bias: Believing that past events were always more likely to occur relative to our actual belief in their likelihood before they happened (“I knew it all along!”) (Slovic and Fischhoff 1977).\nMotivated reasoning: Rationalizing prior decisions so they are framed in a favorable light, even if they were irrational (Kunda 1990).\n\n\n\nApophenia: Detecting seemingly meaningful patterns in noise (Gilovich, Vallone, and Tversky 1985).\n\nTo make matters worse, the incentive structure of the scientific ecosystem often adds additional motivation to get things wrong. The allocation of funding, awards, and publication prestige is often based on the nature of research results rather than research quality (Smaldino and McElreath 2016; Nosek, Spies, and Motyl 2012). For example, many academic journals, especially those that are widely considered to be the most prestigious, appear to have a preference for novel, positive, and statistically significant results over incremental, negative, or null results (Bakker, Dijk, and Wicherts 2012). There is also pressure to write articles with concise, coherent, and compelling narratives (Giner-Sorolla 2012). This set of forces incentivizes scientists to be “impressive” over being right and encourages questionable research practices. The process of iteratively \\(p\\)-hacking and HARKing one’s way to a “beautiful” scientific paper has been dubbed “The Chrysalis Effect” (O’Boyle, Banks, and Gonzalez-Mulé 2017), illustrated in figure 11.4.\n\n\n\n\n\n\nFigure 11.4: The Chrysalis Effect, when ugly truth becomes a beautiful fiction.\n\n\n\nIn sum, scientists’ human flaws—and the scientific ecosystem’s flawed incentives—highlight the need for transparency and intellectual humility when reporting the findings of our research (Hoekstra and Vazire 2021).",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#reducing-risk-of-bias-increasing-transparency-and-calibrating-confidence-with-preregistration",
    "href": "011-prereg.html#reducing-risk-of-bias-increasing-transparency-and-calibrating-confidence-with-preregistration",
    "title": "11  Preregistration",
    "section": "11.2 Reducing risk of bias, increasing transparency, and calibrating confidence with preregistration",
    "text": "11.2 Reducing risk of bias, increasing transparency, and calibrating confidence with preregistration\nYou can counter the problem of researcher degrees of freedom and data-dependent decision-making by making research decisions before you have seen the data—like planning your route through the garden of forking paths before you start your journey (Wagenmakers et al. 2012; Hardwicke and Wagenmakers 2023). If you stick to the planned route, then you have eliminated the possibility that your decisions were influenced by the data.\nPreregistration is the process of declaring your research decisions in a public registry before you analyze (and often before you collect) the data. Preregistration ensures that your research decisions are data-independent, which reduces risk of bias arising from the issues described above. Preregistration also transparently conveys to others what you planned, helping them to determine the risk of bias and calibrate their confidence in the research results. In other words, preregistration can dissuade researchers from engaging in questionable research practices like \\(p\\)-hacking and HARKing, because they can be held accountable to their original plan while also providing the context needed to properly evaluate and interpret research.\n\n\n\n\n\n\nFigure 11.6: Preregistration clarifies where research activities fall on the continuum of prespecification. When the preregistration provides little constraint over researcher degrees of freedom (i.e., more exploratory research), decisions are more likely to be data-dependent, and consequently there is a higher risk of bias. When preregistration provides strong constraint over researcher degrees of freedom (i.e., more confirmatory research), decisions are less likely to be data dependent, and consequently there is a lower risk of bias. Exploratory research activities are more sensitive to serendipitous discovery but also have a higher risk of bias relative to confirmatory research activities. Preregistration transparently communicates where particular results are located along the continuum, helping readers to appropriately calibrate their confidence.\n\n\n\nPreregistration does not require that you specify all research decisions in advance, only that you are transparent about what was planned, and what was not planned. This transparency helps to make a distinction between which aspects of the research were exploratory and which were confirmatory (figure 11.6). All else being equal, we should have more confidence in confirmatory results, because there is a lower risk of bias. Exploratory results have a higher risk of bias, but they are also more sensitive to serendipitous (unexpected) discoveries. So the confirmatory mode is best suited to testing hypotheses, and the exploratory mode is best suited to generating them. Therefore, exploratory and confirmatory research are both valuable activities—it is just important to differentiate them (Tukey 1980)! Preregistration offers the best of both worlds by clearly separating one from the other.\nIn addition to the benefits described above, preregistration may improve the quality of research by encouraging closer attention to study planning. We’ve found that the process of writing a preregistration really helps facilitate communication between collaborators, and can catch addressable problems before time and resources are wasted on a poorly designed study. Detailed advanced planning can also create opportunities for useful community feedback, particularly in the context of registered reports (see the Depth box below), where dedicated peer reviewers will evaluate your study before it has even begun.\n\n\n\n\n\n\ndepth\n\n\n\n\n\nPreregistration and friends: A toolbox to address researcher degrees of freedom\nSeveral useful tools can be used to complement or extend preregistration. In general, we would recommend that these tool are combined with preregistration, rather than used as a replacement because preregistration provides transparency about the research and planning process (Hardwicke and Wagenmakers 2023). The first two of these are discussed in more detail in the last section of chapter 7.\nRobustness checks. Robustness checks (also called “sensitivity analyses”) assess how different decision choices in the garden of forking paths affect the eventual pattern of results. This technique is particularly helpful when you have to choose between several justifiable analytic choices, neither of which seem superior to the other, or which have complementary strengths and weaknesses. For example, you might run the analysis three times using three different methods for handling missing data. Robust results should not vary substantially across the three different choices.\nMultiverse analyses. Recently, some researchers have started running large-scale robustness checks called “multiverse” (Steegen et al. 2016) or “specification curve” (Simonsohn, Simmons, and Nelson 2020) analyses. We discussed these a bit in chapter 7.  Some have argued that these large-scale robustness checks make preregistration redundant; after all, why prespecify a single path if you can explore them all (Rubin 2020; Oberauer and Lewandowsky 2019)? But interpreting the results of a multiverse analysis is not straightforward; for example, it seems unlikely that all of the decision choices are equally justifiable (Giudice and Gangestad 2021). Furthermore, if multiverse analyses are not preregistered, then they introduce researcher degrees of freedom and create an opportunity for selective reporting, which increases risk of bias.\nHeld-out sample. One option to benefit from both exploratory and confirmatory research modes is to split your data into training and test samples. (The test sample is commonly called “held out” because it is “held out” from the exploratory process.) You can generate hypotheses in an exploratory mode in the training sample and use that as the basis to preregister confirmatory analyses in the held-out sample. A notable disadvantage of this strategy is that splitting the data reduces statistical power, but in cases where data are plentiful—including in much of machine learning—this technique is the gold standard.\nMasked analysis (traditionally called “blind analysis”). Sometimes problems, such as missing data, attrition, or randomization failure that you did not anticipate in your preregistered plan, can arise during data collection. How do you diagnose and address these issues without increasing risk of bias through data-dependent analysis? One option is masked analysis, which disguises key aspects of the data related to the results (for example, by shuffling condition labels or adding noise) while still allowing some degree of data inspection (Dutilh, Sarafoglou, and Wagenmakers 2019). After diagnosing a problem, you can adjust your preregistered plan without increasing risk of bias, because your decisions have not been influenced by the results.\nStandard operating procedures. Community norms, perhaps at the level of your research field or lab, can act as a natural constraint on researcher degrees of freedom. For example, there may be a generally accepted approach for handling outliers in your community. You can make these constraints explicit by writing them down in a standard operating procedures (SOP) document—a bit like a living meta-preregistration (Lin and Green 2016).\nOpen lab notebooks. Maintaining a lab notebook can be a useful way to keep a record of your decisions as a research project unfolds. Preregistration is a bit like taking a snapshot of your lab notebook at the start of the project, when all you have written down is your research plan. Making your lab notebook publicly available is a great way to transparently document your research and departures from the preregistered plan.\n\n\n\n\n\n\nFigure 11.7: Registered reports (from https://www.cos.io/initiatives/registered-reports, licensed under CC BY 4.0).\n\n\n\nRegistered reports. Registered reports (figure 11.7) are a type of article format that embeds preregistration directly into the publication pipeline (Chambers and Tzavella 2020). The idea is that you submit your preregistered protocol to a journal and it is peer reviewed before you’ve even started your study. If the study is approved, the journal agrees to publish it, regardless of the results. This is a radical departure from traditional publication models where peer reviewers and journals evaluate your study after its been completed and the results are known. Because the study is accepted for publication independently of the results, registered reports can offer the benefits of preregistration with additional protection against publication bias. They also provide a great opportunity to obtain feedback on your study design while you can still change it!",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#preregistration-and-friends-a-toolbox-to-address-researcher-degrees-of-freedom",
    "href": "011-prereg.html#preregistration-and-friends-a-toolbox-to-address-researcher-degrees-of-freedom",
    "title": "11  Preregistration",
    "section": "Preregistration and friends: A toolbox to address researcher degrees of freedom",
    "text": "Preregistration and friends: A toolbox to address researcher degrees of freedom\nSeveral useful tools can be used to complement or extend preregistration. In general, we would recommend that these tool are combined with preregistration, rather than used as a replacement because preregistration provides transparency about the research and planning process (Hardwicke and Wagenmakers 2023). The first two of these are discussed in more detail in the last section of chapter 7.\nRobustness checks. Robustness checks (also called “sensitivity analyses”) assess how different decision choices in the garden of forking paths affect the eventual pattern of results. This technique is particularly helpful when you have to choose between several justifiable analytic choices, neither of which seem superior to the other, or which have complementary strengths and weaknesses. For example, you might run the analysis three times using three different methods for handling missing data. Robust results should not vary substantially across the three different choices.\nMultiverse analyses. Recently, some researchers have started running large-scale robustness checks called “multiverse” (Steegen et al. 2016) or “specification curve” (Simonsohn, Simmons, and Nelson 2020) analyses. We discussed these a bit in chapter 7.  Some have argued that these large-scale robustness checks make preregistration redundant; after all, why prespecify a single path if you can explore them all (Rubin 2020; Oberauer and Lewandowsky 2019)? But interpreting the results of a multiverse analysis is not straightforward; for example, it seems unlikely that all of the decision choices are equally justifiable (Giudice and Gangestad 2021). Furthermore, if multiverse analyses are not preregistered, then they introduce researcher degrees of freedom and create an opportunity for selective reporting, which increases risk of bias.\nHeld-out sample. One option to benefit from both exploratory and confirmatory research modes is to split your data into training and test samples. (The test sample is commonly called “held out” because it is “held out” from the exploratory process.) You can generate hypotheses in an exploratory mode in the training sample and use that as the basis to preregister confirmatory analyses in the held-out sample. A notable disadvantage of this strategy is that splitting the data reduces statistical power, but in cases where data are plentiful—including in much of machine learning—this technique is the gold standard.\nMasked analysis (traditionally called “blind analysis”). Sometimes problems, such as missing data, attrition, or randomization failure that you did not anticipate in your preregistered plan, can arise during data collection. How do you diagnose and address these issues without increasing risk of bias through data-dependent analysis? One option is masked analysis, which disguises key aspects of the data related to the results (for example, by shuffling condition labels or adding noise) while still allowing some degree of data inspection (Dutilh, Sarafoglou, and Wagenmakers 2019). After diagnosing a problem, you can adjust your preregistered plan without increasing risk of bias, because your decisions have not been influenced by the results.\nStandard operating procedures. Community norms, perhaps at the level of your research field or lab, can act as a natural constraint on researcher degrees of freedom. For example, there may be a generally accepted approach for handling outliers in your community. You can make these constraints explicit by writing them down in a standard operating procedures (SOP) document—a bit like a living meta-preregistration (Lin and Green 2016).\nOpen lab notebooks. Maintaining a lab notebook can be a useful way to keep a record of your decisions as a research project unfolds. Preregistration is a bit like taking a snapshot of your lab notebook at the start of the project, when all you have written down is your research plan. Making your lab notebook publicly available is a great way to transparently document your research and departures from the preregistered plan.\n\n\n\n\n\n\nFigure 11.7: Registered reports (from https://www.cos.io/initiatives/registered-reports, licensed under CC BY 4.0).\n\n\n\nRegistered reports. Registered reports (figure 11.7) are a type of article format that embeds preregistration directly into the publication pipeline (Chambers and Tzavella 2020). The idea is that you submit your preregistered protocol to a journal and it is peer reviewed before you’ve even started your study. If the study is approved, the journal agrees to publish it, regardless of the results. This is a radical departure from traditional publication models where peer reviewers and journals evaluate your study after its been completed and the results are known. Because the study is accepted for publication independently of the results, registered reports can offer the benefits of preregistration with additional protection against publication bias. They also provide a great opportunity to obtain feedback on your study design while you can still change it!",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#how-to-preregister",
    "href": "011-prereg.html#how-to-preregister",
    "title": "11  Preregistration",
    "section": "11.3 How to preregister",
    "text": "11.3 How to preregister\nHigh-stakes studies such as medical trials must be preregistered (Dickersin and Rennie 2012). In 2005, a large international consortium of medical journals decided that they would not publish unregistered trials. The discipline of economics also has strong norms about study registration (see, e.g., https://www.socialscienceregistry.org). But preregistration is pretty new to psychology (Nosek et al. 2018), and there’s still no standard way of doing it—you’re already at the cutting edge!\nWe recommend using the Open Science Framework (OSF) as your registry. OSF is one of the most popular registries in psychology, and you can do lots of other useful things on the platform to make your research transparent, like sharing data, materials, analysis scripts, and preprints. On OSF, it’s possible to “register” any file you have uploaded. When you register a file, it creates a time-stamped, read-only copy, with a dedicated link. You can add this link to articles reporting your research.\n\n\n\nTable 11.1: Preregistration template outline.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\n\n1\nData collection. Have any data been collected for this study already?\n\n\n\n\n2\nHypothesis. What’s the main question being asked or hypothesis being tested in this study?\n\n\n\n\n3\nDependent variable. Describe the key dependent variable(s) specifying how they will be measured.\n\n\n\n\n4\nConditions. How many and which conditions will participants be assigned to?\n\n\n\n\n5\nAnalyses. Specify exactly which analyses you will conduct to examine the main question/hypothesis.\n\n\n\n\n6\nOutliers and Exclusions. Describe exactly how outliers will be defined and handled, and your precise rule(s) for excluding observations.\n\n\n\n\n7\nSample Size. How many observations will be collected, or what will determine sample size? No need to justify decision, but be precise about exactly how the number will be determined.\n\n\n\n\n8\nOther. Anything else you would like to preregister (e.g., secondary analyses, variables collected for exploratory purposes, unusual analyses planned).\n\n\n\n\n\n\n\n\nOne approach to preregistration is to write a protocol document that specifies the study rationale, aims or hypotheses, methods, and analysis plan, and register that document.4 Open Science Framework also has a collection of dedicated preregistration templates that you can use if you prefer. An outline of such a template is shown in table 11.1. These templates are often tailored to the needs of particular types of research. For example, there are templates for general quantitative psychology research (“PRP-QUANT”; Bosnjak et al. 2022), cognitive modeling (Crüwell and Evans 2021), and secondary data analysis (Akker et al. 2019). The OSF interface may change, but currently this guide provides a set of steps to create a preregistration.\n4 You can think of a study protocol as a bit like a research paper without a results and discussion section (here’s an example from one of our own studies: https://osf.io/2cnkq).Once you’ve preregistered your plan, you just go off and run the study and report the results, right? Well hopefully … but things might not turn out to be that straightforward. It’s quite common to forget to include something in your plan or to have to depart from the plan due to something unexpected. Preregistration can actually be pretty hard in practice (Nosek et al. 2019).\nDon’t worry though—remember that a key goal of preregistration is transparency to enable others to evaluate and interpret research results. If you decide to depart from your original plan and conduct data-dependent analyses, then this decision may increase the risk of bias. But if you communicate this decision transparently to your readers, they can appropriately calibrate their confidence in the results. You may even be able to run both the planned and unplanned analyses as a robustness check (see the Depth box) to evaluate the extent to which this particular choice impacts the results.\nWhen you report your study, it is important to distinguish between what was planned and what was not. If you ran a lot of data-dependent analyses, then it might be worth having separate exploratory and confirmatory results sections. On the other hand, if you mainly stuck to your original plan, with only minor departures, then you could include a table (perhaps in an appendix) that outlines these changes (for example, see Supplementary Information A of this article).",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "011-prereg.html#chapter-summary-preregistration",
    "href": "011-prereg.html#chapter-summary-preregistration",
    "title": "11  Preregistration",
    "section": "11.4 Chapter summary: Preregistration",
    "text": "11.4 Chapter summary: Preregistration\nWe’ve advocated here for preregistering your study plan. This practice helps to reduce the risk of bias caused by data-dependent analysis (the “garden of forking paths” that we described) and transparently communicate the risk of bias to other scientists. Importantly, preregistration is a “plan, not a prison”: in most cases, preregistered, confirmatory analyses coexist with exploratory analyses. Both are an important part of good research—the key is to disclose which is which!\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nP-hack your way to scientific glory! To get a feel for how data-dependent analyses might work in practice, have a play around with this app: https://projects.fivethirtyeight.com/p-hacking. Do you think preregistration would affect your confidence in claims made about this dataset?\nPreregister your next experiment! The best way to get started with preregistration is to have a go with your next study. Head over to https://osf.io/registries/osf/new and register your study protocol or complete one of the templates. What aspects of preregistration did you find most difficult, and what benefits did it bring?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nNosek, Brian A., Charles R. Ebersole, Alexander C. DeHaven, and David T. Mellor (2018). “The Preregistration Revolution.” Proceedings of the National Academy of Sciences 115 (11): 2600–2606. https://doi.org/10.1073/pnas.1708274114.\nHardwicke, Tom E., and Eric-Jan Wagenmakers (2023). “Reducing Bias, Increasing Transparency, and Calibrating Confidence with Preregistration.” Nature Human Behaviour 7 (1): 15–26. https://doi.org/10.31222/osf.io/d7bcu.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Preregistration</span>"
    ]
  },
  {
    "objectID": "012-collection.html",
    "href": "012-collection.html",
    "title": "12  Data collection",
    "section": "",
    "text": "12.1 Informed consent and debriefing\nAs we discussed in chapter 4, experimenters must respect the autonomy of their participants: they must be informed about the risks and benefits of participation before they agree to participate. Researchers must also discuss and contextualize the research by debriefing participants after they have completed the study. Here we look at the nuts and bolts of each of these processes, ending with guidance on the special protections that are required to protect the autonomy of especially vulnerable populations.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#the-rise-of-online-data-collection",
    "href": "012-collection.html#the-rise-of-online-data-collection",
    "title": "12  Data collection",
    "section": "The rise of online data collection",
    "text": "The rise of online data collection\nSince the rise of experimental psychology laboratories in university settings during the period after World War II (Benjamin 2000), experiments have typically been conducted by recruiting participants from what has been referred to as the “subject pool.” This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses (Sieber and Saks 1989) who are required to complete a certain number of experiments as part of their course work.  The ready availability of this convenient population inevitably led to the massive overrepresentation of undergraduates in published psychology research, undermining its generalizability (Sears 1986; Henrich, Heine, and Norenzayan 2010).\nYet, over the last couple of decades, there has been a revolution in data collection. Instead of focusing on university undergraduates, increasingly researchers recruit individuals from crowdsourcing websites like Amazon Mechanical Turk and Prolific Academic. Crowdsourcing services were originally designed to recruit and pay workers for ad hoc business tasks like retyping receipts, but they have also become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments (Litman, Robinson, and Abberbock 2017). As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates), and this proportion is likely continuing to grow (Anderson et al. 2019).\nInitially, many researchers worried that crowdsourced data from online convenience samples would lead to a decrease in data quality. However, several studies suggest that data quality from online convenience samples is typically comparable to in-lab convenience samples (Mason and Suri 2012; M. Buhrmester, Kwang, and Gosling 2011). In one particularly compelling demonstration, a set of online experiments were used to replicate a group of classic phenomena in cognitive psychology, with clear successes on every experiment except those requiring sub-50 millisecond stimulus presentation (Crump, McDonnell, and Gureckis 2013). Further, as we discuss below, researchers have developed a suite of tools to ensure that online participants understand and comply with the instructions in complex experimental tasks.\nSince these initial successes, however, attention has moved away from the validity of online experiments to the ethical challenges of engaging with crowdworkers. In 2020, nearly 130,000 people completed MTurk studies (Moss et al. 2020). Of those, an estimated 70% identified as White, 56% identified as women, and 48% had an annual household income below $50,000. A sampling of crowd work determined that the average wage earned was just $2.00 per hour, and less than 5% of workers were paid at least the federal minimum wage (Hara et al. 2018). Further, many experimenters routinely withheld payment from workers based on their performance in experiments. These practices clearly violate ethical guidelines for research with human participants but are often overlooked by institutional review boards who may be unfamiliar with online recruitment platforms or consider that platforms are offering a “service” rather than simply being alternative routes for paying individuals.\nWith greater attention to the conditions of workers (e.g., Salehi et al. 2015), best practices for online research have progressed considerably. As we describe below, working with online populations requires attention to both standard ethical issues of consent and compensation, as well as new issues around the “user experience” of participating in research. The availability of online convenience samples can be transformative for the pace of research, for example, by enabling large studies to be run in a single day rather than over many months. But online participants are vulnerable in different ways than university convenience samples, and we must take care to ensure that research online is conducted ethically.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#informed-consent-and-debriefing",
    "href": "012-collection.html#informed-consent-and-debriefing",
    "title": "12  Data collection",
    "section": "",
    "text": "12.1.1 Getting consent\nExperimental participants must give consent. In most regulatory frameworks, there are clear guidelines about what the process of giving consent should look like. Typically participants are expected to read and sign a consent form: a document that explains the goals of the research and its procedures, describes potential risks and benefits, and asks for participants’ explicit consent to participate voluntarily. Table 12.1 gives the full list of consent form requirements from the US Office for Human Research Protections, and figure 12.1 shows how these individual requirements are reflected in a real consent form used in our research.\n\n\n\nTable 12.1: US Office of Human Research Protections requirements for a consent form (edited for length).\n\n\n\n\n\n\n\n\n\n\nRequirement\n\n\n\n\n1\nA statement that the study involves research\n\n\n2\nAn explanation of the purposes of the research\n\n\n3\nThe expected duration of the subject’s participation\n\n\n4\nA description of the procedures to be followed\n\n\n5\nIdentification of any procedures that are experimental\n\n\n6\nA description of any reasonably foreseeable risks or discomforts to the subject\n\n\n7\nA description of any benefits to the subject or to others that may reasonably be expected from the research\n\n\n8\nA disclosure of appropriate alternative procedures or courses of treatment, if any, that might be advantageous to the subject\n\n\n9\nA statement describing the extent, if any, to which confidentiality of records identifying the subject will be maintained\n\n\n10\nFor research involving more than minimal risk, an explanation as to whether any compensation or medical treatments are available if injury occurs\n\n\n11\nAn explanation of whom to contact for answers to pertinent questions about the research and research subjects’ rights\n\n\n12\nA statement that participation is voluntary, refusal to participate will involve no penalty, and that subject may discontinue participation at any time without penalty\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: A consent form annotated to show how specific text fulfills the requirements in table 12.1. Categories 5, 8, and 10 were not required for this minimal-risk psychology experiment.\n\n\n\nThese are just samples. Since ethics regulation is almost always managed at the institutional level, your local ethics board will often provide guidance on the specific information you should include in the consent form and they will almost always need to approve the form before you are allowed to begin recruiting participants.\nWhen providing consent information, researchers should focus on what someone might think or feel as a result of participating in the study. Are there any physical or emotional risks associated? What should someone know about the study that may give them pause about agreeing to participate in the first place? Our advice is to center the participant in the consent process rather than the research question. Information about specific research goals can typically be provided during debriefing.2\n2 Some experimenters worry that informing participants about the study that they are about to participate in may influence their behavior in the study via so-called demand characteristics, discussed in chapter 9. But the goal of a consent form is not to explain the specific psychological construct being manipulated. Instead, a consent form typically focuses on the experience of being in the study (for example, that a participant would be asked to provide quick verbal responses to pictures). This sort of general explanation should not create demand characteristics.If there are specific pieces of information about study goals or procedures that must be withheld from participants during consent, deception of participants may be warranted. Deception can be approved by ethics boards as long as it poses little risk and is effectively addressed via more extensive debriefing. But an experimental protocol that includes deception will likely undergo greater scrutiny during ethics review, as it must be justified by a specific experimental need.\nDuring the consent process, researchers should explain to participants what will be done with their data. Requirement 9 in table 12.1 asks for a statement about data confidentiality, but such a statement is a mere minimum. Some modern consent forms explicitly describe different uses of the data and ask for consent for each. For example, the form in figure 12.1 asks permission for showing recordings as part of presentations.3\n3 Some ethics boards will ask for consent for sharing even anonymized data files. As we discuss in chapter 13, fully anonymized data can often be shared without explicit consent. You may still choose to ask participants’ permission, but this practice may lead to an awkward situation—for example, a dataset with heterogeneous sharing permissions such that most but not all data can be shared publicly. Norms around anonymized data sharing are shifting, so it’s worth having a conversation with your ethics board about how they interpret your particular regulatory obligations.\n\n12.1.2 Prerequisites of consent\nTo give consent, participants must have the cognitive capacity to make decisions (competence), understand what they are being asked to do (comprehension), and know that they have the right to withdraw consent at any time (voluntariness) (Kadam 2017).\nTypically, we assume competence for adult volunteers in our experiments, but if we are working with children or other vulnerable populations (see below), we may need to consider whether they are legally competent to provide consent. Participants who cannot consent on their own should still be informed about participation in an experiment, and, if possible, you should still obtain their assent (informal agreement) to participate. When a person has no legal ability to consent, you must obtain consent from their legal guardian. But if they do not assent, you should also respect their decision not to participate—even if you previously obtained consent from their guardian.\nThe second prerequisite is comprehension. It is good practice to discuss consent forms verbally with participants, especially if the study is involved and takes place in person. If the study is online, ensure that participants know how to contact you if they have questions about the study. The consent form itself must be readable for a broad audience, meaning care should be taken to use accessible language and clear formatting. Consider giving participants a copy of the consent form in advance so they can read at their own pace, think of any questions they might have, and decide how to proceed without any chance of feeling coerced (Young, Hooker, and Freeberg 1990).\nFinally, participants must understand that their involvement is voluntary, meaning that they are under no obligation to be involved in a study and always have the right to withdraw at any time. Experimenters should not only state that participation is voluntary; they should also pay attention to other features of the study environment that might lead to structural coercion (Fisher 2013). For example, high levels of compensation can make it difficult for lower-income participants to withdraw from research. Similarly, factors like race, gender, and social class can lead participants to feel discomfort around discontinuing a study. It is incumbent on experimenters to provide a comfortable study environment and to avoid such coercive factors wherever possible.\n\n\n12.1.3 Debriefing participants\nOnce a study is completed, researchers should always debrief participants. A debriefing is composed of four parts: (1) gratitude, (2) discussion of goals, (3) explanation of deception (if relevant), and (4) questions and clarification (Allen 2017). Together these serve to contextualize the experience for the participant and to mitigate any potential harms from the study.\n\nGratitude. Thank participants for their contribution! Sometimes thanks is enough (for a short experiment), but many studies also include monetary compensation or course credit. Compensation should be commensurate with the amount of time and effort required for participation. Compensation structures vary widely from place to place; typically local ethics boards will have specific guidelines.\nDiscussion of goals. Researchers should share the purpose of the research with participants in, aiming for a short and accessible statement that avoids technical jargon. Sharing goals is especially important when some aspect of the study appears evaluative—participants will often be interested in knowing how well they performed against their peers. For example, a parent whose child completed a word-recognition task may request information about their child’s performance. It can assuage parents’ worries to highlight that the goals of the study are about measuring a particular experimental effect, not about individual evaluation and ranking.4\nExplanation of deception. Researchers must reveal any deception during debriefing, regardless of how minor the deception seems to the researcher. This component of the debriefing process can be thought of as “dehoaxing” because it is meant to illuminate any aspects of the study that were previously misleading or inaccurate (Holmes 1976). The goal is both to reveal the true intent of the study and to alleviate any potential anxiety associated with the deception. Experimenters should make clear both where in the study the deception occurred and why the deception was necessary for the study’s success.\nQuestions and clarification. Finally, researchers should answer any questions or address any concerns raised by participants. Many researchers use this opportunity to ask participants about their own ideas about the study goals. This practice not only illuminates aspects of the study design that may have been unclear to or hidden from participants, but also begins a discussion where both researchers and participants can communicate about this joint experience. This step is also helpful in identifying negative emotions or feelings resulting from the study (Allen 2017). When participants do express negative emotions, researchers are responsible for sharing resources participants can use to help them.5\n\n4 At the study’s conclusion, you might also consider sharing any findings with participants—many participants appreciate learning about research findings that they contributed to, even months or years after participation.5 In the case that participants report substantial concerns or negative reactions to an experiment—during debriefing or otherwise—researchers will typically have an obligation to report these to their ethics board.\n\n12.1.4 Special considerations for vulnerable populations\nRegardless of who is participating in research, investigators have an obligation to protect the rights and well-being of all participants. Some populations are considered especially vulnerable because of their decreased agency—either in general or in the face of potentially coercive situations. Research with these populations receives additional regulatory oversight. In this section, we will consider several vulnerable populations.\nChildren. Children are some of the most commonly used vulnerable populations in research because the study of development can contribute both to children’s welfare and to our understanding of the human mind. In the US, children under the age of 18 may only participate in research with written consent from a parent or guardian. Unless they are preverbal, children should additionally be asked for their assent. The risks associated with a research study focusing on children also must be no greater than minimal unless participants may receive some direct benefit from participating or participating in the study may improve a disorder or condition the participant was formally diagnosed with.\nPeople with disabilities. There are thousands of disabilities that affect cognition, development, motor ability, communication, and decision-making with varying degrees of interference, so it is first important to remember that considerations for this population will be just as diverse as its members. No laws preclude people with disabilities from participating in research. However, those with cognitive disabilities who are unable to make their own decisions may only participant with written consent from a legal guardian and with their individual assent (if applicable). Those retaining full cognitive capacity but who have other disabilities that make it challenging to participate normally in the study should receive appropriate assistance to access information about the study, including the risks and benefits of participation.\nIncarcerated populations. Nearly 2.1 million people are incarcerated in the United States alone (Gramlich 2021). Due to early (and repugnant) use of prisoners as a convenience population that could not provide consent, the use of prisoners in research has been a key focus of protective efforts. The US Office for Human Research Protections (OHRP) supports their involvement in research under very limited circumstances—typically when the research specifically focuses on issues relevant to incarcerated populations (Office for Human Research Protections 2003). When researchers propose to study incarcerated individuals, the local ethics board must reconfigure to include at least one active prisoner (or someone who can speak from a prisoner’s perspective) and ensure that less than half of the board has any affiliation to the prison system, public or private. Importantly, researchers must not suggest or promise that participation will have any bearing on an individual’s prison sentence or parole eligibility, and compensation must be otherwise commensurate with their contribution.\nLow-income populations. Participants with fewer resources may be more persuaded to participate by monetary incentives, creating a potentially coercive situation. Researchers should consult with their local ethics board to conform to local standards for noncoercive payment.\nIndigenous populations. There is a long and negative history of the involvement of indigenous populations in research without their consent. In the case that research requires the participation of indigenous individuals—because of potential benefits to their communities, rather than due to convenience—then community leadership must be involved to discuss the appropriateness of the research as well as how the consent process should be structured (Fitzpatrick et al. 2016).\nCrowdworkers. Ethics boards do not usually consider crowdworkers on platforms like Amazon Mechanical Turk to be a specific vulnerable population, but many of the same concerns about diminished autonomy and greater need for protection still arise (see the Depth Box below). Without platform or ethics board standards, it is up to individual experimenters to commit to fair pay, which should ideally match or exceed the applicable minimum wage (e.g., the US federal minimum wage). Further, in the context of reputation management systems like those of Amazon Mechanical Turk, participants can be penalized for withdrawing from an experiment—once they have their work “rejected” by an experimenter, it can be harder for them to find new jobs, causing serious long-term harm to their ability to earn on the platform.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#designing-the-research-experience",
    "href": "012-collection.html#designing-the-research-experience",
    "title": "12  Data collection",
    "section": "12.2 Designing the “research experience”",
    "text": "12.2 Designing the “research experience”\nFor the majority of psychology experiments, the biggest factor that governs whether a participant has a positive or negative experience of an experiment is not its risk profile, since for many psychology experiments the quantifiable risk to participants is minimal.6 Instead, it is the participants’ experience. Did they feel welcome? Did they understand the instructions? Did the software work as designed? Was their compensation clearly described and promptly delivered? These aspects of “user experience” are critical both for ensuring that participants have a good experience in the study (an ethical imperative) and for gathering good data. An experiment that leaves participants unhappy typically doesn’t satisfy either the ethical or the scientific goals of research. In this section, we’ll discuss how to optimize the research experience for both in-person and online experiments, as well as providing some guidance on how to decide between these two administration contexts.\n6 There are of course exceptions, including research with more sensitive content. Even in these cases, however, attention to the participant’s experience can be important for ensuring good scientific outcomes.\n12.2.1 Ensuring good experiences for in-lab participants\nA participant’s experience begins even before they arrive at the lab. Negative experiences with the recruitment process (e.g., unclear consent forms, poor communication, or complicated scheduling) or transit to the lab (e.g., difficulty navigating or finding parking) can lead to frustrated participants with a negative view of your research. Anything you can do to make these experiences smoother and more predicable—prompt communication, well-tested directions, reserved parking slots, and so on—will make your participants happier and increase the quality of your data.7\n7 For some reason, the Stanford Psychology Department building is notoriously difficult to navigate. This seemingly minor issue has resulted in a substantial number of late, frustrated, and flustered participants over the years.Once a participant enters the lab, every aspect of the interaction with the experimenter can have an effect on their measured behavior (Gass and Seiter 2018). For example, a likable and authoritative experimenter who clearly describes the benefits of participation is following general principles for persuasion (Cialdini and Goldstein 2004). This interaction should lead to better compliance with experimental instructions, and hence better data, than an interaction with an unclear or indifferent experimenter.\nAny interaction with participants must be scripted and standardized so that all participants have as similar an experience as possible. A lack of standardization can result in differential treatment for participants with different characteristics, which could result in data with greater variability or even specific sociodemographic biases. An experimenter that was kinder and more welcoming to one demographic group would be acting unethically, and they also might find a very different result than they intended.\nEven more importantly, experimenters who interact with participants should ideally be unaware of the experimental condition each participant is assigned to. This practice is often called “blinding” or “masking.” Otherwise it is easy for experimenter knowledge to result in small differences in interaction across conditions, which in turn can influence participants’ behavior, resulting in experimenter expectancy effects (see chapter 9). Even if the experimenter must know a participant’s condition assignment—as is sometimes the case—this information should be revealed at the last possible moment to avoid contamination of other aspects of the experimental session.8\n8 In some experiments, an experimenter delivers a manipulation and, hence, it cannot be masked from them. In such cases, it’s common to have two experimenters such that one delivers the manipulation and another (masked to condition) collects the measurements. This situation often comes up with studies of infancy, since stimuli are often delivered via an in-person puppet show; at a minimum, behavior should be coded by someone other than the puppeteer.\n\n12.2.2 Ensuring good experiences for online participants\nThe design challenges for online experiments are very different than for in-lab experiments. As the experimental procedure is delivered through a web browser, experimenter variability and potential expectancy effects are almost completely eliminated. On the other hand, some online participants do many hours of online tasks a day and many are multi-tasking in other windows or on other devices. It can be much harder to induce interest and engagement in your research when your manipulation is one of dozens the participant has experienced that day and when your interactions are mediated by a small window on a computer screen.\nWhen creating an online experimental experience, we consider four issues: (1) design, (2) communication, (3) payment policies, and (4) effective consent and debriefing.9\n9 For extensive further guidance on this topic, see Litman and Robinson (2020).Basic UX design. Good experiment design online is a subset of good web user experience (UX) design more generally. If your web experiment is unpleasant to interact with, participants will likely become confused and frustrated. They will either drop out or provide data that are lower quality. A good interface should be clean and well-tested and should offer clear places where the participant must type or click to interact. If a participant presses a key at an appropriate time, the experiment should offer a response—otherwise the participant will likely press it again. If the participant is uncertain how many trials are left, they may be more likely to drop out of the experiment so it is also helpful to provide an indication of their progress. And if they are performing a speeded paradigm, they should receive practice trials to ensure that they understand the experiment prior to beginning the critical blocks of trials.\nCommunication. Many online studies involve almost no direct contact with participants. When participants do communicate with you, it is very important to be responsive and polite (as it is with in-lab participants, of course). Unlike the typical undergraduate participant, the work that a crowdworker is doing for your study may be part of how they earn their livelihood, and a small issue in the study for you may feel very important for them. For that reason, rapid resolution of issues with studies—typically through appropriate compensation—is very important. Crowdworkers often track the reputation of specific labs and experimenters (sometimes through forums or specialized software; Irani and Silberman 2013). A quick and generous response to an issue will ensure that future crowdworkers do not avoid your studies.\nPayment policies. Unclear or punitive payment policies can have a major impact on crowdworkers. We strongly recommend always paying workers if they complete your experiment, regardless of result. This policy is comparable to standard payment policies for in-lab work. We assume good faith in our participants: if someone comes to the lab, they are paid for the experiment, even if it turns out that they did not perform correctly. The major counterargument to this policy is that some online marketplaces have a population of workers who are looking to cheat by being noncompliant with the experiment (e.g., entering gibberish or even using scripts or artificial intelligence tools to progress quickly through studies). Our recommendation is to address this issue through the thoughtful use of “check” trials (see below)—not through punitive nonpayment. The easiest way for a participant to complete your experiment should be by complying with your instructions.\n\n\n\nTable 12.2: Sample online consent statement from our course.\n\n\n\n\n\n\n\n\nBy answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at stanfordpsych251@gmail.com. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.\n\n\n\n\n\n\n\n\nConsent and debriefing. Because online studies are typically fully automated, participants do not have a chance to interact with researchers around consent and debriefing. Further, engagement with long consent forms may be minimal. In our work, we have typically relied on short consent statements such as the one from our class that is shown in table 12.2. Similarly, debriefing often occurs through a set of pages that summarize all components of the debriefing process (participation gratitude, discussion of goals, explanation of deception if relevant, and questions and clarification). Because these interactions are so short, it is especially important to include contact information prominently so that participants can follow up.\n\n\n12.2.3 When to collect data online?\nOnline data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser—alongside survey software like Qualtrics or packages like jsPsych (Leeuw 2023)—can be a major aid to transparency in sharing experimental materials. Replication and reuse of experimental materials is vastly simpler if readers and reviewers can click a link and share the same experience as a participant in your experiment. By and large, well-designed studies yield data that are as reliable as in-lab data (M. Buhrmester, Kwang, and Gosling 2011; Mason and Suri 2012; Crump, McDonnell, and Gureckis 2013).\nStill, online data collection is not right for every experiment. Studies that have substantial deception or induce negative emotions may require an experimenter present to alleviate ethical concerns or provide debriefing. Beyond ethical issues, we discuss four broader concerns to consider when deciding whether to conduct data collection online: (1) population availability, (2) the availability of particular measures, (3) the feasibility of particular manipulations, and (4) the length of experiments.\nPopulation. Not every target population can be tested online. Indeed, initially, convenience samples from Amazon Mechanical Turk were the only group easily available for online studies. More recently, new tools have emerged to allow prescreening of crowd participants, including sites like Cloud Research and Prolific (Eyal et al. 2021; Peer et al. 2021).10 And it may initially have seemed implausible that children could be recruited online, but during the COVID-19 pandemic a substantial amount of developmental data collection moved online, with many studies yielding comparable results to in-lab studies (e.g., Chuey et al. 2021).11 Finally, new, non-US crowdsourcing platforms continue to grow in popularity, leading to greater global diversity in the available online populations.\n10 These tools still have significant weaknesses for accessing socio-demographically diverse populations within and outside the US, however—screening tools can remove participants, but if the underlying population does not contain many participants from a particular demographic, it can be hard to gather large enough samples. For an example of using crowdsourcing and social media sites to gather diverse participants, see DeMayo et al. (2021).11 Sites like LookIt (https://lookit.mit.edu) now offer sophisticated platforms for hosting studies for children and families (Scott and Schulz 2017).Online measures. Not all measures are available online, though more and more are. Although online data collection was initially restricted to the use of survey measures—including ratings and text responses—measurement options have rapidly expanded. The widespread use of libraries like jsPsych (De Leeuw 2015) has meant that millisecond accuracy in capturing response times is now possible within web browsers; thus, most reaction time tasks are quite feasible (Crump, McDonnell, and Gureckis 2013). The capture of sound and video is possible with modern browser frameworks (Scott and Schulz 2017). Further, even measures like mouse- and eye-tracking are beginning to become available (Maldonado, Dunbar, and Chemla 2019; Slim and Hartsuiker 2023). In general, almost any variable that can be measured in the lab without specialized apparatus can also be collected online. On the other hand, studies that measure a broader range of physiological variables (e.g., heart rate or skin conductance) or a larger range of physical behaviors (e.g., walking speed or pose) are still likely difficult to implement online.\nOnline manipulations. Online experiments are limited to the set of manipulations that can be created within a browser window—but this restriction excludes many different manipulations that involve real-time social interactions with a human being.12 Synchronous chat sessions can be a useful substitute (Hawkins, Frank, and Goodman 2020), but these focus the experiment on the content of what is said and exclude the broader set of nonverbal cues available to participants in a live interaction (e.g., gaze, race, appearance, accent, and so on). Creative experimenters can circumvent these limitations by using pictures, videos, and other methods. But more broadly, an experimenter interested in implementing a particular manipulation online should ask how compelling the online implementation is compared with an in-lab implementation. If the intention is to induce some psychological state—say stress, fear, or disgust—experimenters must trade off the greater ease of recruitment and larger scale of online studies with the more compelling experience they may be able to offer in a controlled lab context.\n12 So-called moderated experiments—in which the experimental session is administered through a synchronous video chat—have been used widely in online experiments for children, but these designs are less common in experiments with adults because they are expensive and time-consuming to administer (Chuey et al. 2021).The length of online studies. One last concern is about attention and focus in online studies. Early guidance around online studies tended to focus on making studies short and easy, with the rationale that crowdsourcing workers were used to short jobs. Our sense is that this guidance no longer holds. Increasingly, researchers are deploying long and complex batteries of tasks to relatively good effect (e.g., Enkavi et al. 2019) and conducting repeated longitudinal sampling protocols (discussed in depth in Litman and Robinson 2020). Rather than relying on hard-and-fast rules about study length, a better approach for online testing is to ensure that participants’ experience is as smooth and compelling as possible. Under these conditions, if an experiment is viable in the lab, it is likely viable online.\nOnline testing tools continue to grow and change, but they are already mature enough that using them should be part of most behavioral researchers’ basic toolkit.13\n13 It is of course important to keep in mind that if a person works part- or full-time on a crowdsourcing platform, they are not a representative sample of the broader national population. Unfortunately, similar caveats hold true for in-person convenience samples (see chapter 10). Ultimately, researchers must reason about what their generalization goal is and whether that goal is consistent with the samples they can access (online or otherwise).",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#ensuring-high-quality-data",
    "href": "012-collection.html#ensuring-high-quality-data",
    "title": "12  Data collection",
    "section": "12.3 Ensuring high-quality data",
    "text": "12.3 Ensuring high-quality data\nIn the final section of this chapter, we review some key data collection practices that can help researchers collect high-quality data while respecting our ethical obligations to participants. By “high quality,” here we especially mean datasets that are uncontaminated by responses generated by misunderstanding of instructions, fatigue, incomprehension, or intentional neglect of the experimental task.\nWe’ll begin by discussing the issue of pilot testing; we recommend a systematic procedure for piloting that can maximize the chance of collecting high-quality data. Next, we’ll discuss the practice of checking participants’ comprehension and attention and what such checks should and shouldn’t be used for. Finally, we’ll discuss the importance of maintaining consistent data collection records.\n\n12.3.1 Conduct effective pilot studies\nA pilot study is a small study conducted before you collect your main sample. The goal is to ensure smooth and successful data collection by first checking if your experimental procedures and data collection workflow are working correctly. Pilot studies are also an opportunity to get feedback from participants about their experience of the experimental task; for example, is it too easy, too difficult, or too boring.\nBecause pilot studies usually involve a small number of participants, they are not a reliable indicator of the study results, such as the expected effect size or statistical significance (as we discussed in chapter 10). Don’t use pilots to check if your effect is present or to estimate an effect size for power analysis. What pilots can do is tell you about whether your experimental procedure is viable. For example, pilots studies can reveal:\n\nif your code crashes under certain circumstances\nif your instructions confuse a substantial portion of participants\nif you have a very high dropout rate\nif your data collection procedure fails to log variables of interest\nif participants are disgruntled by the end of the experiment\n\nWe recommend that all experimenters perform—at the very minimum—two pilot studies before they launch a new experiment.14\n14 We mean especially when deploying a new experimental paradigm or when collecting data from a new population. Once you have run many studies with a similar procedure and similar sample, extensive piloting is less important. Any time you change something, it’s always good to run one or two pilots, though, just to check that you didn’t inadvertently mess up your experiment.15 In a pinch you can even run yourself through the experiment a bunch of times (though this isn’t preferable because you’re likely to miss a lot of aspects of the experience that you are habituated to, especially if you’ve been debugging the experiment already).The first pilot, which we call your non-naive participant pilot, can make use of participants who know the goals of the experiment and understand the experimental manipulation—this could be a friend, collaborator, colleague, or family member.15 The goal of this pilot study is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. You must analyze the data from the non-naive pilot, at least to the point of checking that the relevant data about each trial is logged.\nThe second pilot, your naive participant pilot, should consist of a test of a small set of participants recruited via the channel you plan to use for your main study. The number of participants you should pilot depends on the cost of the experiment in time, money, and opportunity as well as its novelty. A brand new paradigm is likely more prone to error than a tried and tested paradigm. For a short online survey-style experiment, a pilot of 10–20 people is reasonable. A more time-consuming laboratory study might require piloting just two or three people.16\n16 In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility is to plan to include the pilot participants in the main dataset if no major procedural changes are required. In this case, it is helpful to preregister a contingent testing strategy to avoid introducing data-dependent bias (see chapter 11). For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate—and not at any condition differences. Then the preregistration can state that, if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset, including the initial pilot, but if dropout rate is higher than 25%, you will discard the pilot sample and make changes. This kind of strategy can help you split the difference between cautious piloting and conservation of rare or costly data.The goal of the naive pilot study is to understand properties of the participant experience. Were participants confused? Did they withdraw before the study finished? Even a small number of pilots can tell you that your dropout rate is likely too high: for example, if five of ten pilot participants withdraw, you likely need to reconsider aspects of your design. It’s critical for your naive participant pilot that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over. “What did you think the study was about?” and “Is there any way we could improve the experience of being in the study?” can be helpful questions. Often this debriefing is more effective if it is interactive, so even if you are running an online study you may want to find some way to chat with your participants.\nPiloting—especially piloting with naive participants to optimize the participant experience—is typically an iterative process. We frequently launch an experiment for a naive pilot, then recognize from the data or from participant feedback that the experience can be improved. We make tweaks and pilot again. Be careful not to over-fit to small differences in pilot data, however. Piloting should be more like workshopping a manuscript to remove typos than doing statistical analysis. If someone has trouble understanding a particular sentence—whether in your manuscript or in your experiment instructions—you should edit to make it clearer!\n\n\n\n\n\n\naccident report\n\n\n\n\n\nData logging much?\nWhen Mike was in graduate school, his lab got a contract to test a very large group of participants in a battery of experiments, bringing them into the lab over the course of a series of intense bursts of participant testing. He got the opportunity to add an experiment to the battery, allowing him to test a much larger sample than resources would otherwise allow. He quickly coded up a new experiment as part of a series of ongoing studies and began deploying it, coming to the lab every weekend for several months to help move participants through the testing protocol. Eagerly opening up the data file to reap the reward of this hard work, he found that the condition variable was missing from the data files. Although the experimental manipulation had been deployed properly, there was no record of which condition each participant had been run in, and so the data were essentially worthless. Had he run a quick pilot (even with non-naive participants) and attempted to analyze the data, this error would have been detected and many hours of participant and experimenter effort would not have been lost.\n\n\n\n\n\n12.3.2 Measure participant compliance\nYou’ve constructed your experiment and piloted it. You are almost ready to go—but there is one more family of tricks for helping to achieve high-quality data: integrating measures of participant compliance into your paradigm. Collecting data on compliance (whether participants followed the experimental procedures as expected) can help you quantify whether participants understood your task, engaged with your manipulation, and paid attention to the full experimental experience. These measures in turn can be used both to modify your experimental paradigm and to exclude specific participants that were especially noncompliant (Hauser, Ellsworth, and Gonzalez 2018; Ejelöv and Luke 2020).\nBelow we discuss four types of compliance checks: (1) passive measures, (2) comprehension checks, (3) manipulation checks, and (4) attention checks. Passive measures and comprehension checks are very helpful for enhancing data quality. Manipulation checks also often have a role to play. In contrast, we typically caution in the use of attention checks.\n\nPassive measures of compliance. Even if you do not ask participants anything extra in an experiment, it is often possible to tell if they have engaged with the experimental procedure simply by how long it takes them to complete the experiment. If you see participants with completion times substantially above or below the median, there is a good chance that they are either multi-tasking or rushing through the experiment without engaging.17 Passive measures cost little to implement and should be inserted whenever possible in experiments.18\nComprehension checks. For tasks with complex instructions or experimental materials (say a passage that must be understood for a judgment to be made about it), it can be very helpful to get a signal that participants have understood what they have read or viewed. Comprehension checks, which ask about the content of the experimental instructions or materials, are often included for this purpose. For the comprehension of instructions, the best kinds of questions simply query the knowledge necessary to succeed in the experiment: for example, “What are you supposed to do when you see a red circle flash on the screen?” In many platforms, it is possible to make participants reread the instructions again until they can answer these correctly. This kind of repetition is nice because it corrects participants’ misconceptions rather than allowing them to continue in the experiment when they do not understand.19\nManipulation checks. If your experiment involves more than a very transient manipulation—for example, if you plan to induce some state in participants or have them learn some content—then you can include a measure in your experiment that confirms that your manipulation succeeded (Ejelöv and Luke 2020). This measure is known as a manipulation check because it measures some prerequisite difference between conditions that is not the key causal effect of interest but is causally prerequisite to this effect. For example, if you want to see if anger affects moral judgment, then it makes sense to measure whether participants in your anger induction condition rate themselves as angrier than participants in your control condition. Manipulation checks are useful in the interpretation of experimental findings because they can decouple the failure of a manipulation from the failure of a manipulation to affect your specific measure of interest.20\nAttention checks. A final type of compliance check is a check that participants are paying attention to the experiment at all. One simple technique is to add questions that have a known and fairly obvious right answer (e.g., “What’s the capital of the United States?”). These trials can catch participants that are simply ignoring all text and “mashing buttons,” but they will not find participants who are mildly inattentive. Sometimes experimenters also use trickier compliance checks, such as putting an instruction for participants to click a particular answer deep within a question text that otherwise would have a different answer (figure 12.2). Such compliance checks decrease so-called satisficing behavior, in which participants read as quickly as they can to get away with doing only the minimum. On the other hand, participants may see such trials as indications that the experimenter is trying to trick the, and adopt a more adversarial stance toward the experiment, which may result in less compliance with other aspects of the design, unless they are at the end of the experiment (Hauser, Ellsworth, and Gonzalez 2018). If you choose to include attention checks like these, be aware that you are likely reducing variability in your sample—trading off representativeness for compliance.\n\n17 Measurements of per-page or per-element completion times can be even more specific since they can, for example, identify participants that simply did not read an assigned passage.18 One variation that we endorse in certain cases is to force participants to engage with particular pages for a certain amount of time through the use of timers. Though, beware, this kind of feature can lead to an adversarial relationship with participants—in the face of this kind of coercion, many will opt to pull out their phone and multi-task until the timer runs down.19 If you are querying comprehension of experimental materials rather than instructions, you may not want to reexpose participants to the same passage again in order to avoid confounding a participants’ initial comprehension and the amount of exposure that they receive.20 Hauser, Ellsworth, and Gonzalez (2018) worry that manipulation checks can themselves change the effect of a manipulation—this worry strikes us as sensible, especially for some types of manipulations like emotion inductions. Their recommendation is to test the efficacy of the manipulation in a separate study, rather than trying to nest the manipulation check within the main study.\n\n\n\n\n\nFigure 12.2: An attention check trial based on Oppenheimer, Meyvis, and Davidenko (2009). These trials can decrease variability in participant attention, but at the cost of selecting a subsample of participants, so they should be used cautiously.\n\n\n\nData from all of these types of checks are used in many different—often inconsistent—ways in the literature. We recommend that you:\n\nUse passive measures and comprehension checks as preregistered exclusion criteria to eliminate a (hopefully small) group of participants who might be noncompliant with your experiment.\nCheck that exclusions are low and that they are uniform across conditions. If exclusion rates are high, your design may have deeper issues. If exclusions are asymmetric across conditions, you may be compromising your randomization by creating a situation in which (on average) different kinds of participants are included in one condition compared with the other. Both of these situations substantially compromise any estimate of the causal effect of interest.\n\n\n\n\n\n\n\naccident report\n\n\n\n\n\nDoes data quality vary throughout the semester?\nEvery lab that collects empirical data repeatedly using the same population builds up lore about how that population varies in different contexts. Many researchers who conducted experiments with college undergraduates were taught never to run their studies at the end of the semester. Exhausted and stressed students would likely yield low-quality data, or so the argument went. Until the rise of multi-lab collaborative projects like ManyLabs (see chapter 3), such beliefs were almost impossible to test.\nManyLabs 3 aimed specifically to evaluate data quality variation across the academic calendar (Ebersole et al. 2016). With 2,696 participants at 20 sites, the study conducted replications of 13 previously published findings. Although only six of these findings showed strong evidence of replicating across sites, none of the six effects was substantially moderated by being collected later in the semester. The biggest effect they observed was a change in the Stroop effect from \\(d=0.89\\) during the beginning and middle of the semester to \\(d=0.92\\) at the end. There was some evidence that participants reported being less attentive at the end of the semester, but this trend wasn’t accompanied by a moderation of experimental effects.\nResearchers are subject to the same cognitive illusions and biases as any human. One of these biases is the search to find meaning in the random fluctuations they sometimes observe in their experiments. The intuitions formed through this process can be helpful prompts for generating hypotheses—but beware of adopting them into your “standard operating procedures” without further examination. Labs that avoided data collection during the end of the semester might have sacrificed 10%–20% of their data collection capacity for no reason!\n\n\n\n\nDeploy manipulation checks if you are concerned about whether your manipulation effectively induces a difference between groups. Analyze the manipulation check separately from the dependent variable to test whether the manipulation was causally effective (Ejelöv and Luke 2020).\nMake sure that your attention checks are not confounded in any way with condition—remember our cautionary tale from chapter 9, in which an attention check that was different across conditions actually created an experimental effect.\nDo not include any of these checks in your analytic models as a covariate, as including this information in your analysis compromises the causal inference from randomization and introduces bias in your analysis (Montgomery, Nyhan, and Torres 2018).21\n\n21 Including this information means you are “conditioning on a post-treatment variable,” as we described in chapter 7. In medicine, analysts distinguish “intent-to-treat” analysis, where you analyze data from everyone you gave a drug, and “as-treated” analysis, where you analyze data depending on how much of the drug people actually took. In general, intent-to-treat analysis gives you the generalizable causal estimate. In our current situation, if you include compliance as a covariate, you are essentially doing an “as-treated” analysis and your estimate can be biased as a result. Although there is occasional need for such analyses, in general you probably want to avoid them.Used appropriately, compliance checks can provide both a useful set of exclusion criteria and a powerful tool for diagnosing potential issues with your experiment during data analysis and correcting them down the road.\n\n\n12.3.3 Keep consistent data collection records\nAs an experimentalist, one of the worst feelings is to come back to your data directory and see a group of data files, run1.csv, run2.csv, run3.csv and not know what experimental protocol was run for each. Was run1 the pilot? Maybe a little bit of personal archaeology with timestamps and version history can tell you the answer, but there is no guarantee.22\n22 We’ll have a lot to say about this issue in chapter 13.\n\n\n\n\n\nFigure 12.3: Part of a run sheet for a developmental study.\n\n\n\nAs well as collecting the actual data in whatever form they take (e.g., paper surveys, videos, or files on a computer), it is important to log metadata—data about your data—including relevant information like the date of data collection, the sample that was collected, the experiment version, and the research assistants who were present. The relevant metadata will vary substantially from study to study—the important part is that you keep detailed records. Figure 12.3 and figure 12.4 give two examples from our own research. The key feature is that they provide some persistent metadata about how the experiments were conducted.\n\n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nAdded a simple familiarization slide substitute that presents Bob and\nshows that the experiment is about a person talking to you. Before\nthat, the familiarization slide was simply skipped.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n----------------------------\nNovember 18 2013\n50 subjects | Betting | No familiarization | Friend\nvar participant_response_type = 1;\nvar participant_feature_count = 1;\nvar linguistic_framing = 0;\nvar question_type = 0;\n----------------------------\nNovember 18 2013\n50 subjects | Likert | No familiarization | Friend\nvar participant_response_type = 2;\nvar participant_feature_count = 1;\nvar linguistic_framing = 0;\nvar question_type = 2;\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nThe experiment now asked the subjects the referent of Bobs statement\nat the bottom of the page. The previous experiments always had the\ninput field just below the stimuli or, in the case of 3fc hoovering\nover the images did highlighted possible ones.\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n----------------------------\nNovember 30 2013 ~ 7 pm:\n50 subjects | 3 forced choice condition | No familiarization | Friend\nvar participant_response_type = 0;\nvar participant_feature_count = 1;\nvar linguistic_framing = 0;\nvar question_type = 0;\n\n\n\nFigure 12.4: An excerpt of a log for an iterative run of online experiments.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#data-logging-much",
    "href": "012-collection.html#data-logging-much",
    "title": "12  Data collection",
    "section": "Data logging much?",
    "text": "Data logging much?\nWhen Mike was in graduate school, his lab got a contract to test a very large group of participants in a battery of experiments, bringing them into the lab over the course of a series of intense bursts of participant testing. He got the opportunity to add an experiment to the battery, allowing him to test a much larger sample than resources would otherwise allow. He quickly coded up a new experiment as part of a series of ongoing studies and began deploying it, coming to the lab every weekend for several months to help move participants through the testing protocol. Eagerly opening up the data file to reap the reward of this hard work, he found that the condition variable was missing from the data files. Although the experimental manipulation had been deployed properly, there was no record of which condition each participant had been run in, and so the data were essentially worthless. Had he run a quick pilot (even with non-naive participants) and attempted to analyze the data, this error would have been detected and many hours of participant and experimenter effort would not have been lost.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#does-data-quality-vary-throughout-the-semester",
    "href": "012-collection.html#does-data-quality-vary-throughout-the-semester",
    "title": "12  Data collection",
    "section": "Does data quality vary throughout the semester?",
    "text": "Does data quality vary throughout the semester?\nEvery lab that collects empirical data repeatedly using the same population builds up lore about how that population varies in different contexts. Many researchers who conducted experiments with college undergraduates were taught never to run their studies at the end of the semester. Exhausted and stressed students would likely yield low-quality data, or so the argument went. Until the rise of multi-lab collaborative projects like ManyLabs (see chapter 3), such beliefs were almost impossible to test.\nManyLabs 3 aimed specifically to evaluate data quality variation across the academic calendar (Ebersole et al. 2016). With 2,696 participants at 20 sites, the study conducted replications of 13 previously published findings. Although only six of these findings showed strong evidence of replicating across sites, none of the six effects was substantially moderated by being collected later in the semester. The biggest effect they observed was a change in the Stroop effect from \\(d=0.89\\) during the beginning and middle of the semester to \\(d=0.92\\) at the end. There was some evidence that participants reported being less attentive at the end of the semester, but this trend wasn’t accompanied by a moderation of experimental effects.\nResearchers are subject to the same cognitive illusions and biases as any human. One of these biases is the search to find meaning in the random fluctuations they sometimes observe in their experiments. The intuitions formed through this process can be helpful prompts for generating hypotheses—but beware of adopting them into your “standard operating procedures” without further examination. Labs that avoided data collection during the end of the semester might have sacrificed 10%–20% of their data collection capacity for no reason!",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "012-collection.html#chapter-summary-data-collection",
    "href": "012-collection.html#chapter-summary-data-collection",
    "title": "12  Data collection",
    "section": "12.4 Chapter summary: Data collection",
    "text": "12.4 Chapter summary: Data collection\nIn this chapter, we took the perspective of both the participant and the researcher. Our goal was to discuss how to achieve a good research outcome for both. On the side of the participant, we highlighted the responsibility of the experimenter to ensure a robust consent and debriefing process. We also discussed the importance of a good experimental experience in the lab and online—ensuring that the experiment is not only conducted ethically but is also pleasant to participate in. Finally, we discussed how to address some concerns about data quality from the researcher perspective, recommending both the extensive use of non-naive and naive pilot participants and the use of comprehension and manipulation checks.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\n“Citizen science” is a movement to have a broader base of individuals participate in research because they are interested in discoveries and want to help. In practice, citizen science projects in psychology like Project Implicit (https://implicit.harvard.edu/implicit), Children Helping Science (https://lookit.mit.edu), and TheMusicLab.org (https://themusiclab.org) have all succeeded by offering participants a compelling experience. Check one of these out, participate in a study, and make a list of the features that make it fun and easy to contribute data.\nBe a Turker! Sign up for an account as an Amazon Mechanical Turk or Prolific Academic worker and complete a couple of tasks. How did you feel about browsing the list of tasks looking for work? What features of tasks attracted your interest? How hard was it to figure out how to participate in each task? And how long did it take to get paid?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nAn introduction to online research: Buhrmester, Michael D., Sanaz Talaifar, and Samuel D. Gosling (2018). “An Evaluation of Amazon’s Mechanical Turk, Its Rapid Rise, and Its Effective Use.” Perspectives on Psychological Science 13 (2): 149–154. https://doi.org/10.1177/1745691617706516.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data collection</span>"
    ]
  },
  {
    "objectID": "013-management.html",
    "href": "013-management.html",
    "title": "13  Project management",
    "section": "",
    "text": "13.1 Principles of project management\nA lot of project management problems can be avoided by following a very simple file organization system.4 For those researchers who “grew up” managing their files locally on their own computers and emailing colleagues versions of data files and manuscripts with names like manuscript-FINAL-JS-rev1.xlsx, a few aspects of this system may seem disconcerting. However, with a little practice, this new way of working will start to feel intuitive and have substantial benefits.\nHere are the principles:\nKeeping these principles in mind, we discuss best practices for project organization, version control, and file naming.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#manybabies-manyspreadsheetformats",
    "href": "013-management.html#manybabies-manyspreadsheetformats",
    "title": "13  Project management",
    "section": "ManyBabies, ManySpreadsheetFormats!",
    "text": "ManyBabies, ManySpreadsheetFormats!\nThe ManyBabies project is an example of “Big Team Science” in psychology. A group of developmental psychology researchers (including some of us) were worried about many of the issues of reproducibility, replicability, and experimental methods that we’ve been discussing throughout this book, so they set up a large-scale collaboration to replicate key effects in developmental science. The first of these studies was ManyBabies 1 (The ManyBabies Consortium et al. 2020), a study of infants’ preference for baby-talk (also known as “infant directed speech”).\nThe core team expected a handful of labs to contribute, but after a year-long data collection period, they ended up receiving data from 69 labs around the world! The outpouring of interest signaled a lot of enthusiasm from the community for this kind of collaborative science. Unfortunately, it also made for a tremendous data management headache. All kinds of complications and hilarity ensued as the idiosyncratic data formatting preferences of the various labs were reorganized to fit into a single standardized analysis pipeline (Byers-Heinlein et al. 2020).\nAll of the specific formatting changes that individual labs made were reasonable—altering column names for clarity, combining templates into a single Excel file, changing units (e.g., from seconds to milliseconds)—but together they created a very challenging data validation problem for the core analysis team, requiring many dozens of hours of coding and hand-checking. The data checking was critical: an error in one lab’s data was flagged during validation and led to the painful decision to drop those data from the final dataset. In future ManyBabies projects, the group has committed to using shared data validation software (https://manybabies.org/validator) to ensure that data files uploaded by individual labs conform to a shared standard.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#principles-of-project-management",
    "href": "013-management.html#principles-of-project-management",
    "title": "13  Project management",
    "section": "",
    "text": "4 We’re going to talk in this chapter about managing research products, which is one important part of project management. We won’t talk about some other aspects of managing projects such as calendaring, managing tasks, or project communications. These are all important, they are just a bit out of scope for a book on doing experiments!\n\nThere should be exactly one definitive copy of each document in the project, with its name denoting what it is. For example, fifo_manuscript.Rmd or fifo_manuscript.docx is the write-up of the “fifo” project as a journal manuscript.\nThe location of each document should be within a folder that serves to uniquely identify the document’s function within the project. For example,analysis/experiment1/eye_tracking_preprocessing.Rmd is clearly the file that performs preprocessing for the analysis of eye-tracking data from experiment 1.\nThe full project should be accessible to all collaborators via the cloud, either using a version control platform (e.g., GitHub) or another cloud storage provider (e.g., Dropbox, Google Drive).\nThe revision history of all text and text-based documents (minimally, data, analysis code, and manuscript files) should be archived automatically. Automatic versioning is the key feature of all version control systems and is often included by cloud storage providers.\n\n\n\n13.1.1 Organizing your project\nTo the greatest extent possible, all files related to a project should be stored in the same project folder (with appropriate subfolders), and on the same storage provider. There are cases where this is impractical due to the limitations of different software packages. For example, in many cases, a team will manage its data and analysis code via GitHub but decide to write collaboratively using Google Docs, Overleaf, or another collaborative platform. (It can also be hard to ask all collaborators to use a version control system they are unfamiliar with.) In that case, the final paper should still be linked in some way to the project repository.5\n5 The biggest issue that comes up in using a split workflow like this is the need to ensure reproducible written products, a process we cover in chapter 14.Figure 13.3 shows an example project stored on the Open Science Framework. The top-level folder contains subfolders for analyses, materials, raw and processed data (kept separately). It also contains the paper manuscript and, critically, a README file in a text format that describes the project. A README is a great way to document any other metadata that the authors would like to be associated with the research products, for example a license, explained below.\n\n\n\n\n\n\nFigure 13.3: Sample top-level folder structure for a project. From Klein et al. (2018). Original visible on the Open Science Framework (https://osf.io/xf6ug).\n\n\n\nThere are many reasonable ways to organize the subfolders of a research project, but the broad categories of materials, data, analysis, and writing are typically present.6 In some projects—such as those involving multiple experiments or complex data types—you may have to adopt a more complex structure. In many of our projects, it’s not uncommon to find paths like /data/raw_data/exp1/demographics. The key principle is to create a hierarchical structure in which subfolders uniquely identify the part of the broader space of research products that are found inside them—that is, /data/raw_data/exp1 contains all the raw data from experiment 1, and /data/raw_data/exp1/demographics contains all the raw demographics data from that particular experiment.\n6 We like the scheme followed by Project TIER (https://www.projecttier.org), which provides very clear guidance about file structure and naming conventions. TIER is primarily designed for a copy-and-paste workflow, which is slightly different from the “dynamic documents” workflow that we primarily advocate for (e.g., using R Markdown or Quarto as in appendix C).\n\n13.1.2 Versioning\nProbably everyone who has ever collaborated electronically has experienced the frustration of editing a document, only to find out that you are editing the wrong version—perhaps some of the problems you are working on have already been corrected, or perhaps the section you are adding has already been written by someone else. A second common source of frustration comes when you take a wrong turn in a project, perhaps by reorganizing a manuscript in a way that doesn’t work or refactoring code in a way that turns out to be short-sighted.\nThese two problems are solved by modern version control systems. Here we focus on the use of Git, which is the most widely used version control system. Git is a great general solution for version control, but many people—including several of us—don’t love it for collaborative manuscript writing. We’ll introduce Git and its principles here, while noting that online collaboration tools like Google Docs and Overleaf7 can be easier for writing prose (as opposed to code); we cover this topic in a bit more depth in chapter 14.\n7 Overleaf is actually supported by Git on the backend!{#fig-management-git .column-margin fig-alt=“A diagram of connected circles where”your work” and “someone else’s work” branch off of “main branch” then merge back in.”}\nGit is a tool for creating and managing projects, which are called repositories. A Git repository is a directory whose revision history is tracked via a series of commits—snapshots of the state of the project. These commits can form a tree with different branches, as when two contributors to the project are working on two different parts simultaneously (?fig-management-git). These branches can later be merged either automatically or via manual intervention in the case of conflicting changes.\nCommonly, Git repositories are hosted by an online service like GitHub to facilitate collaboration. With this workflow, a user makes changes to a local version of the repository on their own computer and pushes those changes to the online repository. Another user can then pull those changes from the online repository to their own local version. The online “origin” copy is always the definitive copy of the project, and a record is kept of all changes. Chapter B provides a practical introduction to Git and GitHub, and there are a variety of good tutorials available online and in print (Blischak, Davenport, and Wilson 2016).\nCollaboration using version control tools is designed to solve many of the problems we’ve been discussing:\n\nA remotely hosted Git repository is a cloud-based backup of your work, meaning it is less vulnerable to accidental erasure.8\nBy virtue of having versioning history, you have access to previous drafts in case you find you have been following a blind alley and want to roll back your changes.\nBy creating new branches, you can create another, parallel history for your project so that you can try out major changes or additions without disturbing the main branch in the process.\nA project’s commit history is labeled with each commit’s author and date, facilitating record-keeping and collaboration.\nAutomatic merging can allow synchronous editing of different parts of a manuscript or codebase.9\n\n8 In 48BC, Julius Caesar accidentally burned down part of the Great Library of Alexandria where the sole copies of many valuable ancient works were stored. To this day, many scientists have apparently retained the habit of storing single copies of important information in vulnerable locations. Even in the age of cloud computing, hard drive failure is a surprisingly common source of problems!9 Version control isn’t magic, and if you and a collaborator edit the same paragraph or function, you will likely have to merge your changes by hand. But Git will at least show you where the conflict is!Organizing a project repository for collaboration and hosting on a remote platform is an important first step toward sharing! Many of our projects (like this book) are actually born open: we do all of our work on a publicly hosted repository for everyone to see (Rouder 2015). This philosophy of “working in the open” encourages good organization practices from the beginning. It can feel uncomfortable at first, but this discomfort soon vanishes as you realize that basically no one is looking at your in-progress project.\nOne concern that many people raise about sharing in-progress research openly is the possibility of “scooping”—that is, other researchers getting an idea or even data from the repository and writing a paper before you do. We have two responses to this concern. First, the empirical frequency of this sort of scooping is difficult to determine but likely very low—we don’t know of any documented cases. Mostly, the problem is getting people to care about your experiment at all, not people caring so much that they would publish using your data or materials! In Gary King’s words (King and Shieber 2013), “The thing that matters the least is being scooped. The thing that matters the most is being ignored.” On the other hand, if you are in an area of research that you perceive to be competitive, or where there is some significant risk of this kind of shenanigans, it’s very easy to keep part, or all, of a repository private among your collaborators until you are ready to share more widely. All of the benefits we described still accrue. For an appropriately organized and hosted project, often the only steps required to share materials, data, and code are (1) to make the hosted repository public and (2) to link it to an archival storage platform like the Open Science Framework.\n\n\n13.1.3 File names\nAs Phil Karlton reportedly said, “There are only two hard things in Computer Science: cache invalidation and naming things.” What’s true for computer science is true for research in general.10 Naming files is hard! Some very organized people survive on systems like INFO-r1-draft-2020-07-13-js.docx, meaning “the INFO project revision 1 draft of July 13th, 2020, with edits by JS.” But this kind of system needs a lot of rules and discipline, and it requires everyone in a project to buy in completely.\n10 We won’t talk about cache invalidation; that’s a more technical problem in computer science that is beyond the scope of this book.On the other hand, if you are naming a file in a hierarchically organized version control repository, the naming problem gets dramatically easier. All of a sudden, you have a context in which names make sense. data.csv is a terrible name for a data file on its own. But the name is actually perfectly informative—in the context of a project repository with a README that states that there is only a single experiment, a repository structure such that the file lives in a folder called raw_data, and a commit history that indicates the file’s commit date and author.\nAs this example shows, naming is hard out of context. So here’s our rule: name a file with what it contains. Don’t use the name to convey the context of who edited it, when, or where it should go in a project. That is metadata that the platform should take care of.11\n11 The platform won’t take care of it if you email it to a collaborator—precisely why you should share access to the full platform, not just the out-of-context file!",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#data-management",
    "href": "013-management.html#data-management",
    "title": "13  Project management",
    "section": "13.2 Data Management",
    "text": "13.2 Data Management\nWe’ve just discussed how to manage projects in general; in this section we zoom in on datasets specifically. Data are often the most valuable research product because they represent the evidence generated by our research. We maximize the value of the evidence when other scientists can reuse it for independent verification or generation of novel discoveries. Yet, lots of research data are not reusable, even when they are shared. In chapter 3, we discussed Hardwicke et al.’s (2018) study of analytic reproducibility. But before we were even able to try and reproduce the analytic results, we had to look at the data. When we did that, we found that only 64% of shared datasets were both complete and understandable.\nHow can you make sure that your data are managed so as to enable effective sharing? We make four primary recommendations:\n\nsave your raw data\ndocument your data collection process\norganize your raw data for later analysis\ndocument your data using a codebook or other metadata\n\nLet’s look at each in turn.\n\n13.2.1 Save your raw data\nRaw data take many forms. For many of us, the raw data are those returned by the experimental software; for others, the raw data are videos of the experiment being carried out. Regardless of the form of these data, save them! They are often the only way to check issues in whatever processing pipeline brings these data from their initial state to the form you analyze. They also can be invaluable for addressing critiques or questions about your methods or results later in the process. If you need to correct something about your raw data, do not alter the original files. Make a copy, and make a note about how the copy differs from the original.12\n12 Future you will thank present you for explaining why there are two copies of subject 19’s data after you went back and corrected a typo.13 The precise repository you use for this task is likely to vary by the kind of data that you’re trying to store and the local regulatory environment. For example, in the United States, to store de-anonymized data with certain fields requires a server that is certified for HIPAA (the relevant privacy law). Many—but by no means all—universities provide HIPAA-compliant cloud storage.Raw data are often not anonymized—or even anonymizable. Anonymizing them sometimes means altering them (e.g., in the case of downloaded logs from a service that might include IDs or IP addresses). Or in some cases, anonymization is difficult or impossible without significant effort and loss of some value from the data, for example, for video data or MRI data (Bischoff-Grethe et al. 2007). Unless you have specific permission for broad distribution of these identifiable data, the raw data may then need to be stored in a different way. In these cases, we recommend saving your raw data in a separate repository with the appropriate permissions. For example, in the ManyBabies 1 study we described above, the public repository does not contain the raw data contributed by participating labs, which the team could not guarantee was anonymized; these data are instead stored in a private repository.13\nYou can use your repository’s README to describe what is and is not shared. For example, a README might state, “We provide anonymized versions of the files originally downloaded from Qualtrics” or “Participants did not provide permission for public distribution of raw video recordings, which are retained on a secure university server.” Critically, if you share the derived tabular data, it should still be possible to reproduce the analytic results in your paper, even if checking the provenance of those numbers from the raw data is not possible for every reader.14\n14 One way we organize the raw data in some of our paper is to have three different subfolders in the data/ directory: raw/, for the original data; processed/, for the anonymized or otherwise preprocessed data; and /scripts, for the code that does the preprocessing. Since these folders are in a Git repository, we can then add raw/* to the .gitignore file, ensuring that they are never added to the public version of the repository even though they sit within our local file hierarchy in the appropriate place.15 A word about subject identifiers. These should be anonymous identifiers, like randomly generated numbers, that cannot be linked to participant identities (like date of birth) and are unique. You laugh, but one of us was in a lab where all the subject IDs were the date of test and the initials of the participant. These were neither unique nor anonymous. One common convention is to give your study a code-name and to number participants sequentially, so your first participant in a sequence of experiments on information processing might be INFO-1-01.One common practice is the use of participant identifiers to link specific experimental data—which, if they are responses on standardized measures, rarely pose a significant identifiability risk—to demographic data sheets that might include more sensitive and potentially identifiable data.15 Depending on the nature of the analyses being reported, the experimental data can then be shared with limited risk. Then a selected set of demographic variables—for example, those that do not increase privacy risks but are necessary for particular analyses—can be distributed as a separate file and joined back into the data later.\n\n\n13.2.2 Document your data collection process\nIn order to understand the meaning of the raw data, it’s helpful to share as much as possible about the context in which they were collected. This practice also helps communicate the experience that participants had in your experiment. Documentation of this experience can take many forms.\nIf the experimental experience was a web-based questionnaire, archiving this experience can be as simple as downloading the questionnaire source.16 For more involved studies, it can be more difficult to reconstruct what participants went through. This kind of situation is where video data can shine (Gilmore and Adolph 2017). A video recording of a typical experimental session can provide a valuable tutorial for other experimenters—as well as good context for readers of your paper. This is doubly true if there is a substantial interactive element to your experimental experience, as is often the case for experiments with children. For example, in our ManyBabies case study, the project shared “walk-through” videos of experimental sessions for many of the participating labs, creating a repository of standard experiences for infant development studies. If nothing else, a video of an experimental session can sometimes be a very nice archive of a particular context.17\n16 If it’s in a proprietary format like a Qualtrics .QSF file, a good practice is to convert it to a simple plain text format as well so it can be opened and reused by folks who do not have access to Qualtrics (which may include future you!).17 Videos of experimental sessions also are great demos to show in a presentation about your experiment, provided you have permission from the participant.Regardless of what specific documentation you keep, it’s critical to create some record linking your data to the documentation. For a questionnaire study, for example, this documentation might be as simple as a README that says that the data in the data/raw/ directory were collected on a particular date using the file named experiment1.qsf. This kind of “connective tissue” linking data to materials can be very important when you return to a project with questions. If you spot a potential error in your data, you will want to be able to examine the precise version of the materials that you used to gather those data in order to identify the source of the problem.\n\n\n13.2.3 Organize your data for later analysis: Spreadsheets\nData come in many forms, but chances are that at some point during your project you will end up with a spreadsheet full of information. Well-organized spreadsheets can mean the difference between project success and failure! A wonderful article by Broman and Woo (2018) lays out principles of good spreadsheet design. We highlight some of their principles here (with our own, opinionated ordering):\n\nMake it a rectangle.18 Nearly all data analysis software, like SPSS, Stata, Jamovi, and JASP (and many R packages), require data to be in a tabular format.19 If you are used to analyzing data exclusively in a spreadsheet, this kind of tabular data isn’t quite as readable, but readable formatting gets in the way of almost any analysis you want to do. Figure 13.4 gives some examples of nonrectangular spreadsheets. All of these will cause any analytic package to choke because of inconsistencies in how rows and columns are used!\n\n18 Think of your data like a well-ordered plate of sushi, neatly packed together without any gaps.19 Tabular data is a precursor to “tidy” data, which we describe in more detail in appendix D.\n\n\n\n\n\nFigure 13.4: Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. Adapted from Broman and Woo (2018).\n\n\n\n\nChoose good names for your variables. No one convention for name formatting is best, but it’s important to be consistent. We tend to follow the tidyverse style guide and use lowercase words separated by underscores (_). It’s also helpful to give units where these are available—for example, whether reaction times are in seconds or milliseconds. Table 13.1 gives some examples of good and bad variable names.\n\n\n\n\nTable 13.1: Examples of good and bad variable names. Adapted from Broman and Woo (2018).\n\n\n\n\n\nGood name\nGood alternative\nAvoid\n\n\n\n\nsubject_id\nSubID\nsubject #\n\n\nsex\nfemale\nM/F\n\n\nrt_msec\nreaction_time_ms\nreaction time (millisec.)\n\n\n\n\n\n\n\nBe consistent with your cell formatting. Each column should have one kind of thing in it. For example, if you have a column of numerical values, don’t all of a sudden introduce text data like “missing” into one of the cells. This kind of mixing of data types can cause havoc down the road. Mixed or multiple entries also don’t work, so don’t write “0 (missing)” as the value of a cell. Leaving cells blank is also risky because it is ambiguous. Most software packages have a standard value for missing data (e.g., NA is what R uses). If you are writing dates, please be sure to use the “global standard” (ISO 8601), which is YYYY-MM-DD. Anything else can be misinterpreted easily.20\nDecoration isn’t data. Decorating your data with bold headings or highlighting may seem useful for humans, but it isn’t uniformly interpreted or even recognized by analysis software (e.g., reading an Excel spreadsheet into R will scrub all your beautiful highlighting and artistic fonts), so do not rely on it.\nSave data in plain text files. The CSV (comma-delimited) file format is a common standard for data that is uniformly understood by most analysis software (it is an “interoperable” file format).21 The advantage of CSVs is that they are not proprietary to Microsoft or another tech company and can be inspected in a text editor, but be careful: they do not preserve Excel formulas or formatting!\n\n20 Dates in Excel deserve special mention as a source of terribleness. Excel has an unfortunate habit of interpreting information that has nothing to do with dates as dates, destroying the original content in the process. Excel’s issue with dates has caused unending horror in the genetics literature, where gene names are automatically converted to dates, sometimes without the researchers noticing (Ziemann, Eren, and El-Osta 2016). In fact, some gene names have had to be changed in order to avoid this issue!21 Be aware of some interesting differences in how these files are output by European vs American versions of Microsoft Excel! You might find semicolons instead of commas in some datasets.Given the points above, we recommend that you avoid analyzing your data in Excel. If it is necessary to analyze your data in a spreadsheet program, we urge you to save the raw data as a separate CSV and then create a distinct analysis spreadsheet so as to be sure to retain the raw data unaltered by your (or Excel’s) manipulations.\n\n\n13.2.4 Organize your data for later analysis: Software\nMany researchers do not create data by manually entering information into a spreadsheet. Instead they receive data as the output from a web platform, software package, or device. These tools typically provide researchers limited control over the format of the resulting tabular data export. Case in point is the survey platform Qualtrics, which—at least at the moment—provides data with not one but two header rows, complicating import into almost all analysis software!22\n22 The R package qualtRics (Ginn, O’Brien, and Silge 2024) can help with this.That said, if your platform does allow you to control what comes out, you can try to use the principles of good tabular data design outlined above. For example, try to give your variables (e.g., questions in Qualtrics) sensible names!\n\n\n\n\n\n\naccident report\n\n\n\n\n\nBad variable naming can lead to analytic errors!\nIn our methods class, students often try to reproduce the original analyses from a published study before attempting to replicate the results in a new sample of participants. When Kengthsagn Louis looked at the code for the study she was interested in, she noticed that the variables in the analysis code were named horribly (presumably because they were output this way by the survey software). For example, one piece of Stata code looked like this:\ngen recall1=.\nreplace recall1=0 if Q21==1 \nreplace recall1=1 if Q21==3 | Q21==5 | Q21==6\nreplace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8\nreplace recall1=0 if Q69==1 \nreplace recall1=1 if Q69==3 | Q69==5 | Q69==6\nreplace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8\nta recall1\nIn the process of translating this code into R in order to reproduce the analyses, Kengthsagn and a course teaching assistant, Andrew Lampinen, noticed that some participant responses had been assigned to the wrong variables. Because the variable names were not human-readable, this error was almost impossible to detect. Since the problem affected some of the inferential conclusions of the article, the article’s author—to their credit—issued an immediate correction (M. B. Petersen 2019).\nThe moral of the story: obscure variable names can hide existing errors and create opportunities for further error! Sometimes you can adjust these within your experimental software, avoiding the issue. If not, make sure to create a “key” and translate the names immediately, double checking after you are done.\n\n\n\n\n\n13.2.5 Document the format of your data\nEven the best-organized tabular data are not always easy to understand by other researchers, or even yourself, especially after some time has passed. For that reason, you should make a codebook (also known as a data dictionary) that explicitly documents what each variable is. Figure 13.6 shows an example codebook for the trial-level data in the bottom of figure 13.5. Each row represents one variable in the associated dataset. Codebooks often describe what type of variable a column is (e.g., numeric, string), and what values can appear in that column. A human-readable explanation is often given as well, providing units (e.g., “seconds”) and a translation of numeric codes (e.g., “test condition is coded as 1”) where relevant.\n\n\n\n\n\n\nFigure 13.5: Example participant (top) and trial (bottom) level data from the ManyBabies (2020) case study.\n\n\n\n\n\n\n\n\n\nFigure 13.6: Codebook for trial-level data (see above) from the ManyBabies (2020) case study.\n\n\n\nCreating a codebook need not require a lot of work. Almost any documentation is better than nothing! There are also several R packages that can automatically generate a codebook for you, for example codebook (Arslan 2019), dataspice (Boettiger et al. 2021), and dataMaid (A. H. Petersen and Ekstrøm 2019). Adding a codebook can substantially increase the reuse value of the data and prevent hours of frustration as future you and others try to decode your variable names and assumptions.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#bad-variable-naming-can-lead-to-analytic-errors",
    "href": "013-management.html#bad-variable-naming-can-lead-to-analytic-errors",
    "title": "13  Project management",
    "section": "Bad variable naming can lead to analytic errors!",
    "text": "Bad variable naming can lead to analytic errors!\nIn our methods class, students often try to reproduce the original analyses from a published study before attempting to replicate the results in a new sample of participants. When Kengthsagn Louis looked at the code for the study she was interested in, she noticed that the variables in the analysis code were named horribly (presumably because they were output this way by the survey software). For example, one piece of Stata code looked like this:\ngen recall1=.\nreplace recall1=0 if Q21==1 \nreplace recall1=1 if Q21==3 | Q21==5 | Q21==6\nreplace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8\nreplace recall1=0 if Q69==1 \nreplace recall1=1 if Q69==3 | Q69==5 | Q69==6\nreplace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8\nta recall1\nIn the process of translating this code into R in order to reproduce the analyses, Kengthsagn and a course teaching assistant, Andrew Lampinen, noticed that some participant responses had been assigned to the wrong variables. Because the variable names were not human-readable, this error was almost impossible to detect. Since the problem affected some of the inferential conclusions of the article, the article’s author—to their credit—issued an immediate correction (M. B. Petersen 2019).\nThe moral of the story: obscure variable names can hide existing errors and create opportunities for further error! Sometimes you can adjust these within your experimental software, avoiding the issue. If not, make sure to create a “key” and translate the names immediately, double checking after you are done.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#sharing-research-products",
    "href": "013-management.html#sharing-research-products",
    "title": "13  Project management",
    "section": "13.3 Sharing Research Products",
    "text": "13.3 Sharing Research Products\nAs we’ve been discussing throughout this chapter, if you’ve managed your research products effectively, sharing them with others is a far less daunting prospect, and usually just requires uploading them to an online repository like the Open Science Framework. This section addresses some potential limitations on sharing that you should bear in mind and discusses where and how to share research products.\n\n13.3.1 What you can and can’t share\nWe’ve been advocating that you share all of your research products, especially your data. In practice, however, participant privacy (as well as a few other constraints) limits what you can share. Luckily, there are some concrete steps you can take to make sure that you protect participants and comply with your obligations while still realizing the benefits of data sharing.\nUnless they explicitly waive their rights, participants in psychology experiments have the expectation of privacy—that is, no one should be able to identify them from the data they have provided. Protecting participant privacy is an important part of researchers’ ethical responsibilities (Ross, Iguchi, and Panicker 2018) and needs to be balanced against the ethical imperatives to share (see chapter 4).23\n23 Meyer (2018) gives an excellent overview of how to navigate various legal and ethical issues around data sharing in the US context.Furthermore, there are legal regulations that protect participants’ data, though these vary from country to country. In the US, the relevant regulation is HIPAA, the Health Insurance Portability and Accountability Act, which limits disclosures of private health information (PHI). In the European Union, the relevant regulation is the European GDPR (General Data Protection Regulation). It’s beyond the scope of this book to give a full treatment of these regulatory frameworks; you should consult with your local ethics board regarding compliance, but here is the way we have navigated this situation while still sharing data.\nUnder both frameworks, anonymization (or equivalently de-identification) of data is a key concept, such that data sharing is generally just fine if the data meet the relevant standard. Under US guidelines, researchers can follow the “safe harbor” standard24 under which data are considered to be anonymized if they do not contain identifiers like names, telephone numbers, email addresses, social security numbers, dates of birth, faces, and others. Thus, data that only contain participant IDs and nothing from this list can typically be shared without participant consent without a problem.25\n24 As described on the relevant DHHS page (https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html).25 US IRBs are a very decentralized bunch, and their interpretations often vary considerably. For reasons of liability or ethics, they may not allow data sharing even though it is permitted by US law. If you feel like arguing with an IRB that takes this kind of stand, you could mention that the DHHS rule actually doesn’t consider de-identified data to be “human subjects” data at all, and thus the IRB may not have regulatory authority over it. We’re not lawyers, and we’re not sure if you’ll succeed, but it could be worth a try.The EU’s GDPR also allows fully anonymized data sharing, with one big complication. Putting anonymous identifiers in a data file and removing identifiable fields does not itself suffice for GDPR anonymization if the data are still in principle reidentifiable because you have maintained documentation linking IDs to identifiable data like names or email addresses. Only when the key linking identifiers to data has been destroyed are the data truly de-identified according to this standard.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nReally anonymous?\nWhen we first began teaching Psych 251, our experimental methods course at Stanford, one of the biggest contributions of the course was simply showing students how to do experiments online. Amazon’s Mechanical Turk crowdsourcing service was relatively new, and our IRB did not have a good sense of what this service really was. We proposed that we would share data from the class and received approval for this practice. Our datasets were downloaded directly from Mechanical Turk and included participants’ MTurk IDs (long alphanumeric strings that seemed completely anonymous). Several experiences caused us to reconsider this practice!\nFirst, we discovered that MTurk IDs were in some cases linked to study participants’ public Amazon “wish lists,” which could both inadvertently provide information about the participant and also even potentially provide a basis for reidentification (in rare cases). This discovery led us to consult with our IRB and provide more explicit consent language in our class experiments, linking to instructions for making Amazon profiles private.\nThen, a little later we received an irate email from an MTurk participant who had discovered their data on GitHub via a search for their MTurk ID. Although they were not identified in this dataset, it convinced us that at least some participants would not like this ID shared. After another consultation with the IRB, we apologized to this individual and removed their and others’ IDs from our GitHub commit histories across that and other repositories. Prior to posting data, we now take care to anonymize IDs by creating a secret mapping between the IDs we post and the actual MTurk IDs.\n\n\n\nDe-identification is not always enough. As datasets get richer, statistical reidentification risks go up substantially such that, with a little bit of outside information, data can be matched with a unique individual. These risks are especially high with linguistic, physiological, and geospatial data, but they can be present even for simple behavioral experiments. In one influential demonstration, knowing a person’s location on two occasions was often enough to identify their data uniquely in a huge database of credit card transactions (De Montjoye et al. 2015).26 Thus, simply removing fields from the data is a good starting point—but if you are collecting richer data about participants’ behavior you may need to consult an expert.\n26 For an example closer to home, many of the contributing labs in the ManyBabies project logged the date of test for each participant. This useful and seemingly innocuous piece of information is unlikely to identify any particular participant—but alongside a social media post about a lab visit or a dataset about travel records, it could easily reveal a particular participant’s identity.Privacy issues are ubiquitous in data sharing, and almost every experimental research project will need to solve them before sharing data. For simple projects, often these are the only issues that preclude data sharing. However, in more complex projects, other concerns can arise. Funders may have specific mandates regarding where your data should be shared. Data use agreements or collaborator preferences may restrict where and when you can share. And certain data types require much more sensitivity since they are more consequential than, say, the reaction times on a Stroop task. We include here a set of questions to walk through to plan your sharing (figure 13.7). When in doubt, it’s often a good idea to consult with the relevant local authority—for example, your ethics board for ethical issues or your research management office for regulatory issues.\n\n\n\n\n\n\nFigure 13.7: A decision chart for thinking about sharing research products. Adapted from Klein et al. (2018).\n\n\n\n\n\n13.3.2 Where and how to share: the FAIR principles\n\n\n\n\nFor shared research products27 to be usable by others, they should meet the FAIR standard by being findable, accessible, interoperable, and reusable (Wilkinson et al. 2016).\n27 Most of this discussion is about data, because that’s where the community has focused its efforts. That said, almost everything here applies to other research products as well!\nFindable products are easily discoverable to both humans and machines. That means linking to them in research reports using unique persistent identifiers (e.g., a digital object identifier [DOI])28 and attaching them with metadata describing what they are so they can be indexed by search engines.\nAccessibility means that research products need to be preserved across the long term and are retrievable via their standardized identifier.\nInteroperability means that the research products needs to be in a format that people and machines (e.g., search engines and analysis software) can understand.\nReusable means that the research products need to be well organized, documented, and licensed so that others know how to use them.\n\n28 DOIs are those long URL-like things that are often used to link to papers. Turns out they can also be associated with datasets and other research products. Critically, they are guaranteed to work to find stuff, whereas standard web URLs often go stale after several years when people refactor their website. Most online repositories, like the Open Science Framework, will issue DOIs for the research products you store there.29 You can get a DOI for GitHub software through a partnership with Zenodo (https://zenodo.org), a FAIR-compliant repository.If you’ve followed the guidance in the rest of this chapter, then you will already be well on your way to making your research products FAIR. There are a few final steps to consider. An important decision is where you are going to share the research products. We recommend uploading the files to a repository that’s designed to support FAIR principles. Personal websites don’t cut it, since these sites tend to go out of date and disappear. There’s also no easy way to find research products on personal sites unless you know who created them. GitHub, though it’s a great platform for collaboration, isn’t a FAIR repository—for one thing, products there don’t necessarily have DOIs29—and there are no archival guarantees on files that are shared there. Perhaps surprisingly for some researchers, journal supplementary materials are also not a great place to put research products. Often supplementary materials are assigned no unique DOI or metadata, have limited supported formats, and have no persistence guarantees (Evangelou, Trikalinos, and Ioannidis 2005).\nFortunately, there are many repositories that help you conform to FAIR standards. Zenodo, Figshare, the Open Science Framework (OSF), and the various Dataverse sites are designed for this purpose, though there are many other domain-specific repositories that are particularly relevant for different research fields. We often use the OSF as it makes it easy to share all research products connected to a project in one place. Open Science Framework is FAIR compatible and allows users to assign DOIs to their data and provide appropriate metadata.\nWe recommend you attach a license to your research products. Academic culture is (usually) unburdened by discussion of intellectual property and legal rights and instead relies on scholarly norms about citation and attribution. The basic expectation is that if you rely on someone else’s research, you explicitly acknowledge the relevant journal article through a citation. Although norms are still evolving, using research products created by others generally adheres to the same scholarly principle. Research products can also be useful in nonacademic contexts, however. Perhaps you created software that a company would like to use. Maybe a pediatrician would like to use a research instrument you’ve been working on to assess their patients. These applications (and many other reuses of the data) require a legal license. In practice, there are a number of simple, open-source licenses that permit reuse. We tend to favor Creative Commons licenses, which come in a variety of flavors such as CC0 (which allows all reuse), CC-BY (which allows reuse as long as there is attribution), and CC-BY-NC (which only allows attributed, noncommercial reuse).30 Regardless of what license you choose, having a license means that your products won’t be in a “not sure what I’m allowed to do with this” limbo for others who are interested in reusing them.\n30 Klein et al. (2018) recommend the CC0 license, which puts no limits on what can be done with your data. At first glance, it may seem like a license that requires attribution is useful. But academic norms, rather than the threat of litigation, lead to good citation practices. In addition, more restrictive licenses can mean that some legitimate uses of your data or research can be blocked.As we have discussed, you may want to consider storing your work in a public repository from the outset of the project. If you are using GitHub to manage your project, you can link the Git repository to the Open Science Framework so it automatically syncs. This provides a valuable incentive to organize your work properly throughout your project and makes sharing super easy, because you’ve already done it! On the other hand, this way of working can feel exposed for some researchers, and it does carry some risks, however small, of “scooping” or preemption by other groups working in the same space. Fortunately you can set up the same Git-OSF workflow and keep it private until you’re ready to make it public later on.\nThe next stage at which you should consider sharing your research products is when you submit your study to a journal. If you’re still hesitant to make the project entirely public, many repositories (including OSF) will allow you to create special links that facilitate limited access to, for example, reviewers and editors. In general, the earlier you share your research products the better because there are more opportunities for others to learn from, build on, and verify your research.31 But if neither of these options seems appealing, please do share your research products once your paper is accepted. Doing so will increase the value (and the impact) of your publication.\n31 If there are errors in our work, we’d certainly love to hear about it before the article is published in a journal rather than after!",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#really-anonymous",
    "href": "013-management.html#really-anonymous",
    "title": "13  Project management",
    "section": "Really anonymous?",
    "text": "Really anonymous?\nWhen we first began teaching Psych 251, our experimental methods course at Stanford, one of the biggest contributions of the course was simply showing students how to do experiments online. Amazon’s Mechanical Turk crowdsourcing service was relatively new, and our IRB did not have a good sense of what this service really was. We proposed that we would share data from the class and received approval for this practice. Our datasets were downloaded directly from Mechanical Turk and included participants’ MTurk IDs (long alphanumeric strings that seemed completely anonymous). Several experiences caused us to reconsider this practice!\nFirst, we discovered that MTurk IDs were in some cases linked to study participants’ public Amazon “wish lists,” which could both inadvertently provide information about the participant and also even potentially provide a basis for reidentification (in rare cases). This discovery led us to consult with our IRB and provide more explicit consent language in our class experiments, linking to instructions for making Amazon profiles private.\nThen, a little later we received an irate email from an MTurk participant who had discovered their data on GitHub via a search for their MTurk ID. Although they were not identified in this dataset, it convinced us that at least some participants would not like this ID shared. After another consultation with the IRB, we apologized to this individual and removed their and others’ IDs from our GitHub commit histories across that and other repositories. Prior to posting data, we now take care to anonymize IDs by creating a secret mapping between the IDs we post and the actual MTurk IDs.",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "013-management.html#chapter-summary",
    "href": "013-management.html#chapter-summary",
    "title": "13  Project management",
    "section": "13.4 Chapter summary",
    "text": "13.4 Chapter summary\nAll of the hard work you put into your experiments—not to mention the contributions of your participants—can be undermined by bad data and project management. As our accident reports and case study show, bad organizational practices can at a minimum cause huge headaches. Sometimes the consequences can be even worse. On the flip side, starting with a firm organizational foundation sets your experiment up for success. These practices also make it easier to share all of the products of your research, not just your findings. Such sharing is useful both for individual researchers and for the field as a whole.\n\n\n\n\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nFind an Open Science Framework repository that corresponds to a published paper. What is their strategy for documenting what is shared? How easy is it to figure out where everything is and if the data and materials sharing is complete?\nOpen up the US Department of Health and Human Services “safe harbor” standards (https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html) and navigate to the section called “The De-identification Standard.” Go through the list of identifiers that must be removed. Are there any on this list that you would need to include in your dataset in order to conduct your own research? Can you think of any others that do not fall on this list?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA more in-depth tutorial on various aspects of scientific openness: Klein, Olivier, Tom E Hardwicke, Frederik Aust, Johannes Breuer, Henrik Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf Vanpaemel, and Michael C Frank (2018). “A Practical Guide for Transparency in Psychological Science.” Collabra: Psychology 4 (1): 20. https://doi.org/10.1525/collabra.158",
    "crumbs": [
      "Execution",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Project management</span>"
    ]
  },
  {
    "objectID": "014-writing.html",
    "href": "014-writing.html",
    "title": "14  Writing",
    "section": "",
    "text": "14.1 Writing clearly\nWhat is the purpose of writing? “Telepathy, of course,” says Stephen King (King 2000). The goal of writing is to transfer information from your mind to the reader’s as effectively as possible. Unfortunately, for most of us, writing clearly does not come naturally; it is a craft we need to work at.\nOne of the most effective ways to learn to write clearly is to read and to imitate the writing you admire. Many scientific articles are not clearly written, so you will need to be selective in which models you imitate. Fortunately, as a reader, you will know good writing when you see it—you will feel like the writer is sending ideas directly from their mind to yours. When you come across writing like that, try to find more work by the same author. The more good scientific writing you are exposed to, the more you will develop a sense of what works and what does not. You may pick up bad habits as well as good ones (we sure have!), but over time, your writing will improve if you make a conscious effort to weed out the bad, and keep the good.\nThere are no strict rules of clear writing, but there are some generally accepted conventions that we will share with you here, drawing from both general style guides and those specific to scientific writing (Zinsser 2006; Heard 2016; Gernsbacher 2018; Savage and Yeh 2019).",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "014-writing.html#writing-clearly",
    "href": "014-writing.html#writing-clearly",
    "title": "14  Writing",
    "section": "",
    "text": "14.1.1 The structure of a scientific paper\nA scientific paper is not a novel. Rather than reading from beginning to end, readers typically jump between sections to extract information efficiently (Doumont 2009). This “random access” is possible because research articles typically follow the same conventional structure (see figure 14.1). The main body of the article includes four main sections: introduction, methods, results, and discussion (IMRaD).2 This structure has a narrative logic: What’s the knowledge gap? (introduction); how did you address it? (methods); what did you find? (results); what do the results mean? (discussion).\n2 In the old old days, there were few conventions—scientists would share their latest findings by writing letters to each other. But as the number of scientists and studies increased, this approach became unsustainable. The IMRaD structure gained traction in the 1800s and became dominant in the mid-1900s as scientific productivity rapidly expanded in the post-war era. We think IMRaD style articles are a big improvement, even if it is nice to receive a letter every now and again.Structure helps writers as well as readers. Try starting the writing process with section headings as a structure, then flesh it out, layer by layer. In each section, start by making a list of the key points you want to convey, each representing the first sentence of a new paragraph. Then add the content of each paragraph, and you’ll be well on your way to having a full first draft of your article.\nImagine that the breadth of focus in the body of your article has an “hourglass” structure (figure 14.1). The start of the introduction should have a broad focus, providing the reader with the general context of your study. From there, the focus of the introduction should get increasingly narrow until you are describing the specific knowledge gap or problem you will address and (briefly) how you are going to address it. The methods and results sections are at the center of the hourglass because they are tightly focused on your study alone. In the discussion section, the focus shifts in the opposite direction, from narrow to broad. Begin by summarizing the results of your study, discuss limitations, then integrate the findings with existing literature and describe practical and theoretical implications.\n\n\n\n\n\n\n\nFigure 14.1: Conventional structure of a research article. The main body of the article consists of introduction, methods, results, and discussion (IMRaD) sections.\n\n\nResearch articles are often packed with complex information; it is easy for readers to get lost. A “cross-reference” is a helpful signpost that tells readers where they can find relevant additional information without disrupting the flow of your writing. For example, you can refer the reader to data visualizations by cross-referencing to figures or tables (e.g., “see Figure 1”), or additional methodological information in the supplementary information (e.g., “see Supplementary Information A”).\nOne useful trick for structuring complex arguments is to cross-reference your research aims/hypotheses with your results. For example, you could introduce numbered hypotheses in the introduction of an article and then refer to them directly when reporting the relevant analyses and results. These cross-references can serve to remind readers how different results or analyses relate back to your research goals.\n\n\n14.1.2 Paragraphs, sentences, and words\nWriting an article is like drawing a human form. If you begin by sketching the clothes, you risk adding beautiful textures onto an impossible shape. Instead, you have to start by understanding the underlying skeleton, and then gradually adding layers until you can visualize how cloth hangs on the body. The structure of an article is the “skeleton” and the paragraphs and sentences are the “flesh.” Only start thinking about paragraphs and sentences once you have a solid outline in place.\nIdeally, each paragraph should correspond to a single point in the article’s outline, with the specifics necessary to convince the reader embedded within. “P-E-E-L” (point - explain - evidence - link) is a useful paragraph structure, particularly in the introduction and discussion sections. First, state the paragraph’s message succinctly in the first sentence (P). The core of the paragraph is dedicated to further explaining the point and providing evidence (E-E; you can also include a third “E”—an example). At the end of the paragraph, take a couple of sentences to remind the reader of your point and set up a link to the next paragraph.\nSince each sentence in a paragraph has a purpose, you can compose and edit the sentence by asking how its form serves that purpose. For example, short sentences are great for making strong initial points. On the other hand, if you only use short sentences, your writing may come across as monotonous and robotic. Try varying sentence lengths to give your writing a more natural rhythm. Just avoid cramming too much information into the same sentence; very long sentences can be confusing and difficult to process.\nYou can also use sentence structure as a scaffold to support the reader’s thinking. Start sentences with something the reader already knows. For example, rather than writing “We performed a between-subjects \\(t\\)-test comparing performance in the experimental and control groups to address the cognitive dissonance hypothesis,” write “To address the cognitive dissonance hypothesis, we compared performance in the experimental group and control group using a between-subjects t-test.”\nHuman readers are good at processing narratives about people. Yet, often scientists compromise the research narrative by removing themselves from the process, sometimes even using awkward grammatical constructions to do so. For example, scientists sometimes write “The data were analysed” or, worse, “An analysis of the data was carried out.” Many of us were taught to write sentences like these, but it’s much clearer to say “We analyzed the data.”\nSimilarly, many of us tend to hide our views with frames and caveats: “[It is believed that/Research indicates that/Studies show that] money leads to increased happiness (Frog & Toad, 1963).” If you truly do believe that money causes happiness, simply assert it—with a citation if necessary. Save caveats for cases where someone believes that money causes happiness, but it’s not you. Emphasize uncertainty where you in fact feel that uncertainty is warranted, and readers will take your doubts more seriously.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "014-writing.html#advice",
    "href": "014-writing.html#advice",
    "title": "14  Writing",
    "section": "14.2 Advice",
    "text": "14.2 Advice\nScientific writing has a reputation for being dry, dull, and soulless. While it’s true that writing research articles is more constrained than writing fiction, there are still ways to surprise and entertain your reader with metaphor, alliteration, and even humor. As long as your writing is clear and accurate, we see no reason why you cannot also make it enjoyable. Enjoyable articles are easier to read and more fun to write.3\n3 One of our favorite examples of an enjoyable article is Cutler (1994), a delightful piece that uses the form of the article to make a point about human language processing. Read it: you’ll see!Here are a few more pieces of advice about expressing yourself clearly.\nBe explicit. Avoid vagueness and ambiguity. The more you leave the meaning of your writing to your reader’s imagination, the greater the danger that different readers will imagine different things! So be direct and specific.\nBe concise. Maximize the signal-to-noise ratio in your writing by omitting needless words and removing clutter (Zinsser 2006). For example, say we investigated rather than we performed an investigation of and say if rather than in the event that. Don’t try to convey everything you know about a topic—a research report is not an essay. Include only what you need to achieve the purpose of the article and exclude everything else.\nBe concrete. Concrete examples make abstract ideas easier to grasp. But some ideas are just hard to express in prose, and diagrams can be very helpful in these cases. For example, it may be clearer to illustrate a complex series of exclusion criteria using a flow chart rather than text. You can even use photos, videos, and screenshots to illustrate experimental tasks (Heycke and Spitzer 2019).\nBe consistent. Referring to the same concept using different words can be confusing because it may not be clear if you are referring to a different concept or just using a synonym. For example, in everyday conversation, “replication” and “reproducibility” may sound like two different ways to refer to the same thing, but in scientific writing, these two concepts have different technical definitions, so we should not use them interchangeably. Define each technical term once and then use the same term throughout the manuscript.\nAdjust to your audience. Most of us adjust our conversation style depending on who we’re talking to; the same principle applies to good writing. Knowing your audience is more difficult with writing, because we cannot see the reader’s reactions and adjust accordingly. Nevertheless, we can make some educated guesses about who our readers might be. For example, if you are writing an introductory review article, you may need to pay more attention to explaining technical terms than if you are writing a research article for a specialty journal.\nCheck your understanding. Unclear writing can be a symptom of unclear thinking. If an idea doesn’t make sense in your head, how will it ever make sense on the page? In fact, trying to communicate something in writing is an excellent way to probe your understanding and expose logical gaps in your arguments. So, if you are finding it difficult to write clearly, stop and ask yourself, do I know what I want to say? If the problem is unclear thinking, then it might be worth talking out the ideas with a colleague or advisor before you try to write them down.\n\nUse acronyms sparingly. It’s tempting to replace lengthy terminology with short acronyms–why say “cognitive dissonance theory” when you can say “CDT”? Unfortunately, acronyms can increase the reader’s cognitive burden and cause misunderstandings.4 For example, if you shorten “odds ratio” to “OR,” the reader has to take the extra step of translating “OR” back to “odds ratio” every time they encounter it. The problem multiplies as you introduce more acronyms into your article. Worse, for some readers, “OR” tends to mean “operating room,” not “odds ratio.” Acronyms can be useful, but usually only when they are widely used and understood.\n\n4 Barnett and Doubleday (2020) found that acronyms are widely used in research articles and argued that they undermine clear communication. Here is one example of text Barnett and Doubleday extracted from a 2019 publication to illustrate the point: “Applying PROBAST showed that ADO, B-AE-D, B-AE-D-C, extended ADO, updated ADO, updated BODE, and a model developed by Bertens et al. were derived in studies assessed as being at low risk of bias.”\n14.2.1 Drafting and revision\nThe clearest and most effortless-seeming scientific writing has probably gone through extensive revision to appear that way. It can surprise many students to know the amount of revision that has gone into many “breezy” articles. For example, Tversky and Kahneman repeatedly drafted and redrafted each word of their famous (and highly readable) articles on judgment and decision-making, hunched over the typewriter together (Lewis 2016).\nThink of the article you are writing as a garden. Your first draft may be an unruly mess of intertwined fronds and branches. Several rounds of pruning and sculpting will be needed before your writing reaches its most effective form. You’ll be amazed how often you find words you can omit or elaborate sentences you can simplify.\nIt can be difficult to judge if your own writing has achieved its telepathic goal, especially after several rounds of revision. Try to get feedback from somebody in your target audience. Their comments—even if not wholly positive—will give you a good sense of how much of your argument they understood (and agreed with).5\n5 Seek out people who are willing to tell you that your writing is not good! They may not make you feel good, but they will help you improve.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "014-writing.html#writing-reproducibly",
    "href": "014-writing.html#writing-reproducibly",
    "title": "14  Writing",
    "section": "14.3 Writing reproducibly",
    "text": "14.3 Writing reproducibly\nMany research results are not reproducible—that is, the numbers and graphs that they report can’t be recreated by repeating the original analyses—even on the original data. As we discussed in chapter 3, a lack of reproducibility is a big problem for the scientific literature; if you can’t trust the numbers in the articles you read, it’s much harder to build on the literature.\nFortunately, there are number of tools and techniques available that you can use to write fully reproducible research reports. The basic idea is to create an unbroken chain that links every single part of the data analysis pipeline, from the raw data through to the final numbers reported in your research article. This linkage enables you—and hopefully others as well—to trace the provenance of every number and recreate (reproduce) it from scratch.\n\n14.3.1 Why write reproducible reports?\nThere are (at least) three reasons to write reproducible reports. First, data analysis is an error-prone activity. Without safeguards in place, it can be easy to accidentally overwrite data, mislabel experimental conditions, or copy and paste the wrong statistics. As we discussed in chapter 3, one study found that nearly half of a sample of psychology papers contained obvious statistical reporting errors (Nuijten et al. 2016). You can reduce opportunities for error by adopting a reproducible analysis workflow that avoids error-prone manual actions, like copying and pasting.\nSecond, technical information about data analysis can be difficult to communicate in writing. Prose is often ambiguous, and authors can inadvertently leave out important details (Hardwicke et al. 2018). By contrast, a reproducible workflow documents the entire analysis pipeline from raw data to research report exactly as it was implemented, describing the origin of any reported values and allowing readers to assess, verify, and repeat the analysis process.\nFinally, reproducible workflows are typically more efficient workflows. For example, you may realize you forgot to perform data exclusions and need to rerun the analysis. You may produce a graph and then decide you’d prefer a different color scheme. Or perhaps you want to output the same results table in a PDF document and in a PowerPoint slide. In a reproducible workflow, all of the analysis steps are scripted and can be easily rerun at the click of a button. You (and others) can also reuse parts of your code in other projects, rather than having to write from scratch.\n\n\n14.3.2 Principles of reproducible writing\nBelow we outline some general principles of reproducible writing. These can be put in practice in a number of different software ecosystems. We recommend R Markdown and its successor, Quarto, which are ways of writing data analysis code in R so that it compiles into spiffy documents or even websites. (This book was written in Quarto.) Chapter C gives an introduction to the nuts and bolts of using these tools to create scientific papers.\n\nNever break the chain. Every part of the analysis pipeline—from raw data6 to final product—should be present in the project repository. By consulting the repository documentation, a reader should be able to follow the steps to go from the raw data to the final manuscript, including tables and figures.\nScript everything. Try to ensure that each step of the analysis pipeline is executed by computer code rather than manual actions such as copying and pasting or directly editing spreadsheets. This practice ensures that every step is documented via executable code rather than ambiguous description, ensuring that it can be reproduced. Imagine, for example, that you decided to recode a variable in your dataset. You could use the “find and replace” function in Excel, but this action would not be documented—you might even forget that you did it! A better option would be to write an R script. While a scripted pipeline can be a pain to set up the first time, by the third time you rerun it, it will save you time.\nUse literate programming. The meaning of a chunk of computer code is not always obvious to another user, especially if they’re not an expert. Indeed, we frequently look at our own code and scratch our heads, wondering what on earth it’s doing. To avoid this problem, try to structure your code around plain language comments that explain what it should be doing, a technique known as “literate programming” (Knuth 1992).\nUse defensive programming. Errors can still occur in scripted analyses. Defensive programming is a series of strategies to help anticipate, detect, and avoid errors in advance. A typical defensive programming tool is the inclusion of tests in your code, snippets that check if the code or data meet some assumptions. For example, you might test if a variable storing reaction times has taken on values below zero (which should be impossible). If the test passes, the analysis pipeline continues; if the test fails, the pipeline halts and an error message appears to alert you to the problem.\nUse free/open-source software and programming languages. If possible, avoid using commercial software, like SPSS or Matlab, and instead use free, open-source software and programming languages, like JASP, Jamovi, R, or Python. This practice will make it easier for others to access, reuse, and verify your work—including yourself!7\nUse version control. In chapter 13, we introduced the benefits of version control—a great way to save your analysis pipeline incrementally as you build it, allowing you to roll back to a previous version if you accidentally introduce errors.\nPreserve the computational environment. Even if your analysis pipeline is entirely reproducible on your own computer, you still need to consider whether it will run on somebody else’s computer, or even your own computer after software updates. You can address this issue by documenting and preserving the computational environment in which the analysis pipeline runs successfully. Various tools are available to help with this, including Docker, Code Ocean, renv (for R), and pip (for Python).8\n\n6 Modulo the privacy concerns discussed in chapter 13, of course.7 Several of us have libraries of old Matlab code. While discounted licenses are available to students, a full-price software license can be a major barrier to researchers with limited resources. if you move away from Matlab, it’s also terrible to have to ask yourself whether it’s worth the price of another year’s license just to rerun one old analysis.8 If you are interested in going in this direction, we recommend Peikert and Brandmaier (2021), which gives an advanced tutorial for complete computational reproducibility using Docker and make as tools to supplement Git and R Markdown.\n\n14.3.3 The reproducibility-collaboration trade-off\nWe would love to leave it there and watch you walk off into the sunset with a spring in your step and a reproducible report under your arm. Unfortunately, we have to admit that writing reproducibly can create a few practical difficulties when it comes to collaboration.\nA major aspect of collaboration is exchanging comments and inline text edits with coauthors. You can do this exchange with R Markdown files and Git, but these tools are not as user-friendly as, say, Word or Google Docs, and some collaborators will be completely unfamiliar with them. Most journals also expect articles to be submitted as Word documents. Outputting R Markdown files to Word can often introduce formatting issues, especially for moderately complex tables. So, until more user-friendly tools are introduced, some compromise between reproducibility and collaboration may be necessary. Here are two workflow styles for you to consider.\nFirst, the maximal reproducibility approach. If your collaborators are familiar with R Markdown and you don’t mind exchanging comments and edits via Git—or if they don’t mind giving you lists of comments and changes that you implement in the R Markdown document—then you can maintain a fully reproducible workflow for your project. The journal submission and publication process may still introduce some issues, such as incorporating changes made by the copy editor, but at least your submitted manuscript (and the preprint you have hopefully posted) will be fully reproducible.\nSecond, the two worlds approach. This workflow is a bit clunky, but it facilitates collaboration and maintains reproducibility. First, write your results section in R Markdown and generate a Word document (see appendix C). Then, write the remainder of the manuscript in Word, including incorporating comments and changes from collaborators. When you have a final version, copy and paste the abstract, introduction, methods, and discussion into the R Markdown document.9 Integrating any changes made to the results section back into the R Markdown requires a bit more effort, either using manual checking or Word’s “compare documents” feature.10 The advantage of this approach is that you have a reproducible document and your collaborators have not had to deviate from their preferred workflow. Unfortunately, it requires more effort from you and is slightly more error-prone than the maximal reproducibility approach.\n9 You can also incorporate Google Docs into this workflow—we find that cloud platforms like Docs are especially useful when gathering comments from multiple collaborators on the same document. Unfortunately, you cannot generate a Google Doc from R Markdown, so you will need to import and convert or else copy and paste.10 Packages such as trackdown (Kothe et al. 2021) could help as well (https://claudiozandonella.github.io/trackdown).",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "014-writing.html#writing-responsibly",
    "href": "014-writing.html#writing-responsibly",
    "title": "14  Writing",
    "section": "14.4 Writing responsibly",
    "text": "14.4 Writing responsibly\nAs a scientific writer, you have both professional and ethical responsibilities. You must communicate all relevant information about your research so as to enable proper evaluation and verification by other scientists. It is also important not to overstate your findings and calibrate your conclusions to the available evidence (Hoekstra and Vazire 2021). If errors are found in your work, you must respond and correct them when possible (Bishop 2018). Finally, you must meet scholarly obligations with regards to authorship and citation practices.\n\n14.4.1 Responsible disclosure and interpretation\nBack in school, we all learned that getting the right answer is not enough—you need to demonstrate how you arrived at that answer in order to get full marks. The same expectation applies to research reports. Don’t just tell the reader what you found, tell them how you found it.11 That means describing the methods in full detail, as well as sharing data, materials, and analysis scripts.\n11 It can be easy to overlook important details, especially when you reach the end of a project. Looking back at your study preregistration can be a helpful reminder. Reporting guidelines for different research designs can also provide useful checklists (Appelbaum et al. 2018).In a journal article, you typically have some flexibility in terms of how much detail you provide in the main body of the article and how much you relegate to the supplementary information. Readers have different needs; some may just want to know the highlights, and some will need detailed methodological information in order to replicate your study. As a rule of thumb, try to make sure there is nothing relegated to the supplementary information that might surprise the reader. You certainty should not use the supplementary information to hide important details deliberately or use it as a disorganized dumping ground—the principles of clear writing still apply!\nHere are a few more guidelines for responsible writing:\n\nDon’t overclaim. Scientists often feel they are (and unfortunately, often are) evaluated based on the results of their research, rather than the quality of their research. Consequently, it can be tempting to make bigger and bolder claims than are really justified by the evidence. Think carefully about the limitations of your research and calibrate your conclusions to the evidence, rather than what you wish you were able to claim. Ensure that your conclusions are appropriately stated throughout the manuscript, especially in the title and abstract.\nAcknowledge limitations on interpretation and generalizability. Even if you calibrate your claims appropriately throughout, there are likely specific limitations that are worth discussing, either as you introduce the design of the study in the introduction or as you interpret it in the discussion section. For example, if your experiment used one particular manipulation to instantiate a construct of interest, you might discuss this limitation and how it might be addressed by future work. Think carefully about the limitations of your study, state them clearly, and consider how they impact your conclusions (Clarke et al. 2023).12 Discussions of limitations are a great point to make an explicit statement about the generalizability of your findings (see Simons, Shoda, and Lindsay 2017 for guidance about these kinds of “Constraints on Generality” statements).\nDiscuss, don’t debate. The purpose of the discussion section is to help the reader interpret your research. Importantly, a journal article is not a debate—don’t feel the need to argue dogmatically for a particular position or interpretation. You should discuss the strengths and weaknesses of the evidence, and the relative merits of different interpretations. For example, perhaps there is a potential confounding variable that you were unable to eliminate with your research design. The reader might be able to spot this themselves, but regardless, its your responsibility to highlight it. Perhaps on balance you think the confound is unlikely to explain the results—that’s fine, but you need to explain your reasoning to the reader.\nDisclose conflicts of interest and funding. Researchers are usually personally invested in the outcomes of their research, and this investment can lead to bias (for example, overclaiming or selective reporting). But sometimes your potential personal gains from a piece of research rise above a threshold and are considered conflicts of interest. Where this threshold lies is not always completely clear. The most obvious conflicts of interest occur when you stand to benefit financially from the outcomes of your research (for example, a drug developer evaluating their own drug). If you are in doubt about whether you have a potential conflict of interest, then you should disclose it. You should also disclose any funding you received for the research, partly because this is often a requirement of the funder and partly because it may represent a conflict of interest, for example, if the funder has a particular stake in the outcome of the research. To avoid ambiguity, you should also disclose when you do not have a conflict of interest or funding to declare.\nReport transparently. In chapter 11, you learned about the problem of selective reporting and how this practice can bias the research literature. There are several ways to avoid this issue in your own work. First, assuming you have reported everything, include a statement in the methods section that explicitly says so. A statement suggested by Simmons, Nelson, and Simonsohn (2012) is “We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.” If you have preregistered your study, clearly link to the preregistration and state whether you deviated from your original plan. You can include a detailed preregistration disclosure table in the supplementary information and highlight any major deviations in the methods section. In the results section, clearly identify (e.g., with subheadings) which analyses were preplanned and included in the preregistration (confirmatory) and which were not planned (exploratory).\n\n12 Should you just make your claims more modest and avoid writing about your study’s limitations? The balance between claims and limitations is tricky. One way to navigate this issue is to ask yourself, “Is it OK to say X in the abstract of my article, if I later go on to say state a limitation relevant to that claim, or will the reader feel tricked?”\n\n14.4.2 Responsible handling of errors\nIt is not your responsibility to never make mistakes. But it is your responsibility to respond to errors in a timely, transparent, and professional manner (Bishop 2018).13 Regardless of how the error was identified (e.g., by yourself or by a reader), we recommend contacting the journal and requesting that they publish a correction statement (sometimes called an erratum). Several of us have corrected papers in the past. If the error is serious and cannot be fixed, you should consider retracting the article.\n13 As jazz musician Miles Davis once said, “If you hit a wrong note, it’s the next note that you play that determines if it’s good or bad.”A correction/retraction statement should include the following information:\n\nAcknowledge the error. Be clear that an error has occurred.\nDescribe the error. Readers need to know the exact nature of the error.\nDescribe the implications of the error. Readers need to know how the error might affect their interpretation of the results.\nDescribe how the error occurred. Knowing how the error happened may help others avoid the same error.\nDescribe what you have done to address the error. Others may learn from solutions you’ve implemented.\nAcknowledge the person who identified the error. Identifying errors can take a lot of work; if the person is willing to be identified, give credit where credit is due.\n\n\n\n\n\n\n\naccident report\n\n\n\n\n\nIn 2018, at a crucial stage of her career, Julia Strand published an important study in the prestigious journal Psychonomic Bulletin & Review. She presented the work at conferences and received additional funding to do follow-up studies. But several months later, her team found that they could not replicate the result.\nPuzzled, she began searching for the cause of the discrepant results. Eventually, she found the culprit—a programming error. As she sat staring at her computer in horror, she realized that it was unlikely anyone else would ever find the bug. Hiding the error must have seemed like the easiest thing to do.\nBut she did the right thing. She spent the next day informing her students, her coauthors, the funding officer, the department chair overseeing her tenure review, and the journal—to initiate a retraction of the article. And … it didn’t ruin her career. Everybody was understanding and appreciated that she was doing the right thing. The journal corrected the article. She didn’t lose her grant. She got tenure. And a lot of scientists, including us, admire her for what she did.\nHonest mistakes happen—it’s how you respond to them that matters (Strand 2021). In fact, survey research with both scientists and the general public suggests that scientists’ reputations are built on the perception that they try to “get it right,” not just to “be right” (Ebersole, Axt, and Nosek 2016).\n\n\n\n\n\n14.4.3 Responsible citation\nCiting prior work that your study builds upon ensures that researchers receive credit for their contributions and helps readers to verify the basis of your claims. You should certainly avoid copying the work of others and presenting it as your own (see chapter 4 for more on plagiarism). Try to be explicit about why you are citing a source. For example, does it provide evidence to support your point? Is it a review paper that gives the reader useful background? Or is it a description of a theory you are testing?\nMake sure you read articles before you cite them. Stang, Jonas, and Poole (2018) reports a cautionary tale in which a commentary criticizing a methodological tool was frequently cited as supporting the use of that tool! It seems that many authors had not read the paper they were citing, which is both misleading and embarrassing.\nTry to avoid selective or uncritical citation. It is misleading to cite only research that supports your argument and ignoring research that doesn’t. You should provide a balanced account of prior work, including contradictory evidence. Make sure to evaluate and integrate evidence from prior studies, rather than simply describing them. Remember—every study has limitations.\n\n\n14.4.4 Responsible authorship practices\nIt is an ethical responsibility to credit the individuals who worked on a research project—so that they can reap the benefits if the work is influential, but also so that they can take responsibility for errors.14\n14 In 1975, physicist and mathematician Jack H. Hetherington wrote a paper he intended to submit to the journal Physical Review Letters. We’re not sure why, but Hetherington wrote the paper in first person plural (i.e., referring to himself as “we” rather than “I”). He subsequently discovered that the journal would not accept the use of “we” for single-authored articles. Hetherington had painstakingly tapped out the article on his typewriter, an exercise he was not keen to repeat. Instead, he opted for a less taxing solution and named his cat—a feline by the name of F. D. C. Willard—as a coauthor. The paper was accepted and published (Hetherington and Willard 1975).Currently in academia, the authorship model is dominant. Under this model, authorship and authorship order are important signals about researchers’s contributions to a project. It is generally expected that to qualify for authorship, an individual should have made a substantial contribution to the research (e.g., design, data collection, analysis) and assisted with writing the research report, and that they take joint responsibility for the research along with the other coauthors. Individuals who worked on the project who do not reach this threshold are instead mentioned in a separate acknowledgements section and not considered authors.\nAuthorship order is often understood to signal the nature and extent of an author’s contribution. In psychology (and neighboring disciplines), the first author and last author are typically the project leaders. Typically—though certainly not always!—the first author is a junior colleague who implements the project and the last author is a senior colleague who supervises the project.\nIt has been argued that the authorship model should be replaced with a more inclusive contributorship model in which all individuals who worked on the project are acknowledged as “contributors.” Unlike the authorship model, there is no arbitrary threshold for contributorship. The actual contributions of each individual are explicitly described, rather than relying on the implicit conventions of authorship order. The contributorship model may facilitate collaboration and ensure student assistants are properly credited.\nYou will probably find that most journals still expect you to use the authorship model. Nevertheless, it is usually possible—and sometimes required—to include a contributorship statement in your article that describes what everybody did. For example, the CREDIT taxonomy provides a structured taxonomy of research tasks, making for uniform contributorship reporting.15\n15 For larger projects, the tool Tenzing allows for CREDIT statements to be generated automatically from standardized forms (Holcombe et al. 2020).16 If you find yourself in a situation where all authors have contributed equally, you may have to draw inspiration from historical examples and determine authorship order based on a 25-game croquet series (Hassell and May 1974); rock, paper, scissors (Kupfer, Webbeking, and Franklin 2004); or a brownie bake-off (Young and Young 1992). Alternatively, you can adopt the method of Lakens, Scheel, and Isager (2018) and randomize the authorship order in R!Because authorship is such an important signal in academia, it’s important to agree on an authorship plan with your collaborators (particularly who will be the first and last authors) as early as possible.16",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "014-writing.html#chapter-summary-writing",
    "href": "014-writing.html#chapter-summary-writing",
    "title": "14  Writing",
    "section": "14.5 Chapter summary: Writing",
    "text": "14.5 Chapter summary: Writing\nWriting a scientific article can be a rewarding endpoint for the process of doing experimental research. But writing is a craft, and writing clearly—especially about complex and technical topics—can require substantial practice and many drafts. Further, writing about research comes with ethical and professional responsibilities that are different than the burdens of other kinds of writing. A scientific author must work to ensure the reproducibility of their findings and report on those findings responsibly, noting limitations and weaknesses as well as strengths.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nFind a writing buddy and exchange feedback on a short piece of writing (the abstract of a paper in progress, a conference abstract, or even a class project proposal would be good examples). Think about how to improve each other’s writing using the advice offered in this chapter.\nIdentify a published research article with openly available data and see if you can reproduce an analysis in their paper by recovering the exact numerical values they report. You can find support for this exercise at the Social Science Reproduction Platform (https://www.socialsciencereproduction.org) or ReproHack (https://www.reprohack.org). Discuss with a friend what challenges you faced in this exercise and how they might be avoided in your own work.\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nZinsser, William (2006). On Writing Well: The Classic Guide to Writing Nonfiction. 7th ed. Harper Collins.\nGernsbacher, Morton Ann (2018). “Writing Empirical Articles: Transparency, Reproducibility, Clarity, and Memorability.” Advances in Methods and Practices in Psychological Science 1 (3): 403–414. https://doi.org/10.1177/2515245918754485.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Writing</span>"
    ]
  },
  {
    "objectID": "015-viz.html",
    "href": "015-viz.html",
    "title": "15  Visualization",
    "section": "",
    "text": "15.1 Basic principles of (confirmatory) visualization\nIn this section, we begin by introducing a few simple guidelines to keep in mind when making informative visualizations in the context of experimental psychology.1 Remember that our needs may be distinct from other fields, such as journalism or public policy. You may have seen beautiful and engaging full-page graphics with small print and a wealth of information. The art of designing and producing these graphics is typically known as infoviz and should be distinguished from what we call statistical visualization (Gelman and Unwin 2013).\nRoughly, infoviz aims to construct rich and immersive worlds to visually explore: a reader can spend hours pouring over the most intricate graphics and continue to find new and intriguing patterns. Statistical visualization, on the other hand, aims to crisply convey the logic of a specific inference at a glance. These visualizations are the production-ready figures that anchor the results section of a paper and accompany the key, preregistered analyses of interest. In this section, we review several basic principles of making statistical visualizations. We then return below to the role of visualization in more exploratory analyses.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#mapping-a-pandemic",
    "href": "015-viz.html#mapping-a-pandemic",
    "title": "15  Visualization",
    "section": "Mapping a pandemic",
    "text": "Mapping a pandemic\nIn 1854, a deadly outbreak of cholera was sweeping through London. The scientific consensus at the time was that diseases like cholera spread through breathing poisonous and foul-smelling vapors, an idea known as the “miasma theory” (Halliday 2001). An obstetrician and anesthesiologist named John Snow, however, had proposed an alternative theory: rather than spreading through foul air, he thought that cholera was spreading through a polluted water supply (Snow 1855). To make a public case for this idea, he started counting cholera deaths. He marked each case on a map of the area and indicated the locations of the water pumps for reference. Furthermore, a line could be drawn representing the region that was closest to each water pump, a technique that is now known as a Voronoi diagram (https://en.wikipedia.org/wiki/Voronoi_diagram). The resulting illustration clearly reveals that cases clustered around an area called Golden Square, which received water from a pump on Broad Street (figure 15.1). Although the precise causal role of these maps in Snow’s own thinking is disputed, and it is likely that he produced them well after the incident (Brody et al. 2000), they nonetheless played a significant role in the history of data visualization (Friendly and Wainer 2021).\n\n\n\n\n\n\nFigure 15.1: Mapping out a cholera epidemic (Snow 1854). The dotted line shows the region for which Broad Street pump is nearest.\n\n\n\nNearly two centuries later, as the COVID-19 pandemic swept through the world, governmental agencies like the CDC produced maps of the outbreak that became much more familiar (figure 15.2).\n\n\n\n\n\n\nFigure 15.2: A map showing the counts of COVID hospitalizations by state since August 2020 as of January 2024 (from the CDC COVID Data Tracker, https://covid.cdc.gov/covid-data-tracker). Usage does not constitute endorsement or recommendation by the US Government, Department of Health and Human Services, or Centers for Disease Control and Prevention.\n\n\n\nThese maps make abstract statistics visible: By assigning higher cumulative case rates to darker colors, we can see at a glance which areas have been most affected. And we’re not limited by the spatial layout of a map. We’re now also used to seeing the horizontal axis correspond to time and the vertical axis correspond to some value at that time. Curves like the following, showing the weekly counts of new cases, allow us to see other patterns, like the rate of change. Even though more and more cases accumulate every day, we can see at a glance the different “waves” of cases and when they peaked (figure 15.3).\n\n\n\n\n\n\nFigure 15.3: Weekly counts of new reported COVID hospital admissions in the US between August 2020 and January 2024 (from the CDC COVID Data Tracker, https://covid.cdc.gov/covid-data-tracker). Usage does not constitute endorsement or recommendation by the US Government, Department of Health and Human Services, or Centers for Disease Control and Prevention.\n\n\n\nWhile these visualizations capture purely descriptive statistics, we often want our visualizations to answer more specific questions. For example, we may ask about the effectiveness of vaccinations: How do case rates differ across vaccinated and unvaccinated populations? In this case, we may talk about “breaking out” a curve by some other variable, like vaccination status (figure 15.4).\n\n\n\n\n\n\nFigure 15.4: Rates of COVID cases by vaccination status (from the CDC COVID Data Tracker, https://covid.cdc.gov/covid-data-tracker). Usage does not constitute endorsement or recommendation by the US Government, Department of Health and Human Services, or Centers for Disease Control and Prevention.\n\n\n\nFrom this visualization, we can see that unvaccinated individuals are about six times more likely to test positive. At the same time, these visualizations were produced using observational data, which makes it challenging to draw causal inferences. For example, people were not randomly assigned to vaccination conditions, and those who have avoided vaccinations may differ in other ways than those who sought out vaccinations. Additionally, you may have noticed that these visualizations typically do not give a sense of the raw data, the sample sizes of each group, or uncertainty about the estimates. In this chapter, we will explore how to use visualizations to communicate the results of carefully controlled psychology experiments, which license stronger causal inferences.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#basic-principles-of-confirmatory-visualization",
    "href": "015-viz.html#basic-principles-of-confirmatory-visualization",
    "title": "15  Visualization",
    "section": "Visualizing uncertainty with error bars",
    "text": "1 For the purposes of understanding the examples in this chapter, it should be sufficient to work through the tutorials on data manipulation and visualization in appendix D and appendix E.\n\n15.1.1 Principle 1: Show the design\nThere are so many different kinds of graphs (bar graphs, line graphs, scatter plots, and pie charts) and so many different possible attributes of those graphs (colors, sizes, line types). How do we begin to decide how to navigate these decisions? The first principle guiding good statistical visualizations is to show the design of your experiment.\nThe first confirmatory plot you should have in mind for your experiment is the design plot. Analogous to the “default” or “saturated” model in chapter 7, the design plot should show the key dependent variable of the experiment, broken down by all of the key manipulations. Critically, design plots should neither omit particular manipulations because they didn’t yield an effect nor include extra covariates because they seemed interesting after looking at the data. Both of these steps are the visual analogue of \\(p\\)-hacking! Instead, the design plot is the “preregistered analysis” of your visualization: it illustrates a first look at the estimated causal effects from your experimental manipulations. In the words of Coppock (2019), “Visualize as You Randomize”!\nIt can sometimes be a challenge to represent the full pattern of manipulations from an experiment in a single plot. Below we give some tricks for maximizing the legible information in your plot. But if you have tried these and your design plot still looks crowded and messy, that could be an indication that your experiment is manipulating too many things at once!\nThere are strong (unwritten) conventions about how your confirmatory analysis is expected to map onto graphical elements, and following these conventions can minimize confusion. Start with the variables you manipulate, and make sure they are clearly visible. Conventionally, the primary manipulation of interest (e.g., condition) goes on the x-axis, and the primary measurement of interest (e.g., responses) goes on the y-axis. Other critical variables of interest (e.g., secondary manipulations and demographics) are then assigned to “visual variables” (e.g., color, shape, or size).\n\n\n\n\n\n\ncode\n\n\n\n\n\nThe visualization library ggplot (see appendix E) makes the mapping of variables in the data to visual data. Part of a ggplot call is an aes (short for aesthetics) mapping:\n\naes(\n  x = ...,\n  y = ...,\n  color = ...,\n  linetype = ...,\n)\n\nThe aesthetics argument serves as a statement of how data are mapped to “marks” on the plot. This transparent mapping makes it very easy to explore different plot types by changing that aes() statement, as we’ll see below.\n\n\n\nAs an example, we will consider the data from Stiller, Goodman, and Frank (2015) that we explored back in chapter 7. Because this experiment was a developmental study, the primary independent variable of interest was the age group of participants (ages two, three, or four). So age gets assigned to the horizontal (x) axis. The dependent variable is accuracy: the proportion of trials that a participant made the correct response (out of four trials). So accuracy goes on the vertical (y) axis. Now, we have two other variables that we might want to show: the condition (experimental vs control) and the type of stimuli (houses, beds, and plates of pasta). When we think about it, though, only condition is central to exposing the design. While we might be interested in whether some types of stimuli are systematically easier or harder than others, condition is more central for understanding the logic of the study.\n\n\n\n\n\n\ncode\n\n\n\n\n\nAs a reminder, here’s our code for loading the Stiller, Goodman, and Frank (2015) data:\n\nrepo &lt;- \"https://raw.githubusercontent.com/langcog/experimentology/main\"\nsgf &lt;- read_csv(file.path(repo, \"data/tidyverse/stiller_scales_data.csv\")) |&gt;\n  mutate(age_group = cut(age, 2:5, include.lowest = TRUE), \n         condition = condition |&gt;\n           fct_recode(\"Experimental\" = \"Label\", \"Control\" = \"No Label\"))\n\nsgf_cond_means &lt;- sgf |&gt;\n  group_by(condition, age_group) |&gt;\n  summarize(rating = mean(correct))\n\n\n\n\n\n\n15.1.2 Principle 2: Facilitate comparison\n\n\n\n\n\n\n\nFigure 15.5: Principles of visual perception can help guide visualization choices. Based on Mackinlay (1986; see also Cleveland and McGill 1984).\n\n\nNow that you’ve mapped elements of your design to the figure’s axes, how do you decide which graphical elements to display? You might think: well, in principle, these assignments are all arbitrary anyway. As long as we clearly label our choices, it shouldn’t matter whether we use lines, points, bars, colors, textures, or shapes. It’s true that there are many ways to show the same data. But being thoughtful about our choices can make it much easier for readers to interpret our findings. The second principle of statistical visualizations is to facilitate comparison along the dimensions relevant to our scientific questions. It is easier for our visual system to accurately compare the location of elements (e.g., noticing that one point is a certain distance away from another) than to compare their areas or colors (e.g., noticing that one point is bigger or brighter than another). Figure 15.5 shows an ordering of visual variables based on how accurate our visual system is in making comparisons.\nFor example, we could start by plotting the accuracy of each age group as colors (figure 15.6).\n\n\n\n\n\n\n\n\nFigure 15.6: A first visualization of the Stiller, Goodman, and Frank (2015) data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nTo make this (bad) visualization, we used a ggplot function called geom_tile().\n\nggplot(sgf_cond_means, aes(x = age_group, y = condition, fill = rating)) +\n  geom_tile() + \n  labs(x = \"Age group\", y = \"Condition\") \n\ngeom_tile() is commonly used to make heat maps (https://en.wikipedia.org/wiki/Heat_map): for each value of some pair of variables (x, y), it shows a color representing the magnitude of a third variable (z). While a heat map is a silly way to visualize the Stiller, Goodman, and Frank (2015) data, consider using geom_tile() when you have a pair of continuous variables, each taking a large range of values. In these cases, bar plots and line plots tend to get extremely cluttered, making it hard to see the relationship between the variables. Heat maps help these relationships to pop out as clear “hot” and “cold” regions. For example, in Barnett, Griffiths, and Hawkins (2022), a heatmap was used to show a specific range of parameters where an effect of interest emerged (see figure 15.7).\n\n\n\n\n\n\nFigure 15.7: A heatmap showing a specific range of continuous parameters where an effect emerged. Barnett, Griffiths, and Hawkins (2022), Figure 3 (licensed under CC BY 4.0).\n\n\n\n\n\n\nOr we could plot the accuracy of each age group as sizes/areas (figure 15.8).\n\n\n\n\n\n\n\n\nFigure 15.8: Iterating on the Stiller data using size.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nTo make this (bad) visualization, we mapped the rating DV to the size element in our aes() call.\n\nggplot(sgf_cond_means, aes(x = age_group, y = condition, size = rating)) +\n  geom_point() + \n  labs(x = \"Age group\", y = \"Condition\") \n\n\n\n\nThese plots allow us to see that one condition is (qualitatively) bigger than others, but it’s hard to tell how much bigger. Additionally, this way of plotting the data places equal emphasis on age and condition, but we may instead have in mind particular contrasts, like the change across ages and how that change differs across conditions. An alternative is to show six bars: three on the left showing the “experimental” phase and three on the right showing the “control” phase. Maybe the age groups then are represented as different colors, as in figure 15.9.\n\n\n\n\n\n\n\n\nFigure 15.9: A bar graph of the Stiller data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nWe make bar plots using the ggplot function geom_col(). By default, it creates “stacked” bar plots, where all values associated with the same x value (here, condition) get stacked up on top of one another. Stacked bar plots can be useful if, for example, you’re plotting proportions that sum up to 1, or want to show how some big count is broken down into subcategories. It’s also common to use geom_area() for this purpose, which connects adjacent regions. But the more common bar plot used in psychology puts the bars next to one another, or “dodges” them. To accomplish this, we use the position = \"dodge\" argument:\n\nggplot(sgf_cond_means, aes(x = condition, y = rating, fill = age_group)) +\n  geom_col(position = \"dodge\") + \n  labs(x = \"Condition\", y = \"Mean accuracy\") \n\n\n\n\nThis plot is slightly better: it’s easier to compare the heights of bars than the “blueness” of squares, and mapping age to color draws our eye to those contrasts. However, we can do even better by noticing that our experiment was designed to test an interaction. That statistic of interest is a difference of differences. To what extent does the developmental change in performance on the experimental condition differ from developmental change in performance on the control condition? Some researchers have gotten proficient at reading off interactions from bar plots, but they also require a complex set of eye movements. We have to look at the pattern across the bars on the left, and then jump over to the bars on the right, and implicitly judge one difference against the other: the actual statistic isn’t explicitly shown anywhere! What could help facilitate this comparison? Consider the line plot in figure 15.10.\n\n\n\n\n\n\n\n\nFigure 15.10: A line graph of the Stiller data promotes comparison.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nUsing a combination of geom_point() and geom_line():\n\nggplot(sgf_cond_means, aes(x = age_group, y = rating, color = condition)) +\n  geom_point() +\n  geom_line(aes(group = condition)) + \n  labs(x = \"Age group\", y = \"Mean accuracy\") \n\n\n\n\nThe interaction contrast we want to interpret is highlighted visually in this plot. It is much easier to compare slopes of lines than mentally compute a difference of differences between bars. Here are a few corollaries of this principle (adapted from a presentation by Karl Broman).\n\nIt is easier to compare values that are adjacent to one another. This is especially important when there are many different conditions included on the same plot. If particular sets of conditions are of theoretical interest, place them close to one another. Otherwise, sort conditions by a meaningful value (rather than alphabetically, which is usually the default for plotting software).\nWhen possible, color-code labels and place them directly next to data rather than in a separate legend. Legends force readers to glance back and forth to remember what different colors or lines mean.\nWhen making histograms or density plots, it is challenging to compare distributions when they are placed side by side. Instead, facilitate comparison of distributions by vertically aligning them, or making them transparent and placed on the same axes.\nIf the scale makes it hard to see important differences, consider transforming the data (e.g., taking the logarithm).\nWhen making bar plots, be very careful about the vertical y-axis. A classic “misleading visualization” mistake is to cut off the bottom of the bars by placing the endpoint of the y-axis at some arbitrary value near the smallest data point. This is misleading because people interpret bar plots in terms of the relative area of the bars (i.e., the amount of ink taken up by the bar), not just their absolute y-values.\nIf a key variable from your design is mapped to color, choose the color scale carefully. For example, if the variable is binary or categorical, choose visually distinct colors to maximize contrast (e.g., black, blue, and orange). If the variable is ordinal or continuous, use a color gradient. If there is a natural midpoint (e.g., if some values are negative and some are positive), consider using a diverging scale (e.g., different colors at each extreme). Remember also that a portion of your audience may be colorblind.2\n\n\n2 Palettes like viridis have been designed to be colorblind-friendly and also perceptually uniform (i.e., the perceived difference between 0.1 and 0.2 is approximately the same as the difference between 0.8 and 0.9).\n\n15.1.3 Principle 3: Show the data\nLooking at older papers, you may be alarmed to notice how little information is contained in the graphs. The worst offenders might show just two bars, representing average values for two conditions. This kind of plot adds very little beyond a sentence in the text reporting the means, but it can also be seriously misleading. It hides real variation in the data, making a noisy effect based on a few data points look the same as a more systematic one based on a larger sample. Additionally, it collapses the distribution of the data, making a multimodal distribution look the same as a unimodal one. The third principle of modern statistical visualization is to show the data and visualize variability in some form.\nThe most minimal form of this principle is to always include error bars.3 Error bars turn a purely descriptive visualization into an inferential one. They represent a minimal form of uncertainty about the possible statistics that might have been observed, not just the one that was actually observed. figure 15.11 shows the data with (bootstrapped) error bars. \n3 And be sure to tell the reader what the error bars represent—a 95% confidence interval? A standard error of the mean?—without this information, error bars are hard to interpret (see the Depth box below).\n\n\n\n\n\n\n\nFigure 15.11: Error bars (95% CIs) added to the Stiller data line graph.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nA common problem arises when we want to add error bars to a dodged bar plot. Naively, we’d expect we could just dodge the error bars in the same way we dodged the bars themselves:\n\ngeom_col(position = \"dodge\") +\ngeom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), position = \"dodge\")\n\nBut this doesn’t work! The rationale is kind of technical, but the width of the error bars is much narrower than the width of the bars, so we need to manually specify how much to dodge the error bars with the position_dodge() function:\n\ngeom_col(position = position_dodge()) +\ngeom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n              position = position_dodge(width = 0.9)) \n\nThis does the trick!\n\n\n\nBut we can do even better. By overlaying the distribution of the actual data points on the same plot, we can give the reader information about not just the statistical inferences but also the underlying data supporting those inferences. In the case of the Stiller, Goodman, and Frank (2015) study, data points for individual trials are binary (correct or incorrect). It’s technically possible to show individual responses as dots at 0s and 1s, but this doesn’t tell us much (we’ll just get a big clump of 0s and a big clump of 1s). The question to ask yourself when “showing the data” is: What are the theoretically meaningful units of variation in the data? This question is closely related to our discussion of mixed-effects models in chapter 7, when we considered which random effects we should include. Here, a reader is likely to wonder how much variance was found across different children in a given age group. To show such variation, we aggregate to calculate an accuracy score for each participant.4\n4 While participant-level variation is a good default, the relevant level of aggregation may differ across designs. For example, collective behavior studies may choose to show the data point for each group. This choice of unit is also important when generating error bars: if you have a small number of participants but many observations per participant, you are faced with a choice. You may either bootstrap over the flat list of all individual observations (yielding very small error bars), or you may first aggregate within participants (yielding larger error bars that account for the fact that repeated observations from the same participant are not independent).There are many ways of showing the resulting distribution of participant-level data. For example, a boxplot shows the median (a horizontal line) in the center of a box extending from the lower quartile (25%) to the upper quartile (75%). Lines then extend out to the biggest and smallest values (excluding outliers, which are shown as dots). Figure 15.12 gives the boxplots for the Stiller data, which don’t look that informative—perhaps because of the coarseness of individual participant averages due to the small number of trials.\n\n\n\n\n\n\n\n\nFigure 15.12: A boxplot of the Stiller data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nIn ggplot, we can make box plots using geom_boxplot():\n\ngeom_boxplot(alpha = 0.8)\n\nA common problem to run into is that geom_boxplot() requires the variable assigned to x to be discrete. If you have discrete levels of a numeric variable (e.g., age groups), make sure you’ve actually converted that variable to a factor. Otherwise, if it’s still coded as numeric, ggplot will collapse all of the levels together!\n\n\n\nIt is also common to show the raw data as jittered values with low transparency. In figure 15.13, we jitter the points because many participants have the same numbers (e.g., 50%), and if they overlap it is hard to see how many points there are.\n\n\n\n\n\n\n\n\nFigure 15.13: Jittered points representing the data distribution of the Stiller data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nAdding the jittered points is simple using geom_jitter(), but we are starting to have a fairly complex plot, so maybe it’s worth taking stock of how we get there.\nTo plot both condition means and participant means, we need to create two different data frames. Here sgf_subj_means is a data frame of means for each participant; sgf_subj_ci is a data frame with means and confidence intervals across participants. For this purpose, we use the tidyboot package (Braginsky and Yurovsky 2018) and the tidyboot_mean() function, which gives us bootstrapped 95% confidence intervals for the means.\n\nsgf_subj_means &lt;- sgf |&gt; \n  group_by(condition, age_group, subid) |&gt;\n  summarize(rating = mean(correct)) \n\nsgf_subj_ci &lt;- sgf_subj_means |&gt;\n  group_by(condition, age_group) |&gt;\n  tidyboot::tidyboot_mean(rating) |&gt;\n  rename(rating = empirical_stat)\n\nggplot(sgf_subj_ci, aes(x = age_group, y = rating, color = condition)) +\n  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  geom_line(aes(group = condition)) +\n  geom_jitter(data = sgf_subj_means, alpha = 0.25, width = 0.1, height = .03) +\n  labs(x = \"Age group\", y = \"Mean accuracy\") \n\nThe most noteworthy aspect of this code is that the geom_jitter() function doesn’t just take a different aesthetic; it also takes a different dataframe altogether! Mixing dataframes can be an important tool for creating complex plots.\n\n\n\nPerhaps the format that takes this principle the furthest is the so-called raincloud plot (Allen et al. 2019) shown in figure 15.14. A raincloud plot combines the raw data (“rain”) with a smoothed density (“cloud”) and a boxplot giving the median and quartiles of the distribution.\n\n\n\n\n\n\n\n\nFigure 15.14: A raincloud plot of the Stiller data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nThis raincloud plot requires two additional plotting packages: ggridges (Wilke 2023) for the densities and ggstance (Henry, Wickham, and Chang 2022) for the horizontal boxplots.\n\nlibrary(ggridges)\nlibrary(ggstance)\nggplot(sgf_subj_means, aes(y = age_group, x = rating, color = condition)) +\n  geom_density_ridges(aes(fill = condition), alpha = 0.2, scale = 0.7,\n                      jittered_points = TRUE, point_alpha = 0.7,\n                      position = position_raincloud(width = 0.05, height = 0.15,\n                                                    ygap = 0.1)) +\n  geom_boxploth(width = 0.1, alpha = 0.2, outlier.shape = NA, show.legend = FALSE) +\n  scale_y_discrete(expand = expansion(mult = c(0.2, 0.4))) +\n  guides(fill = \"none\", color = guide_legend(reverse = TRUE)) +\n  labs(x = \"Mean accuracy\", y = \"Age group\", color = \"Condition\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\ndepth\n\n\n\n\n\nVisualizing uncertainty with error bars\nOne common misconception is that error bars are a measure of variance in the data, like the standard deviation of the response variable. Instead, they typically represent a measure of precision extracted from the statistical model. In older papers, for example, it was common to use the standard error of the mean (SEM; see chapter 6). Remember that this is not the standard deviation of the data distribution but of the sampling distribution of the mean that is being estimated. Given the central limit theorem, which tells us that this sampling distribution is asymptotically normal, it was straightforward to estimate the standard error analytically using the empirical standard deviation of the data divided by the square root of the sample size: sd(x) / sqrt(length(x)). Error bars based on the SEM often looked misleadingly small, as they only represent a 68% interval of the sampling distribution and go to zero quickly as a function of sample size. As a result, it became more common to show the 95% confidence interval instead: [-1.96 \\(\\times\\) SEM, 1.96 \\(\\times\\) SEM].\nWhile these analytic equations remain common, an increasingly popular alternative is to bootstrap confidence intervals (see the Depth box in chapter 6 for more on bootstrapping).  The bootstrap is a powerfully generic technique, especially when you want to show error bars for summary statistics that are more complex than means, where we do not have such convenient asymptotic guarantees and “closed-form” equations. An example would be if you’re working with a skewed response variable or a dataset with clear outliers and you want to estimate medians and quartiles.\nOr, suppose you want to estimate proportions from categorical data, or a more ad hoc statistic like the AUC (area underneath the curve) in a hierarchical design where it is not clear how to aggregate across items or participants in a mixed-effects model. Analytic estimators of confidence intervals can in principle be derived for these statistics, subject to different assumptions, but it is often more transparent and reliable in practice to use the bootstrap. As long as you can write a code snippet to compute a value from a dataset, you can use the bootstrap.\n\n\n\n\n\n\nFigure 15.15: Three different error bars for the Stiller data: bootstrapped 95% confidence intervals (left), standard error of the mean (middle), and analytically computed confidence intervals (right).\n\n\n\nAs we can see, the bootstrapped 95% CI looks similar to the analytic 95% CI derived from the standard error, except the upper and lower limits are slightly asymmetric (reflecting outliers in one direction or another). Of course, the bootstrap is not a silver bullet and can be abused in particularly small samples. This is because the bootstrap is fundamentally limited to the sample we run it on. It can be expected to be reasonably accurate if the sample is reasonably representative of the population. But at the end of the day, as they say, “There’s no such thing as a free lunch.” In other words, we cannot magically pull more information out of a small sample without making additional assumptions about the data generating process.\n\n\n\n\n\n15.1.4 Principle 4: Maximize information, minimize ink\n\n\n\n\n\n\n\nFigure 15.16: This figure uses a lot to ink to show three numbers, for a “ddi” of 0.2 (from the Washington Post, 1978; see Wainer (1984) for other examples).\n\n\nNow that we have the basic graphical elements in place to show our design and data, it might seem like the rest is purely a matter of aesthetic preference, like choosing a pretty color scheme or font. Not so.\nThere are well-founded principles to make the difference between an effective visualization and a confusing or obfuscating one. Simply put, we should try to use the simplest possible presentation of the maximal amount of information: we should maximize the “data-ink ratio.” To calculate the amount of information shown, Tufte (2001) suggests a measure called the “data density index,” the “numbers plotted per square inch.” The worst offenders have a very low density while also using a lot of excess ink (e.g., figure 15.16 and figure 15.17)\n\n\n\n\n\n\n\nFigure 15.17: This figure uses complicated 3D ribbons to compare distributions across four countries (from Roeder 1994). How could the same data have been presented more legibly?\n\n\n5 See the ggthemes package (Arnold 2023) for a good collection of themes.The defaults in modern visualization libraries like ggplot prevent some of the worst offenses but are still often suboptimal. For example: consider whether the visual complexity introduced by the default grey background and grid lines in figure 15.18 is justified, or whether a more minimal theme would be sufficient.5\n\n\n\n\n\n\n\n\nFigure 15.18: Standard “gray”-themed Stiller figure.\n\n\n\n\n\nFigure 15.19 shows a slightly more “styled” version of the same plot with labels directly on the plot and a lighter-weight theme.\n\n\n\n\n\n\n\n\nFigure 15.19: Custom-themed Stiller figure with direct labels.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nTo produce the plot in figure 15.19, we’ve added a few styling elements including:\n\nThe nice and minimal custom theme, with a larger font size.\nA more accessible color palette (scale_colour_ptol()) from the ggthemes package (Arnold 2023).\nDirect labels using geom_dl() from the directlabels package (Hocking 2023).\n\n\n\ngeom_dl(aes(label = condition), method = list(\"last.points\", dl.trans(x = x + 0.5)))\n\n\n\n\nHere are a few final tips for making good confirmatory visualizations:\n\nMake sure the font size of all text in your figures is legible and no smaller than other text in your paper (e.g., 10 pt). This change may require, for example, making the axis breaks sparser, rotating text, or changing the aspect ratio of the figure.\nAnother important tool to keep in your visualization arsenal is the facet plot. When your experimental design becomes more complex, consider breaking variables out into a grid of facets instead of packing more and more colors and line-styles onto the same axis. In other words, while higher information density is typically a good thing, you want to aim for the sweet spot before it becomes too dense and confusing. Remember principle 2. When there is too much going on in every square inch, it is difficult to guide your reader’s eye to the comparisons that actually matter, and spreading it out across facets gives you additional control over the salient patterns.\nSometimes these principles come into conflict, and you may need to prioritize legibility over, for example, showing all of the data. For example, suppose there is an outlier orders of magnitude away from the summary statistics. If the axis limits are zoomed out to show that point, then most of the plot will be blank space! It is reasonable to decide that it is not worth compressing the key statistical question of your visualization into the bottom centimeter just to show one point. It may suffice to truncate the axes and note in the caption that a single point was excluded.\nFix the axis labels! A common mistake is to keep the default shorthand you used to name variables in your plotting software instead of more descriptive labels (e.g., “RT” instead of “Reaction Time”). Use consistent terminology for different manipulations and measures in the main text and figures. If anything might be unclear in the figure, explain it in the caption.\nDifferent audiences may require different levels of detail. Sometimes it is better to collapse over secondary variables (even if they are included in your statistical models) in order to control the density of the figure and draw attention to the key question of interest.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#visualizing-uncertainty-with-error-bars",
    "href": "015-viz.html#visualizing-uncertainty-with-error-bars",
    "title": "15  Visualization",
    "section": "",
    "text": "One common misconception is that error bars are a measure of variance in the data, like the standard deviation of the response variable. Instead, they typically represent a measure of precision extracted from the statistical model. In older papers, for example, it was common to use the standard error of the mean (SEM; see chapter 6). Remember that this is not the standard deviation of the data distribution but of the sampling distribution of the mean that is being estimated. Given the central limit theorem, which tells us that this sampling distribution is asymptotically normal, it was straightforward to estimate the standard error analytically using the empirical standard deviation of the data divided by the square root of the sample size: sd(x) / sqrt(length(x)). Error bars based on the SEM often looked misleadingly small, as they only represent a 68% interval of the sampling distribution and go to zero quickly as a function of sample size. As a result, it became more common to show the 95% confidence interval instead: [-1.96 \\(\\times\\) SEM, 1.96 \\(\\times\\) SEM].\nWhile these analytic equations remain common, an increasingly popular alternative is to bootstrap confidence intervals (see the Depth box in chapter 6 for more on bootstrapping).  The bootstrap is a powerfully generic technique, especially when you want to show error bars for summary statistics that are more complex than means, where we do not have such convenient asymptotic guarantees and “closed-form” equations. An example would be if you’re working with a skewed response variable or a dataset with clear outliers and you want to estimate medians and quartiles.\nOr, suppose you want to estimate proportions from categorical data, or a more ad hoc statistic like the AUC (area underneath the curve) in a hierarchical design where it is not clear how to aggregate across items or participants in a mixed-effects model. Analytic estimators of confidence intervals can in principle be derived for these statistics, subject to different assumptions, but it is often more transparent and reliable in practice to use the bootstrap. As long as you can write a code snippet to compute a value from a dataset, you can use the bootstrap.\n\n\n\n\n\n\nFigure 15.15: Three different error bars for the Stiller data: bootstrapped 95% confidence intervals (left), standard error of the mean (middle), and analytically computed confidence intervals (right).\n\n\n\nAs we can see, the bootstrapped 95% CI looks similar to the analytic 95% CI derived from the standard error, except the upper and lower limits are slightly asymmetric (reflecting outliers in one direction or another). Of course, the bootstrap is not a silver bullet and can be abused in particularly small samples. This is because the bootstrap is fundamentally limited to the sample we run it on. It can be expected to be reasonably accurate if the sample is reasonably representative of the population. But at the end of the day, as they say, “There’s no such thing as a free lunch.” In other words, we cannot magically pull more information out of a small sample without making additional assumptions about the data generating process.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#exploratory-visualization",
    "href": "015-viz.html#exploratory-visualization",
    "title": "15  Visualization",
    "section": "15.2 Exploratory visualization",
    "text": "15.2 Exploratory visualization\nSo far in this chapter we have focused on principles of confirmatory data visualization: how to make production-quality figures that convey the key preregistered analyses without hiding sources of variability or misleading readers about the reliability of the results. Yet, this is only one role that data visualization plays when doing science. An equally important role is called exploratory visualization: the more routine practice of understanding one’s own data by visualizing it. This role is analogous to the sense of exploratory data analyses discussed in chapter 11. We typically do not preregister exploratory visualizations, and when we decide to include them in a paper they are typically in the service of a secondary argument (e.g., checking the robustness of an effect or validating that some assumption is satisfied).\nThis kind of visualization plays a ubiquitous role in a researcher’s day-to-day activities. While confirmatory visualization is primarily audience-driven and concerned with visual communication, exploratory visualization is first and foremost a “cognitive tool” for the researcher. The first time we load in a new dataset, we start up a new feedback loop—we ask ourselves questions and answer them by making visualizations. These visualizations then raise further questions and are often our best tool for debugging our code. In this section, we consider some best practices for exploratory visualization.\n\n15.2.1 Examining distributional information\nThe primary advantage of exploratory visualization—the reason it is uniquely important for data science—is that it gives us access to holistic information about the distribution of the data that cannot be captured in any single summary statistic. The most famous example is known as “Anscombe’s quartet,” a set of four datasets with identical statistics (figure 15.20). They have the same means, the same variances, the same correlation, the same regression line, and the same \\(R^2\\) value. Yet, when they are plotted, they reveal striking structural differences. The first looks like a noisy linear relationship—the kind of idealized relationship we imagine when we imagine a regression line. But the second is a perfect quadratic arc, the third is a perfectly noiseless line with a single outlier, and the fourth is nearly categorical: every observation except one shares exactly the same x-value.\n\n\n\n\n\n\n\n\nFigure 15.20: Anscombe’s quartet (Anscombe 1973).\n\n\n\n\n\nIf our analyses are supposed to help us distinguish between different data-generating processes, corresponding to different psychological theories, it is clear that these four datasets would correspond to dramatically different theories even though they share the same statistics. Of course, there are arbitrarily many datasets with the same statistics, and most of these differences don’t matter (this is why they are called “summary” statistics, after all!). Figure 15.21 and table 15.1 show just how bad things can get when we rely on summary statistics. When we operationalize a theory’s predictions in terms of a single statistic (e.g., a difference between groups or a regression coefficient), we can lose track of everything else that may be going on. Good visualizations force us to zoom out and take in the bigger picture.\n\n\n\n\n\n\n\n\n\nFigure 15.21: Originally inspired by the Datasaurus figure constructed by @albertocairo on Twitter using the DrawMyData tool (http://robertgrantstats.co.uk/drawmydata.html), we can construct an arbitrary number of different graphs with exactly the same statistics (Matejka and Fitzmaurice 2017; Murray and Wilson 2021), such as the Datasaurus Dozen (Matejka and Fitzmaurice 2017).\n\n\n\n\n\n\n\n\n\nTable 15.1: Summary statistics for each dataset in the Datasaurus Dozen (Matejka 2017).\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nsd_x\nsd_y\ncor_xy\n\n\n\n\naway\n54.3\n47.8\n16.8\n26.9\n-0.064\n\n\nbullseye\n54.3\n47.8\n16.8\n26.9\n-0.069\n\n\ncircle\n54.3\n47.8\n16.8\n26.9\n-0.068\n\n\ndino\n54.3\n47.8\n16.8\n26.9\n-0.064\n\n\ndots\n54.3\n47.8\n16.8\n26.9\n-0.060\n\n\nh_lines\n54.3\n47.8\n16.8\n26.9\n-0.062\n\n\nhigh_lines\n54.3\n47.8\n16.8\n26.9\n-0.069\n\n\nslant_down\n54.3\n47.8\n16.8\n26.9\n-0.069\n\n\nslant_up\n54.3\n47.8\n16.8\n26.9\n-0.069\n\n\nstar\n54.3\n47.8\n16.8\n26.9\n-0.063\n\n\nv_lines\n54.3\n47.8\n16.8\n26.9\n-0.069\n\n\nwide_lines\n54.3\n47.8\n16.8\n26.9\n-0.067\n\n\nx_shape\n54.3\n47.8\n16.8\n26.9\n-0.066\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naccident report\n\n\n\n\n\n[Distributional] gorillas in our midst.\nMany data scientists don’t bother checking what their data looks like before proceeding to test specific hypotheses. Yanai and Lercher (2020) cleverly designed an artificial dataset for their students to test for such oversight. Each row of the dataset contained an individual’s body mass index (BMI) and the number of steps they walked on a given day. While the spreadsheet looked innocuous, the data was constructed such that simply plotting the raw data revealed a picture of a gorilla. One group of 19 students was given an explicit set of hypotheses to test (e.g., about the relationship between BMI and steps). Fourteen of these students failed to notice a gorilla, suggesting that they evaluated these hypotheses without ever visualizing their data. Another group of 14 students were simply asked what, if anything, they could conclude (without being given explicit hypotheses). More of these students apparently made the visualization, but five of them still failed to notice the gorilla (figure 15.22)!\n\n\n\n\n\n\nFigure 15.22: A dataset constructed by Yanai and Lercher (2020) that revealed a picture of a gorilla when the raw data were plotted.\n\n\n\nWhile it may not be surprising that a group of students would take the shortest path to completing their assignment, similar concerns have been raised in much more serious cases concerning how experienced researchers could fail to notice obviously fraudulent data. For example, when the Datacolada bloggers (2021) made a simple histogram of the car mileage data reported in Shu et al. (2012; released publicly by Kristal et al. 2020), they were immediately able to observe that it followed a perfectly uniform distribution, truncated at exactly 50,000 miles (figure 15.23). Given a little thought, this pattern should be extremely puzzling. Over a given period of time, we would typically expect something more bell-shaped: a small number of people will drive very little (e.g., 1,000 miles), a small number of people will drive a lot (e.g., 50,000 miles), and most people will fall between these tails. So it is highly surprising to find exactly the same number of drivers in every mileage bin. While further specialized analyses revealed additional evidence of fraud (e.g., based on patterns of rounding and pairs of duplicated data points), this humble histogram was already enough to set off alarm bells. A recurring regret raised by the coauthors of this paper is that they never thought to make this visualization before reporting their statistical tests.\n\n\n\n\n\n\nFigure 15.23: A suspiciously uniform distribution abruptly cutting off at 50K miles. Ring the alarm!\n\n\n\nOur data are always messier than we expect. There might be a bug in our coding scheme, a column might be mislabeled, or it might contain a range of values that we didn’t expect. Maybe our design wasn’t perfectly balanced, or something went wrong with a particular participant’s keyboard presses. Most of the time, it’s not tractable to manually scroll through our raw data looking for such problems. Visualization is our first line of defense for the all-important process of running “data diagnostics.” If there is a weird artifact in our data, it will pop out if we just make the right visualizations.\n\n\n\n\n\n15.2.2 Data diagnostics\nSo, which visualizations should we start with? The best practice is to always start by making histograms of the raw data. As an example, let’s consider the rich and interesting dataset shared by Blake, McAuliffe, and colleagues (2015) in their article “Ontogeny of Fairness in Seven Societies.” This article studies the emergence of children’s reasoning about fairness—both when it benefits them and when it harms them—across cultures.\n\n\n\n\n\n\ncode\n\n\n\n\n\nIf you want to follow along with this example at home, you can load the data from our repository!\n\nrepo &lt;- \"https://raw.githubusercontent.com/langcog/experimentology/main\"\nfairness_raw &lt;- read_csv(file.path(repo, \"data/viz/ontogeny_of_fairness.csv\"))\n\nfairness &lt;- fairness_raw |&gt;\n  mutate(trial_num = trial |&gt; str_remove(\"t\") |&gt; as.numeric(),\n         trial_type = eq.uneq |&gt; fct_recode(\"Equal\" = \"E\", \"Unequal\" = \"U\"),\n         condition = condition |&gt; fct_recode(\"Advantageous\" = \"AI\",\n                                             \"Disadvantageous\" = \"DI\"),\n         age = floor(actor.age.years),\n         reject = decision == \"reject\") |&gt;\n  select(subj_id = actor.id, age, country, condition, trial_num, trial_type, reject) |&gt;\n  arrange(country, condition, subj_id, trial_num)\n\n\n\n\nIn this study, pairs of children played the “inequity game”: they sat across from one another and were given a particular allocation of snacks. On some trials, each participant was allocated the same amount (“equal” trials) and on some trials they were allocated different amounts (“unequal” trials). One participant was chosen to be the “actor” and got to choose whether to accept or reject the allocation: in the case of rejection, neither participant got anything. The critical manipulation was between two forms of inequity. Some pairs were assigned to the “disadvantageous” condition, where the actor was allocated less than their partner on unequal trials (e.g., one vs four). Others were assigned to the “advantageous” condition, where they were allocated more (e.g., four vs one).\nThe confirmatory design plot for this study would focus on contrasting developmental trajectories for advantageous vs disadvantageous inequality. However, this is a complex, multivariate dataset, including 866 pairs from different age groups and different testing sites across the world which used subtly different protocols. How might we go about the process of exploratory visualization for this dataset?\n\n\n15.2.3 Plot data collection details\nLet’s start by getting a handle on some of the basic sample characteristics. For example, how many participants were in each age bin (figure 15.24)?\n\n\n\n\n\n\n\n\nFigure 15.24: Participants by age in the Blake data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nExploratory histograms are often a combination of an aggregation step and a plotting step. In the aggregation step, we make use of the convenience count() function, which gives the number (n) of rows in a particular grouping. Here we count() twice in order to get first one row per participant and then count the number of participants within each age group.\n\nfairness_by_age &lt;- fairness |&gt;\n  count(age, subj_id) |&gt;\n  count(age)\n\nAnd then we plot using ggplot():\n\nggplot(fairness_by_age, aes(x = age, y = n)) +\n  geom_col() +\n  xlim(0, 18) +\n  labs(x = \"Age (years)\", y = \"Count\")\n\nAn alternative (perhaps more elegant) workflow here would be to use a histogram:\n\nfairness_by_age &lt;- fairness |&gt;\n  count(age, subj_id) \n  \nggplot(fairness_by_age, aes(x = age)) +\n  geom_histogram(binwidth = 1) +\n  labs(x = \"Age (years)\", y = \"Count\")\n\nHistograms are intended by ggplot to be for continuous data, however, and so they don’t give the discrete bars that our earlier workflow did.\n\n\n\nHow many participants were included from each country (figure 15.25)?\n\n\n\n\n\n\n\n\nFigure 15.25: Participants by country in the Blake data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nHere we are going to make things even terser and use a pipe chain that includes the ggplot() call, just so we are writing only a single call to produce our plot. It’s up to you whether you think this enhances the readability of your code or decreases it. We find that it’s sometimes useful when you don’t plan on keeping the intermediate data frame for any other use than plotting.\n\nfairness |&gt;\n  count(country, subj_id) |&gt;\n  count(country) |&gt;\n  mutate(country = fct_reorder(country, -n)) |&gt;\n  ggplot(aes(x = country, y = n)) +\n    geom_col() +\n    labs(x = \"Country\", y = \"Count\")\n\nIf you use this technique, be careful to use pipe (|&gt; or %&gt;%) between function calls but use (+) between ggplot layers!\nThe only other trick to point out here is that we use the fct_reorder() call to order the levels of the country factor in descending order. This function is found in the very useful forcats package (Wickham 2023) of the tidyverse, which contains all sorts of functions for working with factors.\n\n\n\nAre ages roughly similar across each country (figure 15.26)?\n\n\n\n\n\n\n\n\nFigure 15.26: Age distribution across countries in the Blake data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nThis next plot simply combines the grouping factors of each of the last two plots, and uses facet_wrap() to show a separate histogram by country:\n\nfairness |&gt;\n  count(country, age, subj_id) |&gt;\n  count(country, age) |&gt;\n  mutate(country = fct_reorder(country, -n)) |&gt;\n  ggplot(aes(x = age, y = n)) +\n    facet_wrap(vars(country), ncol = 4) +\n    geom_col() +\n    xlim(0, 18) +\n    labs(x = \"Age (years)\", y = \"Count\")\n\n\n\n\nThese exploratory visualizations help us read off some descriptive properties of the sample. For example, we can see that age ranges differ somewhat across sites: the maximum age is 11 in India but 15 in Mexico. We can also see that age groups are fairly imbalanced: in Canada, there are 18 eleven-year-olds but only 5 six-year-olds.\nNone of these properties are problematic, but seeing them gives us a degree of awareness that could shape our downstream analytic decisions. For example, if we did not appropriately model random effects, our estimates would be dominated by the countries with larger sample sizes. And if we were planning to compare specific groups of six-year-olds (for some reason), this analysis would be underpowered.\n\n\n15.2.4 Explorating distributions\nNow that we have a handle on the sample, let’s get a sense of the dependent variable: the participant’s decision to accept or reject the allocation. Before we start taking means, let’s look at how the “rejection rate” variable is distributed. We’ll aggregate at the participant level, and check the frequency of different rejection rates, overall (figure 15.27).\n\n\n\n\n\n\n\n\nFigure 15.27: Rejection rates in the Blake data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nRejection rate is a continuous variable, so we switch to using a histogram in this case, choosing 0.05 as a reasonable bin width to see the distribution.\n\nfairness_by_subj &lt;- fairness |&gt;\n  filter(!is.na(trial_type)) |&gt;\n  group_by(subj_id) |&gt;\n  summarize(mean_reject = mean(reject, na.rm = TRUE))\n\nggplot(fairness_by_subj, aes(x = mean_reject)) +\n  geom_histogram(binwidth = .05) +\n  labs(x = \"Proportion of offers rejected\", y = \"Count\")\n\n\n\n\nWe notice that many participants (27%) never reject in the entire experiment. This kind of “zero-inflated” distribution is not uncommon in psychology, and may warrant special consideration when designing the statistical model. We also notice that there is clumping around certain values. This clumping leads us to check how many trials each participant is completing (figure 15.28).\n\n\n\n\n\n\n\n\nFigure 15.28: Trials per participant in the Blake data.\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nThis histogram is very similar to the ones above; however, we now use count() twice, first getting the trial counts for each participant and then counting how many times each count occurs overall!\n\nfairness |&gt;\n  filter(!is.na(trial_type)) |&gt;\n  count(subj_id) |&gt;\n  count(n) |&gt;\n  ggplot(aes(x = n, y = nn)) +\n    geom_col() +\n    labs(x = \"Number of trials\", y = \"Count of participants\")\n\n\n\n\nThere’s some variation here: most participants completed 17 trials, but some participants completed 8 trials, and a small number of participants have 14 or 15. Given the logistical complexity of large multi-site studies, it is common to have some changes in experimental protocol across data collection. Indeed, looking at the supplement for the study, we see that while India and Peru had 12 trials, additional trials were added at the other sites. In a design where the number of trials was carefully controlled, seeing unexpected numbers here (like the 14 or 15 trial bins) are clues that something else may be going on in the data. In this case, it was a small number of trials with missing data. More generally, seeing this kind of signal in a visualization of our own data typically leads us to look up the participant IDs in these bins and manually inspect their data to see what might be going on.\n\n\n\n\n\n\n\n15.2.5 Hypothesis-driven exploration\nFinally, we can make a few versions of the design plot that are broken out by different variables. Let’s start by just looking at the data from the largest site (figure 15.29).\n\n\n\n\n\n\n\n\nFigure 15.29: Rejection rates in the US data from Blake, plotted by age.\n\n\n\n\n\nFigure 15.29 is not a figure we’d put in a paper, but it helps us get a sense of the pattern in the data. There appears to be an age trend that’s specific to the Unequal trials, with rejection rates rising over time (compared to roughly even or decreasing rates in the Equal trials). Meanwhile, rejection rates for the disadvantageous group also seem slightly higher than those in the advantageous group.\n\n\n\n\n\n\ncode\n\n\n\n\n\nHere, we are using geom_smooth() to overlay regression trends over the raw data. geom_smooth() takes a number of different options corresponding to different smoothing techniques. Nonparametric smoothing can be a good choice for exploratory visualizations if you have a lot of data and want to make minimal assumptions about the form of the trend.\nHere, however, we show the linear regression trend, geom_smooth(method = \"lm\"), which better corresponds to the predictions of the study and the statistical model being used (see chapter 7). Other regression forms can be specified with the formula argument. For example, we could show quadratic smoothing with geom_smooth(method = \"lm\", formula = y ~ poly(x, 2)). The form of smoothing you use may differ across exploratory and confirmatory visualizations. In a confirmatory visualization—if you are going to include a smoothing curve—it is typically best to use the one specified by your statistical model, as the slopes will correspond to the inferences being testing.\nWe begin by making a summary dataset:\n\nfairness_by_age &lt;- fairness |&gt;\n  filter(!is.na(reject)) |&gt;\n  group_by(country, trial_type, condition, age, subj_id) |&gt;\n  summarize(mean_reject_subj = mean(reject, na.rm = TRUE)) |&gt;\n  group_by(country, trial_type, condition, age) |&gt;\n  summarize(mean_reject_age = mean(mean_reject_subj, na.rm = TRUE),\n            n_subj = n()) |&gt;\n  ungroup()\n\nThen we can create the visualization:\n\nfairness_by_age |&gt; filter(country == \"US\") |&gt;\n  ggplot(aes(x = age, y = mean_reject_age, color = condition)) +\n    facet_grid(vars(trial_type), vars(condition)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_point(aes(size = n_subj), alpha = .5) +\n    ylim(c(0, 1)) +\n    labs(x = \"Age (years)\", y = \"Proportion of offers rejected\",\n         color = \"Condition\", size = \"N subjects\") +\n    theme(legend.position = \"bottom\", legend.box = \"vertical\")\n\nWe often find it convenient to filter the summary dataset in the plotting call, so that we can reuse it again.\n\n\n\nNow let’s rebin the data into two-year age groups so that individual point estimates are a bit more reliable, and add the other countries back in.6\n6 Binning data is a trick that we often use for reducing complexity in a plot when data are noisy. It should be used with care, however, since different binning decisions can sometimes lead to different conclusions. Here we tried several binning intervals and decided that two-year age bins showed the underlying trends pretty well.\n\n\n\n\n\n\n\nFigure 15.30: Rejection rates by age for all data in the Blake dataset.\n\n\n\n\n\nFigure 15.30 is now looking much closer to a quick-and-dirty version of a “design plot” we might include in a paper. The DV (rejection rate) is on the y-axis, and the primary variable of interest (age) is on the x-axis. Other elements of the design (country and trial type) are mapped to color and facets, respectively.\n\n\n\n\n\n\ncode\n\n\n\n\n\nDespite the difference between the plot above and this one, the code to produce them is actually very similar. The only difference is the creation of the binned variable and a slight shift of aesthetic and faceting variables.\n\nfairness_by_age_binned &lt;- fairness |&gt;\n  filter(!is.na(reject)) |&gt;\n  mutate(age_binned = floor(age / 2) * 2) |&gt;\n  group_by(country, trial_type, condition, age_binned, subj_id) |&gt;\n  summarize(mean_reject_subj = mean(reject, na.rm = TRUE)) |&gt;\n  group_by(country, trial_type, condition, age_binned) |&gt;\n  summarize(mean_reject_age = mean(mean_reject_subj, na.rm = TRUE),\n            n = n())  |&gt;\n  ungroup()\n\nggplot(fairness_by_age_binned,\n       aes(x = age_binned, y = mean_reject_age, color = condition)) +\n  facet_grid(vars(trial_type), vars(country)) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(weight=n)) +\n  geom_point(alpha = .5, aes(size = n)) +\n  scale_x_continuous(breaks = seq(4, 12, 4), limits = c(3,13)) +\n  scale_y_continuous(limits = c(0, 1), breaks = c(0, .5, 1)) +\n  labs(x = \"Age (years)\", y = \"Proportion of offers rejected\",\n       color = \"Condition\", size = \"N subjects\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n15.2.6 Visualization as debugging\nThe point of exploratory visualization is to converge toward a better understanding of what’s going on in your data. As you iterate through different exploratory visualizations, stay vigilant! Think about what you expect to see before making the plot, then ask yourself whether you got what you expected. You can think of this workflow as a form of “visual debugging.” You might notice a data point with an impossible value, such as a proportion greater than one or a reaction time less than zero. Or you might notice weird clusters or striations, which might indicate heterogeneity in data entry (perhaps different coders used slightly different rubrics or rounded in different ways). You might notice that an attribute is missing for some values and trace it back to a bug reading in the data or merging data frames (maybe there was a missing comma in our csv file). If you see anything that looks weird, track it down until you understand why it’s happening. Bugs that are subtle and invisible in other parts of the analysis pipeline will often pop out as red flags in visualizations.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#distributional-gorillas-in-our-midst.",
    "href": "015-viz.html#distributional-gorillas-in-our-midst.",
    "title": "15  Visualization",
    "section": "[Distributional] gorillas in our midst.",
    "text": "[Distributional] gorillas in our midst.\nMany data scientists don’t bother checking what their data looks like before proceeding to test specific hypotheses. Yanai and Lercher (2020) cleverly designed an artificial dataset for their students to test for such oversight. Each row of the dataset contained an individual’s body mass index (BMI) and the number of steps they walked on a given day. While the spreadsheet looked innocuous, the data was constructed such that simply plotting the raw data revealed a picture of a gorilla. One group of 19 students was given an explicit set of hypotheses to test (e.g., about the relationship between BMI and steps). Fourteen of these students failed to notice a gorilla, suggesting that they evaluated these hypotheses without ever visualizing their data. Another group of 14 students were simply asked what, if anything, they could conclude (without being given explicit hypotheses). More of these students apparently made the visualization, but five of them still failed to notice the gorilla (figure 15.22)!\n\n\n\n\n\n\nFigure 15.22: A dataset constructed by Yanai and Lercher (2020) that revealed a picture of a gorilla when the raw data were plotted.\n\n\n\nWhile it may not be surprising that a group of students would take the shortest path to completing their assignment, similar concerns have been raised in much more serious cases concerning how experienced researchers could fail to notice obviously fraudulent data. For example, when the Datacolada bloggers (2021) made a simple histogram of the car mileage data reported in Shu et al. (2012; released publicly by Kristal et al. 2020), they were immediately able to observe that it followed a perfectly uniform distribution, truncated at exactly 50,000 miles (figure 15.23). Given a little thought, this pattern should be extremely puzzling. Over a given period of time, we would typically expect something more bell-shaped: a small number of people will drive very little (e.g., 1,000 miles), a small number of people will drive a lot (e.g., 50,000 miles), and most people will fall between these tails. So it is highly surprising to find exactly the same number of drivers in every mileage bin. While further specialized analyses revealed additional evidence of fraud (e.g., based on patterns of rounding and pairs of duplicated data points), this humble histogram was already enough to set off alarm bells. A recurring regret raised by the coauthors of this paper is that they never thought to make this visualization before reporting their statistical tests.\n\n\n\n\n\n\nFigure 15.23: A suspiciously uniform distribution abruptly cutting off at 50K miles. Ring the alarm!\n\n\n\nOur data are always messier than we expect. There might be a bug in our coding scheme, a column might be mislabeled, or it might contain a range of values that we didn’t expect. Maybe our design wasn’t perfectly balanced, or something went wrong with a particular participant’s keyboard presses. Most of the time, it’s not tractable to manually scroll through our raw data looking for such problems. Visualization is our first line of defense for the all-important process of running “data diagnostics.” If there is a weird artifact in our data, it will pop out if we just make the right visualizations.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "015-viz.html#chapter-summary-visualization",
    "href": "015-viz.html#chapter-summary-visualization",
    "title": "15  Visualization",
    "section": "15.3 Chapter summary: Visualization",
    "text": "15.3 Chapter summary: Visualization\nThis chapter has given a short review of the principles of data visualization, especially focusing on the needs of experimental psychology, which are often quite different than those of other fields. We particularly focused on the need to make visualization part of the experimenter’s analytic workflow. Picking up the idea of a “default model” from chapter 7, we discussed a default “design plot” that reflects the key choices made in the experimental design. Within this framework, we then discussed different visualizations of distribution and variability that better align our graphics with the principles of measurement and attention to raw data that we have been advocating throughout.\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nChoose a recent piece of research that you’ve heard about and try to sketch the “design plot” with pencil and paper. What does and doesn’t work? How does your sketch differ from the visualizations in the paper?\nThe “design plot” idea that we’ve discussed here can run into problems when an experimental design is too complex to show on a single plot. Imagine you had data from a trial of attention deficit hyperactivity disorder (ADHD) treatment that manipulated both whether a medication was given and whether patients received therapy in a crossed design. The researchers measured two different outcomes: parent report symptom severity and teacher report symptom severity in four different time-points (baseline, three months, six months, and nine months). How could you show the data from such an experiment in a transparent way?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\nThere are many good introductions to data visualization. Here are two social-science focused books whose advice we agree with and that also contain a lot of practical information and helpful R code for the same packages we use here.\n\nHealy, Kieran (2018). Data Visualization: A Practical Introduction. Princeton University Press. Available free online at https://socviz.co.\nWilke, Claus O. (2019). Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. Available free online at https://clauswilke.com/dataviz.\n\nFor a more classical treatment, see:\n\nTukey, John W. (1977). Exploratory Data Analysis. Pearson.\nTufte, Edward R. (2001). The Visual Display of Quantitative Information. 2nd ed. Graphics Press.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "016-meta.html",
    "href": "016-meta.html",
    "title": "16  Meta-analysis",
    "section": "",
    "text": "16.1 The basics of evidence synthesis\nAs we explore the details of conducting a meta-analysis, we’ll turn to another running example: a meta-analysis of studies investigating the “contact hypothesis” on intergroup relations.\nAccording to the contact hypothesis, prejudice toward members of minority groups can be reduced through intergroup contact interventions, in which members of majority and minority groups work together to pursue a common goal (Allport, Clark, and Pettigrew 1954). To aggregate the evidence on the contact hypothesis, Paluck, Green, and Green (2019) meta-analyzed studies that tested the effects of randomized intergroup contact interventions on long-term prejudice-related outcomes.\nUsing a systematic literature search, Paluck, Green, and Green (2019) searched for all papers that tested these effects and then extracted effect size estimates from each paper.3 Because not every paper reports standardized effect sizes—or even means and standard deviations for every group—this process can often involve scraping information from plots, tables, and statistical tests to try to reconstruct effect sizes.4\nFollowing best practices for meta-analysis (where there are almost never privacy concerns to worry about), Paluck, Green, and Green (2019) shared their data openly. The first few lines are shown in table 16.1. We’ll use these data as our running example throughout.\nTable 16.1: The first few lines of extracted effect sizes (d) and their variances (var_d) in the Paluck, Green, and Green (2019) meta-analysis.\n\n\n\n\n\n\n\nname\npub_date\ntarget\nn_total\nd\nvar_d\n\n\n\n\nBoisjoly 06 B\n2006\nrace\n1243\n0.030\n0.006\n\n\nSorensen 10\n2010\nrace\n597\n0.302\n0.007\n\n\nScacco 18\n2018\nreligion\n474\n0.000\n0.010\n\n\nFinseraas 2017\n2017\nforeigners\n577\n0.000\n0.011\n\n\nSheare 74\n1974\ndisability\n400\n1.059\n0.011\n\n\nBarnhardt 09\n2009\nreligion\n312\n0.395\n0.015\nAs we’ve seen throughout this book, visualizing data before and after analysis helps benchmark and check our intuitions about the formal statistical results. In a meta-analysis, a common way to plot effect sizes is the forest plot, which depicts individual studies’ estimates and confidence intervals. In the forest plot in figure 16.1,5 the larger squares correspond to more precise studies; notice how much narrower their confidence intervals are than the confidence intervals of less precise studies.\nFigure 16.1: A forest plot for Paluck, Green, and Green (2019) meta-analysis. Studies are ordered from smallest to largest standard error.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#towel-reuse-by-hotel-guests",
    "href": "016-meta.html#towel-reuse-by-hotel-guests",
    "title": "16  Meta-analysis",
    "section": "Towel reuse by hotel guests",
    "text": "Towel reuse by hotel guests\nImagine you are staying in a hotel and you have just taken a shower. Do you throw the towels on the floor or hang them back up again? In a widely cited study on the power of social norms, Goldstein, Cialdini, and Griskevicius (2008) manipulated whether a sign encouraging guests to reuse towels focused on environmental impacts (e.g., “help reduce water use”) or social norms (e.g., “most guests reuse their towels”). Across two studies, they found that guests were significantly more likely to reuse their towels after receiving the social norm message (Study 1: odds ratio [OR] = 1.46, 95% CI [1.00, 2.16], \\(p = 0.05\\); Study 2: OR = 1.35, 95% CI [1.04, 1.77], \\(p = 0.03\\)).\nHowever, five subsequent studies by other researchers did not find significant evidence that social norm messaging increased towel reuse. (ORs ranged from 0.22 to 1.34, and no hypothesis-consistent \\(p\\)-value was less than 0.05). This caused many researchers to wonder if there is any effect at all. To examine this question, Scheibehenne, Jamil, and Wagenmakers (2016) statistically combined evidence across the studies via meta-analysis. This meta-analysis indicated that using social norm messages did significantly increase hotel towel reuse, on average (OR = 1.26, 95% CI [1.07, 1.46], \\(p &lt; 0.005\\)). This case study demonstrates an important strength of meta-analysis: by pooling evidence from multiple studies, meta-analysis can generate more powerful insights than any one study alone. We will also see how meta-analysis can be used to assess variability in effects across studies.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#the-basics-of-evidence-synthesis",
    "href": "016-meta.html#the-basics-of-evidence-synthesis",
    "title": "16  Meta-analysis",
    "section": "Single-paper meta-analysis and pooled analysis",
    "text": "3 This book will not cover the process of conducting a systematic literature search and extracting effect sizes, but these topics are critical to understand if you plan to conduct a meta-analysis or other evidence synthesis. Our experience is that extracting effect sizes from papers with inconsistent reporting standards can be especially tricky, so it can be helpful to talk to someone with experience in meta-analysis to get advice about this.4 For example, if the outcome variable is continuous, we could estimate Cohen’s \\(d\\) from group means and standard deviations reported in the paper, even without having access to raw data.\n\n\n5 You can ignore for now the final line, “RE Model”; we will return to this later.\n\n\n\n\n\n\ncode\n\n\n\n\n\nIn this chapter, we use the wonderful metafor package (Viechtbauer 2010). With this package, you must first fit your meta-analytic model. But once you’ve fit your model mod, you can simply call forest(mod) to create a plot like the one above.\n\n\n\n\n\n16.1.1 How not to synthesize evidence\nMany people’s first instinct in evidence synthesis is to count how many studies supported versus did not support the hypothesis under investigation. This technique usually amounts to counting the number of studies with “significant” \\(p\\)-values, since—for better or for worse—“significance” is largely what drives the take-home conclusions researchers report (McShane and Gal 2017; Nelson, Rosenthal, and Rosnow 1986). In meta-analysis, we call this practice of counting the number of significant \\(p\\)-values vote-counting (Borenstein et al. 2021). For example, in the Paluck, Green, and Green (2019) meta-analysis, almost all studies had a positive effect size, but only 12 of 27 were significant. So, based on this vote-count, we would have the impression that most studies do not support the contact hypothesis.\nMany qualitative literature reviews use this vote-counting approach, although often not explicitly. Despite its intuitive appeal, vote-counting can be very misleading because it characterizes evidence solely in terms of dichotomized \\(p\\)-values, while entirely ignoring effect sizes. In chapter 3, we saw how fetishizing statistical significance can mislead us when we consider individual studies. These problems also apply when considering multiple studies.\nFor example, small studies may consistently produce nonsignificant effects due to their limited power. But when many such studies are combined in a meta-analysis, the meta-analysis may provide strong evidence of a positive average effect. Inversely, many studies might have statistically significant effects, but if their effect sizes are small, then a meta-analysis might indicate that the average effect size is too small to be practically meaningful. In these cases, vote-counting based on statistical significance can lead us badly astray (Borenstein et al. 2021). To avoid these pitfalls, meta-analysis combines the effect size estimates from each study (not just their \\(p\\)-values), weighting them in a principled way.\n\n\n16.1.2 Fixed-effects meta-analysis\nIf vote-counting is a bad idea, how should we combine results across studies? Another intuitive approach might be to average effect sizes from each study. For example, in Paluck et al.’s meta-analysis, the mean of the studies’ effect size estimates is 0.44. This averaging approach is a step in the right direction, but it has an important limitation: averaging effect size estimates gives equal weight to each study. A small study (e.g., Clunies-Ross and O’Meara 1989 with \\(N = 30\\)) contributes as much to the mean effect size as a large study (e.g., Boisjoly et al. 2006 with \\(N = 1,243\\)). Larger studies provide more precise estimates of effect sizes than small studies, so weighting all studies equally is not ideal. Instead, larger studies should carry more weight in the analysis.\nTo address this issue, fixed-effects meta-analysis uses a weighted average approach. Larger, more precise studies are given more weight in the calculation of the overall effect size. Specifically, each study is weighted by the inverse of its variance (i.e., the inverse of its squared standard error). This makes sense because larger, more precise studies have smaller variances, and thus get more weight in the analysis.\nIn general terms, the fixed-effect pooled estimate is: \\[\\widehat{\\mu} = \\frac{ \\sum_{i=1}^k w_i \\widehat{\\theta}_i}{\\sum_{i=1}^k w_i}\\] where \\(k\\) is the number of studies, \\(\\widehat{\\theta}_i\\) is the point estimate of the \\(i^{th}\\) study, and \\(w_i = 1/\\widehat{\\sigma}^2_i\\) is study \\(i\\)’s weight in the analysis (i.e., the inverse of its variance).6\n\n\n\n\n6 If you are curious, the standard error of the fixed-effect \\(\\widehat{\\mu}\\) is \\(\\frac{1}{\\sum_{i=1}^k w_i}\\). This standard error can be used to construct a confidence interval or \\(p\\)-value, as described in chapter 6.Using the fixed-effects formula, we can estimate that the overall effect size in Paluck et al.’s meta-analysis is a standardized mean difference of \\(\\widehat{\\mu}\\) = 0.28; 95% confidence interval [0.23, 0.34]; \\(p &lt; 0.001\\). Because Cohen’s \\(d\\) is our effect size index, this estimate would suggest that intergroup contact decreased prejudice by 0.28 standard deviations.\n\n\n\n\n\n\ncode\n\n\n\n\n\nFitting meta-analytic models in metafor is quite simple. For example, for the fixed-effects model above, we simply ran the rma() function and specified that we wanted a fixed-effects analysis.\n\nfe_model &lt;- rma(yi = d, vi = var_d, data = paluck, method = \"FE\")\n\nThen summary(fe_model) gives us the relevant information about the fitted model.\n\n\n\n\n\n16.1.3 Limitations of fixed-effects meta-analysis\nOne of the limitations of fixed-effect meta-analysis is that it assumes that the true effect size is, well, fixed! In other words, fixed-effect meta-analysis assumes that there is a single effect size that all studies are estimating. This is a stringent assumption. It’s easy to imagine that it could be violated. Imagine, for example, that intergroup contact decreased prejudice when the group succeeded at its joint goal but increased prejudice when the group failed. If we meta-analyzed two studies under these conditions—one in which intergroup contact substantially increased prejudice and one in which intergroup contact substantially decreased prejudice—it might appear that the true effect of intergroup contact was close to zero, when in fact both of the meta-analyzed studies had large effects.\nIn Paluck et al.’s meta-analysis, studies differed in several ways that could lead to different true effects. For example, some studies recruited adult participants while others recruited children. If intergroup contact is more or less effective for adults versus children, then it is misleading to talk about a single (i.e., “fixed”) intergroup contact effect. Instead, we would say that the effects of intergroup contact vary across studies, an idea called heterogeneity.\nDoes the concept of heterogeneity remind you of anything from when we analyzed repeated-measures data in chapter 7 on models? Recall that, with repeated-measures data, we had to deal with the possibility of heterogeneity across participants—and of the ways we did so was by introducing participant-level random intercepts to our regression model. It turns out that we can do a similar thing in meta-analysis to deal with heterogeneity across studies.\n\n\n16.1.4 Random-effects meta-analysis\nWhile fixed-effect meta-analysis essentially assumes that all studies in the meta-analysis have the same population effect size, \\(\\mu\\), random-effects meta-analysis instead assumes that study effects come from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\tau\\).7 The larger the standard deviation, \\(\\tau\\), the more heterogeneous the effects are across studies. A random-effects model then estimates both \\(\\mu\\) and \\(\\tau\\), for example by maximum likelihood (DerSimonian and Laird 1986; Brockwell and Gordon 2001). \n7 Technically, other specifications of random-effects meta-analysis are possible. For example, robust variance estimation does not require making assumptions about the distribution of effects across studies (Hedges, Tipton, and Johnson 2010). These approaches also have other substantial advantages, like their ability to handle effects that are clustered, e.g., because some papers contribute multiple estimates (Hedges, Tipton, and Johnson 2010; Pustejovsky and Tipton 2021), and their ability to provide better inference in meta-analyses with relatively few studies (Tipton 2015). For these reasons, we often use these robust methods.Like fixed-effect meta-analysis, the random-effects estimate of \\(\\widehat{\\mu}\\) is still a weighted average of studies’ effect size estimates: \\[\\widehat{\\mu} = \\frac{ \\sum_{i=1}^k w_i \\widehat{\\theta}_i}{\\sum_{i=1}^k w_i}\\] However, in random-effects meta-analysis, the inverse-variance weights now incorporate heterogeneity: \\(w_i = 1/\\left(\\widehat{\\tau}^2 + \\widehat{\\sigma}^2_i \\right)\\). Where before we had one term in our weights, now we have two. That is because these weights represent the inverse of studies’ marginal variances, taking into account both statistical error due to their finite sample sizes (\\(\\widehat{\\sigma}^2_i\\) as before) and also genuine effect heterogeneity (\\(\\widehat{\\tau}^2\\)). \nConducting a random-effects meta-analysis of Paluck et al.’s dataset yields \\(\\widehat{\\mu}\\) = 0.4; 95% confidence interval [0.2, 0.61]; \\(p &lt; 0.001\\). That is, on average across studies, intergroup contact was associated with a decrease in prejudice of 0.4 standard deviations, substantially larger than the estimate from the fixed-effects model. This meta-analytic estimate is shown as the bottom line of figure 16.1.\n\n\n\n\n\n\ncode\n\n\n\n\n\nFitting a random-effects model requires only a small change to the methods argument of rma(). (We also include the knha flag that adds a correction to the computation of standard errors and p-values.)\n\nre_model &lt;- rma(yi = d, vi = var_d, data = paluck, method = \"REML\", knha = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: Estimated distribution of population effects from random-effects meta-analysis of Paluck et. al’s dataset (heavy red curve) and estimated density of studies’ point estimates (thin black curve).\n\n\n\n\nBased on the random-effects model, intergroup contact effects appear to differ across studies. Paluck et al. estimated that the standard deviation of effects across studies was \\(\\widehat{\\tau}\\) = 0.44 ; 95% confidence interval [0.25, 0.57]. This estimate indicates a substantial amount of heterogeneity! To visualize these results, we can plot the estimated density of the population effects, which is just a normal distribution with mean \\(\\widehat{\\mu}\\) and standard deviation \\(\\widehat{\\tau}\\) (figure 16.2).\nThis meta-analysis highlights an important point: that the overall effect size estimate \\(\\widehat{\\mu}\\) represents only the mean population effect across studies. It tells us nothing about how much the effects vary across studies. Thus, we recommend always reporting the heterogeneity estimate \\(\\widehat{\\tau}\\), preferably along with other related metrics that help summarize the distribution of effect sizes across studies (Riley, Higgins, and Deeks 2011; Wang and Lee 2019; Mathur and VanderWeele 2019, 2020a). Reporting the heterogeneity helps readers know how consistent or inconsistent the effects are across studies, which may point to the need to investigate moderators of the effect (i.e., factors that are associated with larger or smaller effects, such as whether participants were adults or children).8\n8 One common approach to investigating moderators in meta-analysis is meta-regression, in which moderators are included as covariates in a random-effects meta-analysis model (Thompson and Higgins 2002). As in standard regression, coefficients can then be estimated for each moderator, representing the mean difference in population effect between studies with versus without the moderator.\n\n\n\n\n\ndepth\n\n\n\n\n\nSingle-paper meta-analysis and pooled analysis\nThus far, we have described meta-analysis as a tool for summarizing results reported across multiple papers. However, some people have argued that meta-analysis should also be used to summarize the results of multiple studies reported in a single paper (Goh, Hall, and Rosenthal 2016). For instance, in a paper where you describe three different experiments on a hypothesis, you could (1) extract summary information (e.g., means and standard deviations) from each study, (2) compute the effect size, and then (3) combine the effect sizes in a meta-analysis.\nSingle-paper meta-analyses come with many of the same strengths and weaknesses we have discussed thus far. One unique weakness, though, is that having a small number of studies means that you typically have low power to detect heterogeneity and moderators. This lack of power sometimes leads researchers to claim that there are no significant differences between their studies. But an alternative explanation is that there simply wasn,t enough power to detect those differences!\nAs an alternative, you can also pool the actual data from the three studies, as opposed to just pooling summary statistics. For example, if you have data from 10 participants in each of the three experiments, you could pool them into a single dataset with 30 participants and include random effects of your condition manipulation across experiments (as described in chapter 7). This strategy is often referred to as pooled or integrative data analysis (and occasionally as “mega-analysis,” which sounds cool).\n\n\n\n\n\n\nFigure 16.3: Meta-analysis vs pooled data analysis.\n\n\n\nOne of the benefits of pooled data analysis is that it can give you more power to detect moderators. For instance, imagine that the effect of an intergroup contact treatment is moderated by age. If we performed a traditional meta-analysis, we would only have three observations in our data set, yielding very low power. However, we have many more observations (and much more variation in the moderator) in the pooled data analysis, which can lead to higher power (figure 16.3).\nPooled data analysis is not without its own limitations (Cooper and Patall 2009). And, of course, sometimes it doesn,t make as much sense to pool datasets (e.g., when measures are different from one another). Nonetheless, we believe that pooled data analysis and meta-analysis are both useful tools to keep in mind in a paper reporting multiple experiments!",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#single-paper-meta-analysis-and-pooled-analysis",
    "href": "016-meta.html#single-paper-meta-analysis-and-pooled-analysis",
    "title": "16  Meta-analysis",
    "section": "",
    "text": "Thus far, we have described meta-analysis as a tool for summarizing results reported across multiple papers. However, some people have argued that meta-analysis should also be used to summarize the results of multiple studies reported in a single paper (Goh, Hall, and Rosenthal 2016). For instance, in a paper where you describe three different experiments on a hypothesis, you could (1) extract summary information (e.g., means and standard deviations) from each study, (2) compute the effect size, and then (3) combine the effect sizes in a meta-analysis.\nSingle-paper meta-analyses come with many of the same strengths and weaknesses we have discussed thus far. One unique weakness, though, is that having a small number of studies means that you typically have low power to detect heterogeneity and moderators. This lack of power sometimes leads researchers to claim that there are no significant differences between their studies. But an alternative explanation is that there simply wasn,t enough power to detect those differences!\nAs an alternative, you can also pool the actual data from the three studies, as opposed to just pooling summary statistics. For example, if you have data from 10 participants in each of the three experiments, you could pool them into a single dataset with 30 participants and include random effects of your condition manipulation across experiments (as described in chapter 7). This strategy is often referred to as pooled or integrative data analysis (and occasionally as “mega-analysis,” which sounds cool).\n\n\n\n\n\n\nFigure 16.3: Meta-analysis vs pooled data analysis.\n\n\n\nOne of the benefits of pooled data analysis is that it can give you more power to detect moderators. For instance, imagine that the effect of an intergroup contact treatment is moderated by age. If we performed a traditional meta-analysis, we would only have three observations in our data set, yielding very low power. However, we have many more observations (and much more variation in the moderator) in the pooled data analysis, which can lead to higher power (figure 16.3).\nPooled data analysis is not without its own limitations (Cooper and Patall 2009). And, of course, sometimes it doesn,t make as much sense to pool datasets (e.g., when measures are different from one another). Nonetheless, we believe that pooled data analysis and meta-analysis are both useful tools to keep in mind in a paper reporting multiple experiments!",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#bias-in-meta-analysis",
    "href": "016-meta.html#bias-in-meta-analysis",
    "title": "16  Meta-analysis",
    "section": "16.2 Bias in meta-analysis",
    "text": "16.2 Bias in meta-analysis\nMeta-analysis is a great tool for synthesizing evidence across studies, but the accuracy of a meta-analysis can be compromised by bias. We’ll talk about two categories of bias here: within-study and across-study biases. Either type can lead to meta-analytic estimates that are too large, too small, or even in the wrong direction altogether. \n\n16.2.1 Within-study biases\nWithin-study biases—such as demand characteristics, confounds, and order effects, all discussed in chapter 9—not only impact the validity of individual studies but also any attempt to synthesize those studies. And of course, if individual study results are affected by analytic flexibility (\\(p\\)-hacking), meta-analyzing these will result in inflated effect size estimates. In other words: garbage in, garbage out.\nFor example, Paluck, Green, and Green (2019) noted that early studies on intergroup contact almost exclusively used nonrandomized designs. Imagine a hypothetical study where researchers studied a completely ineffective intergroup contact intervention, and nonrandomly assigned low-prejudice people to the intergroup contact condition and high-prejudice people to the control condition. In a scenario like this, the researcher would of course find that the prejudice was lower in the intergroup contact condition. But the effect would not be a true contact intervention effect, but rather a spurious effect of nonrandom assignment (i.e., confounding). Now imagine meta-analyzing many studies with similarly poor designs. The meta-analyst might find impressive evidence of an intergroup contact effect, even if none existed.\nTo mitigate this problem, meta-analysts often exclude studies that may be especially affected by within-study bias. (For example, Paluck, Green, and Green 2019 excluded nonrandomized studies). Of course, these decisions can’t be made on the basis of their effects on the meta-analytic estimate or else this post hoc exclusion itself will lead to bias! For this reason, inclusion and exclusion criteria for meta-analyses should be preregistered whenever possible.\nSometimes certain sources of bias cannot be eliminated by excluding studies—often because studies in a particular domain share certain fundamental limitations (for example, attrition in drug trials). After data have been collected, meta-analysts should also assess studies’ risks of bias qualitatively using established rating tools (Sterne et al. 2016). Doing so allows the meta-analyst to communicate how much within-study bias there may be.9 \n9 If you’re interested in assessing within-study bias, you can take a look at the Risk of Bias tool (https://sites.google.com/site/riskofbiastool/welcome/rob-2-0-tool) developed by Cochrane, an organization devoted to evidence synthesis.Meta-analysts can also conduct sensitivity analyses to assess how much results might be affected by different within-study biases or by excluding certain types of studies (Mathur and VanderWeele 2022). For example, if nonrandom assignment is a concern, a meta-analyst may run the analyses including only randomized studies, versus including all studies, in order to determine how much including nonrandomized studies changes the meta-analytic estimate. These two options parallel our discussion of experimental preregistration in chapter 11: to allay concerns about results-dependent meta-analysis, researchers can either preregister their analyses ahead of time or else be transparent about their choices after the fact. Sensitivity analyses can allay concerns that a specific choice of exclusion criteria is critically related to the reported results.\n\n\n16.2.2 Across-study biases\nAcross-study biases occur if, for example, researchers selectively report certain types of findings or selectively publish certain types of findings (publication bias, as discussed in chapter 3 and chapter 11). Often, these across-study biases favor statistically significant positive results, which means the meta-analytic estimate based on those studies will be inflated relative to the true effect.\n\n\n\n\n\n\naccident report\n\n\n\n\n\nQuantifying publication bias in the social sciences\nIt’s typically very hard to quantify publication bias because you don’t know how many studies are out there in researchers’ “file drawers”—unpublished studies are by definition not available. But a recent study took advantage of a unique opportunity to try and quantify publication bias within a known pool of studies.\nTime-sharing Experiments in the Social Sciences (TESS) is an innovative project that lets researchers apply to run experiments on nationally representative samples in the US. In 2014, Franco, Malhotra, and Simonovits (2014) and colleagues took advantage of this application process by examining the entire population of 221 studies conducted through TESS.\nUsing this information, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and nonsignificant results, or only nonsignificant results. Then, they examined the likelihood that these results were published in the scientific literature.\nOver 60% of studies with statistically significant results were published, compared to a mere 25% of studies that produced only statistically nonsignificant results. This finding was important because it quantified how strong publication bias actually was, at least in one particular population of studies. This estimate may not be general: for example, perhaps TESS studies were easier to put in the file drawer because they cost nothing for the researchers to run. But even a lower level of publication bias can have a substantial effect on a meta-analysis, meaning that it is crucial to check for—and potentially, correct for—publication bias.\n\n\n\nLike within-study biases, meta-analysts often try to mitigate across-study biases by being careful about what studies make it into the meta-analysis. Meta-analysts don’t only want to capture high-profile, published studies on their effect of interest but also studies published in low-profile journals and the so-called gray literature (i.e., unpublished dissertations and theses; Lefebvre et al. 2019).10\n10 Evidence is mixed regarding whether including gray literature actually reduces across-study biases in meta-analysis (Tsuji et al. 2020; Mathur and VanderWeele 2021), but it is still common practice to try to include this literature.There are also statistical methods to help assess how robust the results may be to across-study biases. Among the most popular tools to assess and correct for publication bias is the funnel plot (Duval and Tweedie 2000; Egger et al. 1997). A funnel plot shows the relationship between studies’ effect estimates and their precision (usually their standard error). These plots are called “funnel plots” because, if there is no publication bias, then as precision increases, the effects “funnel” toward the meta-analytic estimate. As the precision is smaller, they spread out more because of greater measurement error. Figure 16.4] is an example of one type of funnel plot (Mathur and VanderWeele 2020b) for a simulated meta-analysis of 100 studies with no publication bias.\n\n\n\n\n\n\n\n\nFigure 16.4: A significance funnel plot for a meta-analysis simulated to have no publication bias. Orange points: studies with \\(p &lt; 0.05\\) and positive estimates. Grey points: studies with \\(p\\) \\(\\ge\\) \\(0.05\\) or negative estimates. Black diamond: random-effects estimate of \\(\\widehat{\\mu}\\).\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nFor this plot, we use the PublicationBias package (Braginsky, Mathur, and VanderWeele 2023) and the significance_funnel() function. (An alternative function is the metafor function funnel(), which results in a more “classic” funnel plot.) We use our fitted model re_model:\n\nsignificance_funnel(yi = re_model$yi, vi = re_model$vi)\n\nBecause meta-analysis is such a well-established method, many of the relevant operations are “plug and play.”\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: A classic funnel plot.\n\n\n\n\n\nAs implied by the “funnel” moniker, our plot looks a little like a funnel. Larger studies (those with smaller standard errors) cluster more closely around the mean of 0.34 than do smaller studies, but large and small studies alike have point estimates centered around the mean. That is, the funnel plot is symmetric.11\n11 Classic funnel plots look more like figure 16.5). Our version is different in a couple of ways. Most prominently, we don’t have the vertical axis reversed (which we think is confusing). We also don’t have the left boundary highlighted, because we think folks don’t typically select for negative studies.Not all funnel plots are symmetric! figure 16.6 is what happens to our hypothetical meta-analysis if all studies with \\(p&lt;0.05\\) and positive estimates are published, but only 10% of studies with \\(p \\ge 0.05\\) or with negative estimates are published. The introduction of publication bias dramatically inflates the pooled estimate from 0.34 to 1.15. Also, there appears to be a correlation between studies’ estimates and their standard errors, such that smaller studies tend to have larger estimates than do larger studies. This correlation is often called funnel plot asymmetry because the funnel plot starts to look like a right triangle rather than a funnel. Funnel plot asymmetry can be a diagnostic for publication bias, though it isn’t always a perfect indicator, as we’ll see in the next subsection.\n\n\n\n\n\n\n\n\nFigure 16.6: A significance funnel plot for the same simulated meta-analysis after publication bias has occurred. Orange points: studies with \\(p &lt; 0.05\\) and positive estimates. Grey points: studies with \\(p\\) \\(\\ge\\) \\(0.05\\) or negative estimates. Black diamond: random-effects estimate of \\(\\widehat{\\mu}\\).\n\n\n\n\n\n\n\n16.2.3 Across-study bias correction\nHow do we identify and correct bias across studies? Given that some forms of publication bias induce a correlation between studies’ point estimates and their standard errors, several popular statistical methods, such as trim-and-fill (Duval and Tweedie 2000) and Egger’s regression (Egger et al. 1997) are designed to quantify funnel plot asymmetry.\nFunnel plot asymmetry does not always imply that there is publication bias, though. Nor does publication bias always cause funnel plot asymmetry. Sometimes funnel plot asymmetry is driven by genuine differences in the effects being studied in small and large studies (Egger et al. 1997; Lau et al. 2006). For example, in a meta-analysis of intervention studies, if the most effective interventions are also the most expensive or difficult to implement, these highly effective interventions might be used primarily in the smallest studies (“small study effects”).\nFunnel plots and related methods are best suited to detecting publication bias in which (1) small studies with large positive point estimates are more likely to be published than small studies with small or negative point estimates; and (2) the largest studies are published regardless of the magnitude of their point estimates. That model of publication bias is sometimes what is happening, but not always!\nA more flexible approach for detecting publication bias uses selection models. These models can detect other forms of publication bias that funnel plots may not detect, such as publication bias that favors significant results. We won’t cover these methods in detail here, but we think they are a better approach to the question, along with related sensitivity analyses.12\n12 High-level overviews of selection models are given in McShane, Böckenholt, and Hansen (2016) and Maier, VanderWeele, and Mathur (2022). For more methodological detail, see Hedges (1984), Iyengar and Greenhouse (1988), and Vevea and Hedges (1995). For a tutorial on fitting and interpreting selection models, see Maier, VanderWeele, and Mathur (2022). For sensitivity analyses, see Mathur and VanderWeele (2020b).You may also have heard of “\\(p\\)-methods” to detect across-study biases such as \\(p\\)-curve and \\(p\\)-uniform (Simonsohn, Nelson, and Simmons 2014; van Assen, Aert, and Wicherts 2015). These methods essentially assess whether the significant \\(p\\)-values “bunch up” just under 0.05, which is taken to indicate publication bias. These methods are increasingly popular in psychology and have their merits. However, they are actually simplified versions of selection models (e.g., Hedges 1984) that work only under considerably more restrictive settings than the original selection models (for example, when there is not heterogeneity across studies; McShane, Böckenholt, and Hansen 2016). For this reason, it is usually (although not always) better to use selection models in place of the more restrictive \\(p\\)-methods.\nGoing back to our running example, Paluck et al. used a regression-based approach to assess and correct for publication bias. This approach provided significant evidence of a relationship between the standard error and effect size (i.e., an asymmetric funnel plot). Again, this asymmetry could reflect publication bias or other sources of correlation between studies’ estimates and their standard errors. Paluck et al. also used this same regression-based approach to try to correct for potential publication bias. Results from this model indicated that the bias-corrected effect size estimate was close to zero. In other words, even though all studies estimated that intergroup contact decreased prejudice, it is still possible that there are unpublished studies that did not find this (or found that intergroup contact increased prejudice).\n\n\n\n\n\n\naccident report\n\n\n\n\n\nGarbage in, garbage out? Meta-analyzing potentially problematic research\nBotox can help eliminate wrinkles. But some researchers have suggested that, when used to paralyze the muscles associated with frowning, Botox may also help treat clinical depression. As surprising as this claim may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to receive either Botox or saline injections do indeed find that Botox recipients show decreased depression. And when you combine all available evidence in a meta-analysis, you find that this effect is quite large: d = 0.83, 95% CI [0.52, 1.14].\nAs Coles et al. (2019) argued though, this estimated effect may be impacted by both within- and between-study bias. First, participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. But only one of these treatments leads the upper half of your face to be paralyzed! After a couple weeks, you’re pretty likely to know whether you received the Botox treatment or control saline injection. Thus, the apparent effect of Botox on depression could instead be a placebo effect.\nSecond, only 50% of the outcomes that researchers measured were reported in the final publications, raising concerns about selective reporting. Perhaps researchers examining the effects of Botox on depression only reported the measures that showed a positive effect, not the ones that didn’t.\nTaken together, these two criticisms suggest that, despite the impressive meta-analytic estimate, the effect of Botox on depression is far from certain.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#quantifying-publication-bias-in-the-social-sciences",
    "href": "016-meta.html#quantifying-publication-bias-in-the-social-sciences",
    "title": "16  Meta-analysis",
    "section": "Quantifying publication bias in the social sciences",
    "text": "Quantifying publication bias in the social sciences\nIt’s typically very hard to quantify publication bias because you don’t know how many studies are out there in researchers’ “file drawers”—unpublished studies are by definition not available. But a recent study took advantage of a unique opportunity to try and quantify publication bias within a known pool of studies.\nTime-sharing Experiments in the Social Sciences (TESS) is an innovative project that lets researchers apply to run experiments on nationally representative samples in the US. In 2014, Franco, Malhotra, and Simonovits (2014) and colleagues took advantage of this application process by examining the entire population of 221 studies conducted through TESS.\nUsing this information, Franco and colleagues examined the records of these studies to determine whether the researchers found statistically significant results, a mixture of statistically significant and nonsignificant results, or only nonsignificant results. Then, they examined the likelihood that these results were published in the scientific literature.\nOver 60% of studies with statistically significant results were published, compared to a mere 25% of studies that produced only statistically nonsignificant results. This finding was important because it quantified how strong publication bias actually was, at least in one particular population of studies. This estimate may not be general: for example, perhaps TESS studies were easier to put in the file drawer because they cost nothing for the researchers to run. But even a lower level of publication bias can have a substantial effect on a meta-analysis, meaning that it is crucial to check for—and potentially, correct for—publication bias.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#garbage-in-garbage-out-meta-analyzing-potentially-problematic-research",
    "href": "016-meta.html#garbage-in-garbage-out-meta-analyzing-potentially-problematic-research",
    "title": "16  Meta-analysis",
    "section": "Garbage in, garbage out? Meta-analyzing potentially problematic research",
    "text": "Garbage in, garbage out? Meta-analyzing potentially problematic research\nBotox can help eliminate wrinkles. But some researchers have suggested that, when used to paralyze the muscles associated with frowning, Botox may also help treat clinical depression. As surprising as this claim may sound, a quick examination of the literature would lead many to conclude that this treatment works. Studies that randomly assign depressed patients to receive either Botox or saline injections do indeed find that Botox recipients show decreased depression. And when you combine all available evidence in a meta-analysis, you find that this effect is quite large: d = 0.83, 95% CI [0.52, 1.14].\nAs Coles et al. (2019) argued though, this estimated effect may be impacted by both within- and between-study bias. First, participants are not supposed to know whether they have been randomly assigned to receive Botox or a control saline injections. But only one of these treatments leads the upper half of your face to be paralyzed! After a couple weeks, you’re pretty likely to know whether you received the Botox treatment or control saline injection. Thus, the apparent effect of Botox on depression could instead be a placebo effect.\nSecond, only 50% of the outcomes that researchers measured were reported in the final publications, raising concerns about selective reporting. Perhaps researchers examining the effects of Botox on depression only reported the measures that showed a positive effect, not the ones that didn’t.\nTaken together, these two criticisms suggest that, despite the impressive meta-analytic estimate, the effect of Botox on depression is far from certain.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "016-meta.html#chapter-summary-meta-analysis",
    "href": "016-meta.html#chapter-summary-meta-analysis",
    "title": "16  Meta-analysis",
    "section": "16.3 Chapter summary: Meta-analysis",
    "text": "16.3 Chapter summary: Meta-analysis\nTaken together, Paluck and colleagues’ use of meta-analysis provided several important insights that would have been easy to miss in a nonquantitative review. First, despite a preponderance of nonsignificant findings, intergroup contact interventions were estimated to decrease prejudice by on average 0.4 standard deviations. On the other hand, there was considerable heterogeneity in intergroup contact effects, suggesting important moderators of the effectiveness of these interventions. And finally, publication bias was a substantial concern, indicating a need for follow-up research using a registered report format that will be published regardless of whether the outcome is positive (chapter 11).\nOverall, meta-analysis is a key technique for aggregating evidence across studies. Meta-analysis allows researchers to move beyond the bias of naive techniques like vote counting and toward a more quantitative summary of an experimental effect. Unfortunately, a meta-analysis is only as good as the literature it’s based on, so the aspiring meta-analyst must be aware of both within- and between-study biases!\n\n\n\n\n\n\ndiscussion questions\n\n\n\n\n\n\nImagine that you read the following result in the abstract of a meta-analysis: “In 83 randomized studies of middle school children, replacing one hour of class time with mindfulness meditation significantly improved standardized test scores (standardized mean difference \\(\\widehat{\\mu} = 0.05\\); 95% confidence interval: [\\(0.01, 0.09\\)]; \\(p&lt;0.05\\)).” Why is this a problematic way to report on meta-analysis results? Suggest a better sentence to replace this one.\nAs you read the rest of the meta-analysis, you find that the authors conclude that “these findings demonstrate robust benefits of meditation for children, suggesting that test scores improve even when the meditation is introduced as a replacement for normal class time.” You recall that the heterogeneity estimate was \\(\\widehat{\\tau} = 0.90\\). Do you think that this result regarding the heterogeneity tends to support, or rather tends to undermine, the concluding sentence of the meta-analysis? Why?\nWhat kinds of within-study biases would concern you in the meta-analysis described in the prior two questions? How might you assess the credibility of the meta-analyzed studies and of the meta-analysis as a whole in light of these possible biases?\nImagine you conduct a meta-analysis on a literature in which statistically significant results in either direction are much more likely to be published that nonsignificant results. Draw the funnel plot you would expect to see. Is the plot symmetric or asymmetric?\nWhy do you think small studies receive more weight in random-effects meta-analysis than in fixed-effects meta-analysis? Can you see why this is true mathematically based on the equations given above, and can you also explain the intuition in simple language?\n\n\n\n\n\n\n\n\n\n\nreadings\n\n\n\n\n\n\nA nice, free textbook with lots of good code examples: Harrer, Mathias, Pim Cuijpers, Furukawa Toshi A, and David D. Ebert (2021). Doing Meta-Analysis with R: A Hands-On Guide. Chapman & Hall/CRC Press. Available free online at https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R.",
    "crumbs": [
      "Reporting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Meta-analysis</span>"
    ]
  },
  {
    "objectID": "017-conclusion.html",
    "href": "017-conclusion.html",
    "title": "17  Conclusion",
    "section": "",
    "text": "Conclusion\nYou’ve made it to the end of Experimentology, our (sometimes opinionated) guide to how to run good psychology experiments. In this book we’ve tried to present a unified approach to the why and how of running experiments. This approach begins with the goal of doing experiments:\nThis formulation isn’t exactly how experiments are talked about in the broader field, but we hope you’ve started to see some of the rationale behind this approach. In this final chapter, we will briefly discuss some aspects of our approach, as well how this approach connects with our four themes, transparency, measurement precision, bias reduction, and generalizability. We’ll end by mentioning some exciting new trends in the field that give us hope about the future of experimentology and psychology more broadly.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "017-conclusion.html#summarizing-our-approach",
    "href": "017-conclusion.html#summarizing-our-approach",
    "title": "17  Conclusion",
    "section": "17.1 Summarizing our approach",
    "text": "17.1 Summarizing our approach\nThe Experimentology approach is grounded in both an appreciation of the power of experiments to reveal important aspects about human psychology and also an understanding of the many ways that experiments can fail. In particular, the “replication crisis” (chapter 3) has revealed that small samples, a focus on dichotomous statistical inference, and a lack of transparency around data analysis can lead to a literature that is often neither reproducible nor replicable. Our approach is designed to avoid these pitfalls.\nWe focus on measurement precision in service of measuring causal effects. The emphasis on causal effects stems from an acknowledgement of the key role of experiments in establishing causal inferences (chapter 1) and the importance of causal relationships to theories (chapter 2). In our statistical approach, we focus on estimation (chapter 5) and modeling (chapter 7), helping us to avoid some of the fallacies that come along with dichotomous inference (chapter 6). We choose measures to maximize reliability (chapter 8). We prefer simple, within-participant experimental designs because they typically result in more precise estimates (chapter 9). And we think meta-analytically about the overall evidence for a particular effect beyond our individual experiment (chapter 16).\nFurther, we recognize the presence of many potential sources of bias in our estimates, leading us to focus on bias reduction. In our measurements, we identify arguments for the validity of our measures, decreasing bias in estimation of the key constructs of interest (chapter 8); in our designs we seek to minimize bias due to confounding or experimenter effects (chapter 9). We also try to minimize the possibility of bias in our decisions about data collection (chapter 12) and data analysis (chapter 11). Finally, we recognize the possibility of bias in literatures as a whole and consider ways to compensate in our estimates (chapter 16).\nFinally, we consider generalizability throughout the process. We theorize with respect to a particular population (chapter 2) and select our sample in order to maximize the generalizability of our findings to that target population (chapter 10). In our statistical analysis, we take into account multiple dimensions of generalizability, including across participants and experimental stimulus items (chapter 7). And in our reporting, we contextualize our findings with respect to limits on their generalizability (chapter 14).\nWoven throughout this narrative is the hope that embracing transparency throughout the experimental process will help you maximize your work. Not only is sharing your work openly an ethical responsibility (chapter 4), but it’s also a great way to minimize errors while creating valuable products that both advance scientific progress and accelerate your own career (chapter 13).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "017-conclusion.html#forward-the-field",
    "href": "017-conclusion.html#forward-the-field",
    "title": "17  Conclusion",
    "section": "17.2 Forward the field",
    "text": "17.2 Forward the field\nWe have focused especially on reproducibility and replicability issues, but we’ve learned at various points in this book that there’s a replication crisis (Open Science Collaboration 2015), a theory crisis (Oberauer and Lewandowsky 2019), and a generalizability crisis (Yarkoni 2020) in psychology. Based on all these crises, you might think that we are pessimistic about the future of psychology. Not so.\nThere have been tremendous changes in psychological methods since we started teaching Experimental Methods in 2012. When we began, it was common for incoming graduate students to describe the rampant \\(p\\)-hacking they had been encouraged to do in their undergraduate labs. Now, students join the class aware of new practices like preregistration and cognizant of problems of generalizability and theory building. It takes a long time for a field to change, but we have seen tremendous progress at every level—from government policies requiring transparency in the sciences all the way down to individual researchers’ adoption of tools and practices that increase the efficiency of their work and decrease the chances of error.\nOne of the most exciting trends has been the rise of metascience, in which researchers use the tools of science to understand how to make science better (Tom E. Hardwicke et al. 2020). Reproducibility and replicability projects (reviewed in chapter 3) can help us measure the robustness of the scientific literature. In addition, studies that evaluate the impacts of new policies (e.g., Tom E. Hardwicke et al. 2018)—can help stakeholders like journal editors and funders make informed choices about how to push the field toward more robust science.\nIn addition to changes that correct methodological issues, the last ten years have seen the rise of “big team science” efforts that advance the field in new ways (Coles et al. 2022). Collaborations such as the Psychological Science Accelerator (Moshontz et al. 2018) and ManyBabies (Frank et al. 2017) allow hundreds of researchers from around the world to come together to run shared projects. These projects are enabled by open science practices like data and code sharing, and they provide a way for researchers to learn best practices via participating. In addition, by including broader and more diverse samples, they can help address challenges around generalizability (Klein et al. 2018).\nFinally, the last ten years have seen huge progress in the use of statistical models both for understanding data (McElreath 2018) and for describing specific psychological mechanisms (Ma, Körding, and Goldreich 2022). In our own work, we have used these models extensively and we believe that they provide an exciting toolkit for building quantitative theories that allow us to explain and to predict the human mind.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "017-conclusion.html#final-thoughts",
    "href": "017-conclusion.html#final-thoughts",
    "title": "17  Conclusion",
    "section": "17.3 Final thoughts",
    "text": "17.3 Final thoughts\nDoing experiments is a craft, one that requires practice and attention. The first experiment you run will have limitations and issues. So will the 100th. But as you refine your skills, the quality of the studies you design will get better. Further, your own ability to judge others’ experiments will improve as well, making you a more discerning consumer of empirical results. We hope you enjoy this journey!",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "100-instructors.html",
    "href": "100-instructors.html",
    "title": "appendix A — Instructor’s guide",
    "section": "",
    "text": "A.1 Introduction\nThis is an instructor’s guide to conducting replication projects in courses. In addition to benefiting the field in ways that have been previously discussed by some of the authors of this book (e.g., Hawkins et al. 2018; Frank and Saxe 2012), replication-based courses can additionally benefit students in these courses. In this guide, we will describe these benefits, explore different ways in which courses may be modified depending on student level and resources, and provide some guidelines and examples to help you set up the logistics of your course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Instructor's guide</span>"
    ]
  },
  {
    "objectID": "100-instructors.html#why-teach-a-project-based-course",
    "href": "100-instructors.html#why-teach-a-project-based-course",
    "title": "appendix A — Instructor’s guide",
    "section": "A.2 Why Teach a Project-Based Course?",
    "text": "A.2 Why Teach a Project-Based Course?\nOver the years, we have observed many ways in which our replication-based courses benefited students above and beyond a more traditional lecture and problem set-based course. Some of these benefits include:\n\nStudent interest: Since each student will be free to replicate a study that is aligned with their research interests, this freedom facilitates a more direct application of course methods and lessons to a project that is interesting to each student.\nUsefulness: If this course is taught in the first year of the program (as recommended), students may use their replication project as a way to establish robustness of a phenomenon before building studies on top of it.\nRealism: Practice datasets that are typically provided for course exercises lack the complexity and messiness of real data. By conducting a replication project and dealing with real data, students learn to apply the tools provided in the course in a way that more closely demonstrates their usefulness beyond the course.\nIntuition: Presentations of replication outcomes across the class along with a discussion of what factors seemed to predict these outcomes helps students develop a better intuition when reading the literature for how likely studies are to replicate.\nPerspective: Frustrating experiences with ambiguity (whether regarding experimental methods, materials, or analyses) can motivate students to adopt best practices for their own future studies.\n\nA project-based course may look very different depending on student level (undergraduate vs graduate/post-doc level) and availability of resources at your institution for a course like this, namely in terms of TA support and course funding (for data collection). For most of this guide, we will assume that you have a similar setup to ours (i.e., teaching at the graduate/post-doc level and have course funding and TAs to support the course), but we have also spent some time considering ways to adjust the course to fit different student levels and availability of resources (see “Scenarios for different course layouts”).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Instructor's guide</span>"
    ]
  },
  {
    "objectID": "100-instructors.html#logistics",
    "href": "100-instructors.html#logistics",
    "title": "appendix A — Instructor’s guide",
    "section": "A.3 Logistics",
    "text": "A.3 Logistics\n\nA.3.1 Syllabus considerations\nIf it is your first time teaching this course, you may want to decide ahead of time whether your course will mainly focus on content, or whether you will cover both content and relevant practical skills. For instance, if the course is for undergraduate students, you may decide to focus mainly on content, whereas if the course is for graduate students, they may find it more useful if the course covers both content and practical skills they can use in their research.\nAnother important consideration is how long your course will be. Depending on whether your university operates on quarters or semesters, the pace of the course will differ. For Psych 251, since we are on the quarter system, we use the 10-week schedule shown below. However, we have also adapted this schedule to a 16-week course given that it better represents a majority of other institutions’ academic calendars. At the end of this chapter, we give a set of sample class schedules.\n\n\nA.3.2 Grading\nDepending on your course format and teaching philosophy, you may have preferred grading criteria. As a point of reference, in Psych 251, we wanted to encompass both the assignments (problemsets and project components) as well as actual course attendance and participation. In addition, because the replication project is a central part of the course, we weighted the project components slightly more than the problem sets:\n\n40%: Problem sets (four, at 10% each)\n50%: Final project components, including presentations, data collection, analysis, and writeup\n10%: Attendance and participation in class\n\n\n\nA.3.3 Course budget\nFor our course, we usually receive around US$1,000 for course funding from the Psychology Department. In addition, when students from other departments are enrolled, we have been lucky to receive additional funding from those departments as well, to further support the course. Still, making sure that the course funds cover all students’ projects is one of the most challenging parts of the course. Assuming you have a budget to work with, here are some lessons we’ve learned along the way regarding budgeting (and if you don’t have any funding, please refer to the section titled “Course Funding” under “Scenarios for different course layouts”):\n\nBefore students pick their study to replicate, provide them with an estimate of how many participant hours they will be able to receive for their project\nAs soon as students pick a study for their replication project, help each student run a power analysis to confirm that replicating the study would be within the budget (TAs can help with this)\nIf a student feels strongly about a study that does not fit within the budget, consider the following ways to adjust the study: (1) Can the study be made shorter by cutting out unnecessary measures? (2) If it is a multi-trial study, can the number of trials be reduced? (3) Would their advisors be willing to provide additional funding? (4) can the study be run on university participant pools?\nAs mentioned above, if there are students from other departments who are enrolled in your course, one possibility to obtain more funding is to reach out to the heads of those departments to see whether they would be willing to help support your course.\n\nOnce all projects have been approved as within-budget, we encourage you to create a shared spreadsheet containing each student’s name, so that they can fill in the details of their replication project. Ultimately, this will help ensure that students are paying fair wages to their participants and keep track of how the course funds are being divided up.\n\n\nA.3.4 Course-related Institutional Review Board application\nWhile it may be possible to apply for individual IRB approval for each student’s project, we recommend applying for course-wide standard IRB approval for all replication projects that are conducted in your class. Contacting your review board early in the planning stages of the course should clarify what options you have available.\nOne important thing to remember when students run their individual projects is that they should have the course-wide consent form at the beginning of their studies (and TAs should check this when they review the paradigms). For reference, this is the consent form that each student is required to post at the beginning of their study:\n“By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at stanfordpsych251@gmail.com. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Instructor's guide</span>"
    ]
  },
  {
    "objectID": "100-instructors.html#scenarios-for-different-course-layouts",
    "href": "100-instructors.html#scenarios-for-different-course-layouts",
    "title": "appendix A — Instructor’s guide",
    "section": "A.4 Scenarios for different course layouts",
    "text": "A.4 Scenarios for different course layouts\nNow that we have covered the standard format of the course, we want to now turn our attention to ways in which this format can be tweaked in order to fit different needs and resources. We have organized this section into two main categories: student level and course resources (such as TAs and course funding).\n\nA.4.1 Student level\nWhile Psych 251 at Stanford is geared toward graduate students (and is currently a required class for entering first-year graduate students in the Psychology Department), we also accept advanced undergraduate students as well as graduate students from other departments (e.g., education, human-computer interaction, philosophy, computer science). On the first day of our course, we tell students that they should be comfortable with two of the three following topics:\n\nSome knowledge of psychological experimentation & subject matter\nStatistical programming: things like functions and variables\nBasic statistics like ANOVA and t-test\n\nIf students are only comfortable with one of the three topics above, we warn them ahead of time that the course may demand more time from them than the average student.\nNow, if you are planning on catering this course for undergraduate students, chances are that they have had less exposure to these topics overall, so there are multiple ways to calibrate the course accordingly:\nPrerequisites: Require students to have completed courses that cover at least two of the three topics mentioned above (i.e., a psychology class, a class that covers statistical programming, a class that covers basic statistics, any two of the three).\nPace: Unlike Psych 251, where the entire course only lasts 10 weeks, a class for undergraduates may benefit from a slower pace, allowing more time to cover the foundational principles before diving into the project. For instance, the course could be held over multiple academic semesters/quarters, with the project goal of Course #1 being choosing and planning the replication study, and the project goal of Course #2 being the execution and interpretation of the replication.\nPair-group-based projects: In our course, each student is required to conduct their own replication project. However, this structure may be overwhelming for undergraduate students who may have less confidence taking on an entire replication project by themselves. One option that may alleviate this pressure is to have students conduct these projects as pairs or as small teams, so that they can collectively draw on each others’ strengths. When assigning these pairs or teams, it may be especially helpful to try to ensure a relatively even balance of students who are confident in each of the three areas outlined above (psychology, statistical programming, basic statistics).\nNow that we’ve offered a few suggestions to address different student levels, let’s dive into the issue of course resources.\n\n\nA.4.2 Course resources\nWe think there are two main ways in which your course may have different resources from our model: in terms of course assistance (i.e., teaching assistants), and in terms of course funding for student projects. We’ll explore ways to work around each of these in this section.\nTeaching assistants. As a point of comparison, in general, two to three teaching assistants are allocated to Psych 251, which enrolls about 36 students, which comes out to about 12-18 students per TA. Since a project-based course requires individual attention and feedback, we would recommend against a student-TA ratio that is much higher than that. That means that if you know you will have just one TA for the class, you should think about reducing the enrollment cap accordingly. But what if you have no TAs? With some adjustments, there are still ways you can make the course work sans-TA; we outline a few ideas below:\n\nPeer grading. As an instructor with no TAs, the area that will require the biggest lift in terms of time and attention is grading. One way to overcome this is to introduce a peer-grading system, in which students grade each others’ work. If you choose this route, two things that may encourage fair grading among your students is to (1) distribute a clear and specific rubric that reduces the amount of subjectivity in the grading process as much as possible, and (2) anonymize the assignments so that students do not know whose assignment they are grading. If possible, it may again be beneficial to assign grading pairs that consist of students that are relatively knowledgeable in different areas, so that they can provide feedback that address weak points in each others’ work.\nCollective troubleshooting. The second most time intensive area you will have to make up for is the amount of troubleshooting you may have to do for students who run into issues implement their projects, anywhere from getting GitHub and R Markdown up and running on their devices, to trouble with data collection on Mechanical Turk. One way to encourage communal support among your students is to set up a central discussion board for the course (e.g., Piazza or a course channel on Slack) where students can publicly (but anonymously, if desired) post issues they are running into. Then, you can offer extra credit to students who help troubleshoot these issues, in order to further incentivize collective troubleshooting. There will likely still be issues that cannot be addressed by the students, but this system at least frees up your time to focus your attention on those that only you can address.\nSingle class-wide project. Finally, if the collective grading and troubleshooting methods outlined above do not cut down on enough time, you could consider walking through a single replication project as a class.1 To make a single-project course work, you could have students nominate studies they would like to replicate as a class, and then have them vote on the final choice. Once the target study has been selected, every student can individually carry out all the steps of the project, including preregistering and writing up the analysis script. Then, setting up and running the data collection phase can happen during class, and once data has been collected, you can distribute it to the students for them to run it through their analysis script and interpret the result. Whether you choose to have students grade each others’ work or whether you grade their work yourself, the fact that the project is standardized should cut down on a lot of the time you would otherwise spend learning about the details of every individual project.\n\n1 This approach does cancel out some of the benefits of a project-based course we mentioned at the start—namely, the project will likely no longer fit each student’s specific research interest, so there may be less benefit in terms of specific student interest and usefulness for their program of research, but the other two benefits of realism and intuition (especially if the project is discussed in the context of other replication findings) still stand.Course funding. In addition to availability of TAs, another way in which your course may be different from ours is in terms of course funding. If you have little or not funding for your course (even after reaching out to relevant members of your department or institution), we suggest the following adjustments:\n\nPair-group-based projects. Similarly to suggestion 3 for addressing different student levels, one option for limited course budgets is to have students conduct the replication projects as pairs or teams to reduce the cost of data collection. This structure may have the added benefit of encouraging students to problem-solve together. Alternatively, each student in the pairs or teams could complete each step of the replication individually (e.g., writing up the report, analyzing the data, interpreting the result), which would ensure that each student takes full responsibility for every step of the project. This structure may also provide opportunities for interesting discussions at the end of the course around analytic reproducibility, especially if students in the same teams (with the same dataset) differed in the conclusions they drew about the replication outcome.\nFunding from advisors. In some cases, students come to us with target studies that require more funding than we are able to allocate, but that they feel particularly invested in (e.g., because of how relevant the study is to their line of research). Once we rule out other ways of making the study fit our budget (e.g., dropping extra control conditions, running a subset of the study), we often ask students whether their advisor would be willing to fund the study. We have found that advisors are often willing to do this, especially if the replication could serve an important role in the development of the student’s research program. Similarly, one way to reduce the burden on a limited course budget would be to encourage all students to first ask their advisors about whether they would be willing to fund part or all of the data collection for the replication. While chances are that some advisors will be unwilling or unable to do this, there should still be a meaningful reduction in the number of projects the course will need to fund.\nReproduce a replication. The suggestions above apply if you at least have some amount of course funding, but what if you have no funding at all? While there are obvious limitations to this solution, one suggestion is to have students reproduce past public replications. For instance, our course GitHub page contains public repositories of all past replication projects that have been conducted in our course. Since the data for each replication project is available in these repositories, you could provide each of your students with a dataset and the original paper associated with it, and assign them to reproduce the results of the replication. Students should then be able to follow each step of the replication project described below (e.g., writing the report, identifying the key analysis, running the analysis). This format will only work if students do not view the original final replication reports that are posted publicly for their project, so it may be necessary to be clear about this at the beginning of the course.\n\nFor those of you who are working with a different course format (whether in terms of student level or course resources), we hope these suggestions were useful. If you try out a new idea in your course that you found helpful, we would be thrilled if you shared them with us!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Instructor's guide</span>"
    ]
  },
  {
    "objectID": "100-instructors.html#sample-course-schedules",
    "href": "100-instructors.html#sample-course-schedules",
    "title": "appendix A — Instructor’s guide",
    "section": "A.5 Sample course schedules",
    "text": "A.5 Sample course schedules\nThe sample syllabi laid out below are categorized along the following decisions: (1) material: whether the course focuses on just content or both content and skills, and (2) duration: whether the course is 10 weeks long or 16 weeks long.\nFor undergraduate instructors, we have labeled advanced topics in purple. We expect that these topics are best suited for advanced undergraduate students. As for content around statistics (e.g., estimation, inference), instructors should decide how much of this content to teach, depending on how prepared students have been in previous classes.\n\n10 weeks10 weeks, content only16 weeks16 weeks, content only\n\n\n\n\n\n\nTable A.1: A sample 10-week syllabus with both skills and content materials.\n\n\n\n\n \n  \n    Week \n    Day \n    Topic \n    Chapter \n    Appendix \n  \n \n\n  \n    1 \n    M \n    Class introduction \n    1 \n     \n  \n  \n    1 \n    W \n    Theories \n    2 \n     \n  \n  \n    1 \n    F \n    Version control \n     \n    B \n  \n  \n    2 \n    M \n    Reproducible reports \n    14 \n    C \n  \n  \n    2 \n    W \n    Tidyverse tutorial \n     \n    D \n  \n  \n    2 \n    F \n    Tidyverse tutorial continued (with TAs) \n     \n     \n  \n  \n    3 \n    M \n    Measurement, reliability, and validity \n    8 \n     \n  \n  \n    3 \n    W \n    Design of experiments \n    9 \n     \n  \n  \n    3 \n    F \n    Sampling \n    10 \n     \n  \n  \n    4 \n    M \n    Project management \n    13 \n     \n  \n  \n    4 \n    W \n    Experiments 1: Simple survey experiments using Qualtrics \n     \n     \n  \n  \n    4 \n    F \n    Experiments 2: Project-specific implementation (TAs) \n     \n     \n  \n  \n    5 \n    M \n    Estimation \n    5 \n     \n  \n  \n    5 \n    W \n    Inference \n    6 \n     \n  \n  \n    5 \n    F \n    Sample size planning \n     \n     \n  \n  \n    6 \n    M \n    Survey design \n     \n     \n  \n  \n    6 \n    W \n    Midterm presentations 1 \n     \n     \n  \n  \n    6 \n    F \n    Midterm presentations 2 \n     \n     \n  \n  \n    7 \n    M \n    Preregistration \n    11 \n     \n  \n  \n    7 \n    W \n    Meta-analysis \n    16 \n     \n  \n  \n    7 \n    F \n    Open science \n    3 \n     \n  \n  \n    8 \n    M \n    Visualization 1 \n    15 \n    E \n  \n  \n    8 \n    W \n    Visualization 2 \n     \n     \n  \n  \n    8 \n    F \n    Exploratory data analysis workshop \n     \n     \n  \n  \n    9 \n    M \n    Sampling, representativeness, and generalizability \n    4 \n     \n  \n  \n    9 \n    W \n    Data and participant ethics \n    12 \n     \n  \n  \n    9 \n    F \n    Authorship and research ethics \n     \n     \n  \n  \n    10 \n    M \n    Open discussion \n    17 \n     \n  \n  \n    10 \n    W \n    Final Project presentations 1 \n     \n     \n  \n  \n    10 \n    F \n    Final Project presentations 2 \n     \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable A.2: A sample 10-week syllabus with only content materials.\n\n\n\n\n \n  \n    Week \n    Day \n    Topic \n    Chapter \n  \n \n\n  \n    1 \n    M \n    Class introduction \n    1 \n  \n  \n    1 \n    W \n    Theories \n    2 \n  \n  \n    1 \n    F \n    Replication and reproducibility \n    3 \n  \n  \n    2 \n    M \n    Open Science \n     \n  \n  \n    2 \n    W \n    Measurement \n    8 \n  \n  \n    2 \n    F \n    Design of experiments 1 \n    9 \n  \n  \n    3 \n    M \n    Design of experiments 2 \n     \n  \n  \n    3 \n    W \n    Sampling \n    10 \n  \n  \n    3 \n    F \n    Experimental strategy \n     \n  \n  \n    4 \n    M \n    Preregistration \n    11 \n  \n  \n    4 \n    W \n    Data collection \n    12 \n  \n  \n    4 \n    F \n    Visualization 1 \n    15 \n  \n  \n    5 \n    M \n    Visualization 2 \n     \n  \n  \n    5 \n    W \n    Midterm exam \n     \n  \n  \n    5 \n    F \n    Introduction to statistics \n     \n  \n  \n    6 \n    M \n    Estimation 1 \n    5 \n  \n  \n    6 \n    W \n    Estimation 2 \n     \n  \n  \n    6 \n    F \n    Inference 1 \n    6 \n  \n  \n    7 \n    M \n    Inference 2 \n     \n  \n  \n    7 \n    W \n    Models 1 \n    7 \n  \n  \n    7 \n    F \n    Models 2 \n     \n  \n  \n    8 \n    M \n    Meta-analysis \n    16 \n  \n  \n    8 \n    W \n    Project management \n    13 \n  \n  \n    8 \n    F \n    [Instructor-specific topics] \n     \n  \n  \n    9 \n    M \n    Sampling, representativeness, and generalizability \n    4 \n  \n  \n    9 \n    W \n    Data and participant ethics \n    12 \n  \n  \n    9 \n    F \n    Authorship and research ethics \n     \n  \n  \n    10 \n    M \n    Conclusion \n    17 \n  \n  \n    10 \n    W \n    Conclusion \n     \n  \n  \n    10 \n    F \n    Final exam \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable A.3: A sample 16-week syllabus with both skills and content materials.\n\n\n\n\n \n  \n    Week \n    Day \n    Topic \n    Chapter \n    Appendix \n  \n \n\n  \n    1 \n    1 \n    Class introduction \n    1 \n     \n  \n  \n    1 \n    2 \n    Theories \n    2 \n     \n  \n  \n    2 \n    1 \n    Version control \n     \n    B \n  \n  \n    2 \n    2 \n    Reproducible reports \n    14 \n    C \n  \n  \n    3 \n    1 \n    Tidyverse tutorial \n     \n    D \n  \n  \n    3 \n    2 \n    Tidyverse tutorial continued (with TAs) \n     \n     \n  \n  \n    4 \n    1 \n    Measurement, reliability, and validity \n    8 \n     \n  \n  \n    4 \n    2 \n    Design of experiments \n    9 \n     \n  \n  \n    5 \n    1 \n    Sampling \n    10 \n     \n  \n  \n    5 \n    2 \n    Project management \n    13 \n     \n  \n  \n    6 \n    1 \n    Experiments 1: Simple survey experiments using Qualtrics \n     \n     \n  \n  \n    6 \n    2 \n    Experiments 2: Project-specific implementation (TAs) \n     \n     \n  \n  \n    7 \n    1 \n    Estimation \n    5 \n     \n  \n  \n    7 \n    2 \n    Inference \n    6 \n     \n  \n  \n    8 \n    1 \n    Sample size planning \n     \n     \n  \n  \n    8 \n    2 \n    Survey design \n     \n     \n  \n  \n    9 \n    1 \n    Midterm presentations 1 \n     \n     \n  \n  \n    9 \n    2 \n    Midterm presentations 2 \n     \n     \n  \n  \n    10 \n    1 \n    Preregistration \n    11 \n     \n  \n  \n    10 \n    2 \n    Meta-analysis \n    16 \n     \n  \n  \n    11 \n    1 \n    Open science \n    3 \n     \n  \n  \n    11 \n    2 \n    Visualization 1 \n    15 \n    E \n  \n  \n    12 \n    1 \n    Visualization 2 \n     \n     \n  \n  \n    12 \n    2 \n    Exploratory data analysis workshop \n     \n     \n  \n  \n    13 \n    1 \n    Sampling, representativeness, and generalizability \n    4 \n     \n  \n  \n    13 \n    2 \n    Data and participant ethics \n    12 \n     \n  \n  \n    14 \n    1 \n    Authorship and research ethics \n     \n     \n  \n  \n    14 \n    2 \n    [Instructor-specific topics] \n     \n     \n  \n  \n    15 \n    1 \n    Open discussion \n    17 \n     \n  \n  \n    15 \n    2 \n    Open discussion \n     \n     \n  \n  \n    16 \n    1 \n    Final project presentations 1 \n     \n     \n  \n  \n    16 \n    2 \n    Final project presentations 2 \n     \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable A.4: A sample 16-week syllabus with only content materials.\n\n\n\n\n \n  \n    Week \n    Day \n    Topic \n    Chapter \n  \n \n\n  \n    1 \n    1 \n    Class introduction \n    1 \n  \n  \n    1 \n    2 \n    Theories \n    2 \n  \n  \n    2 \n    1 \n    Replication and reproducibility \n    3 \n  \n  \n    2 \n    2 \n    Open science \n     \n  \n  \n    3 \n    1 \n    Measurement \n    8 \n  \n  \n    3 \n    2 \n    Design of experiments 1 \n    9 \n  \n  \n    4 \n    1 \n    Design of experiments 2 \n     \n  \n  \n    4 \n    2 \n    Sampling \n    10 \n  \n  \n    5 \n    1 \n    Experimental strategy \n     \n  \n  \n    5 \n    2 \n    Preregistration \n    11 \n  \n  \n    6 \n    1 \n    Data collection \n    12 \n  \n  \n    6 \n    2 \n    Visualization 1 \n    15 \n  \n  \n    7 \n    1 \n    Visualization 2 \n     \n  \n  \n    7 \n    2 \n    Midterm exam \n     \n  \n  \n    8 \n    1 \n    Introduction to statistics \n     \n  \n  \n    8 \n    2 \n    Estimation 1 \n    5 \n  \n  \n    9 \n    1 \n    Estimation 2 \n     \n  \n  \n    9 \n    2 \n    Inference 1 \n    6 \n  \n  \n    10 \n    1 \n    Inference 2 \n     \n  \n  \n    10 \n    2 \n    Models 1 \n    7 \n  \n  \n    11 \n    1 \n    Models 2 \n     \n  \n  \n    11 \n    2 \n    Meta-analysis \n    16 \n  \n  \n    12 \n    1 \n    Project management \n    13 \n  \n  \n    12 \n    2 \n    [Instructor-specific topics] \n     \n  \n  \n    13 \n    1 \n    [Instructor-specific topics] \n     \n  \n  \n    13 \n    2 \n    Sampling, representativeness, and generalizability \n    4 \n  \n  \n    14 \n    1 \n    Data and participant ethics \n     \n  \n  \n    14 \n    2 \n    Authorship and research ethics \n     \n  \n  \n    15 \n    1 \n    Ethics: Open discussion \n     \n  \n  \n    15 \n    2 \n    Conclusion \n    17 \n  \n  \n    16 \n    1 \n    Conclusion \n     \n  \n  \n    16 \n    2 \n    Final exam",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Instructor's guide</span>"
    ]
  },
  {
    "objectID": "101-github.html",
    "href": "101-github.html",
    "title": "appendix B — Git and GitHub",
    "section": "",
    "text": "B.1 Introduction\nHave you ever sent a collaborator what you think is a final copy of a manuscript, perhaps titled “Manuscript final” only to get back an updated version from them called “Manuscript final - JC”? So you make the requested changes and send it back to them as “Manuscript final - JC FINAL”, just as you receive an email from another collaborator with more edits in a file called “Manuscript final - MF”? If this sounds familiar, read on - we are here to help!\nTo begin, let us introduce you to Git.\nGit is software that you can install and run locally on your own computer, and it allows you to track changes to your files as you are working on them. Git helps you manage nightmare scenarios like the one described above, because it makes it easy to have different versions of the same document, to easily work by yourself on the same project on multiple computers, and to collaborate with other people (whether you’re working at different times or simultaneously)! In other words, Git is like an undo button, but with labels showing you what changes were made when and with a history that goes back to the very first change you ever made to the project. This makes it ideal for collaborative projects.\nBut Git, as a version control system, is even more powerful in combination with GitHub:\nGitHub provides the service of storing and sharing your Git repositories1 online. Anyone can get a free account on GitHub and they provide free premium accounts to students (see here).\nTo describe some of the most useful functionalities of GitHub, we will set the stage by describing GitHub as a place where you can store your work in folders called repositories. These repositories live in the “cloud,” in that they are accessible via internet (github.com) and therefore allows you to access them from any device.\nFor instance, imagine that your collaborator has a project called “hello” - they have stored all of their code, materials, and analyses into a repository called “hello” on GitHub:\nAs a new member of the project, you need to copy this repository onto your own local device (e.g., your laptop) so that you can inspect the repository and make your own changes. In GitHub speak, this initial step of copying the repository onto your own device is called “cloning” the repository:\nOnce you make your changes to the “hello” repository (e.g., add or delete code, add or remove files), the changes will exist in your local repository but you will need to take steps to have them be reflected in back in the “cloud” where your collaborators can see those changes. The first step you will need to do is to “add” and “commit” your changes. “Adding” your changes will take a snapshot of the changes you made. “Committing” your changes records these added changes. Note that these are preparatory steps and all your changes are still local:\nNow, you are ready to actually share your changes to the cloud, where all of your collaborators can see what you changed This step is called “pushing” to GitHub:\nSo far, we have described one user’s interaction with GitHub. And while using GitHub as a way to track your own changes is a good idea, GitHub’s real potential is unlocked once you have many users collaborating on the same project. This allows any collaborator to “pull” the most recent version of the repository at any time, keep track of who made what change at what time, and easily revert to previous versions:\nNow that we’ve covered the different functionalities of GitHub on a conceptual level, the next section will be a more practical tutorial for how to use GitHub in the ways described above.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#introduction",
    "href": "101-github.html#introduction",
    "title": "appendix B — Git and GitHub",
    "section": "",
    "text": "1 “repo” is short for “repository”– repo is like a folder with files in it (that are usually related as part of a project), with an associated history of changes over time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#review-basic-terminal-commands",
    "href": "101-github.html#review-basic-terminal-commands",
    "title": "appendix B — Git and GitHub",
    "section": "B.2 Review basic terminal commands",
    "text": "B.2 Review basic terminal commands\nIn this tutorial we’ll be working in Terminal. Here are a few useful commands to be aware of:\n\n(more here)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#install-git",
    "href": "101-github.html#install-git",
    "title": "appendix B — Git and GitHub",
    "section": "B.3 Install Git",
    "text": "B.3 Install Git\nGo to https://git-scm.com/downloads and install Git.\n(Windows users, open GitBash for the rest of the tutorial; Mac users, open Terminal.)\n\nB.3.1 Did you successfully install?\nIn terminal, type:\n\ngit --version\n\nto see the current version of Git that is installed.\n\n**Mac troubleshooting**\n\ntry installing: Git version 1.8.4.2\n\n\n\nB.3.2 Other versions\nThis tutorial will focus on how to use GitHub from the terminal, but if you prefer a simple point-and-click experience, you can install the GitHub desktop app. Here is a screenshot of what the desktop version looks like:\n\nOther options include SourceTree (free), Tower (not free, but powerful and ~$25 with a student discount). It’s also possible to set up a Git pane in RStudio.\n\n\nB.3.3 Set your name and email address\nEvery Git commit uses this information. Type:\n\ngit config --global user.name \"John Doe\"\n\n\ngit config --global user.email johndoe@example.com\n\nAlso run this once (it ensures that Git pushes in a sane manner):\n\ngit config --global push.default simple",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#make-a-repo-on-github-clone-it-to-your-computer",
    "href": "101-github.html#make-a-repo-on-github-clone-it-to-your-computer",
    "title": "appendix B — Git and GitHub",
    "section": "B.4 Make a repo on GitHub, clone it to your computer",
    "text": "B.4 Make a repo on GitHub, clone it to your computer\nMake an account on GitHub: https://github.com/.\nCreate a new empty public repository at https://github.com/new\nCall it ‘hello’ and make sure the “Add a README file” checkbox is checked2.\n2 For future reference (feel free to ignore this!): you can also turn any directory that’s already on your computer into a Git repo by going to that directory using cd and then typing git init. Later, when you want to put the repo on GitHub, you go through the steps to make a new repo without initializing with a readme, then from your directory on your computer type the following (replacing the red text):\n\n**git remote add origin git@&lt;span&gt;github.&lt;/span&gt;com:&lt;span style=\"color: red;\"&gt;your_user_name&lt;/span&gt;/&lt;span style=\"color: red;\"&gt;repo_name&lt;/span&gt;.git**\n\n\n**git push -u origin main**\n\n\nB.4.1 Setting up authentication\nGitHub needs a way of knowing whether or not your computer is authorized to read or write to the repository. For public repositories, anyone can clone (without authentication) but only authorized users (like you and collaborators you’ve added) can push changes. For private repos, only authorized users can see, clone, or push to the repo.\nThere are a few ways to autheticate with GitHub, depending on how you are accessing GitHub (all the options are outlined in more detail here, including instructions for GitHub Desktop).\nWe recommend setting up SSH keys since this only has to be set up once for each computer and let’s you interface with GitHub without having to type any passwords or do anything special. To do this you’ll first check if you already have appropriate SSH keys and then create a new key if needed. It’s best if you accept the default location for where the key is saved. If you don’t want to type a password each time, don’t enter a passphrase. Finally, you’ll add this SSH key to your GitHub account.\n\n\nB.4.2 Cloning the repo\nOn your repository website, click the green ‘Code’ button and select the SSH option. Copy the text.\nWhen using SSH keys, when you clone a repo, you’ll want to use the SSH option which will start with “git@github”. If you’re using a different authentication method, you will clone with the HTTPS option.\n\nGo back to Terminal (or GitBash).\nNow we’ll clone the hello folder to your computer. For the purposes of this tutorial, let’s clone the folder to your desktop.\nFirst use cd and ls to navigate to your desktop. (Mac users can type cd ~/Desktop to get there.)\nThen type (replacing the URL with the one you copied above):\n\ngit clone git@github.com:[username]/hello.git\n\nNow you will have a folder called ‘hello’ on your desktop, which contains one filed called ‘Readme’.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#make-some-commits",
    "href": "101-github.html#make-some-commits",
    "title": "appendix B — Git and GitHub",
    "section": "B.5 Make some commits",
    "text": "B.5 Make some commits\nWhat are “commits”? A commit is a snapshot of your project at a certain point in time. Each commit has an author, a time, a unique long ID (also called a ‘SHA’ or ‘hash’), and a message describing what change it makes.\n\nB.5.1 Update your README file\nA README file contains information about other files in a directory, and it’s customary to include one in your Git repo. Your README will be rendered from markdown on the front page of your GitHub repository (see here and here for an example).\nWhen you initialized your repo on GitHub, the site created an empty README file. Let’s write something in it.\nOpen the README file in a text editor3 and write a sentence or two describing your repo, and save your changes. (Read the basics of markdown and use it appropriately in formatting your README.)\n3 There will be lots of files that are simple text files, with .md, .rmd etc. as filetypes. You can open all these with a text editor. Although your computer comes with a default one (e.g., Notepad, TextEdit), we would recommend downloading Sublime Text, which is free and has a lot of powerful tools that will be helpful in the future.\n\nB.5.2 Add and commit changes to Git\nIn Terminal, navigate to the repo by typing:\n\ncd hello\n\nIf you type:\n\ngit status\n\nYou’ll get a message telling you that your README has been modified.\nNow we will add this file and commit it4 so that Git takes a snapshot of the changes we made:\n4 For future reference (feel free to ignore this!): you can add just part of the file by using git add -p README.md and following the instructions at the bottom of the terminal window.\ngit add README.md\n\n\ngit commit -m \"update readme\"\n\n(-m precedes a commit message, which allows you to describe what you changed.)\nIf you now type:\n\ngit status\n\nYou’ll get a message telling you that everything is up to date (“nothing to commit, working tree clean”).\n\n\nB.5.3 Add another file to the repo + commit it\nUse RStudio5 to make a new R script containing one line, e.g., print(\"hello world\"). Save this as ‘pset0.R’ inside the hello folder.\n5 We’ll be using R and RStudio in the future. If you do not yet have those downloaded, you can open up a text editor and do the same thing.Then, back in the terminal, type:\n\ngit status\n\nThis will tell you that a file called pset0.R exists, but isn’t being tracked by Git.6\n6 If you find a file called “.DS_Store” that is being tracked, that is a mac file saving folder preferences. You can add this to a .gitignore file as described in step 7 below so that Git will not track it.Now we will add this file and commit it (so that Git starts to track it):\n\ngit add -A\n\n(the -A specifies that we will add all of the files that have been changed in the repo)\n\ngit commit -m \"initial commit of pset0.R\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#push-your-changes-to-github",
    "href": "101-github.html#push-your-changes-to-github",
    "title": "appendix B — Git and GitHub",
    "section": "B.6 Push your changes to GitHub",
    "text": "B.6 Push your changes to GitHub\nWhat we’ve done so far—add and commit—only affects your local computer. To get your changes on GitHub, use push:\n\ngit push\n\nIf you go to your GitHub account, you can now see the updated files.\nYou can only push to the remote repository if the remote and local copies are in the same state, excepting the changes in the commits. This may be true if you are the only person who makes changes to the repo and you do so from only one computer, but that’s missing out on a lot of the potential to use GitHub for collaboration.\nTo get into a good habit, we recommend that you should pull (get changes from the remote to your local copy) right before you start making changes to files and right before you push your changes. To pull run the command\n\ngit pull --rebase\n\nThis will update your local copy to match the remote, leaving any changes you’ve made that are committed. (If you have uncommitted changes, you’ll get an error.) So, if your collaborator made edits or added files and pushed those changes, when you pull, your local copy with also have those updates.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#make-more-changes-to-the-repo",
    "href": "101-github.html#make-more-changes-to-the-repo",
    "title": "appendix B — Git and GitHub",
    "section": "B.7 Make more changes to the repo",
    "text": "B.7 Make more changes to the repo\nNow make some changes to your pset0.R file (delete and/or add another line or two of code) and save it.\nIn terminal, type\n\ngit status\n\nto see that pset0.R has been modified since the last commit.\nTo see the specific changes since the last commit, type:\n\ngit diff\n\nThen commit:\n\ngit add -A\n\n\ngit commit -m \"[describe change]\"\n\nPush to the repository on GitHub:\n\ngit push\n\nTIP: Commits should be focused. Try to commit little bite-sized changes that are all related to each other together and easy to label, and make separate commits for other changes.\nBest practice for commit messages is to make sure your commit message is not too long and would fit into the sentence: “When you pull this commit, it will ______.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#rolling-back-to-previous-versions",
    "href": "101-github.html#rolling-back-to-previous-versions",
    "title": "appendix B — Git and GitHub",
    "section": "B.8 Rolling back to previous versions",
    "text": "B.8 Rolling back to previous versions\nSometimes you will want to go back to a previous commit. Here’s how to do it:\nTo view previous commits, type:\n\ngit log\n\nTo change the number of displayed commits, type the number you want to see preceded by a dash. For example, to view the three most recent commits, type:\n\ngit log -3\n\n(You can also view the commit history on GitHub.)\nYou can use the long ID numbers attached to commits (also called hashes or SHAs) to roll back to them if you need to see a previous version of the repo. This can be very useful if something breaks and you don’t know how that happened. You can roll back to the last commit where your program wasn’t broken and see what files changed since then, and how.\nFor example, let’s say we wanted to roll back to the very first commit so we could run the code as it was back then. Let’s look at the very first commit. You can find it by typing git log and then pressing the space bar to scroll down to the very first commit. Copy and paste the hash for this commit, (press ‘q’ to get back to the main terminal window), and then type (replacing the hash with the one you copied):\n\ngit checkout a240f92a22cb8e9b1300bfa690e99ef07692151e\n\nor just\n\ngit checkout a240f92\n\n(Git is smart enough to figure out what commit you meant to type if you provide the first 8-10 characters of the hash.)\nIf you open up the hello folder on your desktop, you’ll notice that it’s now in the state it was after you made your first commit.\nIMPORTANT WARNING: After you’ve finished inspecting a checkout, make sure you get back to where you started [the latest commit on the main branch] by typing:\n\ngit checkout main\n\nTo revert your files to the state they were in in an earlier commit, type (replacing 0766c053 with the first 8-10 characters of the hash you copied):\n\ngit revert --no-commit 0766c053..HEAD\n\n\ngit commit -m \"revert all changes since first commit\"\n\nThis will essentially take all of the changes you made since this commit, undo them, and then save this as a new commit. (Your prior commits will still exist.)\n\nFor more info on undoing things in Git, check:\nhttps://github.com/blog/2019-how-to-undo-almost-anything-with-git.\n\nIf you’d like to revert one specific commit (rather than all of the commits after a specific commit), type:\n\ngit revert --no-commit 0766c053\n\n\ngit commit -m \"revert the commit where I did xyz...\"\n\nThese reversions are just more commits, so if you revert something (and commit it) and don’t want that change, you can revert the commit that reverted to get back to where you were before.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#what-not-to-put-on-git",
    "href": "101-github.html#what-not-to-put-on-git",
    "title": "appendix B — Git and GitHub",
    "section": "B.9 What not to put on Git",
    "text": "B.9 What not to put on Git\nThere are some things you don’t want on Git:\n\noutput files (files that are deterministically generated by other files in the repo, e.g., generated PDFs in a LaTeX project repo)\nlog files (like .RData and .Rhistory. You can’t describe what “changes” were made to them and different people’s .RData and .Rhistory files will always conflict.)\nsensitive data (like human subject data and passwords)\nconfiguration files that have configurations specific to your computer (Important: If you are running stuff on Mechanical Turk, make sure your bin/mturk.properties file is NOT on Git, because that file contains an access key to allow you to authenticate with Amazon.)\n\nYou can put these in a special .gitignore file so Git won’t suggest you add them and will even remind you not to add them if you try to. You can create this .gitignore file in a text editor like Sublime Text and update as needed.\nYour .gitignore file might look like this (saved exactly as .gitignore without a file extension):\n\n# R created files\n*.Rproj\n*.Rproj.user\n*.Rhistory\n*.Ruserdata\n*.history\n*.RData\n\n# Image/output files unless otherwise specified\n*.png\n*.docx\n*.doc\n*.jpg\n*.gif\n\n# Misc Knit Files\n*.aux\n*.gz\n*.log\n*.rdx\n*.rdb\n*.knit.md\n*cache\n*.results\n\n# Other\n*.httr-oauth\n*.DS_Store\n\n# MTurk credentials and data\nauth.json\nmy-own-auth.json\nmturk/\nmturk-and-gmail.txt\n\n# Specific file keep\n!README.md\n!/original....pdf\n.Rproj.user",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "101-github.html#further-resources",
    "href": "101-github.html#further-resources",
    "title": "appendix B — Git and GitHub",
    "section": "B.10 Further Resources",
    "text": "B.10 Further Resources\n\nGitHub has many useful guides for learning about branches, pull requests, forking and more: https://guides.github.com/ https://help.github.com/articles/good-resources-for-learning-git-and-github/\nThough the things we covered in this tutorial may seem overwhelming, there are really only a handful of commands that you need to know, which can be found (alongside some commands we didn’t cover) on this handy cheatsheet.\nRequest a premium account at https://education.github.com/ for free private repos.\nFor students seeking deeper Git knowledge, ProGit is a thorough open source book from Scott Chacon. It can be viewed online or downloaded in ePub, Mobi, or PDF formats.\n\nAcknowledgments: Thank you to Cayce Hook, Erin Bennett and Daniel Watson for creating the first version of this tutorial for Psych 251!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Git and GitHub</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html",
    "href": "102-rmarkdown.html",
    "title": "appendix C — R Markdown and Quarto",
    "section": "",
    "text": "C.1 Getting Started\nFire up Rstudio and create a new R Markdown file. Don’t worry about the settings, we’ll get to that later.\nIf you click on “Knit” (or hit CTRL+SHIFT+K) the R Markdown file will run and generate all results and present you with a PDF file, HTML file, or a Word file. If RStudio requests you to install packages, click yes and see whether everything works to begin with.\nWe need that before we teach you more about R Markdown. But you should feel good if you get here already, because honestly, you’re about 80% of the way to being able to write basic R Markdown files. It’s that easy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#getting-started",
    "href": "102-rmarkdown.html#getting-started",
    "title": "appendix C — R Markdown and Quarto",
    "section": "",
    "text": "exercises\n\n\n\n\n\nKnit the R Markdown template to Word and PDF to ensure that you can get this to work. Isn’t it gratifying?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#structure-of-an-r-markdown-file",
    "href": "102-rmarkdown.html#structure-of-an-r-markdown-file",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.2 Structure of an R Markdown file",
    "text": "C.2 Structure of an R Markdown file\nAn R Markdown file contains several parts. Most essential are the header, the body text, and code chunks. When you knit the resulting document, you will get the output—text combined with the results of running the core—in one of a number of output formats.\n\nC.2.1 Header\nHeaders in R Markdown files contain some metadata about your document, which you can customize to your liking. Below is a simple example that purely states the title, author name(s), date, and output format.3\n3 The header is written in “YAML”, which means “yet another markup language.” You don’t need to know that, and don’t worry about it. Just make sure you are careful with indenting, as YAML does care about that.---\ntitle: \"Untitled\"\nauthor: \"NAME\"\ndate: \"July 28, 2017\"\noutput: html_document\n---\n\n\n\nC.2.2 Body text\nThe body of the document is where you actually write your reports. This is primarily written in the Markdown format, which is explained in the Markdown syntax section.\nThe beauty of R Markdown is, however, that you can evaluate R code right in the text. To do this, you start inline code with `r, type the code you want to run, and close it again with a `. Usually, this key is below the escape (ESC) key or next to the left SHIFT button.\nFor example, if you want to have the result of 48 times 35 in your text, you type ` r 48-35`, which returns 13. Please note that if you return a value with many decimals, it will also print these depending on your settings (for example, 3.1415927).\n\n\nC.2.3 Code chunks\nIn the section above we introduced you to running code inside text, but often you need to take several steps in order to get to the result you need. And you don’t want to do data cleaning in the text! This is why there are code chunks. A simple example is a code chunk loading packages.\nFirst, insert a code chunk by going to Code-&gt;Insert code chunk or by pressing CTRL+ALT+I. Inside this code chunk you can then type for example, library(ggplot2) and create an object x.\nIf you do not want to have the contents of the code chunk to be put into your document, you include echo=FALSE at the start of the code chunk. We can now use the contents from the above code chunk to print results (e.g., \\(x=2\\)).\nThese code chunks can contain whatever you need, including tables, and figures (which we will go into more later). Note that all code chunks regard the location of the R Markdown as the working directory, so when you try to read in data use the relative path in.\n\n\nC.2.4 Output formats\nBy default, R Markdown renders to HTML format, the standard format of web pages. These output files are visible in the RStudio viewer and in any web-browser. These files can be shared on the web and are a great way to provide the outputs of your research to collaborators (e.g., sharing intermediate analytic results).\nThrough a program called pandoc, R Markdown can also render to Microsoft Word’s DOCX format. This functionality can be very useful for sharing editable writeups with collaborators (see below).\nFinally, rendering to PDF is useful If you want to create PDFs from R Markdown you need a installation on your computer. (Latex, or tex for short, is a powerful typesetting package). Many tex installations are available. One recent possibility is TinyTEX, a minimal tex installaction made for working with R Markdown. Or if you want a full install, try MikTeX for Windows, MacTeX for Mac, or TeX Live for Linux.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#markdown-syntax",
    "href": "102-rmarkdown.html#markdown-syntax",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.3 Markdown syntax",
    "text": "C.3 Markdown syntax\nMarkdown is one of the simplest document languages around, that is an open standard and can be converted into .tex, .docx, .html, .pdf, etc. This is the main workhorse of R Markdown and is very powerful. You can learn Markdown in five minutes. Other resources include this tutorial, and this cheat sheet.\nThese are the basics:\n\nIt’s easy to get *italic* or **bold**.\nYou can get headings using # heading1 for first level, ## heading2 for second-level, and ### heading3 for third level. Make sure you leave a space after the #!\nLists are delimited with * for each entry.\nYou can write links by writing [here's my link](http://foo.com).\n\nThe great thing about Markdown is that it works almost everywhere! Github, OSF, slack, many wikis, and even in text documents it looks pretty good.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#headers-graphs-and-tables",
    "href": "102-rmarkdown.html#headers-graphs-and-tables",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.4 Headers, graphs, and tables",
    "text": "C.4 Headers, graphs, and tables\n\nC.4.1 Headers\nWe’re going to want more libraries loaded (for now we’re loading them inline).\n\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(broom)\n\nWe often also add chunk options to each code chunk so that, for example:\n\ncode does or doesn’t display inline (echo setting)\nfigures are shown at various sizes (fig.width and fig.height settings)\nwarnings and messages are suppressed (warning and message settings)\ncomputations are cached (cache setting)\n\nThere are many others available as well. Caching can be very helpful for large files, but can also cause problems when there are external dependencies that change. An example that is useful for manuscripts is:\n\nopts_chunk$set(fig.width=8, fig.height=5, \n               echo=TRUE, \n               warning=FALSE, message=FALSE, \n               cache=TRUE)\n\n\n\n\n\n\n\nexercises\n\n\n\n\n\n\nOutlining using headings is a really great way to keep things organized! Try making a bunch of headings, and then recompiling your document.\nTo show off your headings from the previous exercise, add a table of contents. Go to the header of the document (the YAML), and add some options to the html document bit. You want it to look like this (indentation must to be correct):\n\noutput: \n  html_document:\n    toc: true\n\n\n\n\n\nC.4.2 Graphs\nIt’s really easy to include graphs, like this one. (Using the mtcars dataset that comes with ggplot2).\n\n\n\n\n\n\n\n\n\nAll you have to do is make the plot and it will render straight into the text.\nExternal graphics can also be included, as follows:\n\nknitr::include_graphics(\"path/to/file\")\n\n\n\nC.4.3 Tables\nThere are many ways to make good-looking tables using R Markdown, depending on your display purpose.\n\nThe knitr package (which powers R Markdown) comes with the kable function. It’s versatile and makes perfectly reasonable tables. It also has a digits argument for controlling rounding.\nFor HTML tables, there is the DT package, which provides datatable—these are pretty and interactive javascript-based tables that you can click on and search in. Not great for static documents though.\nFor APA manuscripts, it can also be helpful to use the xtable package, which creates very flexible LaTeX tables. These can be tricky to get right but they are completely customizable provided you want to google around and learn a bit about tex.\n\nWe recommend starting with kable. An expression like this:\n\nkable(head(mtcars), digits = 1)\n\nProduces tabular output like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.9\n2.6\n16.5\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.9\n2.9\n17.0\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.9\n2.3\n18.6\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.1\n3.2\n19.4\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.1\n3.4\n17.0\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.8\n3.5\n20.2\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n\nexercises\n\n\n\n\n\nUsing the mtcars dataset, insert a table and a graph of your choice into your R Markdown template document. If you’re feeling uninspired, try hist(mtcars$mpg).\n\n\n\n\n\nC.4.4 Statistics\nIt’s also really easy to include statistical tests of various types. One option is to use the broom package, which formats the outputs of various tests really nicely. Paired with knitr’s kable you can make very simple tables in just a few lines of code. This expression:\n\nmod &lt;- lm(mpg ~ hp + cyl, data = mtcars)\nkable(tidy(mod), digits = 3)\n\nproduces this output:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.908\n2.191\n16.847\n0.000\n\n\nhp\n-0.019\n0.015\n-1.275\n0.213\n\n\ncyl\n-2.265\n0.576\n-3.933\n0.000\n\n\n\n\n\nCleaning these tables up for publication can take some work. For example, we’d need to rename a bunch of fields to make this table have the labels we wanted (e.g., to turn hp into Horsepower).\nWe often need APA-formatted statistics to be printed in text, though. A good approach is to compute them first, and then print them inline. First, we’d run something like this:\n\nts &lt;- with(mtcars,t.test(hp[cyl==4], hp[cyl==6]))\n\nThen we’d print this:\n\nThere’s a statistically-significant difference in horsepower for 4- and 6-cylinder cars (\\(t(11.49) = -3.56\\), \\(p = 0.004\\)).\n\nWe did this via an inline code block: round(ts$parameter, 2).4\n4 APA would require omission of the leading zero. papaja::printp() will let you do that, see below.Rounding \\(p\\) values can occasionally get you in trouble. It’s very easy to have an output of \\(p = 0\\) when in fact \\(p\\) can never be exactly equal to 0. Nonetheless, this can help you prevent the kinds of rounding errors that would get picked up by software like statcheck.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#writing-apa-format-papers",
    "href": "102-rmarkdown.html#writing-apa-format-papers",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.5 Writing APA-format papers",
    "text": "C.5 Writing APA-format papers\nThe end-game of reproducible research is to knit your entire paper into a submittable APA-style writeup. Managing APA format is a pain in the best of times. The papaja package allows you to circumvent this task by rendering your manuscript directly from R Markdown.5\n\n\n\n\n5 Thanks to Frederick Aust for contributing much of the code in this section! For a bit more on papaja, check out this guide.papaja has not yet been released on CRAN but you can install it from GitHub.\n\n# Install devtools package if necessary\nif(!\"devtools\" %in% rownames(installed.packages())) install.packages(\"devtools\")\n\n# Install papaja\ndevtools::install_github(\"crsh/papaja\")\n\nThe APA manuscript template should now be available through the RStudio menus when creating a new R Markdown file.\nWhen you click RStudio’s Knit button papaja, rmarkdown, and knitr work together to create an APA conform manuscript that includes both your manuscript text and the results of any embedded R code.\n\n\n\n\n\n\n\n\n\n\n\nexercises\n\n\n\n\n\nMake sure you’ve got papaja, then open a new APA template file. Compile this document, and look at how awesome it is. Try pasting in your figure and table from your other R Markdown (don’t forget any libraries you need to make it compile).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#bibiographic-management",
    "href": "102-rmarkdown.html#bibiographic-management",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.6 Bibiographic management",
    "text": "C.6 Bibiographic management\nManaging a bibliography by hand is a lot of work. Letting software do this for you is much easier. In R Markdown it’s possible to include references using bibtex, by using @ref syntax. You can do this in papaja but it’s also possible to o it in other packages that have some kind of bibliographic handling.\nIt’s simple. You put together a set of paper citations in a bibtex file—then when you refer to them in text, the citations pop up formatted correctly, and they are also put in your bibliography. As an example, @nuijten2016 results in the in text citation “Nuijten et al. (2016)”, or cite them parenthetically with [@nuijten2016] (Nuijten et al. 2016). Take a look at the papaja APA example to see how this works.\nHow do you make your bibtex file? You can do it by hand but this is a pain. One option for managing references is bibdesk, which integrates with google scholar.6 citr is an R package that provides an easy-to-use RStudio addin that facilitates inserting citations. The addin will automatically look up the Bib(La)TeX-file(s) specified in the YAML front matter. The references for the inserted citations are automatically added to the documents reference section. Once citr is installed (install.packages(\"citr\")) and you have restarted your R session, the addin appears in the menus and you can define a keyboard shortcut to call the addin.\n6 Many other options are possible. For example, some of us use Zotero frequently as well.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#collaboration",
    "href": "102-rmarkdown.html#collaboration",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.7 Collaboration",
    "text": "C.7 Collaboration\nHow do we collaborate using R Markdown? There are lots of different workflows that people use. Here are a few:\n\nThe lead author makes a github repository with the markdown-formatted document in it. Others read the PDF and send text comments or PDF annotations and the lead makes modifications accordingly.7 This workflow works well when the lead author is relatively experienced and wants to keep control of the manuscript without too much line-by-line rewriting.\nThe lead author makes a repository as above, but coauthors collaborate either by pushing changes to master or by creating pull requests. This workflow works well when the authors are all fairly git-savvy, and can be great for quickly writing different parts in parallel because of git’s automatic merging.8\nThe authors work collaboratively together in an editor like Google Docs, Word, or Overleaf. (We favor cloud platforms rather than emailing back and forth, for all the reasons discussed in chapter 13). Once the substantive text sections have converged, the lead author puts that text back into the markdown document and adds references. This workflow is good for very collaborative introduction writing when coauthors don’t use Git or markdown. This workflow is a little clunky, but not too bad. And critically, all the figures and numbers get rendered fresh when you reknit, so nothing can get accidentally altered during the editing process.\nThe lead author renders the results section from markdown, then writes text in the resulting Word document (or uploads it to Google Docs). This workflow is closest to the “old way” that many people are used to, but runs the biggest risk of errors getting introduced and propagated forward, since it’s not possible to rerender the whole document from scratch. If someone makes changes to the results section, it’s critical to propagate these back to the markdown and keep the two in sync.\n\n7 Dropbox has good PDF annotation tools for writing comments on specific lines of text.8 We wrote this book using the all-github workflow, and it was pretty good, modulo some merge conflicts.In sum, there are lots of ways to collaborate—the best thing is to talk with your coauthors to select one that works for the group.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "102-rmarkdown.html#r-markdown-chapter-summary",
    "href": "102-rmarkdown.html#r-markdown-chapter-summary",
    "title": "appendix C — R Markdown and Quarto",
    "section": "C.8 R Markdown: Chapter summary",
    "text": "C.8 R Markdown: Chapter summary\nR Markdown is a great way to write reproducible papers. It is not too tricky to learn, and once you master it you can save time by reformatting quickly and automatically, managing your bibliography automatically, and even creating nice web-compatible documents.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "103-tidyverse.html",
    "href": "103-tidyverse.html",
    "title": "appendix D — Tidyverse",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar-title\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-navbar-title\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-next\"&gt;&lt;span class=\"chapter-number\"&gt;E&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;ggplot&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-prev\"&gt;&lt;span class=\"chapter-number\"&gt;C&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;R Markdown and Quarto&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/index.htmlPreface\"&gt;Preface&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"&gt;Foundations&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/001-experiments.html&lt;span-class=&#39;chapter-number&#39;&gt;1&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Experiments&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Experiments&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/002-theories.html&lt;span-class=&#39;chapter-number&#39;&gt;2&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Theories&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Theories&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/003-replication.html&lt;span-class=&#39;chapter-number&#39;&gt;3&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Replication&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Replication&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/004-ethics.html&lt;span-class=&#39;chapter-number&#39;&gt;4&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Ethics&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;4&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Ethics&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"&gt;Statistics&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/005-estimation.html&lt;span-class=&#39;chapter-number&#39;&gt;5&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Estimation&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;5&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Estimation&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/006-inference.html&lt;span-class=&#39;chapter-number&#39;&gt;6&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Inference&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;6&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Inference&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/007-models.html&lt;span-class=&#39;chapter-number&#39;&gt;7&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Models&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;7&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Models&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-3\"&gt;Planning&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/008-measurement.html&lt;span-class=&#39;chapter-number&#39;&gt;8&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Measurement&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;8&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Measurement&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/009-design.html&lt;span-class=&#39;chapter-number&#39;&gt;9&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Design&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;9&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Design&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/010-sampling.html&lt;span-class=&#39;chapter-number&#39;&gt;10&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Sampling&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;10&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Sampling&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-4\"&gt;Execution&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/011-prereg.html&lt;span-class=&#39;chapter-number&#39;&gt;11&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Preregistration&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;11&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Preregistration&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/012-collection.html&lt;span-class=&#39;chapter-number&#39;&gt;12&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Data-collection&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;12&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Data collection&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/013-management.html&lt;span-class=&#39;chapter-number&#39;&gt;13&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Project-management&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;13&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Project management&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-5\"&gt;Reporting&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/014-writing.html&lt;span-class=&#39;chapter-number&#39;&gt;14&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Writing&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;14&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Writing&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/015-viz.html&lt;span-class=&#39;chapter-number&#39;&gt;15&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Visualization&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;15&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Visualization&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/016-meta.html&lt;span-class=&#39;chapter-number&#39;&gt;16&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Meta-analysis&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;16&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Meta-analysis&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/017-conclusion.html&lt;span-class=&#39;chapter-number&#39;&gt;17&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Conclusion&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;17&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Conclusion&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-6\"&gt;Appendices&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/100-instructors.html&lt;span-class=&#39;chapter-number&#39;&gt;A&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Instructor&#39;s-guide&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;A&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Instructor’s guide&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/101-github.html&lt;span-class=&#39;chapter-number&#39;&gt;B&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Git-and-GitHub&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;B&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Git and GitHub&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/102-rmarkdown.html&lt;span-class=&#39;chapter-number&#39;&gt;C&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;R-Markdown-and-Quarto&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;C&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;R Markdown and Quarto&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/103-tidyverse.html&lt;span-class=&#39;chapter-number&#39;&gt;D&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Tidyverse&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;D&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Tidyverse&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/104-ggplot.html&lt;span-class=&#39;chapter-number&#39;&gt;E&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;ggplot&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;E&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;ggplot&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-breadcrumbs-Appendices\"&gt;Appendices&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-breadcrumbs-&lt;span-class=&#39;chapter-number&#39;&gt;D&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Tidyverse&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;D&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Tidyverse&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"sidebar-footer\"&gt;\n&lt;div class=\"sidebar-footer-item\"&gt;\n&lt;p&gt;© 2024. This work is openly licensed via &lt;a href=\"https://creativecommons.org/licenses/by-nc/4.0\"&gt;CC BY NC 4.0&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-metatitle\"&gt;&lt;span id=\"sec-tidyverse\" class=\"quarto-section-identifier\"&gt;appendix D — Tidyverse&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-twittercardtitle\"&gt;&lt;span id=\"sec-tidyverse\" class=\"quarto-section-identifier\"&gt;appendix D — Tidyverse&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-ogcardtitle\"&gt;&lt;span id=\"sec-tidyverse\" class=\"quarto-section-identifier\"&gt;appendix D — Tidyverse&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-metasitename\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) =&gt; {\n    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';\n    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';\n    let newTheme = '';\n    if(darkModeDefault) {\n      newTheme = isAlternate ? baseTheme : alternateTheme;\n    } else {\n      newTheme = isAlternate ? alternateTheme : baseTheme;\n    }\n    const changeGiscusTheme = () =&gt; {\n      // From: https://github.com/giscus/giscus/issues/336\n      const sendMessage = (message) =&gt; {\n        const iframe = document.querySelector('iframe.giscus-frame');\n        if (!iframe) return;\n        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');\n      }\n      sendMessage({\n        setConfig: {\n          theme: newTheme\n        }\n      });\n    }\n    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;\n    if (isGiscussLoaded) {\n      changeGiscusTheme();\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  const darkModeDefault = false;\n  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n    toggleGiscusIfUsed(toAlternate, darkModeDefault);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const onCopySuccess = function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  }\n  const getTextToCopy = function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {\n    text: getTextToCopy\n  });\n  clipboard.on('success', onCopySuccess);\n  if (window.document.getElementById('quarto-embedded-source-code-modal')) {\n    // For code content inside modals, clipBoardJS needs to be initialized with a container option\n    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)\n    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {\n      text: getTextToCopy,\n      container: window.document.getElementById('quarto-embedded-source-code-modal')\n    });\n    clipboardModal.on('success', onCopySuccess);\n  }\n    var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n    var mailtoRegex = new RegExp(/^mailto:/);\n      var filterRegex = new RegExp('/' + window.location.host + '/');\n    var isInternal = (href) =&gt; {\n        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n    }\n    // Inspect non-navigation links and adorn them if external\n \tvar links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');\n    for (var i=0; i&lt;links.length; i++) {\n      const link = links[i];\n      if (!isInternal(link.href)) {\n        // undo the damage that might have been done by quarto-nav.js in the case of\n        // links that we want to consider external\n        if (link.dataset.originalHref !== undefined) {\n          link.href = link.dataset.originalHref;\n        }\n      }\n    }\n  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n    const config = {\n      allowHTML: true,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start',\n    };\n    if (contentFn) {\n      config.content = contentFn;\n    }\n    if (onTriggerFn) {\n      config.onTrigger = onTriggerFn;\n    }\n    if (onUntriggerFn) {\n      config.onUntrigger = onUntriggerFn;\n    }\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      if (note) {\n        return note.innerHTML;\n      } else {\n        return \"\";\n      }\n    });\n  }\n  const xrefs = window.document.querySelectorAll('a.quarto-xref');\n  const processXRef = (id, note) =&gt; {\n    // Strip column container classes\n    const stripColumnClz = (el) =&gt; {\n      el.classList.remove(\"page-full\", \"page-columns\");\n      if (el.children) {\n        for (const child of el.children) {\n          stripColumnClz(child);\n        }\n      }\n    }\n    stripColumnClz(note)\n    if (id === null || id.startsWith('sec-')) {\n      // Special case sections, only their first couple elements\n      const container = document.createElement(\"div\");\n      if (note.children && note.children.length &gt; 2) {\n        container.appendChild(note.children[0].cloneNode(true));\n        for (let i = 1; i &lt; note.children.length; i++) {\n          const child = note.children[i];\n          if (child.tagName === \"P\" && child.innerText === \"\") {\n            continue;\n          } else {\n            container.appendChild(child.cloneNode(true));\n            break;\n          }\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(container);\n        }\n        return container.innerHTML\n      } else {\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        return note.innerHTML;\n      }\n    } else {\n      // Remove any anchor links if they are present\n      const anchorLink = note.querySelector('a.anchorjs-link');\n      if (anchorLink) {\n        anchorLink.remove();\n      }\n      if (window.Quarto?.typesetMath) {\n        window.Quarto.typesetMath(note);\n      }\n      // TODO in 1.5, we should make sure this works without a callout special case\n      if (note.classList.contains(\"callout\")) {\n        return note.outerHTML;\n      } else {\n        return note.innerHTML;\n      }\n    }\n  }\n  for (var i=0; i&lt;xrefs.length; i++) {\n    const xref = xrefs[i];\n    tippyHover(xref, undefined, function(instance) {\n      instance.disable();\n      let url = xref.getAttribute('href');\n      let hash = undefined; \n      if (url.startsWith('#')) {\n        hash = url;\n      } else {\n        try { hash = new URL(url).hash; } catch {}\n      }\n      if (hash) {\n        const id = hash.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note !== null) {\n          try {\n            const html = processXRef(id, note.cloneNode(true));\n            instance.setContent(html);\n          } finally {\n            instance.enable();\n            instance.show();\n          }\n        } else {\n          // See if we can fetch this\n          fetch(url.split('#')[0])\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.getElementById(id);\n            if (note !== null) {\n              const html = processXRef(id, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      } else {\n        // See if we can fetch a full url (with no hash to target)\n        // This is a special case and we should probably do some content thinning / targeting\n        fetch(url)\n        .then(res =&gt; res.text())\n        .then(html =&gt; {\n          const parser = new DOMParser();\n          const htmlDoc = parser.parseFromString(html, \"text/html\");\n          const note = htmlDoc.querySelector('main.content');\n          if (note !== null) {\n            // This should only happen for chapter cross references\n            // (since there is no id in the URL)\n            // remove the first header\n            if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n              note.children[0].remove();\n            }\n            const html = processXRef(null, note);\n            instance.setContent(html);\n          } \n        }).finally(() =&gt; {\n          instance.enable();\n          instance.show();\n        });\n      }\n    }, function(instance) {\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            div.style.left = 0;\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n        // Handle positioning of the toggle\n    window.addEventListener(\n      \"resize\",\n      throttle(() =&gt; {\n        elRect = undefined;\n        if (selectedAnnoteEl) {\n          selectCodeLines(selectedAnnoteEl);\n        }\n      }, 10)\n    );\n    function throttle(fn, ms) {\n    let throttle = false;\n    let timer;\n      return (...args) =&gt; {\n        if(!throttle) { // first call gets through\n            fn.apply(this, args);\n            throttle = true;\n        } else { // all the others get throttled\n            if(timer) clearTimeout(timer); // cancel #2\n            timer = setTimeout(() =&gt; {\n              fn.apply(this, args);\n              timer = throttle = false;\n            }, ms);\n        }\n      };\n    }\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;nav class=\"page-navigation\"&gt;\n  &lt;div class=\"nav-page nav-page-previous\"&gt;\n      &lt;a  href=\"/102-rmarkdown.html\" class=\"pagination-link\" aria-label=\"R Markdown and Quarto\"&gt;\n        &lt;i class=\"bi bi-arrow-left-short\"&gt;&lt;/i&gt; &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;C&lt;/span&gt;  &lt;span class='chapter-title'&gt;R Markdown and Quarto&lt;/span&gt;&lt;/span&gt;\n      &lt;/a&gt;          \n  &lt;/div&gt;\n  &lt;div class=\"nav-page nav-page-next\"&gt;\n      &lt;a  href=\"/104-ggplot.html\" class=\"pagination-link\" aria-label=\"ggplot\"&gt;\n        &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;E&lt;/span&gt;  &lt;span class='chapter-title'&gt;ggplot&lt;/span&gt;&lt;/span&gt; &lt;i class=\"bi bi-arrow-right-short\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/nav&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "104-ggplot.html",
    "href": "104-ggplot.html",
    "title": "appendix E — ggplot",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar-title\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-navbar-title\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-prev\"&gt;&lt;span class=\"chapter-number\"&gt;D&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Tidyverse&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/index.htmlPreface\"&gt;Preface&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"&gt;Foundations&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/001-experiments.html&lt;span-class=&#39;chapter-number&#39;&gt;1&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Experiments&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Experiments&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/002-theories.html&lt;span-class=&#39;chapter-number&#39;&gt;2&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Theories&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Theories&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/003-replication.html&lt;span-class=&#39;chapter-number&#39;&gt;3&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Replication&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Replication&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/004-ethics.html&lt;span-class=&#39;chapter-number&#39;&gt;4&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Ethics&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;4&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Ethics&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"&gt;Statistics&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/005-estimation.html&lt;span-class=&#39;chapter-number&#39;&gt;5&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Estimation&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;5&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Estimation&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/006-inference.html&lt;span-class=&#39;chapter-number&#39;&gt;6&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Inference&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;6&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Inference&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/007-models.html&lt;span-class=&#39;chapter-number&#39;&gt;7&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Models&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;7&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Models&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-3\"&gt;Planning&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/008-measurement.html&lt;span-class=&#39;chapter-number&#39;&gt;8&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Measurement&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;8&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Measurement&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/009-design.html&lt;span-class=&#39;chapter-number&#39;&gt;9&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Design&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;9&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Design&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/010-sampling.html&lt;span-class=&#39;chapter-number&#39;&gt;10&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Sampling&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;10&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Sampling&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-4\"&gt;Execution&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/011-prereg.html&lt;span-class=&#39;chapter-number&#39;&gt;11&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Preregistration&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;11&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Preregistration&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/012-collection.html&lt;span-class=&#39;chapter-number&#39;&gt;12&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Data-collection&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;12&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Data collection&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/013-management.html&lt;span-class=&#39;chapter-number&#39;&gt;13&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Project-management&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;13&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Project management&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-5\"&gt;Reporting&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/014-writing.html&lt;span-class=&#39;chapter-number&#39;&gt;14&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Writing&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;14&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Writing&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/015-viz.html&lt;span-class=&#39;chapter-number&#39;&gt;15&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Visualization&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;15&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Visualization&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/016-meta.html&lt;span-class=&#39;chapter-number&#39;&gt;16&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Meta-analysis&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;16&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Meta-analysis&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/017-conclusion.html&lt;span-class=&#39;chapter-number&#39;&gt;17&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Conclusion&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;17&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Conclusion&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-6\"&gt;Appendices&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/100-instructors.html&lt;span-class=&#39;chapter-number&#39;&gt;A&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Instructor&#39;s-guide&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;A&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Instructor’s guide&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/101-github.html&lt;span-class=&#39;chapter-number&#39;&gt;B&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Git-and-GitHub&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;B&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Git and GitHub&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/102-rmarkdown.html&lt;span-class=&#39;chapter-number&#39;&gt;C&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;R-Markdown-and-Quarto&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;C&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;R Markdown and Quarto&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/103-tidyverse.html&lt;span-class=&#39;chapter-number&#39;&gt;D&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;Tidyverse&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;D&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Tidyverse&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-int-sidebar:/104-ggplot.html&lt;span-class=&#39;chapter-number&#39;&gt;E&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;ggplot&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;E&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;ggplot&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-breadcrumbs-Appendices\"&gt;Appendices&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-breadcrumbs-&lt;span-class=&#39;chapter-number&#39;&gt;E&lt;/span&gt;--&lt;span-class=&#39;chapter-title&#39;&gt;ggplot&lt;/span&gt;\"&gt;&lt;span class=\"chapter-number\"&gt;E&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;ggplot&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"sidebar-footer\"&gt;\n&lt;div class=\"sidebar-footer-item\"&gt;\n&lt;p&gt;© 2024. This work is openly licensed via &lt;a href=\"https://creativecommons.org/licenses/by-nc/4.0\"&gt;CC BY NC 4.0&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-metatitle\"&gt;&lt;span id=\"sec-ggplot\" class=\"quarto-section-identifier\"&gt;appendix E — ggplot&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-twittercardtitle\"&gt;&lt;span id=\"sec-ggplot\" class=\"quarto-section-identifier\"&gt;appendix E — ggplot&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-ogcardtitle\"&gt;&lt;span id=\"sec-ggplot\" class=\"quarto-section-identifier\"&gt;appendix E — ggplot&lt;/span&gt; – Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-metasitename\"&gt;Experimentology&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) =&gt; {\n    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';\n    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';\n    let newTheme = '';\n    if(darkModeDefault) {\n      newTheme = isAlternate ? baseTheme : alternateTheme;\n    } else {\n      newTheme = isAlternate ? alternateTheme : baseTheme;\n    }\n    const changeGiscusTheme = () =&gt; {\n      // From: https://github.com/giscus/giscus/issues/336\n      const sendMessage = (message) =&gt; {\n        const iframe = document.querySelector('iframe.giscus-frame');\n        if (!iframe) return;\n        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');\n      }\n      sendMessage({\n        setConfig: {\n          theme: newTheme\n        }\n      });\n    }\n    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;\n    if (isGiscussLoaded) {\n      changeGiscusTheme();\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  const darkModeDefault = false;\n  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n    toggleGiscusIfUsed(toAlternate, darkModeDefault);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const onCopySuccess = function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  }\n  const getTextToCopy = function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {\n    text: getTextToCopy\n  });\n  clipboard.on('success', onCopySuccess);\n  if (window.document.getElementById('quarto-embedded-source-code-modal')) {\n    // For code content inside modals, clipBoardJS needs to be initialized with a container option\n    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)\n    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {\n      text: getTextToCopy,\n      container: window.document.getElementById('quarto-embedded-source-code-modal')\n    });\n    clipboardModal.on('success', onCopySuccess);\n  }\n    var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n    var mailtoRegex = new RegExp(/^mailto:/);\n      var filterRegex = new RegExp('/' + window.location.host + '/');\n    var isInternal = (href) =&gt; {\n        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n    }\n    // Inspect non-navigation links and adorn them if external\n \tvar links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');\n    for (var i=0; i&lt;links.length; i++) {\n      const link = links[i];\n      if (!isInternal(link.href)) {\n        // undo the damage that might have been done by quarto-nav.js in the case of\n        // links that we want to consider external\n        if (link.dataset.originalHref !== undefined) {\n          link.href = link.dataset.originalHref;\n        }\n      }\n    }\n  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n    const config = {\n      allowHTML: true,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start',\n    };\n    if (contentFn) {\n      config.content = contentFn;\n    }\n    if (onTriggerFn) {\n      config.onTrigger = onTriggerFn;\n    }\n    if (onUntriggerFn) {\n      config.onUntrigger = onUntriggerFn;\n    }\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      if (note) {\n        return note.innerHTML;\n      } else {\n        return \"\";\n      }\n    });\n  }\n  const xrefs = window.document.querySelectorAll('a.quarto-xref');\n  const processXRef = (id, note) =&gt; {\n    // Strip column container classes\n    const stripColumnClz = (el) =&gt; {\n      el.classList.remove(\"page-full\", \"page-columns\");\n      if (el.children) {\n        for (const child of el.children) {\n          stripColumnClz(child);\n        }\n      }\n    }\n    stripColumnClz(note)\n    if (id === null || id.startsWith('sec-')) {\n      // Special case sections, only their first couple elements\n      const container = document.createElement(\"div\");\n      if (note.children && note.children.length &gt; 2) {\n        container.appendChild(note.children[0].cloneNode(true));\n        for (let i = 1; i &lt; note.children.length; i++) {\n          const child = note.children[i];\n          if (child.tagName === \"P\" && child.innerText === \"\") {\n            continue;\n          } else {\n            container.appendChild(child.cloneNode(true));\n            break;\n          }\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(container);\n        }\n        return container.innerHTML\n      } else {\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        return note.innerHTML;\n      }\n    } else {\n      // Remove any anchor links if they are present\n      const anchorLink = note.querySelector('a.anchorjs-link');\n      if (anchorLink) {\n        anchorLink.remove();\n      }\n      if (window.Quarto?.typesetMath) {\n        window.Quarto.typesetMath(note);\n      }\n      // TODO in 1.5, we should make sure this works without a callout special case\n      if (note.classList.contains(\"callout\")) {\n        return note.outerHTML;\n      } else {\n        return note.innerHTML;\n      }\n    }\n  }\n  for (var i=0; i&lt;xrefs.length; i++) {\n    const xref = xrefs[i];\n    tippyHover(xref, undefined, function(instance) {\n      instance.disable();\n      let url = xref.getAttribute('href');\n      let hash = undefined; \n      if (url.startsWith('#')) {\n        hash = url;\n      } else {\n        try { hash = new URL(url).hash; } catch {}\n      }\n      if (hash) {\n        const id = hash.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note !== null) {\n          try {\n            const html = processXRef(id, note.cloneNode(true));\n            instance.setContent(html);\n          } finally {\n            instance.enable();\n            instance.show();\n          }\n        } else {\n          // See if we can fetch this\n          fetch(url.split('#')[0])\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.getElementById(id);\n            if (note !== null) {\n              const html = processXRef(id, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      } else {\n        // See if we can fetch a full url (with no hash to target)\n        // This is a special case and we should probably do some content thinning / targeting\n        fetch(url)\n        .then(res =&gt; res.text())\n        .then(html =&gt; {\n          const parser = new DOMParser();\n          const htmlDoc = parser.parseFromString(html, \"text/html\");\n          const note = htmlDoc.querySelector('main.content');\n          if (note !== null) {\n            // This should only happen for chapter cross references\n            // (since there is no id in the URL)\n            // remove the first header\n            if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n              note.children[0].remove();\n            }\n            const html = processXRef(null, note);\n            instance.setContent(html);\n          } \n        }).finally(() =&gt; {\n          instance.enable();\n          instance.show();\n        });\n      }\n    }, function(instance) {\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            div.style.left = 0;\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n        // Handle positioning of the toggle\n    window.addEventListener(\n      \"resize\",\n      throttle(() =&gt; {\n        elRect = undefined;\n        if (selectedAnnoteEl) {\n          selectCodeLines(selectedAnnoteEl);\n        }\n      }, 10)\n    );\n    function throttle(fn, ms) {\n    let throttle = false;\n    let timer;\n      return (...args) =&gt; {\n        if(!throttle) { // first call gets through\n            fn.apply(this, args);\n            throttle = true;\n        } else { // all the others get throttled\n            if(timer) clearTimeout(timer); // cancel #2\n            timer = setTimeout(() =&gt; {\n              fn.apply(this, args);\n              timer = throttle = false;\n            }, ms);\n        }\n      };\n    }\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;nav class=\"page-navigation\"&gt;\n  &lt;div class=\"nav-page nav-page-previous\"&gt;\n      &lt;a  href=\"/103-tidyverse.html\" class=\"pagination-link\" aria-label=\"Tidyverse\"&gt;\n        &lt;i class=\"bi bi-arrow-left-short\"&gt;&lt;/i&gt; &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;D&lt;/span&gt;  &lt;span class='chapter-title'&gt;Tidyverse&lt;/span&gt;&lt;/span&gt;\n      &lt;/a&gt;          \n  &lt;/div&gt;\n  &lt;div class=\"nav-page nav-page-next\"&gt;\n  &lt;/div&gt;\n&lt;/nav&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>ggplot</span>"
    ]
  }
]