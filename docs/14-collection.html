<!DOCTYPE html>
<html lang xmlLang><head><meta charSet="utf-8" /><meta name="generator" content="pandoc" /><meta name="viewport" content="width=device-width, initial-scale=1" /><meta property="og:title" content="Chapter 14 Data collection | Experimentology" /><meta property="og:type" content="book" /><meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" /><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><meta name="description" content="Chapter 14 Data collection | Experimentology" /><title>Chapter 14 Data collection | Experimentology</title><script src="libs/header-attrs-2.13/header-attrs.js"></script><link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" /><link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" /><link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" /><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" /><script src="libs/datatables-binding-0.21/datatables.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" /><link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" /><script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" /><script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><style type="text/css">code{white-space: pre;}</style><style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style><style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style><link rel="stylesheet" type="text/css" href="/assets/src/index.page.client.jsx.5f5402d5.css"></head><body>



<div class="row">
<div class="col-sm-12">
<header class="_toc_1lnsy_1" id="toc"><a class="_book_title_1lnsy_24" href="/">Experimentology: An Open Science Approach to Experimental Psychology Methods</a><nav><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Preliminaries</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="1-experiments">Experiments</a><a class="_chapter_title_1lnsy_32" href="2-theories">Theories</a><a class="_chapter_title_1lnsy_32" href="3-replication">Replication and reproducibility</a><a class="_chapter_title_1lnsy_32" href="4-ethics">Ethics</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Statistics</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="5-estimation">Estimation</a><a class="_chapter_title_1lnsy_32" href="6-inference">Inference</a><a class="_chapter_title_1lnsy_32" href="7-models">Models</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Design</div><div class="_part_title_rest_1lnsy_32"> and Planning</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="8-measurement">Measurement</a><a class="_chapter_title_1lnsy_32" href="9-design">Design of experiments</a><a class="_chapter_title_1lnsy_32" href="10-sampling">Sampling</a><a class="_chapter_title_1lnsy_32" href="11-strategy">Experimental strategy</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Execution</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="12-prereg">Preregistration</a><a class="_chapter_title_1lnsy_32" href="13-consent">Recruitment and Consent</a><a class="_chapter_title_1lnsy_32" href="14-collection">Data collection</a><a class="_chapter_title_1lnsy_32" href="15-management">Project management</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Analysis</div><div class="_part_title_rest_1lnsy_32"> and Reporting</div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="16-viz">Visualization</a><a class="_chapter_title_1lnsy_32" href="17-eda">Exploratory data analysis</a><a class="_chapter_title_1lnsy_32" href="18-writing">Writing</a><a class="_chapter_title_1lnsy_32" href="19-meta">Meta-analysis</a><a class="_chapter_title_1lnsy_32" href="20-conclusions">Conclusions</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Appendices</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="A-git">GitHub Tutorial</a><a class="_chapter_title_1lnsy_32" href="B-rmarkdown">R Markdown Tutorial</a><a class="_chapter_title_1lnsy_32" href="C-tidyverse">Tidyverse Tutorial</a><a class="_chapter_title_1lnsy_32" href="D-ggplot">ggplot Tutorial</a><a class="_chapter_title_1lnsy_32" href="E-instructors">Instructor’s guide</a></div></div></nav></header>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="collection" class="section level1" number="14">
<h1><span class="header-section-number">Chapter 14</span> Data collection</h1>
<div class="box learning_goals">
<ul>
<li>Review best practices for online and in person data collection</li>
<li>Implement data integrity checks, manipulation checks, and pilot testing</li>
</ul>
</div>
<p>You have selected your measure and manipulation and planned your sample. Your preregistration is set. You have gone through your recruitment and ethics. Now it’s time to think about the nuts and bolts of collecting data.</p>
<p>While the details of data collection may vary from context to context and sample to sample, this chapter will highlight some general best practices for the data collection process. We organize these practices around two goals. The first section is participant-centric: we review some concerns regarding how to provide a positive experience for participants in both in-person and online experiments. Then in the second section, we discuss the data collection process from the perspective of the experimenter, covering some best practices</p>
<div class="box case_study">
<p>(TITLE) The rise of online data collection</p>
<p>Since the rise of experimental psychology laboratories in university settings during the period after World War 2 <span class="citation">(<a href="#ref-benjamin-jr2000" role="doc-biblioref">Benjamin Jr, 2000</a>)</span>, experiments have typically been conducted by recruiting participants from what has been referred to as the “subject pool.” This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses <span class="citation">(<a href="#ref-sieber1989" role="doc-biblioref">Sieber &amp; Saks, 1989</a>)</span> recruited via the requirement that students complete a certain quantity of experiments as part of their course work.<label for="tufte-sn-163" class="margin-toggle sidenote-number">163</label><input type="checkbox" id="tufte-sn-163" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">163</span> At various times, students have raised ethical concerns about these requirements as being coercive of participationin precisely the way that should be off limits for psychology experiments (see Chapter <a href="4-ethics.html#ethics">4</a>). As a result, most programs now provide some more or less onerous alternative to participation.</span> The ready availability of this convenience population led inevitably to the massive over-representation of US undergraduates in published psychology research, leading to persistent critiques of this practice for the generalizability of research <span class="citation">(<a href="#ref-sears1986" role="doc-biblioref">Sears, 1986</a>; <a href="#ref-henrich2012" role="doc-biblioref"><strong>henrich2012?</strong></a>)</span>.</p>
<p>Yet in the period 2005–2015, there has been a revolution in data collection from convenience populations. Instead of focusing on university undergraduates, increasingly, published psychology work uses convenience samples of online workers recruited from crowdsourcing sites like Amazon Mechanical Turk (AMT). Originally designed to distribute micropayments to workers for business purposes like retyping reciepts, these services have become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates) <span class="citation">(<a href="#ref-anderson2019" role="doc-biblioref">C. A. Anderson et al., 2019</a>)</span> and this proportion is likely continuing to grow.</p>
<p>Online data collection</p>
<p><span class="citation">(<a href="#ref-buhrmester2016" role="doc-biblioref">Buhrmester et al., 2016</a>; <a href="#ref-mason2016" role="doc-biblioref"><strong>mason2016?</strong></a>)</span></p>
<p>Crump et al. (2013) show through a set of beautiful experiments designed in the web browser how online data collection can replicate effects initially found in the lab.</p>
</div>
<div id="the-participants-perspective" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> The participant’s perspective</h2>
<div id="data-collection-in-person" class="section level3" number="14.1.1">
<h3><span class="header-section-number">14.1.1</span> Data collection in person</h3>
</div>
<div id="data-collection-online" class="section level3" number="14.1.2">
<h3><span class="header-section-number">14.1.2</span> Data collection online</h3>
<p>Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser – alongside survey software like Qualtrics – can be a major aid to transparency in sharing experimental materials.</p>
<ul>
<li>Validating the process of collecting data online. We briefly review studies suggesting that for general data collection across many paradigms, online data collection is valid.</li>
<li>When is online not enough? We describe cases where in-person data collection is necessary, highlighting psychophysical and physiological measurement and social interaction as two common classes of experiments that still cannot be done effectively online.</li>
</ul>
<p>Class MTurk Guidelines
Intro
In this class we will be running our replication projects on Amazon’s Mechanical Turk (AMT/mTurk). mTurk is a platform on which Workers (or “Turkers”) can complete Human Intelligence Tasks (HITs) for monetary compensation. HITs are put up by Requesters. mTurk originally was set up to do large-scale tasks that require human intelligence (e.g. labeling photos, finding telephone numbers, etc), but has recently been used by social scientists to conduct (large-scale) online experiments.</p>
<p>In this class, all replication projects will be launched using a common class account. This saves you the hassle of applying for a personal Requester account (which lately has become not-as-straightforward…), and also simplifies funding logistics. We will be giving more detailed instructions in class on how to access the class account and launch HITs etc.</p>
<p>General notes to keep in mind:</p>
<p>Many Turkers multi-task and do tens of HITs for many hours a day, so design your study with this in mind. Include appropriate attention checks, manipulation checks, and exclusion criteria (depending on the original study design as well).
Amazon worker IDs are actually tied to their (public) Amazon account and thus constitute identifiable information. Keep Turk IDs and all other identifiable information private. Anonymize all data before pushing to your git project repo, especially if your repo is public. There are useful tools for quickly automatically anonymizing IDs so reach out to the TAs if you want help with this!
Do all analyses on anonymized data. (This is to prevent cases where others are unable to reproduce your analyses because it might rely somehow on identifiable information. If you start with anonymized data, your analyses would never use any of this information.)
Resources
Beginner:
A gentle guide to MTurk (Links to an external site.)
Guide to Mturk basics (Links to an external site.)
MTurkers are people (Links to an external site.) (&amp; the problem with common paradigms on MTurk)
Survey within MTurk platform (Links to an external site.)
Qualtrics Link (Links to an external site.)
Intermediate:
Turker Nation (Links to an external site.): Discussion board for Turkers
Creating a launch page to external site (Links to an external site.): advantage is that your study is embedded within MTurk, then opens a full page window for your task when required. Note launcher.html and task.html are separate.
Getting around the ridiculous extra 20% fee for 9+ participants (Links to an external site.)
MTurk fee structure (Links to an external site.) (Links to an external site.)
IRB
We have approval under protocol #IRB-23274Links to an external site.: “Reproducibility of psychological science and instruction.”</p>
<p>Before running your study, you will need to complete a short CITI human subjects training if you have not already. (See assignment).</p>
<p>Please include the following text on the first page / consent form of your study:</p>
<p>By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at <a href="mailto:stanfordpsych251@gmail.com" class="email">stanfordpsych251@gmail.com</a>. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you. We have recently been made aware that your public Amazon.com profile can be accessed via your worker ID if you do not choose to opt out. If you would like to opt out of this feature, you may follow instructions available here (Links to an external site.).</p>
<p>Please include a short debriefing in your experiment, thanking the participant, explaining in 2-4 lines what your study was about, and asking them not to share this information with other potential participants.</p>
<p>Additional notes on MTurk
Communicating with participants</p>
<p>When you are running either Pilot B or your actual study, please keep this gmail window open and monitor traffic on it.
If you get complaints about your study, please address them courteously and quickly (ideally, within a few hours). Turkers can be very helpful if you are responsive. Always assure them that they will be paid for their work.
Payment policies</p>
<p>When in doubt as to technical issues, pay the Turker. The TAs can help you bonus those who had technical issues with the experiment or completion code!
If a Turker seems especially difficult, then please bring their complaint to the attention of the course team. Turkers can potentially complain to IRB so err on the side of doing this more frequently if you have issues.</p>
<div class="box ethical_considerations">
<p>(TITLE) Best practices for online research</p>
<p>The rise of Amazon Mechanical Turk was a</p>
<ul>
<li>Fair payment.</li>
<li>Good user experience for participants</li>
<li>Clear communication</li>
</ul>
</div>
</div>
</div>
<div id="ensuring-high-quality-data" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> Ensuring high quality data</h2>
<p>In the second section of this chapter, we review a few key practices for</p>
<div class="box accident_report">
<p>(TITLE) Does data quality vary throughout the semester?</p>
<p>Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies. One infant development lab famously repainted their walls a particularly bright shade of blue and claimed that their studies did not yield significant findings (even replicating highly robust paradigms) until they went back to a more neutral color. …</p>
<p>The ManyLabs studies were a series of large-scale, collaborative studies that involved the same experimental protocol being run at a variety of different sites.</p>
</div>
<div id="run-pilot-studies" class="section level3" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> Run pilot studies</h3>
<p>A <strong>pilot study</strong> is a small study conducted before you collect your main sample. Smooth and successful data collection is typically difficult without piloting, at least the first time you do an experiment of a given type. Fundamentally, experiments induce a particular experience in their participants, and careful attention to the nature of that experience<label for="tufte-sn-164" class="margin-toggle sidenote-number">164</label><input type="checkbox" id="tufte-sn-164" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">164</span> Even if the experience is somewhat tedious, like searching for a T amongst Ls for hundreds of trials!</span> requires iterative development.</p>
<p>Pilot studies cannot tell you about expected effect size (as we discussed in Chapter <a href="10-sampling.html#sampling">10</a>). They also cannot tell you about the significance of your main result. What they <em>can</em> do is tell you about whether your paradigm works. They can reveal:
* if your code crashes under certain circumstances
* if your instructions confuse a substantial portion of your participants
* if you have a very high dropout rate
* if your data collection procedure fails to log variables of interest
* if participants are disgruntled by the end of the experiment</p>
<p>We recommend that all experimenters do – at the very minimum – two pilot studies before they launch their experiment.</p>
<p>The first pilot study, <strong>pilot A</strong><label for="tufte-sn-165" class="margin-toggle sidenote-number">165</label><input type="checkbox" id="tufte-sn-165" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">165</span> Good name, right?</span>, is a test with non-naive participants. Your parents can do this experiment, or in a pinch you can run yourself a bunch of times (though this isn’t preferable because you’re likely to miss a lot of aspects of the experience that you are habituated to, especially if you’ve been debugging the software). The goal of pilot A is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. This last goal means that you must <em>analyze</em> the data from pilot A, at least to the point of checking that the relevant data about each trial is logged.<label for="tufte-sn-166" class="margin-toggle sidenote-number">166</label><input type="checkbox" id="tufte-sn-166" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">166</span> At a minimum, for each trial you need to know a subject ID, a trial ID, the state of any manipulation (condition, trial type, etc.), and the value for the measure.</span></p>
<p>The second pilot study, <strong>pilot B</strong>, consists of a test of a small set of naive participants. Pilot size will depend on the costliness of running the experiment (in time, money, and opportunity cost) as well as your worries about the paradigm. If we’re talking about a short online survey experiment, then running a pilot of 10–20 people is reasonable. A more extensive laboratory study might be better served by piloting just two or three people. The goal of this second study is to understand properties of the participant experience: for example, were they confused? Did they withdraw before the study finished? You won’t have the numbers to make robust statistical inferences about these questions, but even a small number of pilots can tell you that your dropout rate is likely too high: if 5 of 10 pilot participants withdraw you may need to reconsider aspects of your design. It’s critical for pilot B that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over (“what did you think the study was about?” and “is there any way we could improve the experience of being in the study?” can be helpful questions).</p>
<p>Piloting is often an iterative process. We frequently launch studies for a pilot B, then recognise from the data or from participant feedback that they can be improved. We make tweaks and pilot again. Be careful not to overfit to small differences in pilot data – the samples are small and so inferences will not be robust. The process should be more like workshopping a manuscript to remove typos and make it read better than doing a study.</p>
<p>In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility can be to preregister a contingent strategy. For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate and not at any condition differences in the target measure. Then the registration could state that if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset including the initial pilot. This sort of registration can help you split the difference between cautious piloting and conservation of rare or costly data.</p>
</div>
<div id="keep-consistent-data-collection-records" class="section level3" number="14.2.2">
<h3><span class="header-section-number">14.2.2</span> Keep consistent data collection records</h3>
<p>Important to put checks in place on your data collection pipeline early.</p>
</div>
<div id="measure-participant-compliance" class="section level3" number="14.2.3">
<h3><span class="header-section-number">14.2.3</span> Measure participant compliance</h3>
<p>Data collection in the field: An opinionated discussion of common pitfalls of field experiments in psychology.</p>
<ul>
<li>Blinding and randomization. Fieldwork makes it harder to maintain these critical principles of experimental design, potentially leading to bias.</li>
<li>Reasoning about and combatting selection bias.</li>
</ul>
<p><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00998/full" class="uri">https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00998/full</a></p>
</div>
</div>
<div id="chapter-summary-data-collection" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> Chapter summary: Data collection</h2>
<p>In this brief chapter, we reviewed the process of data collection from two perspectives. From the particpant’s perspective, we emphasized fair payment, a good “user experience”, nad clear communication as the three key factors ensuring that they</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-anderson2019" class="csl-entry">
Anderson, C. A., Allen, J. J., Plante, C., Quigley-McBride, A., Lovett, A., &amp; Rokkum, J. N. (2019). The MTurkification of social and personality psychology. <em>Personality and Social Psychology Bulletin</em>, <em>45</em>(6), 842–850.
</div>
<div id="ref-benjamin-jr2000" class="csl-entry">
Benjamin Jr, L. T. (2000). The psychology laboratory at the turn of the 20th century. <em>American Psychologist</em>, <em>55</em>(3), 318.
</div>
<div id="ref-buhrmester2016" class="csl-entry">
Buhrmester, M., Kwang, T., &amp; Gosling, S. D. (2016). <em>Amazon’s mechanical turk: A new source of inexpensive, yet high-quality data?</em>
</div>
<div id="ref-sears1986" class="csl-entry">
Sears, D. O. (1986). College sophomores in the laboratory: Influences of a narrow data base on social psychology’s view of human nature. <em>Journal of Personality and Social Psychology</em>, <em>51</em>(3), 515.
</div>
<div id="ref-sieber1989" class="csl-entry">
Sieber, J. E., &amp; Saks, M. J. (1989). A census of subject pool characteristics and policies. <em>American Psychologist</em>, <em>44</em>(7), 1053.
</div>
</div>
<p style="text-align: center;">
<a href="13-consent.html"><button class="btn btn-default">Previous</button></a>
<a href="15-management.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



<script type="module" src="/assets/src/index.page.client.jsx.84ba7d8b.js"></script><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index"}}</script></body></html>
