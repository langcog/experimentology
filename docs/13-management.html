<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 13 Data management and sharing | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />



<meta name="description" content="Chapter 13 Data management and sharing | Experimentology">

<title>Chapter 13 Data management and sharing | Experimentology</title>

<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />




<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication and reproducibility</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-estimation.html#estimation"><span class="toc-section-number">4</span> Estimation</a></li>
<li><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a></li>
<li><a href="6-models.html#models"><span class="toc-section-number">6</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-preregistration.html#preregistration"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Replicating or extending an existing study</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Data management and sharing</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-github.html#github"><span class="toc-section-number">19</span> Github Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="management" class="section level1" number="13">
<h1><span class="header-section-number">Chapter 13</span> Data management and sharing</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Understand the risks of data identification,</li>
<li>List optimal data management practices,</li>
<li>describe important elements of version control</li>
<li>define FAIR data</li>
</ul>
</div>
<blockquote>
<p>Your closest collaborator is you six months ago, but you don‚Äôt reply to emails.</p>
<footer>
‚Äî Karl Broman (2016)
</footer>
</blockquote>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:versions"></span>
<img src="images/management/versions_xkcd.png" alt="Bad management creates chaos! By xkcd (https://xkcd.com/1459). Shared under CC BY-NC 2.5" width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 13.1: Bad management creates chaos! By xkcd (<a href="https://xkcd.com/1459" class="uri">https://xkcd.com/1459</a>). Shared under CC BY-NC 2.5<!--</p>-->
<!--</div>--></span>
</p>
<p>Have you ever had a filename that ended in <code>-FINAL</code>? And was that file <strong>really</strong> the final version? Have you ever been asked for information or files from a project that you completed months or years ago, and been confused by what you sew when you looked back on your hard drive? These experiences may make you sympathetic to Karl Broman‚Äôs quip: good practices for organizing your data help you first. But these same practices can enable open sharing of your research. This chapter is about the process of managing the documents ‚Äì in practice, computer files ‚Äì that result from research in ways that maximize their value to you and to the broader research community.</p>
<p>When we talk about research products, often our minds go to scientific journal publications, which have been the main method of communication for scientific results since the scientific revolution in the 1600s.<label for="tufte-sn-1" class="margin-toggle sidenote-number">1</label><input type="checkbox" id="tufte-sn-1" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">1</span> Arguably, this is the Transactions of the Royal Society, which first published in 1665.</span> But research produces many other products, from experimental stimulus items and scripts to data and analysis code. When shared appropriately, these other products can be at least as valuable as a paper reporting the primary result <span class="citation">(<label for="tufte-mn-1" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle">Piwowar 2013<span class="marginnote">Piwowar, Heather A. 2013. <span>‚ÄúValue All Research Products.‚Äù</span> <em>Nature</em> 493 (7431): 159‚Äì59. <a href="https://doi.org/10.1038/493159a">https://doi.org/10.1038/493159a</a>.</span>)</span>. Shared stimulus materials can be reused for new studies in creative ways; shared analysis scripts can allow for reproduction of reported results and become templates for new analyses; and shared data can enable new analyses or meta-analyses.</p>
<p>In recent years, there have been widespread calls for data sharing <span class="citation">(<label for="tufte-mn-2" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle">Lindsay 2017<span class="marginnote">Lindsay, D Stephen. 2017. <span>‚ÄúSharing Data and Materials in Psychological Science.‚Äù</span> SAGE PublicationsSage CA: Los Angeles, CA.</span>)</span>. Indeed, some form of data sharing is required of funded projects by many US and European funding agencies. Data sharing has been associated with benefits in terms of downstream citations <span class="citation">(<label for="tufte-mn-3" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle">Piwowar and Vision 2013<span class="marginnote">Piwowar, Heather A, and Todd J Vision. 2013. <span>‚ÄúData Reuse and the Open Data Citation Advantage.‚Äù</span> <em>PeerJ</em> 1: e175.</span>)</span>. And in surveys, researchers report openness to data sharing. Yet many researchers report that they do not share data regularly, and that much of their data is shared exclusively via risky practices such as exclusive storing on a personal computer or external drive where they are easily lost <span class="citation">(<label for="tufte-mn-4" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle">Tenopir et al. 2020<span class="marginnote">Tenopir, Carol, Natalie M. Rice, Suzie Allard, Lynn Baird, Josh Borycz, Lisa Christian, Bruce Grant, Robert Olendorf, and Robert J. Sandusky. 2020. <span>‚ÄúData Sharing, Management, Use, and Reuse: Practices and Perceptions of Scientists Worldwide.‚Äù</span> Edited by Sergi Lozano. <em><span>PLOS</span> <span>ONE</span></em> 15 (3): e0229003. <a href="https://doi.org/10.1371/journal.pone.0229003">https://doi.org/10.1371/journal.pone.0229003</a>.</span>)</span>. And as we have discussed in Chapter <a href="2-replication.html#replication">2</a>, even when data are shared they are not always formatted usably.</p>
<p>A critical benefit of good data management practices is that they enable reproducibility. As we discussed in Chapter <a href="2-replication.html#replication">2</a>, computational reproducibility involves being able to determine the provenance of any reported analytic result in a paper. That means being able to trace the chain from data collection to data files, and from analytic specifications to the reported summaries or visualizations. If data collection is documented appropriately, and if data are stored, organized, and shared, then the provenance of a particular result is relatively easy to verify. But once this chain is broken it can be very hard to reconstruct.</p>
<p>The goal of this chapter is to provide a guide for the management of data and other research products with a dual emphasis on transparency and organization. If you organize your project well, it is easy to share it later, and if you assume that you will be sharing, you will be motivated to organize your work better! This is a virtuous cycle.</p>
<p>We begin by discussing how to manage projects, with discussions of naming, organization, and version control. Then we zoom in specifically on data, which in many cases is the most valuable research product, and discuss best practices for data sharing. We end by discussing the question of what research products to share and some of the risks and benefits of sharing.<label for="tufte-sn-2" class="margin-toggle sidenote-number">2</label><input type="checkbox" id="tufte-sn-2" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">2</span> This chapter ‚Äì especially the last section ‚Äì draws heavily on <span class="citation"><label for="tufte-mn-5" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle">Klein et al. (2018)<span class="marginnote">Klein, Olivier, Tom Elis Hardwicke, Frederik Aust, Johannes Breuer, Henrik Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf Vanpaemel, and Michael C Frank. 2018. <span>‚ÄúA Practical Guide for Transparency in Psychological Science.‚Äù</span></span></span>, an article on research transparency that several us of contributed to.</span></p>
<div class="case-study">
<p>üî¨ Case study: ManyBabies, ManySpreadsheetFormats!</p>
<p>The ManyBabies project is an example of the ‚ÄúBig Team Science‚Äù in psychology. A group of developmental psychology researchers (including some of us) were worried about many of the issues of reproducibility, replicability, and experimental methods that we‚Äôve been discussing throughout this book, so they set up a large-scale collaboration to replicate key effects in developmental science. The first of these studies was ManyBabies 1 <span class="citation">(<label for="tufte-mn-6" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle">The ManyBabies Consortium et al. 2020<span class="marginnote">The ManyBabies Consortium, Michael C Frank, Katherine Jane Alcock, Natalia Arias-Trejo, Gisa Aschersleben, Dare Baldwin, St√©phanie Barbu, et al. 2020. <span>‚ÄúQuantifying Sources of Variability in Infancy Research Using the <span>Infant-Directed-Speech</span> Preference.‚Äù</span> <em>Advances in Methods and Practices in Psychological Science</em>.</span>)</span>, a study of infants‚Äô preference for baby-talk (also known as Infant Directed Speech).</p>
<p>The planning team expected a handful of labs to contribute but after a year-long data collection period, they ended up receiving data from 69 labs around the world! The outpouring of interest signaled a lot of enthusiasm from the community for this kind of collaborative science. But it also made for a tremendous data analysis headache. The organizers had created a set of spreadsheet templates for data upload, but they didn‚Äôt bargain for what they got back from participating labs. As researchers tried to accommodate their lab‚Äôs own idiosyncratic data formatting to the standard template, and then as the central analytic team tried to read all of these into a single analysis pipeline, all kinds of hilarity ensued <span class="citation">(<label for="tufte-mn-7" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle">Byers-Heinlein et al. 2020<span class="marginnote">Byers-Heinlein, Krista, Christina Bergmann, Catherine Davies, Michael C Frank, J Kiley Hamlin, Melissa Kline, Jonathan F Kominsky, et al. 2020. <span>‚ÄúBuilding a Collaborative Psychological Science: Lessons Learned from ManyBabies 1.‚Äù</span> <em>Canadian Psychology/Psychologie Canadienne</em> 61 (4): 349.</span>)</span>.</p>
<p>All of the changes that labs had made were reasonable ‚Äì altering column names for clarity, combining templates into a single Excel file, changing units (e.g., from seconds to milliseconds) ‚Äì but together they created a very challenging <strong>data validation</strong> problem for the analysis team, requiring many dozens of hours of coding and hand-checking. The data checking was critical, however ‚Äì one lab‚Äôs data that had been flagged during validation was eventually identified as having a fundamental issue in the coding of infant videos, leading to the painful decision to drop those data from the final dataset. In future ManyBabies projects, the group has committed to using data validation software to ensure that uploaded data files conform to project-specific templates before going forward with analysis. You can see some of the analysis pipeline and validation process in the <a href="https://github.com/manybabies/mb1-analysis-public">ManyBabies 1 public repository</a>.</p>
</div>
<div id="project-management" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Project Management</h2>
<p>There are a few basic ways that project management goes wrong for scientific projects. The first is that the project is not accessible, whether because a collaborator did not share it to begin with, because it was not backed up and a hard drive failed, because it was linked to one collaborator‚Äôs login and that information has been lost, or otherwise. The second is that the project is accessible but incomprehensible, often because it is unclear what version of a file is definitive, or its function within the project is unknown. This lack of clarity can happen because the project‚Äôs creator(s) are inaccessible for consultation or ‚Äì at least as often ‚Äì because the creators themselves can‚Äôt remember what they were doing back then. These kinds of problems lead to a tremendous loss of scientific value.</p>
<p>Further, they can be avoided via following a very simple schema for organizing project files. For those researchers that ‚Äúgrew up‚Äù managing their files locally and emailing versions of scripts and manuscripts back and forth with names like <code>analysis-FINAL-JS-rev1.xlsx</code>, a few aspects of this schema can be disconcerting. But if you have used a cloud-based provider to collaborate, this way of working will likely be fairly intuitive. Here are the principles:</p>
<ol style="list-style-type: decimal">
<li>There should be exactly one definitive copy of each document in the project, with its name denoting what it is. For example, <code>manuscript.Rmd</code> is the writeup of the project as a journal manuscript.</li>
<li>The location each document should be within a project folder whose sub-folders serve to identify uniquely the document‚Äôs function within the project. For example, <code>/analysis/experiment1/eye_tracking_preprocesssing.Rmd</code> is clearly the Markdown containing eye-tracking pre-processing for the analysis of data from Experiment 1.</li>
<li>The full project should be accessible to all collaborators archived across multiple storage devices, either via a version control platform (e.g., <a href="">github.com</a>) or cloud provider (e.g., dropbox, box, google drive) if possible, or via a distributed, automatic backup system otherwise.</li>
<li>The revision history of all text- and text-based documents (minimally, data, analysis code, and manuscript files) should be archived automatically.</li>
</ol>
<p>Keeping these principles in mind, we discuss best practices for project organization, version control, and file naming.<label for="tufte-sn-3" class="margin-toggle sidenote-number">3</label><input type="checkbox" id="tufte-sn-3" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">3</span> We choose this order because our recommended file naming conventions make more sense in light of our organizational and versioning recommendations.</span></p>
<div id="organizing-your-project" class="section level3" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> Organizing your project</h3>
<p>To the greatest extent possible, all of your files related to a project should be stored in the same place. Ideally, they should all be in the same project folder (with appropriate sub-folders), and they should be stored on the same provider.<label for="tufte-sn-4" class="margin-toggle sidenote-number">4</label><input type="checkbox" id="tufte-sn-4" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">4</span> There are certainly cases where this is impractical due to the limitations of different software packages. For example, in many cases a team will manage its data and analysis code via github but decide to write collaboratively using google docs, overleaf, or another collaborative platform. (It can also be hard to ask all collaborators to buy into a version control system if they are unused to it.) In that case, the final paper should still be linked in some way to the project repository. The only issue that comes up in using a split workflow like this is the need to ensure reproducible written products, a process we cover in Chapter <a href="16-writing.html#writing">16</a>.</span></p>
<p>Figure <a href="13-management.html#fig:management-organization-ex">13.2</a> shows an example project stored using the Open Science Framework. The top level folder contains folders for analyses, materials, raw and processed data (kept separately). It also contains the paper manuscript, and, critically, a README file in a text format that describes the project, the license, and any other meta-data that the authors would like to be associated with the research products.</p>
<div class="figure"><span style="display:block;" id="fig:management-organization-ex"></span>
<p class="caption marginnote shownote">
Figure 13.2: Sample top level folder structure for a project. From Klein et al., 2018. Original visible on the <a href="https://osf.io/xf6ug/">Open Science Framework</a>.
</p>
<img src="images/management/org-ex.png" alt="Sample top level folder structure for a project. From Klein et al., 2018. Original visible on the [Open Science Framework](https://osf.io/xf6ug/)." width="\linewidth"  />
</div>
<p>There‚Äôs no one way to organize folders, but the broad categories of materials, data, analysis, and writing are typically present in almost any experimental project. Further, many projects ‚Äì especially those that include multiple experiments or complex data types ‚Äì require subfolders. In our projects, it‚Äôs not uncommon to find paths like <code>/data/raw_data/exp1/surveys</code>. The key principle here is to create a hierarchical structure in which subfolders uniquely identify the part of the broader space of research products that are found inside them ‚Äì that is, <code>/data/raw_data/exp1</code> contains all the raw data from Experiment 1, and <code>/data/raw_data/exp1/surveys</code> contains all the raw <em>survey</em> data from that particular experiment.<label for="tufte-sn-5" class="margin-toggle sidenote-number">5</label><input type="checkbox" id="tufte-sn-5" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">5</span> If you‚Äôre interested, a more extensive guide to folder organization is found in the <a href="https://psych-transparency-guide.uni-koeln.de/folder-structure.html#root-folder">online supplement</a> to <span class="citation"><label for="tufte-mn-8" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle">Klein et al. (2018)<span class="marginnote">Klein, Olivier, Tom Elis Hardwicke, Frederik Aust, Johannes Breuer, Henrik Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf Vanpaemel, and Michael C Frank. 2018. <span>‚ÄúA Practical Guide for Transparency in Psychological Science.‚Äù</span></span></span>.</span></p>
</div>
<div id="versioning" class="section level3" number="13.1.2">
<h3><span class="header-section-number">13.1.2</span> Versioning</h3>
<p>Distributed version control is one of the great inventions of software engineering practice. Probably everyone who has ever collaborated electronically has experienced the frustration of editing a document, only to find out that you are editing the wrong version ‚Äì perhaps some of the problems you are working on have already been corrected, or perhaps the section you are adding has already been written by someone else. A second source of frustration comes when you take a wrong turn in a project, perhaps by reorganizing a manuscript in a way that doesn‚Äôt work or refactoring code in a way that turns out to be short-sighted.</p>
<p>These two classes of problems are solved effectively by modern version control systems. Here we focus on the use of git, which is perhaps the most widely used version control system. Git is a tool for creating and managing projects, which are called <strong>repositories</strong>. A git repository is a directory whose revision history is tracked via a series of <strong>commits</strong> ‚Äì snapshots of the state of the project. These commits can form a tree with different <strong>branches</strong>, as when two contributors to the project are working on two different parts simultaneously. These branches can later be <strong>merged</strong> either automatically or via manual intervention in the case of conflicting changes.</p>
<p>Practically speaking, a common git workflow is that the repository is hosted by an online service like <a href="http://github.com">github</a>. In that case, changes can be <strong>pushed</strong> to that hosted copy by one user and then <strong>pulled</strong> by another user. The hosted ‚Äúorigin‚Äù copy then is the definitive copy of the project. Appendix <a href="#git"><strong>??</strong></a> provides a practical introduction to how to use Git and Github, and there are a variety of good tutorials available online and in print <span class="citation">(<label for="tufte-mn-9" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle">Blischak, Davenport, and Wilson 2016<span class="marginnote">Blischak, John D, Emily R Davenport, and Greg Wilson. 2016. <span>‚ÄúA Quick Introduction to Version Control with Git and GitHub.‚Äù</span> <em>PLoS Computational Biology</em> 12 (1): e1004668.</span>)</span>.</p>
<p>This model of collaboration is designed to solve many of the problems we‚Äôve been discussing:</p>
<ul>
<li>A remotely hosted git repository is a cloud-based backup of your work, meaning it is less vulnerable to device failure.<label for="tufte-sn-6" class="margin-toggle sidenote-number">6</label><input type="checkbox" id="tufte-sn-6" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">6</span> Hard drive failure is still a surprisingly common source of missed deadlines!</span></li>
<li>By virtue of having versioning history, you have access to previous drafts in case you find you have been following a blind alley and want to roll back your changes.</li>
<li>By creating new branches, you can create another, parallel history for your project, so that you can try out major changes or additions without disturbing the main branch in the process.</li>
<li>A project‚Äôs commit history is labeled with each commit‚Äôs author and date, facilitating record keeping and collaboration.</li>
<li>Automatic merging can allow synchronous editing of different parts of a manuscript or codebase.<label for="tufte-sn-7" class="margin-toggle sidenote-number">7</label><input type="checkbox" id="tufte-sn-7" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">7</span> Version control isn‚Äôt magic, and if you and a collaborator edit the same paragraph or function, you will likely have to merge your changes by hand. But git will at least show you where the conflict is!</span></li>
</ul>
<p>Organizing a project repository for collaboration and hosting on a remote platform is an important first step towards sharing! Many of our projects are actually ‚Äúborn open‚Äù in the sense that we do all of our work on a publicly hosted repository for everyone to see <span class="citation">(<label for="tufte-mn-10" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle">Rouder 2015<span class="marginnote">Rouder, Jeffrey N. 2015. <span>‚ÄúThe What, Why, and How of Born-Open Data.‚Äù</span> <em>Behavior Research Methods</em> 48 (3): 1062‚Äì69. <a href="https://doi.org/10.3758/s13428-015-0630-z">https://doi.org/10.3758/s13428-015-0630-z</a>.</span>)</span>.This practice can feel uncomfortable when you first begin, but this discomfort soon vanishes as you realize that no one is actively looking at your in-progress project.<label for="tufte-sn-8" class="margin-toggle sidenote-number">8</label><input type="checkbox" id="tufte-sn-8" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">8</span> One concern that many people raise about sharing in-progress research openly is the possibility of ‚Äúscooping‚Äù ‚Äì that is, other researchers getting an idea or even data from the repository and writing a paper before you do. We have two responses to this concern. First, the empirical frequency of this sort of scooping is difficult to determine but likely very low ‚Äì we don‚Äôt know of any documented cases. Mostly, the problem is getting people to care about your experiment at all, not people caring so much that they would publish using your data or materials! In Gary King‚Äôs [words](<a href="https://www.youtube.com/watch?v=jD6CcFxRelY" class="uri">https://www.youtube.com/watch?v=jD6CcFxRelY</a></span>, ‚ÄúThe thing that matters the least is being scooped. The thing that matters the most is being ignored.‚Äù On the other hand, if you are in an area of research that you perceive to be competitive or where there is some significant risk of this sort, it‚Äôs very easy to make a repository private or even just to hold back key data files from the repository and share them only among collaborators. All of the benefits we described still accrue.] And the slightly greater level of scrutiny encourages good organization practices from the beginning. If this practice feels uncomfortable, most hosting services allow the creation of private repositories. Then, for an appropriately organized and hosted project, often the only steps required to share materials, data, and code is to make the hosted repository public and link it to an archival storage platform like the Open Science Framework.</p>
</div>
<div id="file-names" class="section level3" number="13.1.3">
<h3><span class="header-section-number">13.1.3</span> File names</h3>
<p>As <a href="https://www.karlton.org/2017/12/naming-things-hard/">Phil Karlton reportedly said</a>, ‚ÄúThere are only two hard things in Computer Science: cache invalidation and naming things.‚Äù What‚Äôs true for computer science is true for research in general.<label for="tufte-sn-9" class="margin-toggle sidenote-number">9</label><input type="checkbox" id="tufte-sn-9" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">9</span> We won‚Äôt talk about cache invalidation though.</span> Naming files is definitely hard (there is a guide to naming variables below). Naming a stand-alone file that you are emailing to someone is a mess ‚Äì our downloads folders are full of <code>manuscript-7 (copy).pdf</code> files. That‚Äôs a nightmare because there is no context to disambiguate! Some very organized people survive on systems like <code>info-r1-draft-2020-07-13-js.docx</code> - meaning, ‚Äúthe info project revision 1 draft of July 13th, 2020, with edits by Jada Smith.‚Äù<label for="tufte-sn-10" class="margin-toggle sidenote-number">10</label><input type="checkbox" id="tufte-sn-10" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">10</span> This kind of system is not bad, but it needs ground rules! What happens when PF also edits ‚Äì should she change the date or the revision number? Only if it‚Äôs major edits?</span></p>
<p>On the other hand, if you are naming a file in a hierarchically organized version control repository, the naming problem gets dramatically easier. All of a sudden, you have a context in which names make sense. <code>data.csv</code> is a terrible name for a data file on its own. But the name is actually perfectly informative ‚Äì in the context of a project repository with a README that states that there is only a single experiment, a repository structure such that the file lives in a folder called <code>raw_data</code>, and a = commit history that indicates the file‚Äôs commit date and author.</p>
<p>As this example shows, naming is hard <em>out of context</em>. So here‚Äôs our rule: name a file what it contains. Don‚Äôt use the name to convey the context of who edited it, when, or where it should go in a project.</p>
</div>
</div>
<div id="data-management" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Data Management</h2>
<p>We‚Äôve just discussed how to manage projects in general; in this section we zoom in on datasets specifically. Data are often the most valuable products for purposes of reproduction and reuse. Yet lots of research data are not reusable, even when they are shared. In Chapter <a href="2-replication.html#replication">2</a>, we discussed <span class="citation"><label for="tufte-mn-11" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle">Hardwicke et al. (2018)<span class="marginnote">Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne, George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al. 2018. <span>‚ÄúData Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal Cognition.‚Äù</span></span></span>‚Äôs study of analytic reproducibility. But before we were able to attempt to reproduce analytic results, we had to examine the data that were shared with a large sample of papers. In that study, although data sharing was mandated by the journal, only 64% of shared datasets were both complete and understandable.</p>
<p>How can you make sure that your data are managed so as to enable appropriate sharing? We make four primary recommendations, which we discuss here in turn. First, save your raw data! Second, document your data collection process. Third, organize your raw data for later analysis ‚Äì we provide guidance on organization for both spreadsheets and for data retrieved from software platforms. Fourth and finally, document your data format using a codebook or other appropriate metadata.</p>
<div id="save-your-raw-data" class="section level3" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Save your raw data</h3>
<p>Raw data take many forms. For many of us, the raw data are those returned by the experimental software; for others, the raw data are videos of the experiment being carried out. Regardless of the form of these data, save them! They are often the only way to check issues in whatever processing pipeline brings these data from their initial state to the form you analyze. They also can be invaluable for addressing critiques or questions about your methods or results later in the process. If you need to correct something about your raw data, <em>do not alter the original files</em>. Make a copy, and make a note about how the copy differs from the original. Future you will thank present you for explaining why there are two copies of subject 19‚Äôs data.</p>
<p>Raw data are often not anonymized. Anonymizing them sometimes means altering them (e.g., in the case of downloaded logs from a service that might include IDs or IP addresses). Or in some cases, anonymization is difficult or impossible without significant effort and loss of some value from the data, e.g.¬†for video data or MRI data <span class="citation">(<label for="tufte-mn-12" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle">Bischoff-Grethe et al. 2007<span class="marginnote">Bischoff-Grethe, Amanda, I Burak Ozyurt, Evelina Busa, Brian T Quinn, Christine Fennema-Notestine, Camellia P Clark, Shaunna Morris, et al. 2007. <span>‚ÄúA Technique for the Deidentification of Structural Brain MR Images.‚Äù</span> <em>Human Brain Mapping</em> 28 (9): 892‚Äì903.</span>)</span>. Unless you have specific permission for broad distribution of these identifiable data, the raw data may then need to be stored in a different way. In these cases, we recommend saving your raw data in a separate repository with the appropriate permissions. For example, in the ManyBabies 1 study we described above, the public repository does not contain the raw data contributed by participating labs, which the team could not guarantee was anonymized; these data are instead stored in a private repository.<label for="tufte-sn-11" class="margin-toggle sidenote-number">11</label><input type="checkbox" id="tufte-sn-11" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">11</span> The precise repository you use for this task is likely to vary by the kind of data that you‚Äôre trying to store and the local regulatory environment. For example, in the United States, to store de-anonymized data with certain fields requires a server that is certified for HIPAA (the relevant medical privacy law). Many ‚Äì but by no means all ‚Äì universities have cloud storage providers who </span> You can then use your repository‚Äôs README to describe what is and is not shared. For example, a note might read ‚ÄúWe provide anonymized versions of the files originally downloaded from Qualtrics‚Äù or ‚ÄúParticipants did not provide permission for public distribution of raw video recordings, which are retained on a secure university server.‚Äù Critically, if you still share the derived tabular data, it should still be possible to reproduce the analytic results in your paper, even if checking the provenance of those numbers from the raw data is not possible for every reader.</p>
<div class="figure"><span style="display:block;" id="fig:management-mb-datafiles"></span>
<p class="caption marginnote shownote">
Figure 13.3: Example participant (top) and trial (bottom) level data from the ManyBabies (2020) case study.
</p>
<img src="experimentology_files/figure-html/management-mb-datafiles-1.png" alt="Example participant (top) and trial (bottom) level data from the ManyBabies (2020) case study." width="\linewidth"  />
</div>
<p>One common practice is the use of participant identifiers to link specific experimental data ‚Äì which, if they are responses on standardized measures, rarely pose as significant an identifiability risk ‚Äì to demographic data sheets that might include more sensitive and potentially identifiable data.<label for="tufte-sn-12" class="margin-toggle sidenote-number">12</label><input type="checkbox" id="tufte-sn-12" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">12</span> A word about subject identifiers. These should be anonymous identifiers that cannot be linked to participant identities and are unique. You laugh, but one of us was in a lab where all the subject IDs were the date of test and the initials of the participant. These were neither unique nor anonymous. One common convention is to give your study a code-name and to number participants sequentially, so your first participant in a sequence of experiments on information processing might be <code>INFO-1-01</code>.</span> Depending on the nature of the analyses being reported, the experimental data can then be shared with limited risk. Then a small and well-vetted set of necessary demographic data can be distributed separately and joined back into the data later. This separation of participant-level information and trial-level information is also quite efficient because it means that if your data are in a ‚Äútidy‚Äù tabular form (see Appendix <a href="21-tidyverse.html#tidyverse">21</a>) the demographics are not repeated for every trial.</p>
</div>
<div id="document-your-data-collection-process" class="section level3" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Document your data collection process</h3>
<p>For a ‚Äúdata detective‚Äù ‚Äì this is often a future version of you ‚Äì raw data are invaluable, but they are not always enough to ensure that you know what happened in an experiment. A critical piece of contextualizing your raw data is making sure to document the data collection process so that you can review the experience that participants had in your experiment. Documentation of this experience can take many forms.</p>
<p>If the experimental experience was a web-based questionnaire, archiving this experience can be as simple as downloading the questionnaire source.<label for="tufte-sn-13" class="margin-toggle sidenote-number">13</label><input type="checkbox" id="tufte-sn-13" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">13</span> If it‚Äôs in a proprietary format like a Qualtrics <code>.QSF</code> file, a good practice is to convert it to a simple plain text format as well.</span> On the other hand, for many more involved studies it can be more difficult to reconstruct what participants went through. This kind of situation is where video data can shine <span class="citation">(<label for="tufte-mn-13" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle">Gilmore and Adolph 2017<span class="marginnote">Gilmore, Rick O, and Karen E Adolph. 2017. <span>‚ÄúVideo Can Make Behavioural Science More Reproducible.‚Äù</span> <em>Nature Human Behaviour</em>.</span>)</span>. A video recording of a typical experimental session can provide a valuable tutorial for other experimenters ‚Äì as well as good context for readers of your paper. This is doubly true if there is a substantial interactive element to your experimental experience, as is often the case for experiments with children. For example, the ManyBabies case study that we examined shared <a href="https://nyu.databrary.org/volume/896">‚Äúwalk through‚Äù videos of experimental sessions</a> for many of the participating labs, creating a repository of standard experiences for infant development studies. If nothing else, a video of an experimental session can sometimes be a very nice archive of a particular context.<label for="tufte-sn-14" class="margin-toggle sidenote-number">14</label><input type="checkbox" id="tufte-sn-14" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">14</span> It also makes a great thing to show in a talk, provided you have permission from your participant at the time.</span></p>
<p>Regardless of what other documentation you keep, it‚Äôs critical to create some record linking your data to the particular documentation you have. For the questionnaire study, for example, this documentation might be as simple as a README that says that the data in the <code>raw_data</code> directory were collected on a particular date using the file named <code>experiment1.qsf</code>. This kind of ‚Äúconnective tissue‚Äù linking data to materials can be very important when you return to a project with questions. If you spot an artifact in your data, you will want to be able to examine the precise version of the materials that you used to gather those data!</p>
</div>
<div id="organize-your-data-for-later-analysis-spreadsheet-version" class="section level3" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Organize your data for later analysis (spreadsheet version)</h3>
<p>There are many forms that data come in, but chances are that at some point during your project you will end up with a spreadsheet full of data. Good spreadsheets mean the difference between success and failure! If you organize your spreadsheet well, it will be easily accessible by future (meta-)analysts as well as your future self. If you organize it poorly, it will be difficult to extract data from automatically and may make it more likely for errors to be introduced into your dataset.</p>
<p>A wonderful article by <span class="citation"><label for="tufte-mn-14" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-14" class="margin-toggle">Broman and Woo (2018)<span class="marginnote">Broman, Karl W, and Kara H Woo. 2018. <span>‚ÄúData Organization in Spreadsheets.‚Äù</span> <em>The American Statistician</em> 72 (1): 2‚Äì10.</span></span> gives a guide to spreadsheet organization that lays out the principles of good spreadsheet design. We review some of the highlights of their principles here (giving our own, opinionated ordering):</p>
<div class="figure"><span style="display:block;" id="fig:management-broman-nonrect"></span>
<p class="caption marginnote shownote">
Figure 13.4: Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. From Broman and Woo (2018).
</p>
<img src="images/management/broman2018.png" alt="Examples of non-rectangular spreadsheet formats that are likely to cause problems in analysis. From Broman and Woo (2018)." width="\linewidth"  />
</div>
<ol style="list-style-type: decimal">
<li><p>Make it a rectangle. Nearly every analysis package from SPSS to R and Stata requires data to be tabular, meaning that each row is an observation or case (trial or subject) and each column is a variable. If you are used to analyzing data exclusively in a spreadsheet, this kind of tabular data isn‚Äôt quite as readable, but readable formatting gets in the way of almost any analysis you want to do. Figure <a href="13-management.html#fig:management-broman-nonrect">13.4</a> gives some examples of non-rectangular spreadsheets. All of these will cause any analytic package to choke because of inconsistencies in how rows and columns are used!</p></li>
<li><p>Choose good names for your variables. No one convention for name formatting is best, but it‚Äôs important to be consistent. We tend to follow the <a href="https://style.tidyverse.org">tidyverse style guide</a> and use lowercase words separated by underscores (<code>_</code>). It‚Äôs also polite (and good science) to give units where these are available, e.g.¬†are reaction times in seconds or milliseconds. Table <a href="13-management.html#tab:management-broman-ex">13.1</a> gives some examples of good and bad variable names.</p></li>
</ol>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:management-broman-ex">Table 13.1: </span>Examples of good and bad variable names. Adapted from Broman and Woo (2018).</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Good name</th>
<th align="left">Good alternative</th>
<th align="left">Avoid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">subject_id</td>
<td align="left">SubID</td>
<td align="left">subject #</td>
</tr>
<tr class="even">
<td align="left">sex</td>
<td align="left">female</td>
<td align="left">M/F</td>
</tr>
<tr class="odd">
<td align="left">rt_msec</td>
<td align="left">reaction_time_ms</td>
<td align="left">reaction time (millisec.)</td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: decimal">
<li><p>Be consistent with your cell formatting. Each column should have one <em>kind</em> of thing in it. For example, if you have numerical values in, don‚Äôt all of a sudden write ‚Äúmissing‚Äù in one of the cells. This kind of mixing of data types can cause havoc down the road. Mixed or multiple entries also don‚Äôt work, so don‚Äôt write ‚Äú0 (missing)‚Äù as the value of a cell. Leaving cells blank is also risky ‚Äì better to use a consistent value for missing data, e.g.¬†<code>NA</code> is what R uses. If you are writing dates, please be sure to use the ‚Äúglobal standard‚Äù (ISO 8601), which is YYY-MM-DD. Anything else can be misinterpreted easily.<label for="tufte-sn-15" class="margin-toggle sidenote-number">15</label><input type="checkbox" id="tufte-sn-15" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">15</span> Dates in Excel deserve special mention as a source of terribleness. Excel will import almost anything as a date and destroy its original content. This has caused unending horror in the genetics literature, where some gene names are routinely converted to dates <span class="citation">(<label for="tufte-mn-15" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-15" class="margin-toggle">Ziemann, Eren, and El-Osta 2016<span class="marginnote">Ziemann, Mark, Yotam Eren, and Assam El-Osta. 2016. <span>‚ÄúGene Name Errors Are Widespread in the Scientific Literature.‚Äù</span> <em>Genome Biology</em> 17 (1): 1‚Äì3.</span>)</span>. In fact, some gene names have had to be changed in order to avoid this issue!</span></p></li>
<li><p>Formatting isn‚Äôt data. If you highlight cells or color them, there is typically no legend for this formatting, so its interpretation is ambiguous. Further, this kind of formatting is typically program-specific so it will be missed by any user who opens the file in a different analysis program (e.g., by reading an Excel spreadsheet into R).</p></li>
<li><p>Save data in plain text files. The CSV (comma-delimited) file format is a common standard for easy-to-read data.<label for="tufte-sn-16" class="margin-toggle sidenote-number">16</label><input type="checkbox" id="tufte-sn-16" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">16</span> Be aware of some interesting differences in how these files are output by European vs.¬†American versions of Microsoft Excel! You might find semi-colons instead of commas in some datasets.</span> The advantage of CSVs is that they are not proprietary to Microsoft or another tech company, can be inspected in a text editor, and will not allow you to save all the kinds of formulas.</p></li>
</ol>
<p>We advise that you don‚Äôt analyze your data in Excel, as you‚Äôve seen in our previous chapters. But if you do feel the need to analyze your data in a spreadsheet program, we urge you to save the raw data as a separate CSV and then create distinct analysis spreadsheets so as to be sure to retain the raw data unaltered by your (or Excel‚Äôs) manipulations.</p>
</div>
<div id="organize-your-data-for-later-analysis-software-version" class="section level3" number="13.2.4">
<h3><span class="header-section-number">13.2.4</span> Organize your data for later analysis (software version)</h3>
<p>Many researchers do not create their data in a spreadsheet by entering individual values. Instead they receive data as the output from a platform, package, or device. These typically provide the researcher with only a modicum of control over the format of the resulting tabular data export. Case in point is the survey platform Qualtrics, which provides data with not one but two header rows, complicating import into almost every analysis package!<label for="tufte-sn-17" class="margin-toggle sidenote-number">17</label><input type="checkbox" id="tufte-sn-17" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">17</span> The R package <code>qualtRics</code> can help with this.</span></p>
<p>That said, if your platform <em>does</em> allow you to control what comes out, you can try to use the same principles of good tabular data design above. For example, try to give your variables (e.g., questions in Qualtrics) sensible names!</p>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Bad variable naming!</p>
<p>In our methods class, students often reproduce the analyses from a study before trying to replicate in a new sample. When Kengthsagn Louis looked at the code for the study she was interested in, she noticed that the inputs to the analysis were unnamed (presumably because they were output this way by the survey software). For example, one piece of Stata code looked like this!</p>
<pre verbatim="TRUE"><code>gen recall1=.
replace recall1=0 if Q21==1 
replace recall1=1 if Q21==3 | Q21==5 | Q21==6
replace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8
replace recall1=0 if Q69==1 
replace recall1=1 if Q69==3 | Q69==5 | Q69==6
replace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8
ta recall1</code></pre>
<p>In the process of translating this code into R in order to reproduce the analyses, Kengthsagn and a course TA, Andrew Lampinen, noticed that some responses had been assigned to derived variables incorrectly. Because the variables was not human-readable, this error was almost impossible to detect, but they double- and triple-checked. After being contacted, the article‚Äôs author ‚Äì to their credit ‚Äì issued an immediate correction since the variable coding problem affected some of the inferential conclusions of the article <span class="citation">(<label for="tufte-mn-16" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-16" class="margin-toggle">Petersen 2019<span class="marginnote">Petersen, Michael Bang. 2019. <span>‚Äú" Healthy Out-Group Members Are Represented Psychologically as Infected in-Group Members": Corrigendum.‚Äù</span></span>)</span>.</p>
<p>The moral of the story: if you can‚Äôt read the variable names your software outputs, it can be very hard to catch errors! Sometimes you can correct this within the software. If not, make sure to create a ‚Äúkey‚Äù and translate the names immediately, double checking after you are done. Otherwise you risk undetectable errors.</p>
</div>
</div>
<div id="document-the-format-of-your-data" class="section level3" number="13.2.5">
<h3><span class="header-section-number">13.2.5</span> Document the format of your data</h3>
<p>Even the best-organized tabular data are not always perfectly transparent after a couple of years. For that reason, best practices for data sharing include making a <strong>codebook</strong> (or equivalently, <strong>data dictionary</strong>) that documents what each variable is. Figure <a href="13-management.html#fig:management-mb-codebook">13.5</a> shows an example for the trial-level data in the bottom of Figure <a href="13-management.html#fig:management-mb-datafiles">13.3</a>. Each row describes one variable in the associated dataset, with some discussion of what variable type it is, what its levels are, and a human-readable explanation.</p>
<div class="figure"><span style="display:block;" id="fig:management-mb-codebook"></span>
<p class="caption marginnote shownote">
Figure 13.5: Codebook for trial-level data (see above) from the ManyBabies (2020) case study.
</p>
<img src="images/management/mb1-codebook.png" alt="Codebook for trial-level data (see above) from the ManyBabies (2020) case study." width="\linewidth"  />
</div>
<p>Creating a codebook need not require a lot of work. Almost any documentation is better than nothing! And there are several R packages that can automatically generate a codebook for you, for example <code>codebook</code>, <code>dataspice</code>, and <code>dataMaid</code> <span class="citation">(<label for="tufte-mn-17" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle">Arslan 2019<span class="marginnote">Arslan, Ruben C. 2019. <span>‚ÄúHow to Automatically Document Data with the Codebook Package to Facilitate Data Reuse.‚Äù</span> <em>Advances in Methods and Practices in Psychological Science</em> 2 (2): 169‚Äì87.</span>)</span>. Adding a codebook can substantially increase the reuse value of your data and prevent hours of frustration as future-you and others try to decode your variable names and assumptions.</p>
</div>
</div>
<div id="sharing-research-products" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Sharing Research Products</h2>
<p>As we‚Äôve been discussing throughout this chapter, once you‚Äôve managed your research products effectively, they are very easy to share with others. Further, sharing these products can be a win both for individual researchers and for the field as a whole. This section discusses some remaining questions about what (and what not) to share as well as where and how to share.</p>
<div id="what-you-can-and-cant-share" class="section level3" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> What you can and can‚Äôt share</h3>
<p>We‚Äôve been advocating that you share everything from your research, especially your data. But in practice there can be some obstacles to sharing that come up, and the first and most important of these is <strong>participant privacy</strong>. Unless they explicitly waive these rights, participants in psychology experiments have the expectation of privacy. Protecting their privacy is an important part of researchers‚Äô ethical burden <span class="citation">(<label for="tufte-mn-18" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle">Ross, Iguchi, and Panicker 2018<span class="marginnote">Ross, Michael W, Martin Y Iguchi, and Sangeeta Panicker. 2018. <span>‚ÄúEthical Aspects of Data Sharing and Research Participant Protections.‚Äù</span> <em>American Psychologist</em> 73 (2): 138.</span>)</span>. Furthermore, there are likely legal regulations that protect participants‚Äô data, though these vary from country to country. In the US, the relevant regulation is <strong>HIPAA</strong>, the Health Insurance Portability and Accountability Act, which limits disclosures of private health information (<strong>PHI</strong>). In the European Union, the relevant regulation is the European GDPR (General Data Protection Regulation). It‚Äôs beyond the scope of this book to give a full treatment of these regulatory frameworks; you should consult with your local IRB regarding compliance.</p>
<p>Under both frameworks, <strong>anonymization</strong> (or equivalently <strong>de-identification</strong>) of data is a key concept, such that data sharing is generally permissible if the data are considered to have met the relevant anonymization standard. Under US Department of Health and Human Services guidelines, researchers can follow the ‚Äúsafe harbor‚Äù standard<label for="tufte-sn-18" class="margin-toggle sidenote-number">18</label><input type="checkbox" id="tufte-sn-18" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">18</span> <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html">As described on the relevant DHHS page</a>.</span> under which data are considered to be anonymized if they do not contain any of 18 identifiers (including fields like names, telephone numbers, email addresses, social security numbers, dates of birth, faces, and other). Thus, data that only contain participant IDs can typically be shared under this framework.<label for="tufte-sn-19" class="margin-toggle sidenote-number">19</label><input type="checkbox" id="tufte-sn-19" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">19</span> US IRBs are a very de-centralized bunch and their interpretations often vary considerably. For reasons of liability or ethics, they may not allow data sharing even though it is permitted by US law. If you feel like arguing with an IRB that takes this kind of stand, you could mention that the DHHS rule actually doesn‚Äôt consider de-identified data to be ‚Äúhuman subjects‚Äù data at all, and thus the IRB may not have regulatory authority over it. We‚Äôre not lawyers, and we‚Äôre not sure if you‚Äôll succeed but it could be worth a try.</span> Under the EU‚Äôs GDPR, putting anonymous identifiers in a data file and removing identifiable fields does not itself suffice for anonymization if the data are still <strong>in-principle re-identifiable</strong> because you have maintained documentation linking IDs to identifiable data like names or email addresses. Only when the key linking identifiers to data has been destroyed are the data truly de-identified.</p>
<p>De-identification via the ‚Äúsafe harbor‚Äù standard described above is not always enough. As datasets get richer, <strong>statistical reidentification risks</strong> go up substantially such that, with a little bit of outside information, data can be matched with a unique individual. These risks are especially high with linguistic, physiological, and geospatial data, but they can be present even for simple behavioral experiments. In one influential demonstration, knowing a person‚Äôs location on two occasions was often enough to identify their data uniquely in a huge database of credit card transactions <span class="citation">(<label for="tufte-mn-19" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle">De Montjoye et al. 2015<span class="marginnote">De Montjoye, Yves-Alexandre, Laura Radaelli, Vivek Kumar Singh, et al. 2015. <span>‚ÄúUnique in the Shopping Mall: On the Reidentifiability of Credit Card Metadata.‚Äù</span> <em>Science</em> 347 (6221): 536‚Äì39.</span>)</span>. For an example closer to home, many of the contributing labs in the ManyBabies project logged the date of test for each participant. This useful and seemingly innocuous piece of information is unlikely to identify any particular participant ‚Äì but alongside a social media post about a lab visit or a dataset about travel records, it could easily reveal a participant‚Äôs identity. Thus, simply removing fields from the data is only a starting point. Before releasing data, researchers need to give some thought to whether any aspects of the data could lead to re-identification.</p>
<div class="ethics-box">
<p>üåø Ethics box: Really anonymous?</p>
<p>When we first began teaching Psych 251, our experimental methods course at Stanford, one of the biggest contributions of the course was simply showing students how to do experiments online. Amazon‚Äôs Mechanical Turk crowdsourcing service was relatively new, and our IRB did not have a good sense of what this service really was. We proposed that we would share data from the class and received approval for this practice. Our datasets were downloaded directly from Mechanical Turk and included participants‚Äô MTurk IDs (long alphanumeric strings that seemed completely anonymous). Several experiences caused us to reconsider this practice!</p>
<p>First, we discovered that MTurk IDs were in some cases linked to study participants‚Äô public Amazon ‚Äúwish lists,‚Äù which could both inadvertently provide information about the participant and also even potentially provide a basis for reidentification (in rare cases). This discovery led us to consult with our IRB and provide more explicit consent language in our class experiments, linking to instructions for making Amazon profiles private.</p>
<p>Then, a little later we received an irate email from an MTurk participant who had discovered their data on github via a search for their MTurk ID. Although they were not identified in this dataset, it convinced us that at least some participants would not like this ID shared. After another consultation with the IRB, we apologized to this individual and removed their and others‚Äô IDs from our github commit histories across that and other repositories. We now take care to anonymize IDs by creating a secret mapping between the IDs we post and the actual MTurk IDs prior to posting data.</p>
</div>
<div class="figure"><span style="display:block;" id="fig:management-sharing-chart"></span>
<p class="caption marginnote shownote">
Figure 13.6: A decision flow chart for thinking about sharing research products. From Klein et al.¬†(2018).
</p>
<img src="images/management/kline1.png" alt="A decision flow chart for thinking about sharing research products. From Klein et al. (2018)." width="\linewidth"  />
</div>
<p>Privacy issues are ubiquitous in data sharing, and almost every experimental research project will need to solve them before sharing data. For simple projects, often these are the only issues that preclude data sharing. However, in more complex projects, other concerns can arise. Funders may have specific mandates regarding where your data should be shared. Data use agreements or collaborator preferences may restrict where and when you can share. And certain data types require much more sensitivity since they are more consequential than, say, the reaction times on a Stroop task. We include here a flow chart (Figure <a href="13-management.html#fig:management-sharing-chart">13.6</a>) that walks through some of the options and mentions relevant concerns. When in doubt, it‚Äôs often a good idea to consult with the relevant local authority, e.g.¬†your IRB for ethical issues or your research management office for regulatory issues.</p>
</div>
<div id="where-and-how-to-share-if-you-like-it-then-youve-gotta-put-a-license-on-it" class="section level3" number="13.3.2">
<h3><span class="header-section-number">13.3.2</span> Where and how to share (‚ÄúIf you like it then you‚Äôve gotta put a license on it‚Äù)</h3>
<p>For shared products<label for="tufte-sn-20" class="margin-toggle sidenote-number">20</label><input type="checkbox" id="tufte-sn-20" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">20</span> Most of this discussion is about data, because that‚Äôs where the community has focused its efforts. That said, almost everything here applies to other research products as well!</span> to be used by others, they should be FAIR: Findable, Accessible, Interoperable, and Reusable <span class="citation">(<label for="tufte-mn-20" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle">Wilkinson et al. 2016<span class="marginnote">Wilkinson, Mark D, Michel Dumontier, I Jsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. <span>‚ÄúThe <span>FAIR</span> Guiding Principles for Scientific Data Management and Stewardship.‚Äù</span> <em>Sci Data</em> 3 (March): 160018.</span>)</span>. Findability means that they need to have clear metadata about what they are and need to be indexed by search engines. Accessibility means that they need to be archived across the long term and retrievable by a standardized identifier, e.g.¬†a digital object identifier (DOI).<label for="tufte-sn-21" class="margin-toggle sidenote-number">21</label><input type="checkbox" id="tufte-sn-21" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">21</span> DOIs are those long URL-like things that are associated with papers. Turns out they can also be associated with datasets and other products. Critically, they are guaranteed to work to find stuff, whereas standard web URLs often go stale after several years when people refactor their website.</span> Interoperability means that the metadata needs to be in a format that people (and search engines) can handle. And reusable means that the data need to be appropriately organized, annotated, and licensed.</p>
<p>To make your research products FAIR, you need to share them in a data repository that‚Äôs designed according to these principles. Personal websites don‚Äôt cut it, since these sites tend to go out of date and disappear. There‚Äôs also no easy way to find data on personal sites unless you know who created them. Github, though it‚Äôs a great platform for collaboration, isn‚Äôt a FAIR repository ‚Äì for one thing, products there don‚Äôt have DOIs ‚Äì and there are no archival guarantees on datasets that are shared there. And ‚Äì perhaps surprisingly for some researchers ‚Äì journal supplementary materials are also not a great place to put data. Often they have no unique DOI or meta-data, and they often change their URLS, leading data becoming unavailable <span class="citation">(<label for="tufte-mn-21" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle">Evangelou, Trikalinos, and Ioannidis 2005<span class="marginnote">Evangelou, Evangelos, Thomas A Trikalinos, and John PA Ioannidis. 2005. <span>‚ÄúUnavailability of Online Supplementary Scientific Information from Articles Published in Major Journals.‚Äù</span> <em>The FASEB Journal</em> 19 (14): 1943‚Äì44.</span>)</span>.</p>
<p>We recommend you share your research products using a repository that is specifically for scientific data sharing. There are many repositories that help you conform to FAIR standards. Zenodo, figshare, the Open Science Framework, and the various Dataverse sites are designed for this purpose, though there are likely other domain-specific repositories that will be relevant for different subfields of psychology. OSF is FAIR compatible and allows users to assign DOIs to their data and provide appropriate metadata.</p>
<p>We also recommend you put a license on your research products. Academic culture is (usually) unburdened by a lot of discussion of intellectual property and legal rights. In the world of ideas, there are scholarly norms about citation and attribution, and someone can be rightly miffed if you don‚Äôt acknowledge their contributions to the literature. But they won‚Äôt usually sue you. On the other hand, when you create research products and let people reuse them, you are entering into a slightly different space. Perhaps you created software that a company would like to use. Maybe a pediatrician would like to use a research instrument you‚Äôve been working on to assess their patients. These applications (and many other reuses of the data) require a legal license. In practice, there are a number of simple, open source licenses that permit reuse. We tend to favor <a href="https://creativecommons.org">Creative Commons licenses</a>, which come in a variety of flavors such as <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY</a> (which allows reuse as long as there is attribution) and <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY-NC</a> (which only allows attributed, non-commercial reuse). Regardless of what license you choose, having a license means that your products won‚Äôt be in a ‚Äúnot sure what I‚Äôm allowed to do with this‚Äù limbo for others who are interested in reusing them.</p>
<p>As we have discussed, we think you should consider simply working with your project in a public repository. This ‚Äúopenness by default‚Äù approach makes things very straightforward and provides a valuable incentive to organize your work. On the other hand, it can feel exposed for some researchers, and it does carry some risks, however small, of ‚Äúscooping‚Äù or pre-emption by other groups working in the same space. For this reason, you can consider working in a private project repository and making it public only later. There are two plausible times when release of materials make sense: either at the time of submission of a paper or at time of publication.<label for="tufte-sn-22" class="margin-toggle sidenote-number">22</label><input type="checkbox" id="tufte-sn-22" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">22</span> If you do not release materials until publication, the lack of accessibility of code and data can play a role in the review process. Norms are changing rapidly on this topic, so your experience will likely differ from journal to journal, but at least some reviewers will wonder why they can‚Äôt see data and code for their review.</span> In general, the earlier you share your materials the more impactful they are likely to be; our view is that if a citeable preprint exists, there is no risk associated with early release.</p>
</div>
</div>
<div id="chapter-summary-1" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Chapter summary</h2>
<p>All of the hard work you put into your experiments ‚Äì not to mention the contributions of your participants ‚Äì can be undermined by bad data and project management. As our accident reports and case study show, bad organizational practices can at a minimum cause huge headaches. Sometimes the consequences can be even worse. On the flip side, starting with a firm organizational foundation sets your experiment up for success. These practices also make it easier to share all of the products of your research, not just your findings. Such sharing is both useful for individual researchers and for the field as a whole.</p>
<!-- TODO: Barriers to adoption of transparent practices. -->
<!-- ::: {.accident-report} -->
<!-- ‚ö†Ô∏è Accident report: security practices for databases (how not to get hit by a ransomware attack) -->
<!-- ::: -->

</div>
</div>



<p style="text-align: center;">
<a href="12-collection.html"><button class="btn btn-default">Previous</button></a>
<a href="14-viz.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
