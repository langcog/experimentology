<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 7 Measurement | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 7 Measurement | Experimentology">

<title>Chapter 7 Measurement | Experimentology</title>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="toc/toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-intro.html#intro"><span class="toc-section-number">1</span> Introduction: theory and experiments</a></li>
<li><a href="2-replication.html#replication"><span class="toc-section-number">2</span> Replication, reproducibility and transparency</a></li>
<li><a href="3-ethics.html#ethics"><span class="toc-section-number">3</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="4-inference.html#inference"><span class="toc-section-number">4</span> Estimation and inference</a></li>
<li><a href="5-models.html#models"><span class="toc-section-number">5</span> Statistical models</a></li>
<li><a href="6-regression.html#regression"><span class="toc-section-number">6</span> Regression Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="7-measurement.html#measurement"><span class="toc-section-number">7</span> Measurement</a></li>
<li><a href="8-design.html#design"><span class="toc-section-number">8</span> Design of experiments</a></li>
<li><a href="9-sampling.html#sampling"><span class="toc-section-number">9</span> Sampling</a></li>
<li><a href="10-preregistration.html#preregistration"><span class="toc-section-number">10</span> Preregistration</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-selection.html#selection"><span class="toc-section-number">11</span> Replicating or extending an existing study</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Data management and sharing</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-eda.html#eda"><span class="toc-section-number">15</span> Exploratory data analysis</a></li>
<li><a href="16-writing.html#writing"><span class="toc-section-number">16</span> Reproducible writing</a></li>
<li><a href="17-meta.html#meta"><span class="toc-section-number">17</span> Meta-analysis</a></li>
<li><a href="18-conclusions.html#conclusions"><span class="toc-section-number">18</span> Conclusions</a></li>
<li class="part"><span><b>VI Appendices</b></span></li>
<li><a href="19-github.html#github"><span class="toc-section-number">19</span> Github Tutorial</a></li>
<li><a href="20-rmarkdown.html#rmarkdown"><span class="toc-section-number">20</span> R Markdown Tutorial</a></li>
<li><a href="21-tidyverse.html#tidyverse"><span class="toc-section-number">21</span> Tidyverse Tutorial</a></li>
<li><a href="22-ggplot.html#ggplot"><span class="toc-section-number">22</span> ggplot Tutorial</a></li>
<li><a href="23-instructors.html#instructors"><span class="toc-section-number">23</span> Instructor‚Äôs Guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="measurement" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Measurement</h1>
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Analyze measurement reliability</li>
<li>Provide validity arguments</li>
<li>Reason about tradeoffs between different measures and measure types</li>
<li>Articulate risks of measurement flexibility and the costs and benefits of multiple measures</li>
<li>Identify well-constructed survey questions</li>
</ul>
</div>
<p>The goal of an experiment is to make a (maximally precise and unbiased) measurement of a particular causal effect of interest. In this next section of the book, we‚Äôre going to try to figure out how to do that. This chapter focuses on the topic of measurement.<label for="tufte-sn-35" class="margin-toggle sidenote-number">35</label><input type="checkbox" id="tufte-sn-35" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">35</span> As a topic, measurement is actually much less well-discussed in experimental contexts compared with, say, observational studies. As far as we can tell, this is a sociological fact, not a scientific one. No matter whether you can manipulate the world directly (as in an experiment) or whether you are doing observational or quasi-experimental research, good measurement is the name of the game.</span> To paraphrase <span class="citation"><label for="tufte-mn-52" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-52" class="margin-toggle">Stevens (1946)<span class="marginnote">Stevens, S S. 1946. <span>‚ÄúOn the Theory of Scales of Measurement.‚Äù</span> <em>Science</em> 103 (2684): 677‚Äì80.</span></span>, measurement is the practice of putting numbers to things.</p>
<!-- TODO: LOOK UP PHIL OF SCIENCE REFS ON THIS. -->
<p>No matter where you are working in the sciences, you need to measure things. If you‚Äôre doing physics or chemistry, you need to be able to measure physical quantities; if you‚Äôre doing biology you might measure populations or lifespan as well as a host of physical quantities. Proper measurement instruments are incredibly important for this kind of work.<label for="tufte-sn-36" class="margin-toggle sidenote-number">36</label><input type="checkbox" id="tufte-sn-36" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">36</span> A lot could be said, of course, about the transformative value of better measurement instruments in the sciences! </span> Psychology and the behavioral sciences are no different ‚Äì we need proper measurement instruments. The difference is that in psychology, we are typically trying to measure something that‚Äôs inside the heads of our participants, which we call a <strong>latent construct</strong>.</p>
<p>Not all measurements are created equal. This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement ‚Äì repeat and assess variation ‚Äì though as we‚Äôll see below it‚Äôs a little trickier. Measurement instruments that have less error are called more <strong>reliable</strong> instruments.<label for="tufte-sn-37" class="margin-toggle sidenote-number">37</label><input type="checkbox" id="tufte-sn-37" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">37</span> Is <strong>reliability</strong> the same as <strong>precision</strong>? Yes, more or less. Confusingly, different fields call these concepts different things <span class="citation">(there‚Äôs a helpful table of these names in <label for="tufte-mn-53" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-53" class="margin-toggle">Brandmaier et al. 2018<span class="marginnote">Brandmaier, Andreas M, Elisabeth Wenger, Nils C Bodammer, Simone K√ºhn, Naftali Raz, and Ulman Lindenberger. 2018. <span>‚ÄúAssessing Reliability in Neuroimaging Research Through Intra-Class Effect Decomposition (ICED).‚Äù</span> <em>Elife</em> 7: e35718.</span>)</span>. Here we‚Äôll talk about reliability as a property of instruments specifically while using the term precision to talk about the measurements themselves.</span></p>
<p>When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But, as we saw in Chapter <a href="1-intro.html#intro">1</a>, things are much trickier when the construct we are trying to measure can‚Äôt be assessed directly. We have to measure something observable ‚Äì our <strong>operationalization</strong> of the construct ‚Äì and then make an argument about how it relates to the construct of interest. We call this argument an argument for the <strong>validity</strong> of the measure.<label for="tufte-sn-38" class="margin-toggle sidenote-number">38</label><input type="checkbox" id="tufte-sn-38" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">38</span> We are also going to talk in Chapter <a href="8-design.html#design">8</a> about the validity of manipulations. The way you identify a causal effect on some measure is by operationalizing some construct as well. If this is done badly, the manipulation can be invalid ‚Äì meaning the causal effect that‚Äôs measured doesn‚Äôt map onto the construct.</span></p>
<p>These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is. Let‚Äôs start by taking a look at an example of the challenge of measuring a particular latent construct, children‚Äôs early language ability. We can use this example to understand the concepts of reliability and validity.</p>
<div class="case-study">
<p>üî¨ Case study: A reliable and valid measure of children‚Äôs vocabulary</p>
<p>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age. Others struggle to produce words but clearly show evidence of understanding. And yet others show deficits in both producing and understanding language. Further, this variation appears to be linked to later outcomes ‚Äì children whose very early language processing is slower and whose vocabularies are smaller tend to do worse in school years later <span class="citation">(<label for="tufte-mn-54" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-54" class="margin-toggle">Marchman and Fernald 2008<span class="marginnote">Marchman, Virginia A, and Anne Fernald. 2008. <span>‚ÄúSpeed of Word Recognition and Vocabulary Knowledge in Infancy Predict Cognitive and Language Outcomes in Later Childhood.‚Äù</span> <em>Developmental Science</em> 11 (3): F9‚Äì16.</span>)</span>. Thus, there are many reasons why you‚Äôd want to make precise measurements of children‚Äôs early language ability as a latent construct of interest.<label for="tufte-sn-39" class="margin-toggle sidenote-number">39</label><input type="checkbox" id="tufte-sn-39" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">39</span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled <span class="citation">(<label for="tufte-mn-55" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-55" class="margin-toggle">Frank et al. 2021<span class="marginnote">Frank, Michael C, Mika Braginsky, Daniel Yurovsky, and Virginia A Marchman. 2021. <em>Variability and Consistency in Early Language Learning: The Wordbank Project</em>. MIT Press.</span>)</span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related ‚Äì you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!</span></p>
<p>As with all developmental research, there are many constraints on measurement that are imposed by the age of the children you want to work with. You can‚Äôt give toddlers a multiple-choice test! So to measure early language ability (to be concrete, let‚Äôs say for children under two and a half years old), you have roughly three options open. First, you can do some kind of observation of them and transcribe their language production ‚Äì this could be a play session in the lab or at home, with an experimenter or with a parent or other caregiver. Second, you could do some kind of direct assessment, e.g.¬†by asking them to point or look at the referent of a word (e.g.¬†‚Äúlook at the kitty‚Äù) and record their responses using video, a tablet, or even eye-tracking technology <span class="citation">(<label for="tufte-mn-56" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-56" class="margin-toggle">Frank et al. 2016<span class="marginnote">Frank, Michael C, Elise Sugarman, Alexandra C Horowitz, Molly L Lewis, and Daniel Yurovsky. 2016. <span>‚ÄúUsing Tablets to Collect Data from Young Children.‚Äù</span> <em>Journal of Cognition and Development</em> 17 (1): 1‚Äì17.</span>)</span>. Or, you could ask their parents about their language, for example sending a questionnaire like the MacArthur Bates Communicative Development Inventory (CDI for short), which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (and other items, which we‚Äôll ignore for now); the first page of an English form is shown in the margin.</p>
<p><label for="tufte-mn-57" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-57" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/measurement/cdi.jpg"/> The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children‚Äôs early language.</span></span></p>
<p>To decide which of these methods to use for a specific study, we need to think through the properties of these measurement instruments ‚Äì both in terms of reliability and validity and also in terms of their practicality for a specific research situation. Practicalities matter! For example, observational measurement can be extremely costly in both time and money because it not only requires a visit of some sort (to the home or the lab) but also transcription of speech, which is quite time-consuming ‚Äì often taking 5-10 minutes of work to transcribe a single minute of speech. Direct assessment still requires a lab or home visit, but scoring is typically more straightforward. Finally, parent report ‚Äì (e)mailing a questionnaire to the parent ‚Äì is extremely time- and cost-effective.</p>
<p>We don‚Äôt want to use CDI questionnaires if they are a bad measurement instrument. How can we tell? This is where assessment of reliability and validity are critical. In practice, the task of selecting and justifying a measurement instrument comes down to an argument about reliability and validity.</p>
<p>How reliable is the CDI? As we‚Äôll discuss more below, there‚Äôs no single answer to this question. Not only are there multiple ways to compute reliability, but also reliability in practice is going to depend on the population being measured, the fidelity with which the instrument is administered, and other factors. That said, a quick and dirty thing we can do is called a <strong>split half correlation</strong>. We can take a dataset of CDI data [from Wordbank; <span class="citation"><label for="tufte-mn-58" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-58" class="margin-toggle">Frank et al. (2017)<span class="marginnote">Frank, Michael C, Mika Braginsky, Daniel Yurovsky, and Virginia A Marchman. 2017. <span>‚ÄúWordbank: An Open Repository for Developmental Vocabulary Data.‚Äù</span> <em>Journal of Child Language</em> 44 (3): 677‚Äì94.</span></span>] and split each test in half. Since there are 680 words on the CDI, that means we pretend that each of our 4214 participants took two versions of the CDI, each with 340 items ‚Äì one test consists of only the even-numbered items and the other is the odd numbered items. Then we just compute the correlation between each participant‚Äôs score on the even CDI and the odd CDI. The resulting correlation is very high: <span class="math inline">\(r = 0.997\)</span>. So that gives us a sense that CDIs are pretty reliable.</p>
<p><label for="tufte-mn-59" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-59" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/psycho-cors.png"/> Longitudinal correlations between a child‚Äôs score on one administration of the CDI and another one several months later.</span></span></p>
<p>On the other hand, a stronger test of reliability is a <strong>test-retest</strong> correlation. Our split-half number only tells us about what was happening within a single administration session, but lots of things vary between administration sessions. For example, maybe the child was having a bad day and not producing as much complex language. Comparing correlations across different days removes this source of correlation ‚Äì at a cost. The longer you wait between observations the more the child has changed! The figure in the margin shows longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases <span class="citation">(<label for="tufte-mn-60" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-60" class="margin-toggle">Frank et al. 2021<span class="marginnote">Frank, Michael C, Mika Braginsky, Daniel Yurovsky, and Virginia A Marchman. 2021. <em>Variability and Consistency in Early Language Learning: The Wordbank Project</em>. MIT Press.</span>)</span>. Overall, this evidence is comforting. It looks like the CDI shows good reliability (<span class="math inline">\(&gt;.9\)</span>) across several methods.</p>
<p>Given that CDI forms are relatively reliable instruments, are they valid? Well, as a starting point, they certainly have reasonable <strong>face validity</strong> ‚Äì they look like they are measuring the construct that they purport to measure. They also arguably have some <strong>ecological validity</strong> in that they measure the child‚Äôs language (as observed by the parent) in their day-to-day experiences, rather than in a particular lab situation. But how well do they really measure the construct of interest, namely children‚Äôs early language ability?</p>
<p><label for="tufte-mn-61" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-61" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/cdi-validity.png"/> Relations between an early form of the CDI (the ELI) and several other measurements of children‚Äôs early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights <span class="citation"><span class="citation">(<label for="tufte-mn-62" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-62" class="margin-toggle">Bornstein and Haynes 1998<span class="marginnote">Bornstein, Marc H, and O Maurice Haynes. 1998. <span>‚ÄúVocabulary Competence in Early Childhood: Measurement, Latent Construct, and Predictive Validity.‚Äù</span> <em>Child Development</em> 69 (3): 654‚Äì71.</span>)</span></span>.</span></span></p>
<p>A study by <span class="citation"><label for="tufte-mn-63" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-63" class="margin-toggle">Bornstein and Haynes (1998)<span class="marginnote">Bornstein, Marc H, and O Maurice Haynes. 1998. <span>‚ÄúVocabulary Competence in Early Childhood: Measurement, Latent Construct, and Predictive Validity.‚Äù</span> <em>Child Development</em> 69 (3): 654‚Äì71.</span></span> answered this question via a common measure validation strategy: concurrently administering a variety of measures that are hypothesized to relate to the same construct. The figure in the the margin shows the results of a structural equation model that measures the shared variance between a variety of different measures (critically including examples of the other two methods of assessment we discussed, direct assessment and observational, transcript-based assessments) and a single hypothesized central construct. The relation between the ELI (an early version of the CDI) and the central construct is quite strong: the ELI score correlated closely with the shared variance among all the different measures. Taken together with the reliability evidence, this kind of <strong>concurrent validity</strong> evidence suggests that, if you want to measure early language, the CDI is a pretty good way to do so.</p>
<p>The story of the CDI is a success story ‚Äì it‚Äôs a relatively inexpensive measure that has some evidence for both reliability and validity. We should celebrate (and also use it as a potential outcome measure in our studies). But there is also plenty more work to do! A critic could very reasonably point out that we haven‚Äôt shown any evidence that reliability and validity extends across different populations or ages. For any measure, it‚Äôs important to start by asking whether there is <em>any</em> evidence for reliability and validity. But once you have a specific target population in mind, you can also ask how likely it is that the measure will yield reliable and valid data with <em>that particular population</em>. Oftentimes you yourself will have to do this measurement work, ‚Äúchecking‚Äù that your instruments function appropriately in the particular situation you want to use them. This is sometimes a lot of hard work, but it‚Äôs an important part of doing good experimental measurement!</p>
</div>
<div id="reliability" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Reliability</h2>
<p><label for="tufte-mn-64" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-64" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/elife-35718-fig1-v2.jpg"/> Precision and bias visualized. The expected precision of measurements from an instrument is its reliabilty. The bias of measurements from an instrument also provide a metaphor for its validity. Figure from <span class="citation"><span class="citation"><label for="tufte-mn-65" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-65" class="margin-toggle">Brandmaier et al. (2018)<span class="marginnote">Brandmaier, Andreas M, Elisabeth Wenger, Nils C Bodammer, Simone K√ºhn, Naftali Raz, and Ulman Lindenberger. 2018. <span>‚ÄúAssessing Reliability in Neuroimaging Research Through Intra-Class Effect Decomposition (ICED).‚Äù</span> <em>Elife</em> 7: e35718.</span></span></span>.</span></span></p>
<p>Having looked at the CDI as an example, we can now talk more generally about what it means for a measure to be reliable. Reliability is a way of describing the extent to which a measure yields signal relative to noise. Intuitively, if there‚Äôs less noise, then there will be more similarity between different measurements of the same quantity.<label for="tufte-sn-40" class="margin-toggle sidenote-number">40</label><input type="checkbox" id="tufte-sn-40" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">40</span> As the graphic above illustrates, a reliable measure that yields high-precision observations can still be highly biased. We‚Äôll talk about bias in a moment when we discuss validity.</span> Here‚Äôs the trouble. How do we measure signal and noise?</p>
<div id="measurement-scales" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Measurement scales</h3>
<p>In the physical sciences, it‚Äôs common to measure the precision of an instrument by quantifying its coefficient of variation <span class="citation">(<label for="tufte-mn-66" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-66" class="margin-toggle">Brandmaier et al. 2018<span class="marginnote">Brandmaier, Andreas M, Elisabeth Wenger, Nils C Bodammer, Simone K√ºhn, Naftali Raz, and Ulman Lindenberger. 2018. <span>‚ÄúAssessing Reliability in Neuroimaging Research Through Intra-Class Effect Decomposition (ICED).‚Äù</span> <em>Elife</em> 7: e35718.</span>)</span>:</p>
<p><span class="math display">\[CV = \frac{\sigma_w}{\mu_w}\]</span>
where <span class="math inline">\(\sigma_w\)</span> is the standard deviation of the measurements within an individual and <span class="math inline">\(\mu_w\)</span> is the mean of those measurements. Imagine we measure the height of a person five times, resulting in measurements of 171cm, 172cm, 171cm, 173cm, and 172cm. Now we can use these measurements to compute the coefficient of variation, which is 0.005. Why can‚Äôt we just do that with psychological measurements?</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:measurement-stevens-table">Table 7.1: </span>Stevens (1946) table of scale types and their associated operations and statistics.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">Scale</th>
<th align="left">Definition</th>
<th align="left">Operations</th>
<th align="left">Statistics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Nominal</td>
<td align="left">Unordered list</td>
<td align="left">Equality</td>
<td align="left">Mode</td>
</tr>
<tr class="even">
<td align="left">Ordinal</td>
<td align="left">Ordered list</td>
<td align="left">Greater than or less than</td>
<td align="left">Median</td>
</tr>
<tr class="odd">
<td align="left">Interval</td>
<td align="left">Numerical</td>
<td align="left">Equality of intervals</td>
<td align="left">Mean, SD</td>
</tr>
<tr class="even">
<td align="left">Ratio</td>
<td align="left">Numerical with zero</td>
<td align="left">Equality of ratios</td>
<td align="left">Coefficient of variation</td>
</tr>
</tbody>
</table>
<p>Thinking about this question takes us on a detour through the different kinds of measurement scales used in psychological research <span class="citation">(<label for="tufte-mn-67" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-67" class="margin-toggle">Stevens 1946<span class="marginnote">Stevens, S S. 1946. <span>‚ÄúOn the Theory of Scales of Measurement.‚Äù</span> <em>Science</em> 103 (2684): 677‚Äì80.</span>)</span>. The height measurements in our example are on what is known as a <strong>ratio</strong> scale: a scale in which numerical measurements are equally spaced and on which there is a true zero point. These scales are common for physical quantities but actually quite infrequent in psychology. More common are <strong>interval</strong> scales, in which there is no true zero point. For example, IQ (and other standardized scores) are intended to capture interval variation on some dimension but 0 is meaningless ‚Äì an IQ of 0 does not correspond to any particular interpretation.</p>
<p><strong>Ordinal</strong> scales are also commonly used. These are scales that are ordered but are not necessarily spaced equally. For example, levels of educational achievement (‚ÄúElementary,‚Äù‚ÄúHigh school,‚Äù‚ÄúSome college,‚Äù‚ÄúCollege,‚Äù‚ÄúGraduate school‚Äù) are ordered, but there is no sense in which ‚ÄúHigh school‚Äù is as far from ‚ÄúElementary‚Äù as ‚ÄúGraduate school‚Äù is from ‚ÄúCollege.‚Äù The last type in Stevens‚Äô hierarchy is <strong>nominal</strong> scales, in which no ordering is possible either. For example, race is an unordered scale in which multiple categories are present but there is no inherent ordering of these categories. The full hierarchy is presented in <a href="7-measurement.html#tab:measurement-stevens-table">7.1</a>.</p>
<p>Critically, different summary measures work for each scale type. If you have an unordered list like a list of options for a question about race on a survey, you can present the modal response (the most likely one).It doesn‚Äôt even make sense to think about what the median was ‚Äì there‚Äôs no ordering! For ordered levels of education, a median is possible but you can‚Äôt compute a mean. And for interval variables like IQ or ‚Äúnumber of correct answers on a math test‚Äù you can compute a mean and a standard deviation.<label for="tufte-sn-41" class="margin-toggle sidenote-number">41</label><input type="checkbox" id="tufte-sn-41" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">41</span> You might be tempted to think that ‚Äúnumber of correct answers‚Äù is a ratio variable ‚Äì but is zero really meaningful? Does it truly correspond to ‚Äúno math knowledge‚Äù or is it just a stand-in for ‚Äúless math knowledge than this test requires.‚Äù</span></p>
<p>But unless you have a ratio scale with a true zero, you can‚Äôt compute a coefficient of variation. Think about it for IQ scores: currently, by convention, standardized IQ scores are set to have a mean of 100. If we tested someone multiple times and found the standard deviation of their test scores was 4 points, then we could estimate the precision of their measurements as ‚ÄúCV‚Äù of 4/100 = .04. But since IQ of 0 isn‚Äôt meaningful, we could just set the mean IQ for the population to 200. Our test would be the same, and so the CV would be 4/200 = .02. On that logic we just doubled the precision of our measurements by rescaling the test! That doesn‚Äôt make any sense.</p>
<p>This digression helps us understand why CV is not a viable measure of the precision of measurements (and hence the reliability of instruments) in psychology: it‚Äôs because <em>most psychological measurements are not on ratio scales where CV makes sense</em>.</p>
</div>
<div id="paradoxes-in-reliability" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Paradoxes in reliability</h3>
<p>So then how do we measure signal and noise when we don‚Äôt have a true zero?<label for="tufte-sn-42" class="margin-toggle sidenote-number">42</label><input type="checkbox" id="tufte-sn-42" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">42</span> For the most part, we‚Äôre only going to talk about methods for interval variables here, but there are generalizations of many of these to ordinal and nominal cases. Our goal here isn‚Äôt to show you all the relevant statistical methods so much as it is to make sure you have a general intuition.</span> We can still look at the variation between repeated measurements, but rather than comparing that variation between measurements to the mean, we can compare it to some other kind of variation (for example between people).</p>
<p>This intuition gives us the intra-class correlation coefficient (ICC):</p>
<p><span class="math display">\[ICC = \frac{\sigma^2_b}{\sigma^2_w + \sigma^2_b}\]</span>
where <span class="math inline">\(\sigma^2_w\)</span> is the within-subject variance in measurements and <span class="math inline">\(\sigma^2_b\)</span> is the between-subject variance in the measurements. So now instead of comparing variation to the mean, we‚Äôre comparing variation on one dimension (between person) to total variation (within and between person).</p>
<p>If you think about it, this is more or less what we were doing when we computed both <strong>split-half</strong> and <strong>test-retest</strong> reliabilities for the CDI in our case study above. In the split half case, we took the correlation between two halves of the test, computed across individuals. That correlation looks at the covariance between the two halves, scaled by the variance <em>across individuals</em> of each half. So now we know why that split half reliability number was so high (<span class="math inline">\(r = 0.997\)</span>)! We were looking at the comparison of the variation between one individual‚Äôs answers and the variation between tons of different kids whose answers are all over the place.<label for="tufte-sn-43" class="margin-toggle sidenote-number">43</label><input type="checkbox" id="tufte-sn-43" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">43</span> If this intuition about the correlation doesn‚Äôt make sense, <span class="citation"><label for="tufte-mn-68" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-68" class="margin-toggle">Lee Rodgers and Nicewander (1988)<span class="marginnote">Lee Rodgers, Joseph, and W Alan Nicewander. 1988. <span>‚ÄúThirteen Ways to Look at the Correlation Coefficient.‚Äù</span> <em>The American Statistician</em> 42 (1): 59‚Äì66.</span></span> give a really nice treatment of different ways of looking at a correlation.</span> Same thing was true with the test-retest example: we were computing the covariation between this individual‚Äôs two CDI scores, scaled by the variation across <em>all the individuals in the sample.</em></p>
<p>This example leads us to a major issue with calculating reliabilities using the relative approach: <em>reliability numbers computed in this way are always relative to the variation in the sample</em>. So if the sample had less variability, reliability would decrease. We can see that in the CDI data by restricting our sample to only 16-month-olds (our prior sample had 16 ‚Äì 30-month-olds) with low maternal education. Within this more restricted subset, our reliability drops to <span class="math inline">\(r = 0.969\)</span> ‚Äì still very high but not quite as high as within the full sample.</p>
<p>But we can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between subjects variability, perhaps by giving different instructions to different families. This practice will actually <em>increase</em> our estimate of split-half reliability ‚Äì while the within-participant variability will remain the same, the between-participant variability will go up! You could call this a ‚Äúreliability paradox‚Äù ‚Äì sloppier data collection can actually lead to higher reliabilities.<label for="tufte-sn-44" class="margin-toggle sidenote-number">44</label><input type="checkbox" id="tufte-sn-44" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">44</span> Much of this discussion comes from a nice blogpost by Steve Luck: <a href="https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision" class="uri">https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision</a>. If you get interested in this topic, a fascinating article by <span class="citation"><label for="tufte-mn-69" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-69" class="margin-toggle">Hedge, Powell, and Sumner (2018)<span class="marginnote">Hedge, Craig, Georgina Powell, and Petroc Sumner. 2018. <span>‚ÄúThe Reliability Paradox: Why Robust Cognitive Tasks Do Not Produce Reliable Individual Differences.‚Äù</span> <em>Behavior Research Methods</em> 50 (3): 1166‚Äì86.</span></span> shows why many robust cognitive tasks like the Stroop task nevertheless show low reliability by the definition we‚Äôve been using here: they don‚Äôt vary very much between individuals!</span></p>
<p>More generally, we need to be sensitive to the sources of variability we‚Äôre quantifying reliability over ‚Äì both the numerator and the denominator. If we‚Äôre computing split-half reliabilities, typically we‚Äôre looking at variability across test questions (from some question bank) vs.¬†across individuals (from some population). Both of these sets matter ‚Äì if the population is more variable <em>or</em> the questions are less variable, we‚Äôll get higher reliability.<label for="tufte-sn-45" class="margin-toggle sidenote-number">45</label><input type="checkbox" id="tufte-sn-45" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">45</span> We‚Äôre not going to cover this, but all the same concepts come up when you quantify the reliability of different observers‚Äô ratings of the same stimulus (<strong>inter-annotator reliability</strong>), say for example when you have two coders rate how aggressive a person seems in a video. The most common measure of inter-annotator agreement is a categorical measure called Cohen‚Äôs <span class="math inline">\(\kappa\)</span>, for categorical agreement, but you can use ICCs and many other measures, and they fall prey to the same paradoxes we‚Äôve discussed here for measure reliability.</span></p>
<p>In sum, it‚Äôs really important to know something about the reliability of the measurement instrument that you want to use. But it‚Äôs also a mistake to think that there‚Äôs one true reliability for that measure. Rather, reasoning about whether a measure is reliable enough for your purposes means you need to think about both the numerator and the denominator for that estimate!</p>
</div>
<div id="practical-advice-for-computing-reliability" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Practical advice for computing reliability</h3>
<p>All of this theoretical talk about reliabilities is somewhat distressing, and doesn‚Äôt alleviate the need to compute numbers. What should a practicing researcher do? Here are a few pieces of advice for choosing (and contextualizing) reliability computations.</p>
<p>First, ignorance is not bliss. If you don‚Äôt know the reliability of your measures for an experiment, you risk wasting your and your participants‚Äô time. As we‚Äôll see in Chapter <a href="9-sampling.html#sampling">9</a>, the necessary sample size for your study is intimately related to the degree of within- and between-participant variance ‚Äì the same quantities that determine reliability. A higher reliability measure will lead to more precise measurements of a causal effect of interest and hence smaller sample sizes. Low-reliability measures are also a graveyard for individual differences studies. One author (MCF) spent several fruitless months in graduate school running dozens of participants through batteries of language processing tasks and correlating the results across tasks. This exercise was a waste of time because most of the tasks were of such low reliability that, even had they been highly correlated with another task, this relationship would have been almost impossible to detect without a huge sample size.<label for="tufte-sn-46" class="margin-toggle sidenote-number">46</label><input type="checkbox" id="tufte-sn-46" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">46</span> One rule of thumb that proves very helpful for individual difference designs of this sort is that the maximal correlation that can be observed between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the square root of the product of their reliabilities: <span class="math inline">\(\sqrt{r_x r_y}\)</span>. So if you have two measures that are reliable at .25, the maximal measured correlation between them is .25 as well!</span></p>
<p>Second, test-retest reliability is generally more conservative (and hence more useful) than split-half reliability. All the old textbooks knew this, and simply tried to derive other measures as stand-ins.<label for="tufte-sn-47" class="margin-toggle sidenote-number">47</label><input type="checkbox" id="tufte-sn-47" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">47</span> Because we are experimentalists, we get to collect new data, so doing a test-retest preliminary study is a totally reasonable option, rather than just blithely moving forward with our planned experiment.</span> In test-retest reliability, your within-participant reliability computation includes variation across different testing sessions (and sometimes different versions of a test, since some tests are hard to repeat). Since others participants‚Äô scores are produced by different test sessions as well, adding in this extra variability makes the comparison to between-participant variability much fairer. It‚Äôs hard work to do real test-retest reliability estimates, but if you plan on using a measure more than once, it will likely be worth-while, not least because your estimates of reliability may make your measure more likely to be adopted by others.<label for="tufte-sn-48" class="margin-toggle sidenote-number">48</label><input type="checkbox" id="tufte-sn-48" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">48</span> Don‚Äôt be fooled by the use of Cronbach‚Äôs <span class="math inline">\(\alpha\)</span>, a commonly reported measure for reliability. <span class="math inline">\(\alpha\)</span> is just split-half reliability averaged across all possible splits. It has all the same problems of split-half reliability. What‚Äôs more, people often report <span class="math inline">\(\alpha\)</span> as an index of internal consistency on the idea that it reflects the relatedness of test items to one another. This is just wrong <span class="citation">(<label for="tufte-mn-70" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-70" class="margin-toggle">Sijtsma 2009<span class="marginnote">Sijtsma, Klaas. 2009. <span>‚ÄúOn the Use, the Misuse, and the Very Limited Usefulness of Cronbach‚Äôs Alpha.‚Äù</span> <em>Psychometrika</em> 74 (1): 107.</span>)</span>!</span></p>
<p>Third, if you have multiple measurement items as part of your instrument and they vary substantially, make sure you evaluate how they contribute to the reliability of the measure. This could happen because you have an inventory comprised of different survey items or because you have multiple vignettes or test stimuli that vary in content or difficulty. In this common situation, some of these items may not contribute to the reliability of your measure ‚Äì and some may even detract. This could be because some items don‚Äôt measure the same construct as the rest, as we‚Äôll discuss below, or it could be because they are noisy. There are many statistical methods that you can use to find such items and remove them from your measure, ranging from the simple (e.g., visualizing distributions to scan for floor and ceiling effects and looking for items that do not correlate with others) to the more complex (e.g., structural equation models). The development of measurement models for binary tests, item response theory <span class="citation">(<label for="tufte-mn-71" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-71" class="margin-toggle">Embretson and Reise 2013<span class="marginnote">Embretson, Susan E, and Steven P Reise. 2013. <em>Item Response Theory</em>. Psychology Press.</span>)</span> can be used to find test items that show limited discrimination between participants of high and low ability.</p>
</div>
</div>
<div id="validity" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Validity</h2>
<p>In Chapter <a href="1-intro.html#intro">1</a>, we talked about the process of theory building as a process of describing the relationships between constructs. But for the theory to be tested, the constructs must be measured so that the posited relationships between them can be probed.<label for="tufte-sn-49" class="margin-toggle sidenote-number">49</label><input type="checkbox" id="tufte-sn-49" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">49</span> If you can‚Äôt measure a construct, then its role in the theory may be more questionable. But there can be a role for unmeasured ‚Äì and unmeasurable ‚Äì entities in a theory!</span> Measurement and measure construction is therefore intimately related to theory construction, and the notion of validity is central.</p>
<p>A valid measure is one that measures the construct of interest.<label for="tufte-sn-50" class="margin-toggle sidenote-number">50</label><input type="checkbox" id="tufte-sn-50" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">50</span> In the target diagram above, invalidity is pictured as bias ‚Äì the holes in the target are tightly grouped but in the wrong place. That can be a helpful guide to thinking about validity, but it also breaks the target metaphor. If distance on the target is variation in the measurement, then the ‚Äúbias‚Äù pictures actually show bias in the measurement, e.g.¬†systematic over- or under-estimation of the true quantity.</span> That sounds simple, but really, how can you tell, given that the construct of interest is unobserved? The answer ‚Äì given by <span class="citation"><label for="tufte-mn-72" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-72" class="margin-toggle">Cronbach and Meehl (1955)<span class="marginnote">Cronbach, L J, and P E Meehl. 1955. <span>‚ÄúConstruct Validity in Psychological Tests.‚Äù</span> <em>Psychol. Bull.</em> 52 (4): 281‚Äì302.</span></span> ‚Äì is that there is no single test of the validity of a measure. Rather, the measure is valid if there is evidence that it fits into the nomological network (the theory).</p>
<p>If you look in a standard research methods textbook, there will be names for a large and somewhat variable number of different kinds of validity. These terms are used in very confusing ways! Below are some definitions, but keep in mind that these these are just names for specific ways that a measure might not match up with a construct, and they are not an exhaustive list.</p>
<ul>
<li><strong>Face validity</strong>: The measure looks like the construct, such that intuitively it is reasonable that it measures the construct. Example: Even if a child‚Äôs height is correlated with their early language ability, we might be skeptical of this measure due to its lack of face validity.</li>
<li><strong>Ecological validity</strong>: The measure incorporates how the construct is used in people‚Äôs lives. Example: Language measures that look at the diversity of language used in children‚Äôs productions at home might be more ecologically valid than a processing measure from a tablet-based game.<br />
</li>
<li><strong>Internal validity</strong>: Usually used negatively. A ‚Äúchallenge to internal validity‚Äù is a description of a case where the measure is administered in such a way as to weaken the relationship between measure and construct.<label for="tufte-sn-51" class="margin-toggle sidenote-number">51</label><input type="checkbox" id="tufte-sn-51" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">51</span> Sometimes this concept is described as only being relevant to the validity of a manipulation, e.g.¬†when the manipulation of the construct is confounded and some other psychological variable is manipulated as well.</span> Example: CDI instructions are given using confusing or overly complex language, leading lower-educated parents to misinterpret the questionnaire and hence for the measure to underestimate language ability in some children.</li>
<li><strong>Convergent validity</strong>: The classic strategy for showing validity is to show that a measure relates (usually, correlates) with other putative measures of the same construct. When these relationships are measured concurrently, this is sometimes called <strong>concurrent validity</strong>. This evidence is most convincing when the other measures themselves have validity evidence.<label for="tufte-sn-52" class="margin-toggle sidenote-number">52</label><input type="checkbox" id="tufte-sn-52" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">52</span> This idea of convergent validity is precisely the circularity of Cronbach and Meehl‚Äôs ‚Äúnomological network‚Äù idea ‚Äì a measure is valid if it relates to other valid measures, which themselves are only valid if the first one is! The measures are valid because the theory works, and the theory works because the measures are valid.</span> Example: CDI scores correlate with word-recognition eye-tracking experiments.</li>
<li><strong>Predictive validity</strong>. If the measure predicts other later measures of the construct; often used in lifespan and developmental studies where it is particularly prized for a measure to be able to predict meaningful life outcomes in the future. Example: relation of CDI scores at age 2 to reading scores during elementary school.^[Both concurrent/convergent and</li>
<li><strong>Divergent validity</strong>. If the measure can be shown to be distinct from measure(s) of a different construct, this evidence can help establish that the measure is specifically linked to the target construct. Example: CDI scores are more related to word recognition reaction times than reaction times in a visual search task.</li>
</ul>
<p>As we mentioned above, some of these forms of validity can also be used as descriptions of the validity of a manipulation (rather than a measure).<label for="tufte-sn-53" class="margin-toggle sidenote-number">53</label><input type="checkbox" id="tufte-sn-53" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">53</span> We discuss this issue in more depth in Chapter <a href="8-design.html#design">8</a>.</span></p>
<p>In our opinion, the most compelling way to think about validity is not to enumerate different varieties of validity as though they are separable from one another. Instead, the users of a measure should make an argument about why a measure is valid, calling on various different sources of support <span class="citation">(<label for="tufte-mn-73" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-73" class="margin-toggle">Kane 1992<span class="marginnote">Kane, Michael T. 1992. <span>‚ÄúAn Argument-Based Approach to Validity.‚Äù</span> <em>Psychological Bulletin</em> 112 (3): 527.</span>)</span>.</p>
<p>The idea of ‚Äúargument-based validity‚Äù is really just a reminder for researchers ‚Äì especially including experimentalists, who have a worrisome tendency to make up ad hoc measures on the fly ‚Äì to think hard about validity and report their thinking about it. Simply reasoning about what construct you want to measure, how your measure relates to the construct, and whether your measure is a valid measure of the construct will go a long way towards ensuring that you make good decisions about measurement. On the flip side, <span class="citation"><label for="tufte-mn-74" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-74" class="margin-toggle">Flake and Fried (2020)<span class="marginnote">Flake, Jessica Kay, and Eiko I Fried. 2020. <span>‚ÄúMeasurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them.‚Äù</span> <em>Advances in Methods and Practices in Psychological Science</em> 3 (4): 456‚Äì65.</span></span> describe what they call ‚Äúquestionable measurement practices‚Äù ‚Äì a suite of practices that can undermine the contribution of a study by reducing validity.</p>
<p>Table <a href="7-measurement.html#tab:flake-questions">7.2</a> gives our adaptation of the set of questions they offer up to try and provoke thoughtful reporting of measurement practices. One big thing on their mind is that researchers have been known to modify their scales and their scale scoring practices (say, omitting items from a survey or rescaling responses) after data collection. This doesn‚Äôt have to be a bad thing ‚Äì sometimes it is justified ‚Äì but it clearly needs to be disclosed, and it may call statistical inferences into question (see Chapter <a href="#prereg"><strong>??</strong></a>).</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:flake-questions">Table 7.2: </span>Questions about measurement that every reseacher should answer in their paper. Adapted from Flake &amp; Fried (2020).</span><!--</caption>--></p>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Question</th>
<th align="left">Information to Report</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">What is your construct?</td>
<td align="left">Define construct, describe theory and research.</td>
</tr>
<tr class="even">
<td align="left">What measure did you use to operationalize your construct?</td>
<td align="left">Describe measure and justify operationalization.</td>
</tr>
<tr class="odd">
<td align="left">Did you select your measure from the literature or create it from scratch?</td>
<td align="left">Justify measure selection and review evidence on reliability and validity (or disclose the lack of such evidence).</td>
</tr>
<tr class="even">
<td align="left">Did you modify your measure during the process?</td>
<td align="left">Describe and justify any modifications; note whether they occurred before or after data collection.</td>
</tr>
<tr class="odd">
<td align="left">How did you quantify your measure?</td>
<td align="left">Describe decisions underlying the calculation of scores on the measure; note whether these were established before or after data collection and whether they are based on standards from previous literature.</td>
</tr>
</tbody>
</table>
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Talk about flexible measurement!</p>
<p>The Competitive Reaction Time Task (CRTT) is a lab-based measure of aggression. Participants are told that they are playing a reaction-time game against another player and are asked to set the parameters of a noise blast that will be played to their opponent. Unfortunately, in an analysis of the literature using CRTT, <span class="citation"><label for="tufte-mn-75" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-75" class="margin-toggle">Elson et al. (2014)<span class="marginnote">Elson, Malte, M. Rohangis Mohseni, Johannes Breuer, Michael Scharkow, and Thorsten Quandt. 2014. <span>‚ÄúPress <span>CRTT</span> to Measure Aggressive Behavior: The Unstandardized Use of the Competitive Reaction Time Task in Aggression Research.‚Äù</span> <em>Psychological Assessment</em> 26 (2): 419‚Äì32. <a href="https://doi.org/10.1037/a0035569">https://doi.org/10.1037/a0035569</a>.</span></span> found that different papers using the CRTT use dramatically different methods for scoring the task. Across trials, both the volume and duration of the noise blast were sometimes analyzed. Sometimes these scores were transformed (via logarithms) or thresholded. Sometimes they were combined into a single score. Elson was so worried by this flexibility, he created a website, <a href="">http://flexiblemeasures.com</a>, to document the variation he observed.</p>
<p><label for="tufte-mn-76" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-76" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/CRTT.png"/> Data on the number of publications using CRTT and the number of different quantifications of CRTT, plotted cumulatively until 2016. Image from <a href="">http://flexiblemeasures.com</a>.</span></span></p>
<p>As of 2016, Elson had found 130 papers using the CRTT. And across these papers, he documented an astonishing 157 quantification strategies. One paper reported ten different strategies for extracting numbers from this measure! More worrisome still, in their work, Elson and colleagues found that these decisions made a major difference to whether the findings of studies were statistically significant.</p>
<p>This examination of the use of the CRTT measure has several implications. First, and most troublingly, there may have been undisclosed flexibility in the analysis of CRTT data across the literature, with investigators taking advantage of the lack of standardization to try many different analysis variants and report the one most favorable to their own hypothesis. Second, it is unknown which quantification of CRTT behavior is in fact most reliable and valid. Since some of these variants are presumably better than others, researchers are effectively ‚Äúleaving money on the table‚Äù by using suboptimal quantifications. Finally, as a consequence, when if researchers adopt the CRTT, they find much less guidance from the literature on what quantification to adopt.</p>
</div>
</div>
<div id="how-to-select-a-good-measure" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> How to select a good measure?</h2>
<p>Ideally you want a measure that is reliable and valid. How do you get one? An important first principle is to use a pre-existing measure. Perhaps someone else has done the hard work of compiling evidence on reliability and validity, and in that case you will most likely want to piggyback on that work. Standardized measures are typically broad in their application and so the tendency can be to discard these because they are not tailored for our studies specifically. But the benefits of a standardized measure are substantial. Not only can you justify the measure using the prior literature, you also have an important index of population variability by comparing absolute scores to other reports.<label for="tufte-sn-54" class="margin-toggle sidenote-number">54</label><input type="checkbox" id="tufte-sn-54" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">54</span> Comparing absolute measurements is a really important trick for ‚Äúsanity-checking‚Äù your data. If your participants are way less accurate than the ones in the paper you‚Äôre following up, that may be a signal that something has gone wrong.</span> If you don‚Äôt use someone else‚Äôs measure, you‚Äôll need to make one up yourself. Most experimenters go down this route at some point, but if you do, remember that you will need to figure out how to estimate its reliability and how to make an argument for its validity!</p>
<div id="what-to-measure" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> What to measure?</h3>
<p>We can measure almost anything about experimental participants‚Äô behavior or even their physiology. We could do an experiment on children‚Äôs exploratory play and measure the number of times they interact with another child <span class="citation">(<label for="tufte-mn-77" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-77" class="margin-toggle">H. S. Ross and Lollis 1989<span class="marginnote">Ross, Hildy S, and Susan P Lollis. 1989. <span>‚ÄúA Social Relations Analysis of Toddler Peer Relationships.‚Äù</span> <em>Child Development</em>, 1082‚Äì91.</span>)</span>, or an experiment on aggression where we measure the amount of hot sauce <span class="citation">(<label for="tufte-mn-78" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-78" class="margin-toggle">Lieberman et al. 1999<span class="marginnote">Lieberman, Joel D, Sheldon Solomon, Jeff Greenberg, and Holly A McGregor. 1999. <span>‚ÄúA Hot New Way to Measure Aggression: Hot Sauce Allocation.‚Äù</span> <em>Aggressive Behavior: Official Journal of the International Society for Research on Aggression</em> 25 (5): 331‚Äì48.</span>)</span>. Yet most of the time we choose a relatively small set to focus on: asking survey questions, collecting choices and reaction times, and measuring physiological variables like eye-movements. Besides following these conventions, how do we choose the right measurement type for a particular experiment?</p>
<p>There‚Äôs no hard and fast rule about what aspect of behavior to measure, but here we will focus on two dimensions that can help us organize the broad space of possible measure targets.<label for="tufte-sn-55" class="margin-toggle sidenote-number">55</label><input type="checkbox" id="tufte-sn-55" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">55</span> Some authors differentiate between ‚Äúself-report‚Äù and ‚Äúobservational‚Äù measures. This distinction seems simple on its face, but actually gets kind of complicated. Is a facial expression a ‚Äúself-report?‚Äù Language is not the only way that people communicate with one another ‚Äì many actions are intended to be communicative <span class="citation">(<label for="tufte-mn-79" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-79" class="margin-toggle">Shafto, Goodman, and Frank 2012<span class="marginnote">Shafto, Patrick, Noah D Goodman, and Michael C Frank. 2012. <span>‚ÄúLearning from Others: The Consequences of Psychological Reasoning for Human Learning.‚Äù</span> <em>Perspectives on Psychological Science</em> 7 (4): 341‚Äì51.</span>)</span>.</span> The first of these is the continuum between simple and complex behaviors (visualized in the margin figure). The second is the focus on explicit, voluntary behaviors vs.¬†implicit or involuntary behaviors.</p>
<p><label for="tufte-mn-80" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-80" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/measurement/measure-considerations.png"/> Often choosing a measure can be consolidated into a choice along a continuum from simple measures that provide a small amount of information but are quick and easy to repeat and those that provide much richer information but require more time.</span></span></p>
<p>The simplest measurable behaviors tend to be button presses, for example:</p>
<ul>
<li>pressing a key to advance to the next word in a word-by-word self-paced reading study <span class="citation">(e.g., <label for="tufte-mn-81" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-81" class="margin-toggle">Warren and Gibson 2002<span class="marginnote">Warren, Tessa, and Edward Gibson. 2002. <span>‚ÄúThe Influence of Referential Processing on Sentence Complexity.‚Äù</span> <em>Cognition</em> 85 (1): 79‚Äì112.</span>)</span></li>
<li>selecting ‚Äúyes‚Äù or ‚Äúno‚Äù in a lexical decision task <span class="citation">(e.g., <label for="tufte-mn-82" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-82" class="margin-toggle">Ratcliff, Gomez, and McKoon 2004<span class="marginnote">Ratcliff, Roger, Pablo Gomez, and Gail McKoon. 2004. <span>‚ÄúA Diffusion Model Account of the Lexical Decision Task.‚Äù</span> <em>Psychological Review</em> 111 (1): 159.</span>)</span></li>
<li>making a forced choice between different alternatives to indicate which has been seen before <span class="citation">(e.g., <label for="tufte-mn-83" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-83" class="margin-toggle">Fiser and Aslin 2001<span class="marginnote">Fiser, J√≥zsef, and Richard N Aslin. 2001. <span>‚ÄúUnsupervised Statistical Learning of Higher-Order Spatial Structures from Visual Scenes.‚Äù</span> <em>Psychological Science</em> 12 (6): 499‚Äì504.</span>)</span></li>
</ul>
<p>These specific measures ‚Äì and many more like them ‚Äì are the bread and butter of many cognitive psychology studies. Because they are quick and easy to explain, these tasks can be repeated over many trials. They can also be executed with a wider variety of populations including with young children and sometimes even with non-human animals with appropriate adaptation. (A further benefit of these paradigms is that they can yield useful reaction time data, which we discuss further below).</p>
<p>In contrast, a huge range of complex behaviors have been studied by psychologists, including:</p>
<ul>
<li>open-ended verbal interviews</li>
<li>written expression, e.g.¬†via handwriting or writing style</li>
<li>body movements, including gestures, walking, and dance <span class="citation">(<label for="tufte-mn-84" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-84" class="margin-toggle">Kirsh 2010<span class="marginnote">Kirsh, David. 2010. <span>‚ÄúThinking with the Body.‚Äù</span></span>)</span></li>
<li>artifact building</li>
</ul>
<p><!-- TODO: add citations [bargh, goldin meadow, judy fan citation] --></p>
<p>There are many reasons to study these kinds of behaviors. First, the behaviors themselves may be examples of tasks of interest (e.g., in studies of tower building). Or, as in the case of studies of skilled typing <span class="citation">(<label for="tufte-mn-85" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-85" class="margin-toggle">Rumelhart and Norman 1982<span class="marginnote">Rumelhart, David E, and Donald A Norman. 1982. <span>‚ÄúSimulating a Skilled Typist: A Study of Skilled Cognitive-Motor Performance.‚Äù</span> <em>Cognitive Science</em> 6 (1): 1‚Äì36.</span>)</span>, the behavior may stand in for other even more complex behaviors of interest. Researchers may also feel that the ecological validity of their measure makes up for the complexity of converting the behavior into a numerical measurement. Of course, that is the major issue with studying complex behaviors: they typically afford a huge variety of different measures. So any experiment that uses a particular measurement of a complex behavior will typically need to do significant work up front to justify the choice of that measurement and provide some assurance about its reliability. Further, it is often much more difficult to have a participant repeat a complex behavior many times under the same conditions ‚Äì imagine asking someone to build a block tower hundreds of times! Thus, the choice of a complex behavior is often a choice to forego a large number of simple trials for a small number of more complex trials.</p>
<p>Our view is that complex behaviors can be useful to study either at the beginning or the end of a set of experiments. At the beginning of a set of experiments, they can provide inspiration about the richness of the target behavior and insight into the many factors that influence it. And at the end of a set of experiments, they can provide an ecologically valid measure to complement a reliable but more artificial, lab-based behavior. But as the primary measure across a multi-experiment set they are often unweildy. As a rule of thumb, the more complex the behavior, the more it will vary across individuals and the more environmental and situational factors will affect it. These can be important parts of the phenomenon, but they will also be nuisances that are difficult to get under experimental control.</p>
<p>The second dimension of organization for measures is the difference between implicit and explicit measures. An explicit measure provides a measurement of a behavior that a participant has conscious awareness of ‚Äì for example, the answer to a question. In contrast, implicit measures provide measurements of psychological processes that participants are unable to report (or occasionally, unwilling to).<label for="tufte-sn-56" class="margin-toggle sidenote-number">56</label><input type="checkbox" id="tufte-sn-56" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">56</span> Implicit/explicit is likely more of a continuum, but one cut-point is whether the participants‚Äô behavior is considered intentional: that is, participants <em>intend</em> to press a key to register a decision, but they likely do not intend to react in 300 as opposed to 350 milliseconds due to having seen a prime.</span> Implicit measures, especially reaction time, have long been argued to reflect internal psychological processes <span class="citation">(<label for="tufte-mn-86" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-86" class="margin-toggle">Donders 1969<span class="marginnote">Donders, Franciscus Cornelis. 1969. <span>‚ÄúOn the Speed of Mental Processes.‚Äù</span> <em>Acta Psychologica</em> 30: 412‚Äì31.</span>)</span>. They also have been proposed as measures of qualities such as racial bias that participants may have motivation not to disclose <span class="citation">(<label for="tufte-mn-87" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-87" class="margin-toggle">Greenwald, McGhee, and Schwartz 1998<span class="marginnote">Greenwald, Anthony G, Debbie E McGhee, and Jordan LK Schwartz. 1998. <span>‚ÄúMeasuring Individual Differences in Implicit Cognition: The Implicit Association Test.‚Äù</span> <em>Journal of Personality and Social Psychology</em> 74 (6): 1464.</span>)</span>.</p>
<p>There are also of course a host of physiological measurements available. Some of these measure eye-movements, heart rate, or skin conductance, which can be linked to aspects of cognitive process. Others reflect underlying brain activity via the signals associated with MRI, MEG, NIRS, and EEG measurements. These methods are outside the scope of this book, though we note that the measurement concerns we discuss here are entirely germane <span class="citation">(e.g., <label for="tufte-mn-88" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-88" class="margin-toggle">Zuo, Xu, and Milham 2019<span class="marginnote">Zuo, Xi-Nian, Ting Xu, and Michael Peter Milham. 2019. <span>‚ÄúHarnessing Reliability for Neuroscience Research.‚Äù</span> <em>Nature Human Behaviour</em> 3 (8): 768‚Äì71. <a href="https://doi.org/10.1038/s41562-019-0655-x">https://doi.org/10.1038/s41562-019-0655-x</a>.</span>)</span>.</p>
<p>Many tasks produce both accuracy and reaction time data. Often these trade off with one another in a classic <strong>speed-accuracy tradeoff</strong>: the faster participants respond, the less accurate they are. For example, to investigate racial bias in policing, <span class="citation"><label for="tufte-mn-89" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-89" class="margin-toggle">Payne (2001)<span class="marginnote">Payne, B Keith. 2001. <span>‚ÄúPrejudice and Perception: The Role of Automatic and Controlled Processes in Misperceiving a Weapon.‚Äù</span> <em>Journal of Personality and Social Psychology</em> 81 (2): 181.</span></span> showed US college students a series of pictures of tools and guns, proceeded by a prime of either a White face or a Black face. In a first study, participants were faster to identify weapons when primed by a Black face but had similar accuracies. A second study added a response deadline to speed up judgments: this manipulation resulted in equal reaction times across conditions but greater errors in weapon identification after Black prime faces. These studies likely revealed the same phenomenon ‚Äì some sort of bias to associate Black faces with weapons ‚Äì but the design of the task moved participants along a speed accuracy tradeoff, yielding effects on different measures. One way of describing the information processing underlying this tradeoff is given by drift diffusion models, which allow joint analysis of accuracy and reaction time <span class="citation">(<label for="tufte-mn-90" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-90" class="margin-toggle">Voss, Nagler, and Lerche 2013<span class="marginnote">Voss, Andreas, Markus Nagler, and Veronika Lerche. 2013. <span>‚ÄúDiffusion Models in Experimental Psychology.‚Äù</span> <em>Experimental Psychology</em> 60 (6): 385‚Äì402. <a href="https://doi.org/10.1027/1618-3169/a000218">https://doi.org/10.1027/1618-3169/a000218</a>.</span>)</span>. Used appropriately, drift diffusion models can provide a way to remove speed-accuracy tradeoffs and extract more reliable signals from tasks where accuracy and reaction time are both measured (see <span class="citation"><label for="tufte-mn-91" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-91" class="margin-toggle">Johnson et al. (2017)<span class="marginnote">Johnson, David J, Christopher J Hopwood, Joseph Cesario, and Timothy J Pleskac. 2017. <span>‚ÄúAdvancing Research on Cognitive Processes in Social and Personality Psychology: A Hierarchical Drift Diffusion Model Primer.‚Äù</span> <em>Social Psychological and Personality Science</em> 8 (4): 413‚Äì23.</span></span> for an example of DDM on a weapon-decision task).</p>
<p>The choice of what to measure is not an easy one, and depends on a host of concerns ‚Äì first and foremost the construct being studied. That said, we have a bias towards simple, explicit behaviors as a starting point. Work using these measures, which are often the least ecologically valid, can then be enriched with implicit measures or measurements of more complex behaviors. In this discussion, however, we have only briefly touched on the most popular simple, explicit tool in the psychologist‚Äôs toolkit: survey questions. Our next section deals with survey measures in more detail.</p>
</div>
<div id="survey-measures" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Survey measures</h3>
<p>Whole books have been written about how to design good survey questions. We are not primarily focused here on survey research, but sometimes such questions are an important part of experimental measurement so we‚Äôll share a few best practices, primarily derived from <span class="citation"><label for="tufte-mn-92" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-92" class="margin-toggle">Krosnick and Presser (2010)<span class="marginnote">Krosnick, Jon A, and Stanley Presser. 2010. <span>‚ÄúQuestion and Questionnaire Design.‚Äù</span> <em>Handbook of Survey Research</em>, 263.</span></span>.</p>
<p>Treat survey questions as a conversation. This is mostly common sense, but the easier your items are to understand, the better. Don‚Äôt repeat variations on the same question unless you want different answers! Try to make the order reasonable. The more you include ‚Äútricky‚Äù items the more you invite tricky answers to straightforward questions. We‚Äôll talk in Chapter <a href="12-collection.html#collection">12</a> about manipulation checks and their strengths and weaknesses.</p>
<p>Open-ended survey questions can be quite rich and informative, especially when an appropriate coding scheme is developed in advance and responses are categorized into a relatively small number of types. On the other hand, they present practical obstacles because they require coding (often by multiple coders to ensure reliability of the coding). Further, they tend to yield nominal data, which are often less useful for quantitative theorizing. Our view is that open-ended questions are a useful tool to add nuance and color to the interpretation of an experiment.</p>
<p>Especially given their ubiquity in commercial survey research, Likert scales with a fixed number of response items are a simple and conventional way of gathering data on attitude and judgment questions. Bipolar scales are those in which the endpoints represent opposites, for example the continuum between ‚Äústrongly dislike‚Äù and ‚Äústrongly like.‚Äù Unipolar scales have one neutral endpoint, like the continuum between ‚Äúno pain‚Äù and ‚Äúvery intense pain.‚Äù Survey best practices suggest that reliability is maximized when bipolar scales have seven points and unipolar scales have five. Labeling every point on the scale with verbal labels is preferable to labeling only the endpoints.<label for="tufte-sn-57" class="margin-toggle sidenote-number">57</label><input type="checkbox" id="tufte-sn-57" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">57</span> One important question is whether to treat data from Likert scales as ordinal or interval. It‚Äôs extremely common (and convenient) to make the assumption that Likert ratings are interval, allowing the use of standard statistical tools like means, standard deviations, linear regression, and the like. The risk in this practice comes from the possibility that scale items are not evenly spaced ‚Äì for example, on a scale labeled ‚Äúnever,‚Äù‚Äúseldom,‚Äù ‚Äúoccasionally,‚Äù‚Äúoften,‚Äù‚Äúalways,‚Äù the distance from ‚Äúoften‚Äù to ‚Äúalways‚Äù may be larger than the distance from ‚Äúseldom‚Äù to ‚Äúoccasionally.‚Äù In practice, you can choose to use regression variants that are appropriate, e.g.¬†ordinal logistic regression and its variants, or they can attempt to assess and mitigate the risks of treating the data as interval. If you choose the second option, it‚Äôs definitely a good idea to look carefully at the raw distributions for individual items (see Chapter <a href="#visualization"><strong>??</strong></a>) to see if their distribution appears approximately normal and not highly skewed or censored. You should also consider the names you give to your scale up front to try to minimize these issues.</span> Recently some researchers have begun to use ‚Äúvisual analog scales‚Äù (or sliders) as a solution. We don‚Äôt recommend these ‚Äì the distribution of the resulting data is often anchored at the starting point or endpoints, and a meta-analysis shows that are quite a bit lower than Likert scales in reliability (REF Krosnick).</p>
<p>It rarely helps matters to add a ‚Äúdon‚Äôt know‚Äù or ‚Äúother‚Äù option to survey questions. These are some of a variety of practices that encourage <strong>satisficing</strong>, where survey takers give answers that are good enough but don‚Äôt reflect substantial thought about the question. Another behavior that results from satisficing is ‚Äústraight-lining‚Äù ‚Äì that is, picking the same option for every question. In general, the best way to prevent straight-lining is to make surveys relatively short, engaging, and well-compensated. The practice of ‚Äúreverse coding‚Äù to make the expected answers to some questions more negative can block straight-lining, but at the cost of making items more confusing [often by introducing pragmatically infelicitous negation; <span class="citation"><label for="tufte-mn-93" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-93" class="margin-toggle">Nieuwland and Kuperberg (2008)<span class="marginnote">Nieuwland, Mante S, and Gina R Kuperberg. 2008. <span>‚ÄúWhen the Truth Is Not Too Hard to Handle: An Event-Related Potential Study on the Pragmatics of Negation.‚Äù</span> <em>Psychological Science</em> 19 (12): 1213‚Äì18.</span></span>]. Some obvious formatting options can reduce straight-lining as well, for example placing scales further apart or on subsequent (web) pages.</p>
<p>In sum, survey questions can be a helpful tool for eliciting graded judgments about explicit questions. The best way to execute them well is to try and make them as clear and easy to answer as possible.</p>
</div>
</div>
<div id="the-temptation-to-measure-lots-of-things" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> The temptation to measure lots of things</h2>
<p>If one measure is good, shouldn‚Äôt two be better? Many experimenters add multiple measurements to their experiments.<label for="tufte-sn-58" class="margin-toggle sidenote-number">58</label><input type="checkbox" id="tufte-sn-58" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">58</span> As usual, we want to qualify that we are only talking about randomized experiments here! In observational studies, often the point is to measure the associations between multiple measures so you typically <em>have</em> to include more than one. We‚Äôve also done plenty of descriptive studies ‚Äì these can be very valuable. In a descriptive context, often the goal is to include as many measures as possible so as to have a holistic picture of the phenomenon of interest.</span> The logic is often simple: more data are better. Sometimes the measures are multiple measures of the same construct; other times these are measures of distinct constructs.</p>
<p>How should you navigate this issue? The decision whether to include multiple measures is an aesthetic and practical issue as well as a scientific one. Throughout this book we have been advocating for a viewpoint in which experiments should be as simple as possible. For us, the best experiment is one that shows that a simple and valid manipulation affects a single, reliable and valid measure. So we‚Äôll try to talk you out of using multiple measures here, but if we fail, then maybe multiple measures are appropriate for your experiment.<label for="tufte-sn-59" class="margin-toggle sidenote-number">59</label><input type="checkbox" id="tufte-sn-59" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">59</span> In an entertaining article called ‚Äúthings I have learned (so far),‚Äù <span class="citation"><label for="tufte-mn-94" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-94" class="margin-toggle">Cohen (1990)<span class="marginnote">Cohen, Jacob. 1990. <span>‚ÄúThings i Have Learned (so Far).‚Äù</span> <em>American Psychologist</em> 45: 1304‚Äì12.</span></span> quips that he leans so far in the direction of large numbers of observations and small numbers of measures, that some students think his perfect study has 10,000 participants and no measures.</span></p>
<p>First, make sure that including more measures doesn‚Äôt compromise each individual measure. This can happen via fatigue or carryover effects. For example, if a brief attitude induction is followed by multiple questionnaire measures, it is a good bet that there is likely to be ‚Äúfade-out‚Äù of the intervention. Further, even if a condition manipulation has a long duration effect on participants, survey fatigue may lead to less meaningful responses to later questions <span class="citation">(<label for="tufte-mn-95" class="margin-toggle">&#8853;</label><input type="checkbox" id="tufte-mn-95" class="margin-toggle">Herzog and Bachman 1981<span class="marginnote">Herzog, A Regula, and Jerald G Bachman. 1981. <span>‚ÄúEffects of Questionnaire Length on Response Quality.‚Äù</span> <em>Public Opinion Quarterly</em> 45 (4): 549‚Äì59.</span>)</span>.</p>
<p>Second, consider whether you have a strong prediction for each measure, or whether you are simply looking for more ways to see an effect of your manipulation. As we‚Äôve discussed, we think of an experiment as a ‚Äúbet.‚Äù On that viewpoint, theories are best tested by observing measurements that they predict but that are low probability according to others. The more measures you add, the more bets you are making but the less value you are putting on each. In essence, you are hedging your bets and so the success of any one bet is less convincing.<label for="tufte-sn-60" class="margin-toggle sidenote-number">60</label><input type="checkbox" id="tufte-sn-60" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">60</span> On the other hand, if you can successfully win a whole series of bets, that can be very convincing evidence!</span></p>
<p>Third, if you include multiple measures in your experiment, you need to think about how you will interpret divergent results. Imagine you have experimental participants engage in a brief written reflection that is hypothesized to affect a construct (vs a control writing exercise, say listing meals). If you include two measures of the construct of interest and one shows a larger effect, what will you conclude? It may be tempting to assume that the one that shows a larger effect is the ‚Äúbetter measure‚Äù but the logic is circular ‚Äì it‚Äôs only better if the manipulation affected the construct of interest, which is what you were testing in the first place! Including multiple measures because you‚Äôre uncertain which one is more related to the construct indulges in this circular logic, since the experiment often can‚Äôt resolve the situation.<label for="tufte-sn-61" class="margin-toggle sidenote-number">61</label><input type="checkbox" id="tufte-sn-61" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">61</span> Except where the manipulation is has been independently investigated with another measure and there is good evidence that it affects the construct!</span> A much better move in this case is to do a preliminary study of the reliability and validity of the two measures so as to be able to select one as the experiment‚Äôs primary endpoint.</p>
<p>The more expensive the experiment, the less likely it is to be repeated to gather a new measurement of the effects of the same manipulation. Thus, larger studies present a stronger rationale for including multiple measures. In contrast, short studies are often easier to repeat multiple times using different measures each time without any worry about fatigue or contamination effects.</p>
<p>If you do include multiple measures, selective reporting of significant or hypothesis-aligned measures becomes a risk. For this reason, preregistration and appropriate reporting of all outcomes becomes even more important. This issue has received a lot of attention in the medical literature. Clinical trials often involve interventions that can have effects on many different measures; imagine a cancer treatment that might affect mortality rates, quality of life, tumor growth rates, etc. Further, such trials are extremely expensive. So there is a strong rationale for including more measures.</p>
<p>A final reason to consider multiple measures is if you intend the design of your study to provide differential validity evidence for your experimental manipulation. That is, if you want to show that your induction affects one construct but not another, it will be critical to measure both constructs.</p>
</div>
<div id="chapter-summary" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Chapter summary</h2>
<p>In olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians thought a lot about how different problems on tests had different measurement properties. They began exploring how to select good and bad items, and how to figure out people‚Äôs ability abstracted away from specific items. This led to a profusion of interesting ideas about measurement and modeling, but these ideas mostly got adopted in observational research.</p>
<p>Cognitive and social psychologists, on the other hand, spurned measure validation and item-level variation. Cognitive psychologists did lots of trials, all generated from the same basic template. They measured quantities of interest with high precision just by generating tons of trials, but they didn‚Äôt really worry about tests of measure validity or experimental reliability. Social psychologists also didn‚Äôt worry as much about these issues and spent more time on issues of ecological validity in their experiments.</p>
<p>These sociological differences between fields has led to an unfortunate divergence, where experimentalists often do not recognize the value of the conceptual tools developed in observational fields, and hence fail to reason about the reliability and validity of their measures in ways that can help them make better experimental measurements. The fundamental insight of the psychometric perspective is that the constructs we study as psychologists are latent, rather than observed. So when we attempt to measure these constructs, we need to understand the properties of our measures and how we hypothesize that they connect to the constructs of interest. As we said in our discussion of reliability, ignorance is not bliss. Even if you fail to make explicit assumptions about how your measure functions and how it connects to your construct of interest, your adoption of defaults still constitutes a choice. Much better to think these choices through!</p>
<!-- ::: {.accident-report} -->
<!-- ‚ö†Ô∏è Accident report: failure of measurement invariance in IQ example (the use of precious stone names as vocabulary items) leading to potentially spurious conclusions (Wicherts and Dolan 2010). -->
<!-- ::: -->

</div>
</div>
<p style="text-align: center;">
<a href="6-regression.html"><button class="btn btn-default">Previous</button></a>
<a href="8-design.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<script src="toc/toc.js"></script>


</body>
</html>
