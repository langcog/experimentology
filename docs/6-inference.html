<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 6 Inference | Experimentology" />
<meta property="og:type" content="book" />





<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 6 Inference | Experimentology">

<title>Chapter 6 Inference | Experimentology</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.18/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Preliminaries</b></span></li>
<li><a href="1-experiments.html#experiments"><span class="toc-section-number">1</span> Experiments</a></li>
<li><a href="2-theories.html#theories"><span class="toc-section-number">2</span> Theories</a></li>
<li><a href="3-replication.html#replication"><span class="toc-section-number">3</span> Replication and reproducibility</a></li>
<li><a href="4-ethics.html#ethics"><span class="toc-section-number">4</span> Ethics</a></li>
<li class="part"><span><b>II Statistics</b></span></li>
<li><a href="5-estimation.html#estimation"><span class="toc-section-number">5</span> Estimation</a></li>
<li><a href="6-inference.html#inference"><span class="toc-section-number">6</span> Inference</a></li>
<li><a href="7-models.html#models"><span class="toc-section-number">7</span> Models</a></li>
<li class="part"><span><b>III Design and Planning</b></span></li>
<li><a href="8-measurement.html#measurement"><span class="toc-section-number">8</span> Measurement</a></li>
<li><a href="9-design.html#design"><span class="toc-section-number">9</span> Design of experiments</a></li>
<li><a href="10-sampling.html#sampling"><span class="toc-section-number">10</span> Sampling</a></li>
<li class="part"><span><b>IV Execution</b></span></li>
<li><a href="11-prereg.html#prereg"><span class="toc-section-number">11</span> Preregistration</a></li>
<li><a href="12-collection.html#collection"><span class="toc-section-number">12</span> Data collection</a></li>
<li><a href="13-management.html#management"><span class="toc-section-number">13</span> Project management</a></li>
<li class="part"><span><b>V Analysis and Reporting</b></span></li>
<li><a href="14-viz.html#viz"><span class="toc-section-number">14</span> Visualization</a></li>
<li><a href="15-writing.html#writing"><span class="toc-section-number">15</span> Writing</a></li>
<li><a href="16-meta.html#meta"><span class="toc-section-number">16</span> Meta-analysis</a></li>
<li><a href="17-conclusions.html#conclusions"><span class="toc-section-number">17</span> Conclusions</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li><a href="A-git.html#git"><span class="toc-section-number">A</span> GitHub Tutorial</a></li>
<li><a href="B-rmarkdown.html#rmarkdown"><span class="toc-section-number">B</span> R Markdown Tutorial</a></li>
<li><a href="C-tidyverse.html#tidyverse"><span class="toc-section-number">C</span> Tidyverse Tutorial</a></li>
<li><a href="D-ggplot.html#ggplot"><span class="toc-section-number">D</span> ggplot Tutorial</a></li>
<li><a href="E-instructors.html#instructors"><span class="toc-section-number">E</span> Instructor‚Äôs guide</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="inference" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Inference</h1>
<!-- ```{r inference-meme} -->
<!-- knitr::include_graphics("images/inference/meme.jpg") -->
<!-- ``` -->
<div class="learning-goals">
<p>üçé Learning goals:</p>
<ul>
<li>Discuss the purpose of statistical inference</li>
<li>Define <span class="math inline">\(p\)</span>-values and Bayes Factors</li>
<li>Consider common fallacies about inference (especially for <span class="math inline">\(p\)</span>-values)</li>
<li>Reason about sampling variability</li>
<li>Define and reason about confidence intervals</li>
</ul>
</div>
<p>We‚Äôve been arguing that experiments are about measuring effects. You might ask then, why does this book even need a chapter about statistical inference? Why can‚Äôt we just report our estimates and be done? The answer is that we are not typically interested in the characteristics of our sample specifically. <strong>Statistical inference</strong> is the process of going beyond the specific characteristics of the sample that you measured to make generalizations about the broader <strong>population</strong>.</p>
<p>Inference methods allow us to ask questions like:</p>
<ol style="list-style-type: decimal">
<li>How likely is it that this pattern of measurements was produced by chance variation?</li>
<li>Do these data provide more support for one hypothesis or another?</li>
<li>How big is the effect of a manipulation, and how precise is our estimate of that effect?</li>
<li>What portion of the variation in the data is due to a particular manipulation (as opposed to variation between participants, stimulus items, or other manipulations)?</li>
</ol>
<p>Question (1) is associated with one particular type of statistical inference method ‚Äì <strong>null hypothesis significance testing</strong> (NHST) in the <strong>frequentist</strong> statistical tradition. NHST has become synonymous with data analysis, such that in the vast majority of research papers (and research methods courses), all of the reported analyses are tests of this type. Yet the equivalence of NHST ‚Äì in particular, the dichotomous inference that something is ‚Äúsignificant‚Äù or not ‚Äì and the idea of data analysis more generally has been quite problematic.</p>
<p>The instinct to ‚Äúgo test for significance‚Äù before visualizing your data and trying to understand sources of variation (participants, items, manipulations, etc.) is one of the most unhelpful things an experimenter can do. Whether <span class="math inline">\(p &lt; .05\)</span> or not, a test of this sort gives you literally <em>one bit</em> of information about your data.<label for="tufte-sn-71" class="margin-toggle sidenote-number">71</label><input type="checkbox" id="tufte-sn-71" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">71</span> In the information theoretic sense, as well as the common sense!</span> Considering effect sizes and their variation more holistically, including using the kinds of visualizations we advocate in Chapter <a href="14-viz.html#viz">14</a>, gives you a much richer sense of what happened in your experiment!</p>
<p>In this chapter, we will describe NHST, the conventional method that many students still learn (and many scientists still use) as their primary method for engaging with data. All practicing experimentalists need to understand NHST, both to read the literature and also to apply this method in appropriate situations. For example, NHST may be a reasonable tool for testing whether an intervention leads to a difference between a treatment condition and an appropriate control, although it still doesn‚Äôt tell you about the size of the intervention effect! But we will also try to contextualize NHST as a very special case of a broader set of strategies around modeling and inference. Further, we will continue to flesh out our account of how some of the pathologies of NHST have been a driver of the replication crisis.</p>
<p><label for="tufte-mn-2" class="margin-toggle">‚äï</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;"><img src="images/inference/krushke.png"/> Clarifying the distinctions between Bayesian and Frequentist paradigms and the ways that they approach inference and estimation. For many settings, we think the estimation mindset is more useful. From Kruschke and Liddell (2018).</span></span></p>
<p>What should replace NHST? There has been a recent move towards the use of Bayes Factors to quantify the evidence in support of a hypothesis. Bayes Factors can help answer questions like (2). We introduce these tools, and believe that they have broader applicability than the NHST framework and should be known by students. On the other hand, Bayes Factors are not a panacea. They have the same problems as NHST when they are applied dichotomously.</p>
<p>Instead of dichotomous frequentist or Bayesian inference, we advocate for <strong>estimation</strong> and <strong>modeling</strong> strategies, which are more suited towards questions (3) and (4) <span class="citation">(<a href="#ref-cumming2014" role="doc-biblioref">Cumming, 2014</a>; <a href="#ref-kruschke2018" role="doc-biblioref">Kruschke &amp; Liddell, 2018</a>)</span>. The goal of these strategies is to yield an accurate and precise estimate of the relationships underlying observed variation in the data. One of these estimates is the causal effect of the experimental manipulation(s), which we introduced in the last chapter. That explains our affection for these approaches: if a good theory predicts these kinds of causal effects, it makes sense that we‚Äôd want to estimate them precisely! What we‚Äôll do in this chapter is build on that estimation strategy to understand what we learn about the population we‚Äôre trying to measure from our sample estimate.</p>
<p>This isn‚Äôt a statistics book and we won‚Äôt attempt to teach the full array of important statistical concepts that will allow students to build good models of a broad array of datasets. (Sorry!).<label for="tufte-sn-72" class="margin-toggle sidenote-number">72</label><input type="checkbox" id="tufte-sn-72" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">72</span> If you‚Äôre interested in going deeper, here are two books that have been really influential for us. The first is <span class="citation">Gelman &amp; Hill (<a href="#ref-gelman2006b" role="doc-biblioref">2006</a>)</span> and its successor <span class="citation">Gelman et al. (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>, which teach regression and multi-level modeling from the perspective of data description. The second is <span class="citation">McElreath (<a href="#ref-mcelreath2018" role="doc-biblioref">2018</a>)</span>, a course on building Bayesian models of the causal structure of your data. Honestly, neither is an easy book to sit down and read (unless you are the kind of person who reads stats books on the subway for fun) but both really reward detailed study. We encourage you to get together a reading group and go through the exercises together. It‚Äôll be well worth while in its impact on your statistical and scientific thinking.</span> But we do want you to be able to reason about inference and modeling. In this chapter, we‚Äôll start by doing some inferences about our tea-tasting example from the last chapter, using this example to build up intuitions about inference and estimation. Then in Chapter <a href="7-models.html#models">7</a>, we‚Äôll start to look at more sophisticated models.</p>
<div id="sampling-variation" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Sampling variation</h2>
<p>In Chapter <a href="5-estimation.html#estimation">5</a>, we introduced Fisher‚Äôs tea-tasting experiment and discussed how to estimate means and differences in means from our observed data. These so-called ‚Äúpoint estimates‚Äù represent our best guesses about the population parameters given the data and possibly also given our prior beliefs. But on their own, point estimates do not say anything about how much statistical uncertainty is involved in these point estimates.<label for="tufte-sn-73" class="margin-toggle sidenote-number">73</label><input type="checkbox" id="tufte-sn-73" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">73</span> The measures of variability that we discuss here account for statistical uncertainty reflecting the fact that we have only a finite sample size. If the sample size were infinite, there would be no uncertainty of this kind. Sampling-based uncertainty is only one kind of uncertainty, though: a more holistic view of the overall credibility of an estimate should also account for study design and bias, among other things</span> Quantifying and reasoning about this uncertainty is an important goal: If we were to repeat the tea-tasting experiment with 500 participants instead of 48, we would have less uncertainty with 500 participants, even if the point estimates happened to be identical.</p>
<div id="standard-errors" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Standard errors</h3>
<p>To characterize the uncertainty in an estimate, it helps to picture what is called its <strong>sampling distribution</strong>, which is the distribution of the estimate across different, hypothetical samples. That is, let‚Äôs imagine ‚Äì purely hypothetically ‚Äì that we conducted the tea experiment with <span class="math inline">\(n=48\)</span> not just once, but hundreds or even thousands of times. This idea is often called <strong>repeated sampling</strong> as a shorthand. For each hypothetical sample, we use similar recruitment methods to recruit a new sample of participants, and we estimate <span class="math inline">\(\widehat{\beta}\)</span> in that sample. Would we get exactly the same answer each time? No, simply because the samples will have some random variability. If we plotted these estimates, <span class="math inline">\(\widehat{\beta}\)</span>, across thousands of samples, we would get the sampling distribution in Figure <a href="6-inference.html#fig:inference-sampling-48">6.1</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-sampling-48"></span>
<img src="experimentology_files/figure-html/inference-sampling-48-1.png" alt="Sampling distribution for the treatment effect in the tea-tasting experiment, given 1000 different experiments, each with N=48." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.1: Sampling distribution for the treatment effect in the tea-tasting experiment, given 1000 different experiments, each with N=48.<!--</p>-->
<!--</div>--></span>
</p>
<p>Now imagine we also did thousands of repetitions of the experiment with <span class="math inline">\(n=500\)</span> instead of <span class="math inline">\(n=48\)</span>. Figure <a href="6-inference.html#fig:inference-sampling-500">6.2</a> shows what the sampling distribution might look like. Notice how much narrower the sampling distribution becomes when we increase the sample size, showing our decreased uncertainty. More formally, the standard deviation of the sampling distribution itself, called the <strong>standard error</strong>, decreases as the sample size increases.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-sampling-500"></span>
<img src="experimentology_files/figure-html/inference-sampling-500-1.png" alt="Sampling distribution for the treatment effect as above, but now with N=500." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.2: Sampling distribution for the treatment effect as above, but now with N=500.<!--</p>-->
<!--</div>--></span>
</p>
<p>The sampling distribution is not the same thing as the distribution of tea ratings in a single sample. Instead, it‚Äôs a distribution of <em>estimates across samples of a given size</em>. Internalizing this distinction is a crucial component to understanding all statistical models and tests. If we were in the classroom with you, we would pound our fist on the lecture podium for emphasis!</p>
</div>
<div id="the-central-limit-theorem" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> The central limit theorem</h3>
<p>We talked in the last chapter about the normal distribution, a convenient and ubiquitous tool for quantifying the distribution of measurements. A shocking thing about sampling distributions for many kinds of estimates ‚Äì and for <em>all</em> maximum likelihood estimates ‚Äì is that they become normally distributed as the sample size gets larger and larger. This result holds even for estimates that are not even remotely normally distributed in small samples!</p>
<p>For example, say we are flipping a coin and we want to estimate the probability that it lands heads (<span class="math inline">\(p_H\)</span>). If we draw samples each consisting of only <span class="math inline">\(n=2\)</span> coin flips, Figure <a href="6-inference.html#fig:inference-coin-2">6.3</a> is the sampling distribution of the estimates (<span class="math inline">\(\widehat{p}_H\)</span>). This sampling distribution doesn‚Äôt look normally distributed at all ‚Äì it doesn‚Äôt have the characteristic ‚Äúbell curve‚Äù look! <span class="math inline">\(\widehat{p}_H\)</span> can only take on the values 0, 0.5, or 1 in a sample of only 2 coin flips.</p>
<!-- Julie's comment: For Figure 6.4, it might be nice to also put the x axis directly below the n = 2 and n = 8 coin flips figures (right now it's only at the bottom of the n = 32 and n = 128 figures). I say this because an important observation that I think helps readers orient to these figures is the fact that for n = 2, only values 0, 0.5 and 1 are possible, as helpfully described above. Right now it looks like "32" which is the heading of the figure below it, is the central value of the x axis which could potentially be confusing. -->
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-coin-2"></span>
<img src="experimentology_files/figure-html/inference-coin-2-1.png" alt="Samping distribution of samples from a biased coin with N=2 flips in each sample." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.3: Samping distribution of samples from a biased coin with N=2 flips in each sample.<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-coin-ns"></span>
<img src="experimentology_files/figure-html/inference-coin-ns-1.png" alt="Sampling distribution for 2, 8, 32, and 128 flips." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.4: Sampling distribution for 2, 8, 32, and 128 flips.<!--</p>-->
<!--</div>--></span>
</p>
<p>But look what happens as we draw increasingly larger samples in Figure <a href="6-inference.html#fig:inference-coin-ns">6.4</a>: We get a normal distribution! This tendency of sampling distributions to become normal as <span class="math inline">\(n\)</span> becomes very large reflects a deep and elegant mathematical law called the <strong>Central Limit Theorem</strong>. Aesthetic swooning aside, the practical upshot is that the Central Limit Theorem directly helps us characterize the uncertainty of sample estimates. For example, when the sample size is reasonably large (approximately <span class="math inline">\(n&gt;30\)</span> in the case of sample means) the standard error (i.e., the standard deviation of the sampling distribution) of a sample mean is approximately <span class="math inline">\(\widehat{SE} = \sigma/\sqrt{n}\)</span>. That is why the sampling distribution becomes narrower as the sample size increases.</p>
</div>
</div>
<div id="from-variation-to-inference" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> From variation to inference</h2>
<p>OK, let‚Äôs go back to Fisher‚Äôs tea-tasting experiment. The first innovation of that experiment was the use of randomization to recover an estimate of the causal effect of milk ordering. But there was more to Fisher‚Äôs analysis than we described.</p>
<p>The second innovation of the tea-tasting experiment was the idea of creating a model of what might happen during the experiment. Specifically, Fisher described a hypothetical <strong>null model</strong> that would arise if the nameless lady had chosen cups by chance rather than because of some tea sensitivity. In our tea-rating experiment, the null model describes what happens when there is no difference in ratings between tea-first and milk-first cups. Under the null model, the treatment effect (<span class="math inline">\(\beta_{\text{milk-first}}\)</span>) is zero.</p>
<p>Even with an actual treatment effect of zero, across repeated sampling, we should see some variation in <span class="math inline">\(\hat{\beta}_{\text{milk-first}}\)</span>, our <em>estimate</em> of the treatment effect. Sometimes we‚Äôll get a small positive effect, sometimes a small negative one. Occasionally just by chance we‚Äôll get a big effect. That‚Äôs the same sampling variation we described above.</p>
<p>Fisher‚Äôs innovation was to quantify the probability of observing various values of <span class="math inline">\(\hat{\beta}\)</span>, given the null model. Then, if the observed data were very low probability under the null model, we could declare that the null was rejected. How unlikely must the observed data be, in order to reject the null? Fisher declared that it is ‚Äúusual and convenient for experimenters to take 5 percent as a standard level of convenience,‚Äù establishing the .05 cutoff that has become gospel throughout the sciences.<label for="tufte-sn-74" class="margin-toggle sidenote-number">74</label><input type="checkbox" id="tufte-sn-74" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">74</span> Actually, right after establishing .05 as a cutoff, Fisher then writes that ‚Äúin the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon‚Ä¶ in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.‚Äù In other words, Fisher was all for replication!</span></p>
<p>Let‚Äôs take a look at what the null model might look like. We already tried out repeating our tea-tasting experiment thousands of times in our discussion of sampling above. Now in Figure <a href="6-inference.html#fig:inference-null-model">6.5</a>, we do the same thing but we assume that the <strong>null hypothesis</strong> of no treatment effect is true. The histogram shows the distribution of treatment effects <span class="math inline">\(\hat{\beta}\)</span> we observe: some a little negative, some a little positive, and a few substantially positive or negative, but mostly zero.</p>
<p>Let‚Äôs apply Fisher‚Äôs standard. If our observation has less than a 5% probability under the null model, then the null model is likely wrong. The red dashed lines on Figure <a href="6-inference.html#fig:inference-null-model">6.5</a> show the point below which only 2.5% of the data are found and the point above which only 2.5% of the data are found. These are called the <strong>tails</strong> of the distribution. Because we‚Äôd be equally willing to accept milk-first tea or tea-first tea being better, we consider both positive and negative observations as possible.<label for="tufte-sn-75" class="margin-toggle sidenote-number">75</label><input type="checkbox" id="tufte-sn-75" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">75</span> Because we‚Äôre looking at both tails of the distribution, this is called a ‚Äútwo-tailed‚Äù test.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-null-model"></span>
<img src="experimentology_files/figure-html/inference-null-model-1.png" alt="Distribution of treatment effects under the null model of no difference in tea preparation order. The red lines indicate the threshold outside of which less than 5\% of observations fall." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.5: Distribution of treatment effects under the null model of no difference in tea preparation order. The red lines indicate the threshold outside of which less than 5% of observations fall.<!--</p>-->
<!--</div>--></span>
</p>
<p>That‚Äôs the logic of Fisherian NHST: if the observed data fall in the region that has a probability of less than .05 under the null model, then we reject the null. So then when we observe some particular treatment effect <span class="math inline">\(\hat{\beta}\)</span> in a single (real) instance of our experiment, we can compute the probability of these data or any data more extreme than ours under the null model. This probability is our <span class="math inline">\(p\)</span>-value, and if it is small, it gives us license to conclude that the null is false.</p>
<p>As we saw before, the larger the sample size, the smaller the standard error. That‚Äôs true for the null model too! Let‚Äôs take a look at running tea-tasting experiments with 12, 24, 48, and 96 participants. Figure <a href="6-inference.html#fig:inference-null-model2">6.6</a> shows the expected null distributions in each of these, again with the 5% boundaries shown in red.</p>
<div class="figure"><span id="fig:inference-null-model2"></span>
<p class="caption marginnote shownote">
Figure 6.6: Distribution of treatment effects under the null model for different numbers of participants (facets). Red lines indicate the p &lt; .05 threshold.
</p>
<img src="experimentology_files/figure-html/inference-null-model2-1.png" alt="Distribution of treatment effects under the null model for different numbers of participants (facets). Red lines indicate the p &lt; .05 threshold." width="\linewidth"  />
</div>
<p>The more participants in the experiment, the tighter the null distribution becomes, and hence the smaller the region in which we should expect a null treatment effect to fall. Because our expectation based on the null becomes more precise, we will be able to reject the null based on smaller treatment effects. In inference of this type, as with estimation, our goals matter. If we‚Äôre merely testing a hypothesis out of curiosity, perhaps we don‚Äôt want to measure too many cups of tea. But if we were designing the tea strategy for a major cafe chain, the stakes would be higher and a more precise estimate might necessary; in that case, maybe we‚Äôd want to do a more extensive experiment!</p>
<p>One last note: You might notice an interesting parallel between Fisher‚Äôs paradigm for NHST and Popper‚Äôs falsificationist philosophy (introduced in Chapter <a href="2-theories.html#theories">2</a>). In both cases, you never get to accept the actual hypothesis of interest. The only thing you can do is observe evidence that is inconsistent. The added limitation of NHST is that the only hypothesis you can falsify is the null!</p>
</div>
<div id="making-inferences" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Making inferences</h2>
<p>In the tea-tasting example we were just considering, we were trying to make an inference from our sample to the broader population. In particular, we were trying to test whether milk-first tea was rated as better than tea-first tea. Our inferential goal was a clear, binary answer: is milk-first tea better?</p>
<p>By defining a <span class="math inline">\(p\)</span>-value, we got one procedure for giving this answer. If <span class="math inline">\(p &lt; .05\)</span>, we reject the null. Then we can look at the direction of the difference and, if it‚Äôs positive, declare that milk-first tea is ‚Äúsignificantly‚Äù better. Let‚Äôs compare this procedure to a different process that builds on the Bayesian estimation ideas we described in the previous chapter. We can then come back to NHST in light of that framework.</p>
<div id="bayes-factors" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Bayes Factors</h3>
<p>Bayes Factors are a method for quantifying the support for one hypothesis over another, based on an observed dataset. Informally, we‚Äôve now talked about two different distinct hypotheses about the tea situation: our participant could have <em>no</em> tea discrimination ability ‚Äì leading to chance performance. We call this <span class="math inline">\(H_0\)</span>. Or they could have some non-zero ability ‚Äì leading to greater than chance performance. We call this <span class="math inline">\(H_1\)</span>. The Bayes Factor is simply the likelihood of the data (in the technical sense used above) under <span class="math inline">\(H_1\)</span> vs.¬†under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[
BF = \frac{\color{red}{p(d | H_1)}}{\color{red}{p(d | H_0)}}.
\]</span></p>
<p>The Bayes Factor is a ratio, so if it is greater than 1, the data are more likely under <span class="math inline">\(H_1\)</span> than they are under <span class="math inline">\(H_0\)</span> ‚Äì and vice versa for values between 1 and 0. A BF of 3 means there is three times as much evidence for <span class="math inline">\(H_1\)</span> than <span class="math inline">\(H_0\)</span>, or equivalently 1/3 as much evidence for <span class="math inline">\(H_0\)</span> as <span class="math inline">\(H_1\)</span>.<label for="tufte-sn-76" class="margin-toggle sidenote-number">76</label><input type="checkbox" id="tufte-sn-76" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">76</span> Sometimes people refer to the BF in favor of <span class="math inline">\(H_1\)</span> as the <span class="math inline">\(BF_{10}\)</span> and the BF in favor of <span class="math inline">\(H_0\)</span> as the <span class="math inline">\(BF_{01}\)</span>. This notation strikes us as a bit confusing because a reader might wonder what the 10 in the subscript means.</span></p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:inference-jeffreys">Table 6.1: </span>Jeffreys‚Äô (1961/1998) interpretation guidelines for Bayes Factors.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="left">BF range</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">&lt; 1</td>
<td align="left">Negative evidence (supports H0)</td>
</tr>
<tr class="even">
<td align="left">1 ‚Äì 3</td>
<td align="left">Barely worth mentioning</td>
</tr>
<tr class="odd">
<td align="left">3 ‚Äì 10</td>
<td align="left">Substantial</td>
</tr>
<tr class="even">
<td align="left">10 ‚Äì 30</td>
<td align="left">Strong</td>
</tr>
<tr class="odd">
<td align="left">30 ‚Äì 100</td>
<td align="left">Very strong</td>
</tr>
<tr class="even">
<td align="left">&gt; 100</td>
<td align="left">Decisive</td>
</tr>
</tbody>
</table>
<p>There are a couple of things to notice about the Bayes Factor. The first is that, like a p-value, it is inherently a continuous measure. You can artificially dichotomize decisions based on the Bayes Factor by declaring a cutoff (say, BF &gt; 3 or BF &gt; 10), there is no intrinsic threshold at which you would say the evidence is ‚Äúsignificant.‚Äù Many people follow guidelines from <span class="citation">Jeffreys (<a href="#ref-jeffreys1998" role="doc-biblioref">1998</a>)</span>, shown in Table <a href="6-inference.html#tab:inference-jeffreys">6.1</a>. On the other hand, cutoffs like BF &gt; 3 or <span class="math inline">\(p &lt; .05\)</span> are not very informative. So although we provide this table to guide interpretation, we caution that you should always report and interpret the actual Bayes Factor, not whether it is above or below some cutoff.</p>
<p>The second thing to notice about the Bayes Factor is that it doesn‚Äôt depend on our prior probability of <span class="math inline">\(H_1\)</span> vs.¬†<span class="math inline">\(H_0\)</span>. We might think of <span class="math inline">\(H_1\)</span> as very implausible. But the BF is independent of that prior belief. So that means it‚Äôs a measure of how much the evidence should shift our beliefs away from our prior. One nice way to think about this is that the Bayes Factor computes how much our beliefs ‚Äì whatever they are ‚Äì should be changed by the data <span class="citation">(<a href="#ref-morey2011" role="doc-biblioref">Morey &amp; Rouder, 2011</a>)</span>.</p>
<p>In practice, the thing that is both tricky and good about Bayes Factors is that you need to define an actual model of what <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are. That process involves making some assumptions explicit. We won‚Äôt go into how to make these models here ‚Äì this is a big topic that is covered extensively in books on Bayesian data analysis.<label for="tufte-sn-77" class="margin-toggle sidenote-number">77</label><input type="checkbox" id="tufte-sn-77" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">77</span> Two good ones are <span class="citation">Gelman et al. (<a href="#ref-gelman1995" role="doc-biblioref">1995</a>)</span> (a bit more statistical) and <span class="citation">Kruschke (<a href="#ref-kruschke2014" role="doc-biblioref">2014</a>)</span> (a bit more focused on psychological data analysis). An <a href="https://vasishth.github.io/bayescogsci/book/">in-prep web-book by Nicenboim et al.</a> also looks great.</span> Below and in Chapter <a href="7-models.html#models">7</a> we will provide some guidance for how to compute Bayes Factors for simple experimental designs. The goal here is just to give a sense of how they work.</p>
<div class="figure"><span id="fig:inference-milk-first"></span>
<p class="caption marginnote shownote">
Figure 6.7: Ratings of the quality of milk-first tea, with the best fitting normal distribution shown in blue (mean shown by the dashed line).
</p>
<img src="experimentology_files/figure-html/inference-milk-first-1.png" alt="Ratings of the quality of milk-first tea, with the best fitting normal distribution shown in blue (mean shown by the dashed line)." width="\linewidth"  />
</div>
<!-- Julie's comment: The caption for Figure 6.7 mentions a best fitting normal distribution in blue and means as dashed lines but the figure only seems to show data points. Not sure if the caption is out-of-date or the figure hasn't been updated, but would be good to align. -->
<p>Figure <a href="6-inference.html#fig:inference-milk-first">6.7</a> shows one example dataset. Once we set up our models, we can compute the relative likelihood of the data under each.<label for="tufte-sn-78" class="margin-toggle sidenote-number">78</label><input type="checkbox" id="tufte-sn-78" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">78</span> In this case, we use the default Bayesian <span class="math inline">\(t\)</span>-test provided by the <code>BayesFactor</code> package.</span> The resulting BF is 1.58, somewhere between anecdotal and substantial evidence. On the other hand, if we double the size of the experiment to 96 participants, the BF goes up to 299.04, which is quite strong evidence.</p>
<p>That‚Äôs the Bayes Factor. Now let‚Äôs turn back to NHST and the <span class="math inline">\(p\)</span>-value.</p>
</div>
<div id="p-values" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> <em>p</em>-values</h3>
<p>We already have a working definition of what a <span class="math inline">\(p\)</span>-value is from our discussion above: it‚Äôs the <strong>probability of the data (or any data that would be more extreme) under the null hypothesis</strong>. How is this quantity related to either our Bayesian estimate or the BF? Well, the first thing to notice is that the <span class="math inline">\(p\)</span>-value is very close (but not identical) to the likelihood itself.<label for="tufte-sn-79" class="margin-toggle sidenote-number">79</label><input type="checkbox" id="tufte-sn-79" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">79</span> The likelihood ‚Äì for both Bayesians and frequentists ‚Äì is the probability of the data, just like the <span class="math inline">\(p\)</span>-value. But unlike the <span class="math inline">\(p\)</span>-value, it doesn‚Äôt include the probability of more extreme data as well.</span></p>
<p>Next we can use a simple statistical test, a <span class="math inline">\(t\)</span>-test, to compute <span class="math inline">\(p\)</span>-values for our experiment. In case you haven‚Äôt encountered one, a <span class="math inline">\(t\)</span>-test is a procedure for computing a <span class="math inline">\(p\)</span>-value by comparing the distribution of two variables using the null hypothesis that there is no difference between them.<label for="tufte-sn-80" class="margin-toggle sidenote-number">80</label><input type="checkbox" id="tufte-sn-80" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">80</span> <span class="math inline">\(t\)</span>-tests can also be used in cases where one sample is being compared to some baseline.</span> The <span class="math inline">\(t\)</span>-test uses the data to compute a <strong>test statistic</strong> whose distribution under the null hypothesis is known. Then the value of this statistic can be converted to <span class="math inline">\(p\)</span>-values for making an inference.</p>
<p>When we conduct a <span class="math inline">\(t\)</span>-test on our experimental results, we see that the difference between the two groups is statistically significant at <span class="math inline">\(p&lt;.05\)</span>: <span class="math inline">\(t(18) = 2.04\)</span>, <span class="math inline">\(p = .056\)</span>. Here we‚Äôve given a standard American Psychological Association report of a <span class="math inline">\(t\)</span>-test. The first part of this report on the test gives the <span class="math inline">\(t\)</span> value, qualified by the <strong>degrees of freedom</strong> for the test in parentheses. We won‚Äôt focus much on the idea of degrees of freedom here, but for now it‚Äôs enough to know that this number quantifies the amount of information given by the data, in this case 48 datapoints minus the two means (one for each of the samples).</p>
<!-- Julie's comment: The data being used through the papaja function doesn't seem to match the text around it. For instance, the p value is p = .056 but the text reports that as being sig at p < .05. Similarly, the df is 18 but the text around it describes the df to be 46 ("48 datapoints minus the two means)". Maybe there is a mismatch somewhere? -->
<p>Let‚Äôs compare <span class="math inline">\(p\)</span> values and default Bayes Factors for a couple of different conditions. In Table <a href="6-inference.html#tab:p-bf-comparison">6.2</a>, you can see a set of simulated experiments with varying total numbers of participants and varying average treatment effects. Both <span class="math inline">\(p\)</span> and BF go up with more participants and larger effects. But in general BFs tend to be a bit more conservative than <span class="math inline">\(p\)</span>-values, such that <span class="math inline">\(p&lt;.05\)</span> can sometimes translate to a BF of less than 3 <span class="citation">(<a href="#ref-benjamin2018" role="doc-biblioref">Benjamin et al., 2018</a>)</span>.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:p-bf-comparison">Table 6.2: </span>Comparison of p-value and BF for several different tea-tasting scenarios.</span><!--</caption>--></p>
<table>
<thead>
<tr class="header">
<th align="right">N</th>
<th align="right">Delta</th>
<th align="right">p value</th>
<th align="right">BF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">12</td>
<td align="right">0.5</td>
<td align="right">0.694</td>
<td align="right">0.493</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="right">1.0</td>
<td align="right">0.701</td>
<td align="right">0.491</td>
</tr>
<tr class="odd">
<td align="right">12</td>
<td align="right">1.5</td>
<td align="right">0.078</td>
<td align="right">1.391</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">0.5</td>
<td align="right">0.002</td>
<td align="right">20.170</td>
</tr>
<tr class="odd">
<td align="right">24</td>
<td align="right">1.0</td>
<td align="right">0.388</td>
<td align="right">0.495</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">1.5</td>
<td align="right">0.007</td>
<td align="right">7.025</td>
</tr>
<tr class="odd">
<td align="right">48</td>
<td align="right">0.5</td>
<td align="right">0.071</td>
<td align="right">1.129</td>
</tr>
<tr class="even">
<td align="right">48</td>
<td align="right">1.0</td>
<td align="right">0.111</td>
<td align="right">0.833</td>
</tr>
<tr class="odd">
<td align="right">48</td>
<td align="right">1.5</td>
<td align="right">0.001</td>
<td align="right">40.667</td>
</tr>
<tr class="even">
<td align="right">96</td>
<td align="right">0.5</td>
<td align="right">0.173</td>
<td align="right">0.493</td>
</tr>
<tr class="odd">
<td align="right">96</td>
<td align="right">1.0</td>
<td align="right">0.000</td>
<td align="right">554.228</td>
</tr>
<tr class="even">
<td align="right">96</td>
<td align="right">1.5</td>
<td align="right">0.000</td>
<td align="right">45172.635</td>
</tr>
</tbody>
</table>
<p>The critical thing about <span class="math inline">\(p\)</span>-values, though, is not just that they are a specific kind of data likelihoods. It is that they are used in a specific inferential procedure. The logic of NHST is that we make a binary decision about the presence of an effect. If <span class="math inline">\(p &lt; .05\)</span>, the null hypothesis is rejected; otherwise not. As <span class="citation">R. A. Fisher (<a href="#ref-fisher1949" role="doc-biblioref">1949</a>)</span> wrote,</p>
<blockquote>
<p>It should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis. (p.¬†19)</p>
</blockquote>
<p>As we mentioned, though, we are usually interested in not just rejecting the null hypothesis but also in the evidence for the alternative (the one we are interested in). The Bayes Factor is one approach to quantifying positive evidence. This issue with the Fisher approach to <span class="math inline">\(p\)</span>-values has been known for a long time, though, and so there is an alternative use of <span class="math inline">\(p\)</span>-values that we introduce here.</p>
<!-- Julie's comment: Maybe it's just me but the last sentence above is a slightly confusing transition to the next section. -->
</div>
<div id="the-neyman-pearson-approach" class="section level3" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> The Neyman-Pearson approach</h3>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-power-alpha"></span>
<img src="images/inference/power-alpha.png" alt="Standard decision matrix for NHST." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.8: Standard decision matrix for NHST.<!--</p>-->
<!--</div>--></span>
</p>
<p>One way to ‚Äúpatch‚Äù NHST is to introduce a decision-theoretic view, shown in Figure <a href="6-inference.html#fig:inference-power-alpha">6.8</a>.[^inference-16] On this view, called the Neyman-Pearson view, there is a real <span class="math inline">\(H_1\)</span>, albeit one that is not specified. Then the true state of the world could be that <span class="math inline">\(H_0\)</span> is true or <span class="math inline">\(H_1\)</span> is true. The <span class="math inline">\(p&lt;.05\)</span> criterion is the threshold at which we are willing to reject the null, and so this constitutes our <strong>false positive</strong> rate <span class="math inline">\(\alpha\)</span>. But we also need to define a <strong>false negative</strong> rate, which is conventionally called <span class="math inline">\(\beta\)</span>. [OH NO! NOW WE HAVE ANOTHER ‚ÄúBETA‚Äù.]</p>
<p>Setting these rates is a decision problem: If you are too conservative in your criteria for the intervention having an effect, then you risk a false negative, where you incorrectly conclude that it doesn‚Äôt work. And if you‚Äôre too liberal in your assessment of the evidence, then you risk a false positive.<label for="tufte-sn-81" class="margin-toggle sidenote-number">81</label><input type="checkbox" id="tufte-sn-81" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">81</span> To make really rational decisions, you could couple this chart to some kind of utility function that assessed the costs of different outcomes. For example, you might think it‚Äôs worse to proceed with an intervention that doesn‚Äôt work than to stay with business as usual. In that case, you‚Äôd assign a higher cost to a false positive and accordingly try to adopt a more conservative criterion. We won‚Äôt cover this kind of decision analysis here, but <span class="citation">Pratt et al. (<a href="#ref-pratt1995" role="doc-biblioref">1995</a>)</span> is a classic textbook on statistical decision theory if you‚Äôre interested.</span> In practice, however, mostly people leave <span class="math inline">\(\alpha\)</span> at .05 and try to control their false negative rate by increasing their sample size.</p>
<p>As we saw in Figure <a href="6-inference.html#fig:inference-null-model">6.5</a>, the greater the sample, the better your chance of rejecting the null for any given non-null effect. But these chances will depend also on the effect size you are using. This formulation gives rise to the idea of classical power analysis, which we cover in Chapter <a href="10-sampling.html#sampling">10</a>. Most folks who defend binary inference using <span class="math inline">\(p\)</span>-values are interested in using the Neyman-Pearson approach. In our view, this approach has its place (it‚Äôs especially useful for power analysis) but it still suffers from the substantial issues that plague all binary inference techniques, especially those that use <span class="math inline">\(p\)</span>-values. We turn to some of these below.<br />
[^inference-16]: A little bit of useful history here is given in <span class="citation">Cohen (<a href="#ref-cohen1990" role="doc-biblioref">1990</a>)</span>.</p>
</div>
</div>
<div id="inference-and-its-discontents" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Inference and its discontents</h2>
<p>In the prior sections of this chapter, we reviewed NHST and Bayesian approaches to inference. Now it‚Äôs time to step back and think about some of the ways that inference practices ‚Äì especially those related to NHST ‚Äì have been problematic for psychology research. We‚Äôll begin with some issues surrounding <span class="math inline">\(p\)</span>-values and then give a specific accident report related to the process of ‚Äú<span class="math inline">\(p\)</span>-hacking.‚Äù We end by discussing the foundations for thinking about probability in light of these issues.</p>
<div id="problems-with-the-interpretation-of-p" class="section level3" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Problems with the interpretation of <em>p</em></h3>
<p><span class="math inline">\(p\)</span>-values are basically likelihoods. The likelihood of the data under the null hypothesis is a critical number to know ‚Äì for computing the Bayes Factor among other reasons. But it doesn‚Äôt tell us a lot of things that we might like to know! For example, it doesn‚Äôt tell us the probability of the data under any positive hypothesis that we might be interested in ‚Äì that‚Äôs the posterior probability <span class="math inline">\(p(H_1 | d)\)</span>. Lots of folks mistake these two for one another, e.g.¬†that the probability of the data given the null is the probability of the null given the data.</p>
<p>Another <span class="math inline">\(p\)</span>-value problem that comes up frequently is what to conclude when <span class="math inline">\(p&gt;.05\)</span>. According to the classical logic of NHST, the answer is ‚Äúnothing‚Äù! That is, a failure to reject the null does not mean that you can <em>accept</em> the null. Even if the probability of the data (or some more extreme data) under <span class="math inline">\(H_0\)</span> is high, their probability might be just as high or higher under <span class="math inline">\(H_1\)</span>.<label for="tufte-sn-82" class="margin-toggle sidenote-number">82</label><input type="checkbox" id="tufte-sn-82" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">82</span> Of course, weighing these two against one another brings you back to the Bayes Factor.</span> But many practicing researchers make this mistake. <span class="citation">Aczel et al. (<a href="#ref-aczel2018" role="doc-biblioref">2018</a>)</span> coded a sample of articles from 2015 and found that 72% of negative statements were inconsistent with the logic of their statistical paradigm of choice ‚Äì most were cases where researchers said that an effect was not present when they had simply failed to reject the null.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:dirty-dozen">Table 6.3: </span>A ‚Äúdirty dozen‚Äù <em>p</em> value misconceptions. Adapted from Goodman (2008).</span><!--</caption>--></p>
<table>
<colgroup>
<col width="2%" />
<col width="97%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"></th>
<th align="left">Misconception</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">If <em>p</em> = .05, the null hypothesis has only a 5% chance of being true.</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">A nonsignificant difference (eg, <em>p</em> ‚â•.05) means there is no difference between groups.</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">A statistically significant finding is clinically important.</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">Studies with <em>p</em> values on opposite sides of .05 are conflicting.</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">Studies with the same <em>p</em> value provide the same evidence against the null hypothesis.</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left"><em>p</em> = .05 means that we have observed data that would occur only 5% of the time under the null hypothesis.</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="left"><em>p</em> = .05 and <em>p</em> ‚â§.05 mean the same thing.</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="left"><em>p</em> values are properly written as inequalities (eg, '<em>p</em> ‚â§.02' when <em>p</em> = .015)</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="left"><em>p</em> = .05 means that if you reject the null hypothesis, the probability of a false positive error is only 5%.</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="left">With a <em>p</em> = .05 threshold for significance, the chance of a false positive error will be 5%.</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="left">You should use a one-sided <em>p</em> value when you don‚Äôt care about a result in one direction, or a difference in that direction is impossible.</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="left">A scientific conclusion or treatment policy should be based on whether or not the <em>p</em> value is significant.</td>
</tr>
</tbody>
</table>
<p>These are not the only issues with <span class="math inline">\(p\)</span>-values. In fact, people have so much trouble understanding what <span class="math inline">\(p\)</span>-values <em>do</em> say that there are whole articles written about these misconceptions. Table <a href="6-inference.html#tab:dirty-dozen">6.3</a> shows a set of misconceptions documented and refuted by <span class="citation">S. Goodman (<a href="#ref-goodman2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>Let‚Äôs take a look at just a few. Misconception 1 is that, if <span class="math inline">\(p= .05\)</span>, the null has a 5% chance of being true. This misconception is a result of confusing <span class="math inline">\(p(H_0 | d)\)</span> (the posterior) and <span class="math inline">\(p(d | H_0)\)</span> (the likelihood ‚Äì also known as the <span class="math inline">\(p\)</span>-value). Misconception 2 ‚Äì that <span class="math inline">\(p &gt; .05\)</span> allows us to <em>accept</em> the null ‚Äì also stems from this reversal of posterior and likelihood. And misconception 3 is a misinterpretation of the <span class="math inline">\(p\)</span>-value as an effect size (which we learned about in the last chapter): a large effect is likely to be clinically important, but with a small enough sample size, you can get a small <span class="math inline">\(p\)</span>-value even for a very small effect. We won‚Äôt go through all the misconceptions here, but we encourage you to challenge yourself to work through them (as in the exercise below).</p>
<!-- NC: The dirty dozen table doesn't seem that useful if we do not explain why these things are incorrect. MM agrees. We can probably discuss the fallacies for one or the other and then just invoke the CI-p-value duality for why the other set of fallacies is also wrong. Mika and I discuss the CI fallacies below. -->
<p>Beyond these misconceptions, there‚Äôs another problem. The <span class="math inline">\(p\)</span>-value is a probability of a certain set of events happening (corresponding to the observed data or any ‚Äúmore extreme‚Äù data, that is to say, data further from the null). Since <span class="math inline">\(p\)</span>-values are probabilities, we can combine them together across different events. If we run a null experiment ‚Äì an experiment where the expected effect is zero ‚Äì the probability of a dataset with <span class="math inline">\(p &lt; .05\)</span> is of course .05. But if we run two such experiments, we can get <span class="math inline">\(p &lt; .05\)</span> with probability 0.1. By the time we run 20 experiments, we have an 0.64 chance of getting a positive result.</p>
<p>It would obviously be a major mistake to run 20 null experiments and then report only the positive ones (which, by design, are false positives) as though these still were ‚Äústatistically significant.‚Äù The same thing applies to doing 20 different statistical tests within a single experiment. There are many statistical corrections that can be made to adjust for this problem.<label for="tufte-sn-83" class="margin-toggle sidenote-number">83</label><input type="checkbox" id="tufte-sn-83" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">83</span> The simplest and most versatile one, the Bonferroni correction, just divides .05 (or technically, whatever your threshold is) by the number of comparisons you are making. Using that correction, if you do 20 null experiments, you would have a 0 chance of a false positive, which is actually a little conservative.</span> But the the broader issue is one of transparency: unless you <em>know</em> what the appropriate set of experiments or tests is, it‚Äôs not possible to implement one of these corrections!<label for="tufte-sn-84" class="margin-toggle sidenote-number">84</label><input type="checkbox" id="tufte-sn-84" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">84</span> This issue is especially problematic with <span class="math inline">\(p\)</span>-values because they are so often presented as an independent set of tests, but the problem of multiple comparisons comes up when you compute a lot of independent Bayes Factors as well. ‚ÄúPosterior hacking‚Äù via selective reporting of Bayes Factors is perfectly possible <span class="citation">(<a href="#ref-simonsohn2014" role="doc-biblioref">Simonsohn, 2014</a>)</span>.</span></p>
<!-- P goes to 0 as data goes to infinity. -->
<div class="accident-report">
<p>‚ö†Ô∏è Accident report: Do extraordinary claims require extraordinary evidence?</p>
<p>In a blockbuster paper that inadvertently kicked off the replication crisis, <span class="citation">Bem (<a href="#ref-bem2011" role="doc-biblioref">2011</a>)</span> presented nine experiments showing evidence for precognition ‚Äì that is, sensing the future. In the first of these experiments, Bem showed each of a group of 100 undergraduates 36 two-alternative forced choice trials in which they had to guess which of two locations on a screen would reveal a picture immediately before the picture was revealed. By chance, participants should choose the correct side 50% of the time of course. Bem found that, specifically for erotic pictures, participants‚Äô guesses were 53.1% correct. This rate of guessing was unexpected under the null hypothesis of chance guessing (<span class="math inline">\(p = .01\)</span>). Eight other studies with a total of more than 1,000 participants yielded apparently supportive evidence, with participants appearing to show a variety of psychological effects even before the stimuli were shown! On this basis, should we conclude that precognition exists?</p>
<p>Probably not. <span class="citation">Wagenmakers et al. (<a href="#ref-wagenmakers2011" role="doc-biblioref">2011</a>)</span> gave an influential critique of Bem‚Äôs findings, arguing that 1) Bem‚Äôs experiments were exploratory in nature, 2) that Bem‚Äôs conclusions were a-priori unlikely, and 3) that the level of statistical evidence from his experiments was quite low. We find each of these arguments alone compelling; together they present a knockdown case.</p>
<p>First, we‚Äôve already discussed the need to be skeptical about situations where experimenters have the opportunity for analytic flexibility in their choice of measures, manipulations, samples, and analyses. Flexibility leads to the possibility of cherry-picking those set of decisions from the ‚Äúgarden of forking paths‚Äù that lead to a positive outcome for the researcher‚Äôs favored hypothesis. And there is plenty of flexibility on display even in Experiment 1 of Bem‚Äôs paper. Although there were 100 participants in the study, they may have been combined post hoc from two distinct samples of 40 and 60, each of which saw different conditions. The 40 made guesses about the location of erotic, negative, and neutral pictures; the 60 saw erotic, positive non-romantic, and positive romantic pictures. The means of each of these conditions was presumably tested against chance (at least 6 comparisons, for a false positive rate of 0.26), and had positive romantic pictures been found significant, an interpretation would have been available about this condition.</p>
<p>Second, as we discussed, a <span class="math inline">\(p\)</span>-value close to .05 does not necessarily provide strong evidence against the null hypothesis. Wagenmakers et al.¬†computed the Bayes Factor for each of experiments in Bem‚Äôs paper and found that, in many cases, the amount of evidence for <span class="math inline">\(H_1\)</span> was quite modest under a default Bayesian <span class="math inline">\(t\)</span>-test. Experiment 1 was no exception: the BF was 1.64, giving ‚Äúanecdotal‚Äù support for the hypothesis of some non-zero effect, even before the multiple-comparisons problem mentioned above.</p>
<p>Finally, since precognition is not attested by any compelling prior scientific evidence ‚Äì and many researchers have tried to provide this evidence ‚Äì perhaps we should assign a low prior probability to Bem‚Äôs <span class="math inline">\(H_1\)</span>, a non-zero precognition effect. Taking a strong Bayesian position, Wagenmakers et al.¬†suggest that we might do well to adopt a prior reflecting how unlikely precognition is, say <span class="math inline">\(p(H_1) = 10^{-20}\)</span>. And if we adopt this prior, even a very well-designed, highly informative experiment (with a Bayes factor conveying substantial or even decisive evidence) would still lead to a very low posterior probability of precognition.</p>
<p>Wagenmakers et al.¬†concluded that, rather than supporting precognition, the conclusion from Bem‚Äôs paper should be psychologists should revise how they think about analyzing their data!<label for="tufte-sn-85" class="margin-toggle sidenote-number">85</label><input type="checkbox" id="tufte-sn-85" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">85</span> If you are intrigued by this set of issues, you might enjoy Slate Star Codex‚Äôs post on ESP research, <a href="https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/">‚ÄúThe Control Group is Out of Control‚Äù</a>. We don‚Äôt agree with everything in it, but it‚Äôs definitely thought-provoking and contains many interesting links.</span></p>
</div>
</div>
<div id="philosophical-and-empirical-views-of-probability" class="section level3" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Philosophical (and empirical) views of probability</h3>
<p>Up until now we‚Äôve presented Bayesian and frequentist tools as two different sets of computations. But in fact, these different tools derive from fundamentally different interpretations of what a probability even is. Very roughly, frequentist approaches tend to believe that probabilities quantify the long-run frequencies of certain events. So, if we say that some outcome of an event has probability .5, we‚Äôre saying that if that event happened thousands of times, the long run frequency of the outcome would be 50% of the total events. In contrast, the Bayesian viewpoint doesn‚Äôt depend on this sense that events could be exactly repeated. Instead, the <strong>subjective Bayesian</strong> interpretation of probability is that it quantifies the person‚Äôs degree of belief in a particular outcome.<label for="tufte-sn-86" class="margin-toggle sidenote-number">86</label><input type="checkbox" id="tufte-sn-86" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">86</span> This is really a very rough description. If you‚Äôre interested in learning more about this philosophical background, we recommend the Stanford Encyclopedia of Philosophy entry on ‚Äúinterpretations of probability‚Äù: <a href="https://plato.stanford.edu/entries/probability-interpret/" class="uri">https://plato.stanford.edu/entries/probability-interpret/</a>.</span></p>
<p>You don‚Äôt have to take sides in this deep philosophical debate about what probability is. But it‚Äôs helpful to know that people actually seem to reason about the world in ways that are well described by the subjective Bayesian view of probability. Recent cognitive science research has made a lot of headway in describing reasoning as a process of Bayesian inference (for review, see <span class="citation">N. D. Goodman et al. (<a href="#ref-probmods2" role="doc-biblioref">2016</a>)</span>) where probabilities describe degrees of belief in different hypotheses. These hypotheses in turn are a lot like the theories we described in Chapter <a href="2-theories.html#theories">2</a>: they describe ways that different abstract entities connect with one another <span class="citation">(<a href="#ref-tenenbaum2011" role="doc-biblioref">Tenenbaum et al., 2011</a>)</span>. You might think that scientists are different from lay-people in this regard, but one of the striking findings from the literature on probabilistic reasoning and judgment is that expertise doesn‚Äôt matter that much. Statistically-trained scientists make many of the same reasoning mistakes as their un-trained students <span class="citation">(<a href="#ref-kahneman1979" role="doc-biblioref">Kahneman &amp; Tversky, 1979</a>)</span>. Even children seem to reason intuitively in a way that looks a bit like creating probabilistic models <span class="citation">(<a href="#ref-gopnik2012" role="doc-biblioref">Gopnik, 2012</a>)</span>.</p>
<p>These cognitive science findings help to explain some of the problems that people (scientists included) have reasoning about <span class="math inline">\(p\)</span>-values. If you are an intuitively Bayesian reasoner, the quantity that you‚Äôre probably tracking is how much you believe in your hypothesis (its posterior probability). So, many people treat the <span class="math inline">\(p\)</span>-value as the posterior probability of the null hypothesis.<label for="tufte-sn-87" class="margin-toggle sidenote-number">87</label><input type="checkbox" id="tufte-sn-87" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">87</span> <span class="citation">Cohen (<a href="#ref-cohen1994" role="doc-biblioref">1994</a>)</span> is a great treatment of this issue.</span> That‚Äôs exactly what fallacy #1 ‚Äì ‚ÄúIf <em>p</em> = .05, the null hypothesis has only a 5% chance of being true.‚Äù ‚Äì states. It‚Äôs not. Written in math, <span class="math inline">\(p(d | H_0)\)</span> (the likelihood that lets us compute the p-value) is not the same thing as <span class="math inline">\(p(H_0 | d)\)</span> (the posterior that we want). Pulling from our accident report above, even if the probability of the data given the null hypothesis of ESP is low, that doesn‚Äôt mean that the probability of ESP is high.</p>
</div>
</div>
<div id="estimating-precision" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Estimating precision</h2>
<p>Our last section presented an argument against using <span class="math inline">\(p\)</span>-values for making <em>dichotomous</em> inferences. But we still want to move from what we know about our own limited sample to some inference about the population. How should we do this?</p>
<div id="confidence-intervals" class="section level3" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Confidence intervals</h3>
<p><img src="images/inference/meme_that_i_know_we_cant_use.jpg" width="\linewidth"  /></p>
<!-- Julie's comment: Based on the title of the meme it sounds like we're not using it, but in case we are, it might be good to place it further down in the chapter after we introduce the concept of confidence intervals, otherwise the joke might not make sense. -->
<p>One thing we often want to do is to make an inference about how similar our estimate from our own sample is to the population parameter of interest. For example, how close is our tea-tasting effect estimate to the true effect of tea ordering in the population? We don‚Äôt know what the true effect is, but our knowledge of sampling distributions lets us make some guesses about how precise our estimate is.</p>
<p>The <strong>confidence interval</strong> is a convenient frequentist way to summarize the variability of the sampling distribution ‚Äì and hence how precise our point estimate is. The confidence interval represents the range of possible values for the parameter of interest that are plausible given the data. More formally, a 95% confidence interval for some estimate (call it <span class="math inline">\(\widehat{\beta}\)</span>, as in the our example) is defined as a range of possible values for <span class="math inline">\(\beta\)</span> such that, if we did repeated sampling, 95% of the intervals generated by those samples would contain the true parameter, <span class="math inline">\(\beta\)</span>.</p>
<p>Here‚Äôs an example. As we will calculate below, the 95% confidence interval for the milk ordering effect is [0.06, 2.74]. If we were to conduct the experiment 100 times and calculate a confidence interval each time, we should expect 95 of the intervals to contain the true <span class="math inline">\(\beta\)</span>, whereas we would expect the remaining 5 to not contain <span class="math inline">\(\beta\)</span>.<label for="tufte-sn-88" class="margin-toggle sidenote-number">88</label><input type="checkbox" id="tufte-sn-88" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">88</span> In case you don‚Äôt have enough tea to do the experiment 100 times to confirm this, you can do it virtually <a href="https://istats.shinyapps.io/ExploreCoverage/">here</a>.</span></p>
<p>Confidence intervals are like betting on the inferences drawn from your sample. The sample you drew is like one pull of a slot machine that will pay off (i.e., have the confidence interval contain the true parameter) 95% of the time. Put more concisely: 95% of 95% confidence intervals contain the true value of the mean.</p>
</div>
<div id="confidence-in-confidence-intervals" class="section level3" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Confidence in confidence intervals?</h3>
<p>In practice, though, confidence intervals are often misinterpreted by students and researchers alike <span class="citation">(<a href="#ref-hoekstra2014" role="doc-biblioref">Hoekstra et al., 2014</a>)</span>. In Hoekstra et al.‚Äôs example, a researcher conducts an experiments and reports ‚ÄúThe 95% confidence interval for the mean ranges from 0.1 to 0.4‚Äù. All of the statements in Table <a href="6-inference.html#tab:inference-ci-false">6.4</a>, though tempting to make, are <em>technically false</em>.</p>
<p><!--
<caption>--><span class="marginnote shownote"><span id="tab:inference-ci-false">Table 6.4: </span>Confidence interval misconceptions for a confidence interval [0.1,0.4]. Adapted from Hoekstra et al.¬†(2014).</span><!--</caption>--></p>
<table>
<colgroup>
<col width="2%" />
<col width="97%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">The probability that the true mean is greater than 0 is at least 95%.</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">The probability that the true mean equals 0 is smaller than 5%.</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">The ‚Äònull hypothesis‚Äô that the true mean equals 0 is likely to be incorrect.</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">There is a 95% probability that the true mean lies between 0.1 and 0.4.</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">We can be 95% confident that the true mean lies between 0.1 and 0.4.</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">If we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.4.</td>
</tr>
</tbody>
</table>
<p>The problem with all of these statements is that, in the frequentist framework, there is only true value of the parameter, and the variability captured in confidence intervals is about the <em>samples</em>, not the parameter itself.<label for="tufte-sn-89" class="margin-toggle sidenote-number">89</label><input type="checkbox" id="tufte-sn-89" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">89</span> In contrast, Bayesians think of parameters themselves as variable rather than fixed.</span> For this reason, we can‚Äôt make any statements about the probability of the value of the mean or of our confidence in specific numbers. To reiterate, what we <em>can</em> say is: if we were to repeat the procedure of conducting the experiment and calculating a confidence interval many times, in the long run, 95% of those confidence intervals would contain the true mean.</p>
<p>Confidence intervals are constructed by estimating the middle 95% of the sampling distribution of <span class="math inline">\(\widehat{\beta}\)</span>. Because of our hero, the Central Limit Theorem, we can treat the sampling distribution as normal for reasonably large samples. Given this, it‚Äôs common to construct a 95% confidence intervals <span class="math inline">\(\widehat{\beta} \pm 1.96 \widehat{SE}\)</span>.<label for="tufte-sn-90" class="margin-toggle sidenote-number">90</label><input type="checkbox" id="tufte-sn-90" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">90</span> This type of CI is called a ‚ÄúWald‚Äù confidence interval.</span> For the tea example, we had <span class="math inline">\(\widehat{SE} = \sigma/\sqrt{n} = 0.68\)</span>, yielding the confidence interval: [0.06, 2.74].</p>
<!-- Julie's comment: I think the paragraph above should be moved higher up, earlier in the introduction of CIs because it clarifies a lot about what a CI is objectively. Otherwise, until we reach this paragraph, CIs feel abstract, e.g., how do we know the "true" parameter? -->
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-condition-cis"></span>
<img src="experimentology_files/figure-html/inference-condition-cis-1.png" alt="Confidence intervals on each of the two condition estimates." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.9: Confidence intervals on each of the two condition estimates.<!--</p>-->
<!--</div>--></span>
</p>
<p>For visualization purposes, we can show the confidence intervals on individual estimates (Figure <a href="6-inference.html#fig:inference-condition-cis">6.9</a>). These tell us about the precision of our estimates of each quantity relative to the population estimate. But we‚Äôve been talking primarily about the CI on the treatment effect <span class="math inline">\(\widehat{\beta}\)</span>, which is shown in Figure <a href="6-inference.html#fig:inference-treatment-ci">6.10</a>. This CI</p>
<!-- Julie's comment: Paragraph above seemed unfinished? Also Figure 6.9 doesn't contain any CI, and figure caption for Figure 6.10 probably needs to be updated (e.g., "Figure 6.10: Confidence intervals on estimate on treatment effect." -->
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:inference-treatment-ci"></span>
<img src="experimentology_files/figure-html/inference-treatment-ci-1.png" alt="Confidence intervals on each of the two condition estimates." width="\linewidth"  />
<!--
<p class="caption marginnote">-->Figure 6.10: Confidence intervals on each of the two condition estimates.<!--</p>-->
<!--</div>--></span>
</p>
<p>The Bayesian analog to a confidence interval is a <strong>credible interval</strong>. Recall that for Bayesians, parameters themselves are considered probabilistic (i.e., subject to random variation), not fixed. A 95% credible interval for an estimate, <span class="math inline">\(\widehat{\beta}\)</span>, represents a range of possible values for <span class="math inline">\(\beta\)</span> such that there is a 95% probability that <span class="math inline">\(\beta\)</span> falls inside the interval. Because we are now wearing our Bayesian hats, we are ‚Äúallowed‚Äù to talk about <span class="math inline">\(\beta\)</span> as if it were probabilistic rather than fixed. In practice, credible intervals are constructed by finding the posterior distribution of <span class="math inline">\(\beta\)</span>, as in Chapter <a href="5-estimation.html#estimation">5</a>, and then taking the middle 95%, for example.</p>
<p>Credible intervals are nice because they don‚Äôt give rise to many of the inference fallacies surrounding confidence intervals . They actually do represent our beliefs about where <span class="math inline">\(\beta\)</span> is likely to be, for example. Despite the technical differences between credible intervals and confidence intervals, in practice ‚Äì with larger sample sizes and weaker priors ‚Äì they turn out to be quite similar to one another in many cases. On the other hand, they can diverge sharply in cases with less data or stronger priors <span class="citation">(<a href="#ref-morey2016" role="doc-biblioref">Morey et al., 2016</a>)</span>.</p>
</div>
</div>
<div id="chapter-summary-inference" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Chapter summary: Inference</h2>
<p>Inference tools help you move from characteristics of the sample to characteristics of the population. This move is a critical part of generalization from research data. But we hope we‚Äôve convinced you that inference doesn‚Äôt have to mean deriving a particular binary decision on the basis of your data. Instead a strategy that seeks to estimate measurements and then make inferences about precision and uncertainty is often much more helpful as a building block for theory.</p>
<p>The problem with binary inferences is that they creates a fragile scientific ecosystem. By the logic of statistical significance, either an experiment ‚Äúworked‚Äù or it didn‚Äôt. Because everyone would usually rather have an experiment that worked than one that didn‚Äôt, inference criteria like <span class="math inline">\(p\)</span>-values often become a target for selection, as we discussed in Chapter <a href="3-replication.html#replication">3</a>.<label for="tufte-sn-91" class="margin-toggle sidenote-number">91</label><input type="checkbox" id="tufte-sn-91" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">91</span> More generally, this is probably an example of Goodhart‚Äôs law, which states that when a measure becomes a target, it ceases to be a good measure <span class="citation">(<a href="#ref-strathern1997" role="doc-biblioref">Strathern, 1997</a>)</span>. Once the outcomes of statistical inference procedures become targets for publication, they are subject to selection biases that make them less meaningful.</span></p>
<p>If you want to quantify evidence for or against a hypothesis, it‚Äôs worth considering whether Bayes Factors address your question better than <span class="math inline">\(p\)</span>-values. In practice, <span class="math inline">\(p\)</span>-values are hard to understand and many people misuse them ‚Äì though to be fair, BFs are misused plenty too. These issues may be rooted in basic facts about how human beings reason about probability.</p>
<p>Despite the reasons to be worried about <span class="math inline">\(p\)</span>-values, for many practicing scientists (at least at time of writing) there is no one right answer as to whether to reject <span class="math inline">\(p\)</span>-values. Even if we‚Äôd like to be Bayesian, there are a number of obstacles. First, though new computational tools make fitting Bayesian models and extracting Bayes Factors much easier than before (more about this below and in the next chapter), it‚Äôs still on average quite a bit harder to fit a Bayesian model than it is a frequentist one. Second, because Bayesian analyses are less familiar, it may be an uphill battle to convince advisors, reviewers, and funders to use them.</p>
<p>As a group, some of us are more Bayesian than frequentist, while others are more frequentist than Bayesian ‚Äì but all of us recognize the need to move flexibly between statistical paradigms. Furthermore, a lot of the time we‚Äôre not so worried about which paradigm we‚Äôre using. The paradigms are at their most divergent when making binary inferences, whether they are at <span class="math inline">\(p &lt; .05\)</span> or <span class="math inline">\(BF &gt; 3\)</span>.
In sum, the practicing experimentalist should see binary inference tools as only one part of a bigger toolkit for data analysis. This toolkit should also include estimation approaches from the previous chapter as well as the methods for quantifying precision and uncertainty that we introduced above. Further, for more complex situations, the toolkit needs to include some tools for building models. Towards this goal, the next chapter provides some initial tools for how to use models for estimation and inference about experimental effects.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-5" class="exercise"><strong>Exercise 6.1  </strong></span>Step away from the computer. Can you write the definition of a <span class="math inline">\(p\)</span>-value and a Bayes Factor?</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-6" class="exercise"><strong>Exercise 6.2  </strong></span>Take three of Goodman‚Äôs (2008) ‚Äúdirty dozen‚Äù in Table <a href="6-inference.html#tab:dirty-dozen">6.3</a> and write a description of why each is a misconception. (These can be checked against the original article, which gives a nice discussion).</p>
</div>
<!-- Julie's general chapter comments: I really loved how clearly the chapter described what sampling distribution and standard error were - I find that these concepts are often one of the most difficult for students to grasp but this chapter explained it extremely well! Another thing I loved was the direct comparison of p values to BF (Table 6.2)! I've never seen such a direct comparison and it helps relate the two more concretely. -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-aczel2018" class="csl-entry">
Aczel, B., Palfi, B., Szollosi, A., Kovacs, M., Szaszi, B., Szecsi, P., Zrubka, M., Gronau, Q. F., Bergh, D. van den, &amp; Wagenmakers, E.-J. (2018). Quantifying support for the null hypothesis in psychology: An empirical investigation. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(3), 357‚Äì366.
</div>
<div id="ref-bem2011" class="csl-entry">
Bem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. <em>Journal of Personality and Social Psychology</em>, <em>100</em>(3), 407.
</div>
<div id="ref-benjamin2018" class="csl-entry">
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., Bollen, K. A., Brembs, B., Brown, L., Camerer, C.others. (2018). Redefine statistical significance. <em>Nature Human Behaviour</em>, <em>2</em>(1), 6‚Äì10.
</div>
<div id="ref-cohen1990" class="csl-entry">
Cohen, J. (1990). Things i have learned (so far). <em>American Psychologist</em>, <em>45</em>, 1304‚Äì1312.
</div>
<div id="ref-cohen1994" class="csl-entry">
Cohen, J. (1994). The earth is round (p&lt;. 05). <em>American Psychologist</em>, <em>49</em>(12), 997.
</div>
<div id="ref-cumming2014" class="csl-entry">
Cumming, G. (2014). The new statistics: Why and how. <em>Psychol. Sci.</em>, <em>25</em>(1), 7‚Äì29.
</div>
<div id="ref-fisher1949" class="csl-entry">
Fisher, R. A. (1949). <em>The design of experiments</em>.
</div>
<div id="ref-gelman1995" class="csl-entry">
Gelman, A., Carlin, J. B., Stern, H. S., &amp; Rubin, D. B. (1995). <em>Bayesian data analysis</em>. Chapman; Hall/CRC.
</div>
<div id="ref-gelman2006b" class="csl-entry">
Gelman, A., &amp; Hill, J. (2006). <em>Data analysis using regression and multilevel/hierarchical models</em>. Cambridge university press.
</div>
<div id="ref-gelman2020" class="csl-entry">
Gelman, A., Hill, J., &amp; Vehtari, A. (2020). <em>Regression and other stories</em>. Cambridge University Press.
</div>
<div id="ref-probmods2" class="csl-entry">
Goodman, N. D., Tenenbaum, J. B., &amp; Contributors, T. P. (2016). <em><span class="nocase">Probabilistic Models of Cognition</span></em> (Second). <a href="http://probmods.org/v2">http://probmods.org/v2</a>.
</div>
<div id="ref-goodman2008" class="csl-entry">
Goodman, S. (2008). A dirty dozen: Twelve p-value misconceptions. <em>Seminars in Hematology</em>, <em>45</em>, 135‚Äì140.
</div>
<div id="ref-gopnik2012" class="csl-entry">
Gopnik, A. (2012). Scientific thinking in young children: Theoretical advances, empirical research, and policy implications. <em>Science</em>, <em>337</em>(6102), 1623‚Äì1627.
</div>
<div id="ref-hoekstra2014" class="csl-entry">
Hoekstra, R., Morey, R. D., Rouder, J. N., &amp; Wagenmakers, E.-J. (2014). Robust misinterpretation of confidence intervals. <em>Psychonomic Bulletin &amp; Review</em>, <em>21</em>(5), 1157‚Äì1164.
</div>
<div id="ref-jeffreys1998" class="csl-entry">
Jeffreys, H. (1998). <em>The theory of probability</em>. OUP Oxford.
</div>
<div id="ref-kahneman1979" class="csl-entry">
Kahneman, D., &amp; Tversky, A. (1979). Prospect theory: An analysis of decision under risk. <em>Econometrica</em>, <em>47</em>(2), 363‚Äì391.
</div>
<div id="ref-kruschke2014" class="csl-entry">
Kruschke, J. K. (2014). <em>Doing bayesian data analysis: A tutorial with r, JAGS, and stan</em>. Academic Press.
</div>
<div id="ref-kruschke2018" class="csl-entry">
Kruschke, J. K., &amp; Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. <em>Psychon. Bull. Rev.</em>, <em>25</em>(1), 178‚Äì206.
</div>
<div id="ref-mcelreath2018" class="csl-entry">
McElreath, R. (2018). <em>Statistical rethinking: A bayesian course with examples in r and stan</em>. Chapman; Hall/CRC.
</div>
<div id="ref-morey2016" class="csl-entry">
Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., &amp; Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. <em>Psychonomic Bulletin &amp; Review</em>, <em>23</em>(1), 103‚Äì123.
</div>
<div id="ref-morey2011" class="csl-entry">
Morey, R. D., &amp; Rouder, J. N. (2011). Bayes factor approaches for testing interval null hypotheses. <em>Psychological Methods</em>, <em>16</em>(4), 406.
</div>
<div id="ref-pratt1995" class="csl-entry">
Pratt, J. W., Raiffa, H., Schlaifer, R.others. (1995). <em>Introduction to statistical decision theory</em>. MIT press.
</div>
<div id="ref-simonsohn2014" class="csl-entry">
Simonsohn, U. (2014). Posterior-hacking: Selective reporting invalidates bayesian results also. <em>Available at SSRN 2374040</em>.
</div>
<div id="ref-strathern1997" class="csl-entry">
Strathern, M. (1997). <span>‚ÄúImproving ratings‚Äù</span>: Audit in the british university system. <em>European Review</em>, <em>5</em>(3), 305‚Äì321.
</div>
<div id="ref-tenenbaum2011" class="csl-entry">
Tenenbaum, J. B., Kemp, C., Griffiths, T. L., &amp; Goodman, N. D. (2011). How to grow a mind: Statistics, structure, and abstraction. <em>Science</em>, <em>331</em>(6022), 1279‚Äì1285.
</div>
<div id="ref-wagenmakers2011" class="csl-entry">
Wagenmakers, E.-J., Wetzels, R., Borsboom, D., &amp; Van Der Maas, H. L. (2011). <em>Why psychologists must change the way they analyze their data: The case of psi: Comment on bem (2011).</em>
</div>
</div>
<p style="text-align: center;">
<a href="5-estimation.html"><button class="btn btn-default">Previous</button></a>
<a href="7-models.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>

<link href="www/global.css" rel="stylesheet">
<script src="www/global.js"></script>


</body>
</html>
