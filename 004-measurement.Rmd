# (PART) Experimental design and planning {-}

<!-- ::: {learning-goals} -->
> Learning goals: 
> * Summarize differences between measurement reliability and validity
> * Reason about appropriate cognitive psychology measures; 
> * Identify well-constructed survey questions.
<!-- ::: -->

# Measurement {#measurement}

The goal of an experiment is to make a (maximally precise and unbiased) measurement of a particular causal effect of interest. In this next section of the book, we're going to try to figure out how to do that. This chapter focuses on the topic of measurement.^[As a topic, measurement is actually much less well-discussed in experimental contexts compared with, say, observational studies. As far as we can tell, this is a sociological fact, not a scientific one. No matter whether you can manipulate the world directly (as in an experiment) or whether you are doing observational or quasi-experimental research, good measurement is the name of the game.]

No matter where you are working in the sciences, you need to measure things. If you're doing physics or chemistry, you need to be able to measure physical quantities; if you're doing biology you might measure populations or lifespan as well as a host of physical quantities. Proper measurement instruments are incredibly important for this kind of work.^[A lot could be said, of course, about the transformative value of better measurement instruments in the sciences -- PHILOSOPHY OF SCIENCE MEASUREMENT REFERENCES?] Psychology and the behavioral sciences are no different -- we need proper measurement instruments. The difference is that in psychology, we are typically trying to measure something that's inside the heads of our participants, which we call a **latent construct**. (See Chapter \@ref(theory)). 

Not all measurements are created equal. This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement -- repeat and assess variation -- though as we'll see below it's a little trickier. Measurement instruments that have less error are called more **reliable** instruments.^[Is **reliability** the same as **precision**? Yes, more or less. Reliability is a property of the instrument while precision is a property of a specific set of measurements using that instrument. In other words, to say that an instrument is reliable is just to say that it typically yields precise measurements.]

When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But, as we saw in Chapter \@ref(theory), things are much trickier when the construct we are trying to measure can't be assessed directly. We have to measure something observable -- our **operationalization** of the construct -- and then make an argument about how it relates to the construct of interest. We call this argument an argument for the **validity** of the measure. 

These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is. Let's start by taking a look at an example of the challenge of measuring a particular latent construct, children's early language ability. We can use this example to understand the concepts of reliability and validity in practice. 

::: {.case-study}
Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age. Others struggle to produce words but clearly show evidence of understanding. And yet others show deficits in both producing and understanding language. Further, this variation appears to be linked to later outcomes -- children whose very early language processing is slower and whose vocabularies are smaller tend to do worse in school years later [@fernald2008]. Thus, there are many reasons why you'd want to make precise measurements of children's early language ability as a latent construct of interest.^[Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled [@frank2021]. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related -- you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!]

As with all developmental research, there are many constraints on measurement that are imposed by the age of the children you want to work with. You can't give toddlers a multiple-choice test! So to measure early language (to be concrete, let's say for children under two and a half years old), you have roughly three options open. First, you can do some kind of observation of them and transcribe their language production -- this could be a play session in the lab or at home, with an experimenter or with a parent or other caregiver. Second, you could do some kind of direct assessment, e.g. by asking them to point or look at the referent of a word (e.g. "look at the kitty") and record their responses using video, a tablet, or even eye-tracking technology [@frank2016]. Or, you could ask thier parents about their language, for example sending a questionnaire like the MacArthur Bates Communicative Development Inventory (CDI for short), which asks parents to check off the words that their child says or understands.


```{marginfigure, echo=TRUE}
<img src="images/cdi.jpg"/>
The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language.
```

To decide which of these methods to use for a specific study, we need to think through the properties of these measurement instruments -- both in terms of reliability and validity and also in terms of their practicality for a specific research situation. Practicalities matter! For example, observational measurement can be extremely costly in both time and money because it not only requires a visit of some sort (to the home or the lab) but also transcription of speech, which is quite time-consuming -- often taking 5-10 minutes of work to transcribe a single minute of speech. Direct assessment still requires a lab or home visit, but scoring is typically more straightforward. Finally, parent report -- (e)mailing a questionnaire to the parent -- is extremely time- and cost-effective. 

On the other hand, we wouldn't want to use CDI questionnaires as a tool if they were a bad measurement instrument. How can we tell? This is where assessment of reliability and validity are critical. In practice, the task of selecting and justifying a measurement instrument comes down to an argument about reliability and validity.


```{r measurement-reliability}
library(wordbankr)
ws <- wordbankr::get_instrument_data(language = "English (American)", 
                                     form = "WS", administrations = TRUE, 
                                     iteminfo = TRUE)

ws_mat <- ws |>
  filter(!longitudinal, type == "word") |>
  mutate(value = value == "produces", 
         even_item = ifelse(num_item_id %% 2 == 0, "even","odd")) |>
  group_by(data_id, even_item) |>
  summarise(prop = mean(value)) |>
  pivot_wider(names_from = even_item, values_from = prop) 

test_retest <- cor.test(ws_mat$even, ws_mat$odd)$estimate
```

How reliable is the CDI? As we'll discuss more below, there's no single answer to this question. Not only are there multiple ways to compute reliability, but also reliability in practice is going to depend on the population being measured, the fidelity with which the instrument is administered, and other factors. 



Given that CDI forms are relatively reliable instruments, are they valid? Well, as a starting point, they certainly have reasonable **face validity** -- they look like they are measuring the construct that they purport to measure. They also arguably have some **ecological validity** in that they measure the child's language (as observed by the parent) in their day-to-day experiences, rather than in a particular lab situation. 



```{marginfigure, echo=TRUE}
<img src="images/cdi-validity.png"/>
Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give stanardized weights [@bornstein1998].
```

:::



## Measurement Validity

Does the measure relate to the construct? Classic concurrent and predictive validation strategies.

Face and ecological validity.
The nomological network (Cronbach and Meehl 1955). In other words, the measure is valid if it fits into the theory and is supported by other aspects of the theory.

## Measurement Reliability


### Computing reliability

Test-retest. Variance related to the test-taker’s performance. 	
Inter-rater. Variance related to the measurement method.

### Reliability paradoxes

https://lucklab.ucdavis.edu/blog/2019/2/19/reliability-and-precision


## Design of measures

Data types: Stevens (1946) framework.
Classic cognitive psychology measures: Forced choices and reaction times
Likert scales and asking good survey-style questions.

The promise and perils of open-ended measures. 

## Survey measures



## Conclusions

In olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians knew that different problems on tests had different measurement properties, and began exploring how to select good and bad items, and how to figure out people's ability abstracted away from specific items. Cognitive psychologists, on the other hand, spurned this item-level variation and embraced the dogma of exchangeable experimental items.

People did Lots Of Trials, all generated from the same basic template. The sumscore^[The sumscore is just what we normal psychologists call "percent correct" -– treating the sum of your correct answers on the test as your score, as opposed to inferring the latent trait (ability) from the performance on the observed variables.] reigned supreme, and yielded important insight into Memory, Attention, and Reasoning (irrespective of what was being remembered, attended to, or reasoned about). 

Psychophysicists diverged from the cognitivist hierarchy. They always knew that they needed to infer a latent relationship. As they got better at doing this, they fit models that included parameters of the decision process (for example, a "lapse" parameter to capture inattention) as well as the quantities of interest. And because they typically fit these curves within individual subjects, these parameters were participant-level estimates. But the models that fit these curves were often specific to particular metric relationships and not appropriate for increasingly complicated domains.

Now in modern cognitive science, we get work on sophisticated constructs – for example, in moral psychology or psycholinguistics – where experimenters break with the cognitivist dogma and use non-exchangeable items. Sometimes items are sentences or even whole vignettes. Yet for the most part these researchers have forgotten to model item variation (except occasionally using a random intercept for items in their linear mixed effects models). @clark1973 scolded them about the problematic statistical inferences that could result from forgetting to model items and this guidance has reappeared in recent exhortations to Keep It Maximal! [@barr2013]. But as far as I can tell, no one really talks about modeling items in more detail *in order to learn more about what is in people's heads*.
