<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Experimentology - 6&nbsp; Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./007-models.html" rel="next">
<link href="./005-estimation.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-659MTW4XZ4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-659MTW4XZ4', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./005-estimation.html">Statistics</a></li><li class="breadcrumb-item"><a href="./006-inference.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Experimentology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/langcog/experimentology" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Experimentology.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-theories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Theories</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-replication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006-inference.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Planning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008-measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Measurement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010-sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Execution</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Preregistration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013-management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Project management</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reporting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014-writing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Writing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100-instructors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Instructor’s guide</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./101-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git and GitHub</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./102-rmarkdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">R Markdown and Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./103-tidyverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tidyverse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./104-ggplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">ggplot</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sampling-variation" id="toc-sampling-variation" class="nav-link active" data-scroll-target="#sampling-variation"><span class="header-section-number">6.1</span> Sampling variation</a>
  <ul class="collapse">
  <li><a href="#standard-errors" id="toc-standard-errors" class="nav-link" data-scroll-target="#standard-errors"><span class="header-section-number">6.1.1</span> Standard errors</a></li>
  <li><a href="#the-central-limit-theorem" id="toc-the-central-limit-theorem" class="nav-link" data-scroll-target="#the-central-limit-theorem"><span class="header-section-number">6.1.2</span> The central limit theorem</a></li>
  </ul></li>
  <li><a href="#from-variation-to-inference" id="toc-from-variation-to-inference" class="nav-link" data-scroll-target="#from-variation-to-inference"><span class="header-section-number">6.2</span> From variation to inference</a></li>
  <li><a href="#making-inferences" id="toc-making-inferences" class="nav-link" data-scroll-target="#making-inferences"><span class="header-section-number">6.3</span> Making inferences</a>
  <ul class="collapse">
  <li><a href="#bayes-factors" id="toc-bayes-factors" class="nav-link" data-scroll-target="#bayes-factors"><span class="header-section-number">6.3.1</span> Bayes Factors</a></li>
  <li><a href="#p-values" id="toc-p-values" class="nav-link" data-scroll-target="#p-values"><span class="header-section-number">6.3.2</span> <em>p</em>-values</a></li>
  <li><a href="#sec-neyman-pearson" id="toc-sec-neyman-pearson" class="nav-link" data-scroll-target="#sec-neyman-pearson"><span class="header-section-number">6.3.3</span> The Neyman-Pearson approach</a></li>
  </ul></li>
  <li><a href="#inference-and-its-discontents" id="toc-inference-and-its-discontents" class="nav-link" data-scroll-target="#inference-and-its-discontents"><span class="header-section-number">6.4</span> Inference and its discontents</a>
  <ul class="collapse">
  <li><a href="#problems-with-the-interpretation-of-p-values" id="toc-problems-with-the-interpretation-of-p-values" class="nav-link" data-scroll-target="#problems-with-the-interpretation-of-p-values"><span class="header-section-number">6.4.1</span> Problems with the interpretation of <em>p</em>-values</a></li>
  <li><a href="#philosophical-and-empirical-views-of-probability" id="toc-philosophical-and-empirical-views-of-probability" class="nav-link" data-scroll-target="#philosophical-and-empirical-views-of-probability"><span class="header-section-number">6.4.2</span> Philosophical (and empirical) views of probability</a></li>
  <li><a href="#what-framework-to-use" id="toc-what-framework-to-use" class="nav-link" data-scroll-target="#what-framework-to-use"><span class="header-section-number">6.4.3</span> What framework to use?</a></li>
  </ul></li>
  <li><a href="#computing-precision" id="toc-computing-precision" class="nav-link" data-scroll-target="#computing-precision"><span class="header-section-number">6.5</span> Computing precision</a>
  <ul class="collapse">
  <li><a href="#confidence-intervals" id="toc-confidence-intervals" class="nav-link" data-scroll-target="#confidence-intervals"><span class="header-section-number">6.5.1</span> Confidence intervals</a></li>
  <li><a href="#confidence-in-confidence-intervals" id="toc-confidence-in-confidence-intervals" class="nav-link" data-scroll-target="#confidence-in-confidence-intervals"><span class="header-section-number">6.5.2</span> Confidence in confidence intervals?</a></li>
  </ul></li>
  <li><a href="#chapter-summary-inference" id="toc-chapter-summary-inference" class="nav-link" data-scroll-target="#chapter-summary-inference"><span class="header-section-number">6.6</span> Chapter summary: Inference</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/langcog/experimentology/blob/main/006-inference.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/langcog/experimentology/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-inference" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled" title="learning goals">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
learning goals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Discuss the purpose of statistical inference</li>
<li>Define <em>p</em>-values and Bayes Factors</li>
<li>Consider common fallacies about inference (especially for <em>p</em>-values)</li>
<li>Reason about sampling variability</li>
<li>Define and reason about confidence intervals</li>
</ul>
</div>
</div>
</div>
<p>We’ve been arguing that experiments are about measuring effects. The effects we are interested in are causal effects for a group of people, but that group is almost always bigger than the participants in an experiment. <strong>Statistical inference</strong> is the process of going beyond the specific characteristics of the sample that you measured to make generalizations about the broader <strong>population</strong>.</p>
<p><a href="005-estimation.html"><span>Chapter&nbsp;5</span></a> already showed us how to make one simple inference: estimating population parameters using both frequentist and Bayesian techniques. Estimating population parameters is an important first step. But often we want to make more sophisticated inferences so that we can answer questions such as:</p>
<ol type="1">
<li>How likely is it that this pattern of measurements was produced by chance variation?</li>
<li>Do these data provide more support for one hypothesis or another?</li>
<li>How precise is our estimate of an effect?</li>
<li>What portion of the variation in the data is due to a particular manipulation (as opposed to variation between participants, stimulus items, or other manipulations)?</li>
</ol>
<p>Question (1) is associated with one particular type of statistical inference method – <strong>null hypothesis significance testing</strong> (NHST) in the <strong>frequentist</strong> statistical tradition. NHST has become synonymous with data analysis, such that in the vast majority of research papers (and research methods courses), all of the reported analyses are tests of this type. Yet this equivalence is quite problematic.</p>
<div class="page-columns page-full"><p>The move to “go test for significance” before visualizing your data and trying to understand sources of variation (participants, items, manipulations, etc.) is one of the most unhelpful strategies for an experimenter. Whether <span class="math inline">\(p &lt; .05\)</span> or not, a test of this sort gives you literally <em>one bit</em> of information about your data.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Considering effect sizes and their variation more holistically, including using the kinds of visualizations we advocate in <a href="015-viz.html"><span>Chapter&nbsp;15</span></a>, gives you a much richer sense of what happened in your experiment!</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;In the information theoretic sense, as well as the common sense!</p></li></div></div>
<p>In this chapter, we will describe NHST, the conventional method that many students still learn (and many scientists still use) as their primary method for engaging with data. All practicing experimentalists need to understand NHST, both to read the literature and also to apply this method in appropriate situations. For example, NHST may be a reasonable tool for testing whether an intervention leads to a difference between a treatment condition and an appropriate control. But we will also try to contextualize NHST as a very special case of a broader set of statistical inference strategies. Further, we will continue to flesh out our account of how some of the pathologies of NHST have been a driver of the replication crisis.</p>

<div class="no-row-height column-margin column-container"><div id="fig-inference-krushke" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/krushke.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.1: Clarifying the distinctions between Bayesian and Frequentist paradigms and the tools they offer for measurement and hypothesis testing. For many settings, we think the measurement mindset is more useful. Adapted from <span class="citation" data-cites="kruschke2018">Kruschke and Liddell (<a href="#ref-kruschke2018" role="doc-biblioref">2018</a>)</span>.</figcaption>
</figure>
</div></div><p>If NHST approaches have so many issues, what should replace them? <a href="#fig-inference-krushke">Figure&nbsp;<span>6.1</span></a> shows one way of organizing different inferential approaches. There has been a recent move towards the use of Bayes Factors to quantify the evidence in support of different candidate hypotheses. Bayes Factors can help answer questions like (2). We introduce these tools, and believe that they have broader applicability than the NHST framework and should be known by students. On the other hand, Bayes Factors are not a panacea. They have many of the same problems as NHST when they are applied dichotomously.</p>
<p>Instead of dichotomous frequentist or Bayesian hypothesis testing, we follow our thematic emphasis on <span class="smallcaps">measurement precision</span> and advocate for a <strong>measurement</strong> strategy, which is more suited towards questions (3) and (4) <span class="citation" data-cites="cumming2014 kruschke2018">(<a href="#ref-cumming2014" role="doc-biblioref">Cumming 2014</a>; <a href="#ref-kruschke2018" role="doc-biblioref">Kruschke and Liddell 2018</a>)</span>. The goal of these strategies is to yield an accurate and precise estimate of the relationships underlying observed variation in the data.</p>
<div class="page-columns page-full"><p>This isn’t a statistics book and we won’t attempt to teach the full array of important statistical concepts that will allow students to build good models of a broad array of datasets. (Sorry!).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> But we do want you to be able to reason about inference and modeling. In this chapter, we’ll start by making some inferences about our tea-tasting example from the last chapter, using this example to build up intuitions about hypothesis testing and inference. Then in <a href="007-models.html"><span>Chapter&nbsp;7</span></a>, we’ll start to look at more sophisticated models and how they can be fit to real datasets.</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;If you’re interested in going deeper, here are two books that have been really influential for us. The first is <span class="citation" data-cites="gelman2006b">Gelman and Hill (<a href="#ref-gelman2006b" role="doc-biblioref">2006</a>)</span> and its successor <span class="citation" data-cites="gelman2020">Gelman, Hill, and Vehtari (<a href="#ref-gelman2020" role="doc-biblioref">2020</a>)</span>, which teach regression and multi-level modeling from the perspective of data description. The second is <span class="citation" data-cites="mcelreath2018">McElreath (<a href="#ref-mcelreath2018" role="doc-biblioref">2018</a>)</span>, a course on building Bayesian models of the causal structure of your data. Honestly, neither is an easy book to sit down and read (unless you are the kind of person who reads statistics books on the subway for fun) but both really reward detailed study. We encourage you to get together a reading group and go through the exercises in one of these together. It’ll be well worth while in its impact on your statistical and scientific thinking.</p></li></div></div>
<section id="sampling-variation" class="level2 page-columns page-full" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sampling-variation"><span class="header-section-number">6.1</span> Sampling variation</h2>
<div class="page-columns page-full"><p>In <a href="005-estimation.html"><span>Chapter&nbsp;5</span></a>, we introduced Fisher’s tea-tasting experiment and discussed how to estimate means and differences in means from our observed data. These so-called “point estimates” represent our best guesses about the population parameters given the data – and possibly also given our prior beliefs. We can also report how much statistical uncertainty is involved in these point estimates.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Quantifying and reasoning about this uncertainty is an important goal: in our original study we only had 9 participants in each group, which will only provide a low precision (i.e., highly uncertain) estimate of the population. By contrast, if we repeated the experiment with 200 participants in each group, the data would be far less noisy, and we would have much less uncertainty, even if the point estimates happened to be identical.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;As in the previous chapter, we’re only capturing <em>statistical</em> uncertainty. A holistic view of a particular estimate’s credibility also include everything else you know about the study design.</p></li></div></div>

<div class="no-row-height column-margin column-container"><div id="fig-inference-sampling-small" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/sampling-small.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.2: Sampling distribution for the treatment effect in the tea-tasting experiment, given many different repetitions of the same experiment, each with N=9 per group. Circles represent average treatment effects from different individual experiments, while the thick line represents the form of the underlying distribution.</figcaption>
</figure>
</div></div><section id="standard-errors" class="level3 page-columns page-full" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="standard-errors"><span class="header-section-number">6.1.1</span> Standard errors</h3>
<p>To characterize the uncertainty in an estimate, it helps to picture its <strong>sampling distribution</strong>, which is the distribution of the estimate across different, hypothetical samples. That is, let’s imagine that we conducted the tea experiment not just once, but dozens, hundreds, or even thousands of times. This idea is often called <strong>repeated sampling</strong> as a shorthand. For each hypothetical sample, we use similar recruitment methods to recruit a new sample of participants, and we compute <span class="math inline">\(\widehat{\beta}\)</span> for that sample. Would we get exactly the same answer each time? No, simply because the samples will have some random variability (noise). If we plotted these estimates, <span class="math inline">\(\widehat{\beta}\)</span>, we would get the sampling distribution in <a href="#fig-inference-sampling-small">Figure&nbsp;<span>6.2</span></a>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this chapter and the subsequent statistics and visualization chapters of the book, we’ll try to facilitate understanding and illustrate how to use these concepts in practice by giving the R code we use in constructing our examples in these code boxes. We’ll assume that you have some knowledge of base R and the Tidyverse – to get started with these, go ahead and take a look at <a href="103-tidyverse.html"><span>Appendix&nbsp;D</span></a> if you haven’t already. Although our figures are often drawn by hand, even the hand-drawn ones are based on actual simulation results!</p>
<p>Since we’re going to be working with lots of data from the tea tasting example, we wrote a function called <code>make_tea_data()</code> that creates a <code>tibble</code> with some (made up) data from our modern tea-tasting experiment. You can find the function on GitHub (<a href="https://github.com/langcog/experimentology/blob/main/helper/tea_helper.qmd" class="uri">https://github.com/langcog/experimentology/blob/main/helper/tea_helper.qmd</a>) if you want to follow along.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-2_859123379be85461dd756db2e550e013">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>tea_data <span class="ot">&lt;-</span> <span class="fu">make_tea_data</span>(<span class="at">n_total =</span> <span class="dv">18</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="fig-inference-sampling-big" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/sampling-big.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.3: Comparing sampling distributions for the treatment effect with smaller and larger size samples.</figcaption>
</figure>
</div></div><p>Now imagine we also did thousands of repetitions of the experiment with <span class="math inline">\(n=200\)</span> per group instead of <span class="math inline">\(n=9\)</span> per group. <a href="#fig-inference-sampling-big">Figure&nbsp;<span>6.3</span></a> shows what the sampling distribution might look like in that case. Notice how much narrower the sampling distribution becomes when we increase the sample size, showing our decreased uncertainty. More formally, the standard deviation of the sampling distribution itself, called the <strong>standard error</strong>, decreases as the sample size increases.</p>
<p>The sampling distribution is not the same thing as the distribution of tea ratings in a single sample. Instead, it’s a distribution of <em>estimates across samples of a given size</em>. In essence, it tells us what the mean of a new experiment might be, if we ran it with a particular sample size.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To do simulations where we repeat the tea-tasting experiment over and over again, we’re using a special tidyverse function from the <code>purrr</code> library: <code>map()</code>. <code>map()</code> is an extremely powerful function that allows us to run another function (in this case, the <code>make_tea_data()</code> function that we introduced last chapter) many times with different inputs. Here we create a tibble made up of a set of 1000 runs of the <code>make_tea_data()</code> function.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-3_7ac56010ae19a84dadf1c2e7e26b9d1a">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>samps <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">sim =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">data =</span> <span class="fu">map</span>(sim, \(i) <span class="fu">make_tea_data</span>(<span class="at">n_total =</span> <span class="dv">18</span>))) <span class="sc">|&gt;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we just use the <code>group_by()</code> and <code>summarise()</code> workflow from <a href="103-tidyverse.html"><span>Appendix&nbsp;D</span></a> to get the estimated treatment effect for each of these simulations.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-4_23880a1763a2700a3c88eafcd4cac084">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tea_summary <span class="ot">&lt;-</span> samps <span class="sc">|&gt;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sim, condition) <span class="sc">|&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean_rating =</span> <span class="fu">mean</span>(rating)) <span class="sc">|&gt;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sim) <span class="sc">|&gt;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">delta =</span> mean_rating[condition <span class="sc">==</span> <span class="st">"milk first"</span>] <span class="sc">-</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>              mean_rating[condition <span class="sc">==</span> <span class="st">"tea first"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This tibble gives us what we would need to plot the sampling distributions above in <a href="#fig-inference-sampling-small">Figure&nbsp;<span>6.2</span></a> and <a href="#fig-inference-sampling-big">Figure&nbsp;<span>6.3</span></a>.</p>
</div>
</div>
</div>
</section>
<section id="the-central-limit-theorem" class="level3 page-columns page-full" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="the-central-limit-theorem"><span class="header-section-number">6.1.2</span> The central limit theorem</h3>
<p>We talked in the last chapter about the normal distribution, a convenient and ubiquitous tool for quantifying the distribution of measurements. A shocking thing about sampling distributions for many kinds of estimates – and for <em>all</em> maximum likelihood estimates – is that they become normally distributed as the sample size gets larger and larger. This result holds even for estimates that are not even remotely normally distributed in small samples!</p>

<div class="no-row-height column-margin column-container"><div id="fig-inference-coin-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/cl-2.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6.4: Samping distribution of samples from a biased coin (N=2 flips per sample). Bar height is the proportion of flips resulting in a particular mean.</figcaption>
</figure>
</div><div id="fig-inference-coin-ns" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/cl-ns.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.5: Sampling distribution for 2, 8, 32, and 128 flips.</figcaption>
</figure>
</div></div>
<p>For example, say we are flipping a coin and we want to estimate the probability that it lands heads (<span class="math inline">\(p_H\)</span>). If we draw samples each consisting of only <span class="math inline">\(n=2\)</span> coin flips, <a href="#fig-inference-coin-2">Figure&nbsp;<span>6.4</span></a> is the sampling distribution of the estimates (<span class="math inline">\(\widehat{p}_H\)</span>). This sampling distribution doesn’t look normally distributed at all – it doesn’t have the characteristic “bell curve” shape! In a sample of only two coin flips, <span class="math inline">\(\widehat{p}_H\)</span> can only take on the values 0, 0.5, or 1.</p>
<p>But look what happens as we draw increasingly larger samples in <a href="#fig-inference-coin-ns">Figure&nbsp;<span>6.5</span></a>: We get a normal distribution! This tendency of sampling distributions to become normal as <span class="math inline">\(n\)</span> becomes very large reflects a deep and elegant mathematical law called the <strong>Central Limit Theorem</strong>.</p>
<p>The practical upshot is that the Central Limit Theorem directly helps us characterize the uncertainty of sample estimates. For example, when the sample size is reasonably large (approximately <span class="math inline">\(n&gt;30\)</span> in the case of sample means) the standard error (i.e., the standard deviation of the sampling distribution) of a sample mean is approximately <span class="math inline">\(\widehat{SE} = \sigma/\sqrt{n}\)</span>. The sampling distribution becomes narrower as the sample size increases because we are dividing by the square root of the number of observations.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Even though our figures are hand-drawn, they’re based on real simulations. For our central limit theorem simulations, we again use the <code>map()</code> function. We set up a tibble with the different values we want to to try (which we call <code>n_flips</code>). Then we make use of the <code>map()</code> function to run <code>rbinom()</code> (random binomial samples) for each value of <code>n_flips</code>.</p>
<p>One trick we make use of here is that <code>rbinom()</code> takes an extra argument that says how many of these random values you want to generate. Here we generate <code>nsamps = 1000</code> samples, giving us 1000 independent replicates at each <code>n</code>. But returning an array of 1000 values for a single value of <code>n_flips</code> results in something odd: the value for each element of <code>flips</code> is an array. To deal with that, we use the <code>unnest()</code> function, which expands the array back into a normal tibble.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-5_dfc439a14cf9511d0cf3594dbd312d6a">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n_samps <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n_flips_list <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">32</span>, <span class="dv">128</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sample_p <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">n_flips =</span> n_flips_list) <span class="sc">|&gt;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">flips =</span> <span class="fu">map</span>(n_flips, \(f) <span class="fu">rbinom</span>(<span class="at">n =</span> n_samps, <span class="at">size =</span> f, <span class="at">prob =</span> .<span class="dv">7</span>))) <span class="sc">|&gt;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> flips) <span class="sc">|&gt;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p =</span> flips <span class="sc">/</span> n_flips)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="from-variation-to-inference" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="from-variation-to-inference"><span class="header-section-number">6.2</span> From variation to inference</h2>
<p>Let’s go back to Fisher’s tea-tasting experiment. The first innovation of that experiment was the use of randomization to recover an estimate of the causal effect of milk ordering. But there was more to Fisher’s analysis than we described.</p>
<p>The second innovation of the tea-tasting experiment was the idea of creating a model of what might happen during the experiment. Specifically, Fisher described a hypothetical <strong>null model</strong> that would arise if the lady had chosen cups by chance rather than because of some tea sensitivity. In our tea-rating experiment, the null model describes what happens when there is no difference in ratings between tea-first and milk-first cups. Under the null model, the true treatment effect (<span class="math inline">\(\beta\)</span>) is zero.</p>
<p>Even with an actual treatment effect of zero, across repeated sampling, we should see some variation in <span class="math inline">\(\widehat{\beta}\)</span>, our <em>estimate</em> of the treatment effect. Sometimes we’ll get a small positive effect, sometimes a small negative one. Occasionally just by chance we’ll get a big effect. This is just sampling variation as we described above.</p>
<div class="page-columns page-full"><p>Fisher’s innovation was to quantify the probability of observing various values of <span class="math inline">\(\hat{\beta}\)</span>, given the null model. Then, if the observed data that were very low probability under the null model, we could declare that the null was rejected. How unlikely must the observed data be, in order to reject the null? Fisher declared that it is “usual and convenient for experimenters to take 5 percent as a standard level of convenience,” establishing the .05 cutoff that has become gospel throughout the sciences.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;Actually, right after establishing .05 as a cutoff, Fisher then writes that “in the statistical sense, we thereby admit that no isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon… in order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.” In other words, Fisher was all for replication!</p></li></div></div>
<p>Let’s take a look at what the null model might look like. We already tried out repeating our tea-tasting experiment thousands of times in our discussion of sampling above. Now in <a href="#fig-inference-null-model">Figure&nbsp;<span>6.6</span></a>, we do the same thing but we assume that the <strong>null hypothesis</strong> of no treatment effect is true. The plot shows the distribution of treatment effects <span class="math inline">\(\hat{\beta}\)</span> we observe: some a little negative, some a little positive, and a few substantially positive or negative, but mostly zero.</p>
<div class="page-columns page-full"><p>Let’s apply the <span class="math inline">\(p&lt;.05\)</span> standard. If our observation has less than a 5% probability under the null model, then the null model is likely wrong. The red dashed lines on <a href="#fig-inference-null-model">Figure&nbsp;<span>6.6</span></a> show the point below which only 2.5% of the data are found and the point above which only 2.5% of the data are found. These are called the <strong>tails</strong> of the distribution. Because we’d be equally willing to accept milk-first tea or tea-first tea being better, we consider both positive and negative observations as possible.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;Because we’re looking at both tails of the distribution, this is called a “two-tailed” test.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To simulate our null model, we can do the same kind of thing we did before, just specifying to our <code>make_tea_data()</code> function that the true difference in effects is zero!</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-6_2aa30eb0e44e0afcad854b8ce524f13b">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>null_model <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">sim =</span> <span class="dv">1</span><span class="sc">:</span>n_sims, <span class="at">n =</span> <span class="dv">18</span>) <span class="sc">|&gt;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">data =</span> <span class="fu">map</span>(sim, \(i) <span class="fu">make_tea_data</span>(<span class="at">n_total =</span> n, <span class="at">delta =</span> <span class="dv">0</span>))) <span class="sc">|&gt;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again we use <code>group_by()</code> and <code>summarise()</code> to get the distribution of treatment effects under the null hypothesis.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-7_82bfacc4ec891cbb2b2d15d6d7d1a591">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>null_model_summary <span class="ot">&lt;-</span> null_model <span class="sc">|&gt;</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sim, condition) <span class="sc">|&gt;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean_rating =</span> <span class="fu">mean</span>(rating)) <span class="sc">|&gt;</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sim) <span class="sc">|&gt;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">delta =</span> mean_rating[condition <span class="sc">==</span> <span class="st">"milk first"</span>] <span class="sc">-</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>              mean_rating[condition <span class="sc">==</span> <span class="st">"tea first"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div id="fig-inference-null-model" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/inference/p-region.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;6.6: One example of the distribution of treatment effects under the null model (with N=9 per group). The red regions indicate the part of the distribution in which less than 5% of observations should fall.</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p><a href="#fig-inference-null-model">Figure&nbsp;<span>6.6</span></a> captures the logic of NHST: if the observed data fall in the region that has a probability of less than .05 under the null model, then we reject the null. So then when we observe some particular treatment effect <span class="math inline">\(\hat{\beta}\)</span> in a single (real) instance of our experiment, we can compute the probability of these data or any data more extreme than ours under the null model.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> This probability is our <span class="math inline">\(p\)</span>-value, and if it is small, it gives us license to conclude that the null is false.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;The “more extreme” part deserves a little explanation. Any individual outcome is relatively unlikely by itself, just because it’s surprising that the estimate is that exact value (we’re simplifying here, it gets a bit trickier when you are talking about real numbers). What we care about instead is a <em>group</em> of values. The ones that are in the middle of the distribution are, considered as a group, quite likely; the ones on the tails are, as a group, less likely. We want to know if the probability of the group of datapoints that includes our observation and anything even further out on the tails is collectively less than .05.</p></li></div></div>
<p>As we saw before, the larger the sample size, the smaller the standard error. That’s true for the null model too! <a href="#fig-inference-null-model2">Figure&nbsp;<span>6.7</span></a> shows the expected null distribution for a bigger experiment.</p>

<div class="no-row-height column-margin column-container"><div id="fig-inference-null-model2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/p-region-bigger.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.7: Example distribution of treatment effects under the null model for a larger experiment.</figcaption>
</figure>
</div></div><p>The more participants in the experiment, the tighter the null distribution becomes, and hence the smaller the region in which we should expect a null treatment effect to fall. Because our expectation based on the null becomes more precise, we will be able to reject the null based on smaller treatment effects. In this type of hypothesis testing, as with estimation, our goals matter. If we’re merely testing a hypothesis out of curiosity, perhaps we don’t want to measure too many cups of tea. But if we were designing the tea strategy for a major cafe chain, the stakes would be higher; in that case, maybe we’d want to do a more extensive experiment!</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can do a more systematic simulation of the null regions for different sample sizes by simply adding a parameter to our simulation.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-8_07fc913e7a794166fd775904803f41a2">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>null_model_multi_n <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(<span class="at">sim =</span> <span class="dv">1</span><span class="sc">:</span>n_sims, <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">24</span>, <span class="dv">48</span>, <span class="dv">96</span>)) <span class="sc">|&gt;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sim_data =</span> <span class="fu">map</span>(n, \(n_i) <span class="fu">make_tea_data</span>(<span class="at">n_total =</span> n_i, <span class="at">delta =</span> <span class="dv">0</span>))) <span class="sc">|&gt;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(<span class="at">cols =</span> sim_data)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>null_model_summary_multi_n <span class="ot">&lt;-</span> null_model_multi_n <span class="sc">|&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n, sim, condition) <span class="sc">|&gt;</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean_rating =</span> <span class="fu">mean</span>(rating)) <span class="sc">|&gt;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n, sim) <span class="sc">|&gt;</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">delta =</span> mean_rating[condition <span class="sc">==</span> <span class="st">"milk first"</span>] <span class="sc">-</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>              mean_rating[condition <span class="sc">==</span> <span class="st">"tea first"</span>])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>null_model_quantiles_multi_n <span class="ot">&lt;-</span> null_model_summary_multi_n <span class="sc">|&gt;</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(n) <span class="sc">|&gt;</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">q_025 =</span> <span class="fu">quantile</span>(delta, .<span class="dv">025</span>),</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            <span class="at">q_975 =</span> <span class="fu">quantile</span>(delta, .<span class="dv">975</span>)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the plotting code to produce a comparable figure to our illustration:</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-9_a07a38aa7905ab0534f1fc3aa5a87c16">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(null_model_summary_multi_n, <span class="fu">aes</span>(<span class="at">x =</span> delta)) <span class="sc">+</span> </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="fu">vars</span>(n), <span class="at">nrow =</span> <span class="dv">1</span>, <span class="at">labeller =</span> label_both) <span class="sc">+</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> .<span class="dv">25</span>) <span class="sc">+</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> pal<span class="sc">$</span>grey, <span class="at">linetype =</span> <span class="st">"dotted"</span>) <span class="sc">+</span> </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> null_model_quantiles_multi_n, </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">xintercept =</span> q_025), <span class="at">color =</span> pal<span class="sc">$</span>red, <span class="at">linetype =</span> <span class="st">"dotted"</span>) <span class="sc">+</span> </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> null_model_quantiles_multi_n, </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">xintercept =</span> q_975), <span class="at">color =</span> pal<span class="sc">$</span>red, <span class="at">linetype =</span> <span class="st">"dotted"</span>) <span class="sc">+</span> </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>) <span class="sc">+</span> </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Difference in rating"</span>, <span class="at">y =</span> <span class="st">"Frequency"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="page-columns page-full"><p>One last note: You might notice an interesting parallel between the NHST paradigm and Popper’s falsificationist philosophy (introduced in <a href="002-theories.html"><span>Chapter&nbsp;2</span></a>). In both cases, you never get to <em>accept</em> the actual hypothesis of interest. The only thing you can do is observe evidence that is inconsistent with the null hypothesis. The added limitation of NHST is that the only hypothesis you can falsify is the null!<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;A historical note: what we describe here as NHST is not what either Fisher’s method <em>or</em> the Neyman-Pearson method that we introduce below. It’s what <span class="citation" data-cites="gigerenzer1989">Gigerenzer (<a href="#ref-gigerenzer1989" role="doc-biblioref">1989</a>)</span> called “the silent hybrid solution,” in which the more continuous approach to <span class="math inline">\(p\)</span>-values that Fisher advocated for got rolled into the hypothesis testing approach of Neyman and Pearson. This hybrid – which neither Fisher nor Neyman and Pearson would have liked – is what we now mostly take for granted as the received NHST approach.</p></li></div></div>
</section>
<section id="making-inferences" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="making-inferences"><span class="header-section-number">6.3</span> Making inferences</h2>
<p>In the tea-tasting example we were just considering, we were trying to make an inference from our sample to the broader population. In particular, we were trying to test whether milk-first tea was rated as better than tea-first tea. Our inferential goal was a clear, binary answer: is milk-first tea better?</p>
<p>By defining a <span class="math inline">\(p\)</span>-value, we got one procedure for giving this answer. If <span class="math inline">\(p &lt; .05\)</span>, we reject the null. Then we can look at the direction of the difference and, if it’s positive, declare that milk-first tea is “significantly” better. Let’s compare this procedure to a different process that builds on the Bayesian estimation ideas we described in the previous chapter. We can then come back to examine NHST in light of that framework.</p>
<section id="bayes-factors" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="bayes-factors"><span class="header-section-number">6.3.1</span> Bayes Factors</h3>
<p>Bayes Factors are a method for quantifying the support for one hypothesis over another, based on an observed dataset. They don’t tell you the probability that a particular hypothesis is right, but they let you compare two different ones.</p>

<div class="no-row-height column-margin column-container"><div id="fig-inference-bf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/bf.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.8: The Bayes Factor (BF).</figcaption>
</figure>
</div></div><div class="page-columns page-full"><p>Informally, we’ve now discussed two different distinct hypotheses about the tea situation: our participants could have <em>no</em> tea discrimination ability – leading to chance performance. We call this <span class="math inline">\(H_0\)</span>. Or they could have some non-zero ability – leading to greater than chance performance. We call this <span class="math inline">\(H_1\)</span>. The Bayes Factor is simply the likelihood of the data (in the technical sense used above) under <span class="math inline">\(H_1\)</span> vs.&nbsp;under <span class="math inline">\(H_0\)</span> (<a href="#fig-inference-bf">Figure&nbsp;<span>6.8</span></a>). The Bayes Factor is a ratio, so if it is greater than 1, the data are more likely under <span class="math inline">\(H_1\)</span> than they are under <span class="math inline">\(H_0\)</span> – and vice versa for values between 1 and 0. A BF of 3 means there is three times as much evidence for <span class="math inline">\(H_1\)</span> than <span class="math inline">\(H_0\)</span>, or equivalently 1/3 as much evidence for <span class="math inline">\(H_0\)</span> as <span class="math inline">\(H_1\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;Sometimes people refer to the BF in favor of <span class="math inline">\(H_1\)</span> as the <span class="math inline">\(BF_{10}\)</span> and the BF in favor of <span class="math inline">\(H_0\)</span> as the <span class="math inline">\(BF_{01}\)</span>. This notation is a bit confusing because the first of these looks like the number 10.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Bayes Factors are delightfully easy to compute using the <code>BayesFactor</code> R package. All we do is feed in the two sets of ratings to the <code>ttestBF()</code> function!</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-10_cb9cde1be4ed968ace01d4f5094457e8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BayesFactor)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tea_bf <span class="ot">&lt;-</span> <span class="fu">ttestBF</span>(<span class="at">x =</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"milk first"</span>)<span class="sc">$</span>rating,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">y =</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"tea first"</span>)<span class="sc">$</span>rating,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">paired =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="page-columns page-full"><p>There are a couple of things to notice about the Bayes Factor. The first is that, like a <span class="math inline">\(p\)</span>-value, it is inherently a continuous measure. You can artificially dichotomize decisions based on the Bayes Factor by declaring a cutoff (say, BF &gt; 3 or BF &gt; 10), but there is no intrinsic threshold at which you would say the evidence is “significant.” Some guidelines for interpretation <span class="citation" data-cites="goodman1999">(from <a href="#ref-goodman1999" role="doc-biblioref">S. N. Goodman 1999</a>)</span> are shown in <a href="#tbl-inference-jeffreys">Table&nbsp;<span>6.1</span></a>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> On the other hand, cutoffs like BF &gt; 5 or <span class="math inline">\(p &lt; .05\)</span> are not very informative. So although we provide this table to guide interpretation, we caution that you should always report and interpret the actual Bayes Factor, not whether it is above or below some cutoff.</p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;Some like the guidelines provided by <span class="citation" data-cites="jeffreys1998">Jeffreys (<a href="#ref-jeffreys1998" role="doc-biblioref">1961</a>)</span>, which include categories such as “barely worth mentioning” (1 &gt; BF &gt; 3).</p></li></div></div>

<div class="no-row-height column-margin column-container"><div class="">
<div id="tbl-inference-jeffreys" class="anchored">
<table class="table">
<caption>Table&nbsp;6.1: <span class="citation" data-cites="goodman1999">S. N. Goodman (<a href="#ref-goodman1999" role="doc-biblioref">1999</a>)</span> interpretation guidelines for Bayes Factors.</caption>
<thead>
<tr class="header">
<th>BF range</th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&lt; 1</td>
<td>Negative (supports <span class="math inline">\(H_0\)</span>)</td>
</tr>
<tr class="even">
<td>1–5</td>
<td>Weak</td>
</tr>
<tr class="odd">
<td>5–10</td>
<td>Moderate</td>
</tr>
<tr class="even">
<td>10–20</td>
<td>Moderate to strong</td>
</tr>
<tr class="odd">
<td>20–100</td>
<td>Strong to very strong</td>
</tr>
</tbody>
</table>
</div>
</div></div><p>The second thing to notice about the Bayes Factor is that it doesn’t depend on our prior probability of <span class="math inline">\(H_1\)</span> vs.&nbsp;<span class="math inline">\(H_0\)</span>. We might think of <span class="math inline">\(H_1\)</span> as very implausible. But the BF is independent of that prior belief. So that means it’s a measure of how much the evidence should shift our beliefs away from our prior. One nice way to think about this is that the Bayes Factor computes how much our beliefs – whatever they are – should be changed by the data <span class="citation" data-cites="morey2011">(<a href="#ref-morey2011" role="doc-biblioref">Morey and Rouder 2011</a>)</span>.</p>
<div class="page-columns page-full"><p>In practice, the thing that is both tricky and good about Bayes Factors is that you need to define an actual model of what <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are. That process involves making some assumptions explicit. We won’t go into how to make these models here – this is a big topic that is covered extensively in books on Bayesian data analysis.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> The goal here is just to give a general sense of what Bayes Factors are.</p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;Two good ones beyond the McElreath book mentioned above are <span class="citation" data-cites="gelman1995">Gelman et al. (<a href="#ref-gelman1995" role="doc-biblioref">1995</a>)</span>, which is a bit more statistical, and <span class="citation" data-cites="kruschke2014">Kruschke (<a href="#ref-kruschke2014" role="doc-biblioref">2014</a>)</span>, which is a bit more focused on psychological data analysis. An in-prep web-book by Nicenboim et al.&nbsp;(<a href="https://vasishth.github.io/bayescogsci/book/" class="uri">https://vasishth.github.io/bayescogsci/book/</a>) also looks great.</p></li></div></div>
<!-- That's the Bayes Factor.  -->
<!-- ```{r inference-milk-first, fig.cap="Ratings of the quality of milk-first tea."} -->
<!-- sigma <- 1.25 -->
<!-- n_total <- 48 -->
<!-- tea_data <- make_tea_data(n_total, sigma) -->
<!-- ggplot(tea_data, aes(x = rating, fill = condition)) +  -->
<!--   geom_dotplot(position = position_dodge(),  -->
<!--                aes(y = ..count..)) +  -->
<!--   xlim(1,7) +  -->
<!--   ylab("Number of ratings") +  -->
<!--   xlab("Quality rating (1-7)") +  -->
<!--   theme(axis.text.y=element_blank(), -->
<!--         axis.ticks.y=element_blank())  -->
<!-- tea_bf <- BayesFactor::ttestBF(x = tea_data$rating[tea_data$condition == "milk first"], -->
<!--                                y = tea_data$rating[tea_data$condition == "tea first"],  -->
<!--                                paired = FALSE) -->
<!-- tea_data_large <- make_tea_data(n_total *2, sigma) -->
<!-- tea_bf_large <- BayesFactor::ttestBF(x = tea_data_large$rating[tea_data_large$condition == "milk first"], -->
<!--                                y = tea_data_large$rating[tea_data_large$condition == "tea first"],  -->
<!--                                paired = FALSE) -->
<!-- ``` -->
</section>
<section id="p-values" class="level3 page-columns page-full" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="p-values"><span class="header-section-number">6.3.2</span> <em>p</em>-values</h3>
<div class="page-columns page-full"><p>Now let’s turn back to NHST and the <span class="math inline">\(p\)</span>-value. We already have a working definition of what a <span class="math inline">\(p\)</span>-value is from our discussion above: it’s the <strong>probability of the data (or any data that would be more extreme) under the null hypothesis</strong>. How is this quantity related to either our Bayesian estimate or the BF? Well, the first thing to notice is that the <span class="math inline">\(p\)</span>-value is very close (but not identical) to the likelihood itself.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;The likelihood – for both Bayesians and frequentists – is the probability of the data, just like the <span class="math inline">\(p\)</span>-value. But unlike the <span class="math inline">\(p\)</span>-value, it doesn’t include the probability of more extreme data as well.</p></li></div></div>
<div class="page-columns page-full"><p>Next we can use a simple statistical test, a <span class="math inline">\(t\)</span>-test, to compute <span class="math inline">\(p\)</span>-values for our experiment. In case you haven’t encountered one, a <span class="math inline">\(t\)</span>-test is a procedure for computing a <span class="math inline">\(p\)</span>-value by comparing the distribution of two variables using the null hypothesis that there is no difference between them.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> The <span class="math inline">\(t\)</span>-test uses the data to compute a <strong>test statistic</strong> whose distribution under the null hypothesis is known. Then the value of this statistic can be converted to <span class="math inline">\(p\)</span>-values for making an inference.</p><div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;<span class="math inline">\(t\)</span>-tests can also be used in cases where one sample is being compared to some baseline.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The standard <code>t.test()</code> function is built into R via the default <code>stats</code> package. Here we simply make sure to specify the variety of test we want by using the flags <code>paired = FALSE</code> and <code>var.equal = TRUE</code> (denoting the assumption of equal variances).</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-11_50b0bdfb216adef01c305193905d5ee8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>tea_t <span class="ot">&lt;-</span> <span class="fu">t.test</span>(<span class="at">x =</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"milk first"</span>)<span class="sc">$</span>rating,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"tea first"</span>)<span class="sc">$</span>rating,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">paired =</span> <span class="cn">FALSE</span>, <span class="at">var.equal =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>Imagine we conduct a tea-tasting experiment with <span class="math inline">\(N=48\)</span> and perform a <span class="math inline">\(t\)</span>-test on our experimental results. In this case, we see that the difference between the two groups is significant at <span class="math inline">\(p&lt;.05\)</span>: <span class="math inline">\(t(46) = 2.86\)</span>, <span class="math inline">\(p = .006\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="cell" data-hash="006-inference_cache/html/tbl-p-bf-comparison_92b078de1250029b173d7f3c44bd527d">
<div class="cell-output-display">
<div id="tbl-p-bf-comparison" class="anchored">
<table class="table table-sm table-striped small">
<caption>Table&nbsp;6.2: Comparison of p-value and BF for several different (randomly-generated) tea-tasting scenarios.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">N</th>
<th style="text-align: right;">Effect size</th>
<th style="text-align: right;">p-value</th>
<th style="text-align: right;">BF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">12</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">&gt; .999</td>
<td style="text-align: right;">0.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">12</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">.076</td>
<td style="text-align: right;">1.4</td>
</tr>
<tr class="odd">
<td style="text-align: right;">12</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">.002</td>
<td style="text-align: right;">18.7</td>
</tr>
<tr class="even">
<td style="text-align: right;">24</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">.858</td>
<td style="text-align: right;">0.4</td>
</tr>
<tr class="odd">
<td style="text-align: right;">24</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">.061</td>
<td style="text-align: right;">1.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">24</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">.009</td>
<td style="text-align: right;">5.6</td>
</tr>
<tr class="odd">
<td style="text-align: right;">48</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">.002</td>
<td style="text-align: right;">17.7</td>
</tr>
<tr class="even">
<td style="text-align: right;">48</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">.033</td>
<td style="text-align: right;">2.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">48</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">&lt; .001</td>
<td style="text-align: right;">133.6</td>
</tr>
<tr class="even">
<td style="text-align: right;">96</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">.038</td>
<td style="text-align: right;">1.5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">96</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">&lt; .001</td>
<td style="text-align: right;">12218.2</td>
</tr>
<tr class="even">
<td style="text-align: right;">96</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">&lt; .001</td>
<td style="text-align: right;">3081.4</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div></div><p>The expression <span class="math inline">\(t(46) = 2.86\)</span>, <span class="math inline">\(p = .006\)</span> is the standard way to report of a <span class="math inline">\(t\)</span>-test according to the American Psychological Association. The first part of this report gives the <span class="math inline">\(t\)</span> value, qualified by the <strong>degrees of freedom</strong> for the test in parentheses. We won’t focus much on the idea of degrees of freedom here, but for now it’s enough to know that this number quantifies the amount of information given by the data, in this case 48 datapoints minus the two means (one for each of the samples).</p>
<p>Let’s compare <span class="math inline">\(p\)</span> values and Bayes Factors (computed using the default setup in the <code>BayesFactor</code> R package). In <a href="#tbl-p-bf-comparison">Table&nbsp;<span>6.2</span></a>), the rows represent simulated experiments with varying total numbers of participants (N and varying average treatment effects. Both <span class="math inline">\(p\)</span> and BF go up with more participants and larger effects. In general, BFs tend to be a bit more conservative than <span class="math inline">\(p\)</span>-values, such that <span class="math inline">\(p&lt;.05\)</span> can sometimes translate to a BF of less than 3 <span class="citation" data-cites="benjamin2018">(<a href="#ref-benjamin2018" role="doc-biblioref">Benjamin et al. 2018</a>)</span>. For example, take a look at the row with 48 participants and an effect size of 1: the <span class="math inline">\(p\)</span> value is less than .05, but the Bayes Factor is only 2.0.</p>
<p>The critical thing about <span class="math inline">\(p\)</span>-values, though, is not just that they are a kind of data likelihoods. It is that they are used in a <em>specific inferential procedure</em>. The logic of NHST is that we make a binary decision about the presence of an effect. If <span class="math inline">\(p &lt; .05\)</span>, the null hypothesis is rejected; otherwise not. As <span class="citation" data-cites="fisher1949">Fisher (<a href="#ref-fisher1949" role="doc-biblioref">1949</a>)</span> wrote,</p>
<blockquote class="blockquote">
<p>It should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation. Every experiment may be said to exist only in order to give the facts a chance of disproving the null hypothesis. (p.&nbsp;19)</p>
</blockquote>
<p>The main problem with <span class="math inline">\(p\)</span>-values from a scientific perspective is that researchers are usually interested in not just rejecting the null hypothesis but also in the evidence for the alternative (the one we are interested in). The Bayes Factor is one approach to quantifying positive evidence <em>for</em> the alternative hypothesis in a Bayesian framework. This issue with the Fisher approach to <span class="math inline">\(p\)</span>-values has been known for a long time, though, and so there is an alternative frequentist approach as well.</p>

<div class="no-row-height column-margin column-container"><div id="fig-inference-power-alpha" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/power-alpha.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6.9: Standard decision matrix for the Neyman-Pearson approach to statistical inference.</figcaption>
</figure>
</div></div></section>
<section id="sec-neyman-pearson" class="level3 page-columns page-full" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored"><span class="header-section-number">6.3.3</span> The Neyman-Pearson approach</h3>
<div class="page-columns page-full"><p>One way to “patch” NHST is to introduce a decision-theoretic view, shown in <a href="#fig-inference-power-alpha">Figure&nbsp;<span>6.9</span></a>.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> On this view, called the Neyman-Pearson view, there is a real <span class="math inline">\(H_1\)</span>, albeit one that is not specified. Then the true state of the world could be that <span class="math inline">\(H_0\)</span> is true or <span class="math inline">\(H_1\)</span> is true. The <span class="math inline">\(p&lt;.05\)</span> criterion is the threshold at which we are willing to reject the null, and so this constitutes our <strong>false positive</strong> rate <span class="math inline">\(\alpha\)</span>. But we also need to define a <strong>false negative</strong> rate, which is conventionally called <span class="math inline">\(\beta\)</span>.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn13"><p><sup>13</sup>&nbsp;A little bit of useful history here is given in <span class="citation" data-cites="cohen1990">Cohen (<a href="#ref-cohen1990" role="doc-biblioref">1990</a>)</span>, and we also recommend <span class="citation" data-cites="gigerenzer1989">Gigerenzer (<a href="#ref-gigerenzer1989" role="doc-biblioref">1989</a>)</span> for a broader perspective.</p></li><li id="fn14"><p><sup>14</sup>&nbsp;Unfortunately, <span class="math inline">\(\beta\)</span> is very commonly used for regression coefficients – and for that reason we’ve used it as our symbol for causal effects. We’ll be using these <span class="math inline">\(\beta\)</span>s in the next chapter as well. Those <span class="math inline">\(\beta\)</span>s are not to be confused with false negative rates. Sorry, this is just a place where statisticians have used the same Greek letter for two different things.</p></li></div></div>
<div class="page-columns page-full"><p>Setting these rates is a decision problem: If you are too conservative in your criteria for the intervention having an effect, then you risk a false negative, where you incorrectly conclude that it doesn’t work. And if you’re too liberal in your assessment of the evidence, then you risk a false positive.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> In practice, however, people usually leave <span class="math inline">\(\alpha\)</span> at .05 and try to control the false negative rate by increasing their sample size.</p><div class="no-row-height column-margin column-container"><li id="fn15"><p><sup>15</sup>&nbsp;To make really rational decisions, you could couple this chart to some kind of utility function that assessed the costs of different outcomes. For example, you might think it’s worse to proceed with an intervention that doesn’t work than to stay with business as usual. In that case, you’d assign a higher cost to a false positive and accordingly try to adopt a more conservative criterion. We won’t cover this kind of decision analysis here, but <span class="citation" data-cites="pratt1995">Pratt et al. (<a href="#ref-pratt1995" role="doc-biblioref">1995</a>)</span> is a classic textbook on statistical decision theory if you’re interested.</p></li></div></div>
<p>As we saw in <a href="#fig-inference-null-model">Figure&nbsp;<span>6.6</span></a>, the larger the sample, the better your chance of rejecting the null for any given non-null effect. But these chances will depend also on the effect size you are estimating. This formulation gives rise to the idea of classical power analysis, which we cover in <a href="010-sampling.html"><span>Chapter&nbsp;10</span></a>. Most folks who defend binary inference are interested in using the Neyman-Pearson approach. In our view, this approach has its place (it’s especially useful for power analysis) but it still suffers from the substantial issues that plague all binary inference techniques.</p>
<!-- \vspace{2.5em} -->
<div class="callout callout-style-default callout-note callout-titled" title="depth">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
depth
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<section id="nonparametric-resampling-under-the-null" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="nonparametric-resampling-under-the-null">Nonparametric resampling under the null</h2>
<p>Hypothesis testing requires knowing the null distribution. In the examples above, it was easy to use statistical theory to work out the null distribution using knowledge of the binomial or normal distribution. But sometimes we don’t know what the null distribution would look like. What if the ratings data from our tea-tasting experiment was very skewed, such that there were many low ratings and a few very high ratings (as in <a href="#fig-inference-permutation">Figure&nbsp;<span>6.10</span></a>)?</p>
<div id="fig-inference-permutation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/inference/skewed.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6.10: A small tea-tasting experiment with a skewed distribution of ratings.</figcaption>
</figure>
</div>
<p>With skewed data like this, we couldn’t proceed with a <span class="math inline">\(t\)</span>-test in good conscience because, with only <span class="math inline">\(n=18\)</span>, we can’t necessarily trust that the Central Limit Theorem has “kicked in” sufficiently for the test to work despite the skewness. Put another way, we can’t be sure that the null distribution is normal (Gaussian) in this case.</p>
<p>An alternative way to approximate a null distribution is through nonparametric resampling. <strong>Resampling</strong> means that we’re going to draw new samples <em>from our existing sample</em>, and <strong>nonparametric</strong> means that we will do this in a way that obviates assumptions about the shape of the null distribution – in contrast to <strong>parametric</strong> approaches that do rely on such assumptions). These techniques are sometimes called “bootstrapping” techniques.</p>
<p>The idea is, if the treatment truly had no effect on the outcome, then the observations would be <strong>exchangeable</strong> between the treatment and control groups. That is, there would not be systematic differences between the treatment and control groups. This property may or may not be true in our observed sample (after all, that’s why we’re doing a hypothesis test in the first place), but we can draw new samples from our existing sample in a manner that forces exchangability.</p>
<p>To perform this kind of test with our tea-tasting data, we would randomly shuffle the ratings in our dataset while leaving the condition assignments fixed. If we did this thousands of times and computed the treatment effect in each case, the result would be a null distribution: what we might expect the treatment effect to look like if there was <em>no</em> condition effect. In essence we’re using a simulated version of “random assignment” here to <em>break</em> the dependency between the condition manipulation and the observed data.</p>
<p>We can then compare our <em>actual</em> treatment effect to this nonparametric null distribution. If the actual treatment was smaller than the 2.5th percentile or larger than the 97.5th percentile in the null distribution, we would reject the null with <span class="math inline">\(p &lt; .05\)</span>, just the same as if we had used a <span class="math inline">\(t\)</span>-test.</p>
<p>Resampling-based tests are extremely useful in a wide variety of cases. They can sometimes be less powerful than parametric approaches and they almost always require more computation, but their versatility makes them a great generic tool for data analysis.</p>
</section>
</div>
</div>
</section>
</section>
<section id="inference-and-its-discontents" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="inference-and-its-discontents"><span class="header-section-number">6.4</span> Inference and its discontents</h2>
<p>In earlier sections of this chapter, we reviewed NHST and Bayesian approaches to inference. Now it’s time to step back and think about some of the ways that inference practices – especially those related to NHST – have been problematic for psychology research. We’ll begin with some issues surrounding <span class="math inline">\(p\)</span>-values and then give a specific accident report related to the process of “<span class="math inline">\(p\)</span>-hacking” and some general philosophical discussion of how statistical testing relates to human reasoning.</p>
<section id="problems-with-the-interpretation-of-p-values" class="level3 page-columns page-full" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored"><span class="header-section-number">6.4.1</span> Problems with the interpretation of <em>p</em>-values</h3>
<div class="page-columns page-full"><p><span class="math inline">\(p\)</span>-values are basically likelihoods, in the sense we introduced in the previous chapter.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> They are the likelihood of the data under the null hypothesis! This likelihood is a critical number to know – for computing the Bayes Factor among other reasons. But it doesn’t tell us a lot of things that we might like to know!</p><div class="no-row-height column-margin column-container"><li id="fn16"><p><sup>16</sup>&nbsp;The only thing that is different is the idea that they are the likelihood of the observed data <em>or any more extreme</em>.</p></li></div></div>
<p>For example, <span class="math inline">\(p\)</span>-values don’t tell us the probability of the data under a specific alternative hypothesis that we might be interested in – that’s the posterior probability <span class="math inline">\(p(H_1 | \text{data})\)</span>. When our tea-tasting <span class="math inline">\(t\)</span>-test yielded <span class="math inline">\(t(46) = 2.86\)</span>, <span class="math inline">\(p = .006\)</span>, that <span class="math inline">\(p\)</span> is <em>not</em> the probability of the null hypothesis being true! And it’s definitely not the probability of milk-first tea being better.</p>
<div class="page-columns page-full"><p>What can you conclude when <span class="math inline">\(p&gt;.05\)</span>? According to the classical logic of NHST, the answer is “nothing”! A failure to reject the null hypothesis doesn’t give you any additional evidence <em>for</em> the null. Even if the probability of the data (or some more extreme data) under <span class="math inline">\(H_0\)</span> is high, their probability might be just as high or higher under <span class="math inline">\(H_1\)</span>.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> But many practicing researchers make this mistake. <span class="citation" data-cites="aczel2018">Aczel et al. (<a href="#ref-aczel2018" role="doc-biblioref">2018</a>)</span> coded a sample of articles from 2015 and found that 72% of negative statements were inconsistent with the logic of their statistical paradigm of choice – most were cases where researchers said that an effect was not present when they had simply failed to reject the null.</p><div class="no-row-height column-margin column-container"><li id="fn17"><p><sup>17</sup>&nbsp;Of course, weighing these two against one another brings you back to the Bayes Factor.</p></li></div></div>
<p>These are not the only issues with <span class="math inline">\(p\)</span>-values. In fact, people have so much trouble understanding what <span class="math inline">\(p\)</span>-values <em>do</em> say that there are whole articles written about these misconceptions. <a href="#tbl-dirty-dozen">Table&nbsp;<span>6.3</span></a> shows a set of misconceptions documented and refuted by <span class="citation" data-cites="goodman2008">S. N. Goodman (<a href="#ref-goodman2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>Let’s take a look at just a few. Misconception 1 is that, if <span class="math inline">\(p= .05\)</span>, the null has a 5% chance of being true. This misconception is a result of confusing <span class="math inline">\(p(H_0 | \text{data})\)</span> (the posterior) and <span class="math inline">\(p(\text{data} | H_0)\)</span> (the likelihood – also known as the <span class="math inline">\(p\)</span>-value). Misconception 2 – that <span class="math inline">\(p &gt; .05\)</span> allows us to <em>accept</em> the null – also stems from this reversal of posterior and likelihood. And misconception 3 is a misinterpretation of the <span class="math inline">\(p\)</span>-value as an effect size (which we learned about in the last chapter): a large effect is likely to be clinically important, but with a large enough sample size, you can get a small <span class="math inline">\(p\)</span>-value even for a very small effect. We won’t go through all the misconceptions here, but we encourage you to challenge yourself to work through them (as in the exercise below).</p>
<div id="tbl-dirty-dozen" class="anchored">
<table class="table">
<caption>Table&nbsp;6.3: A “dirty dozen” <span class="math inline">\(p\)</span>-value misconceptions. Adapted from <span class="citation" data-cites="goodman2008">S. N. Goodman (<a href="#ref-goodman2008" role="doc-biblioref">2008</a>)</span>.</caption>
<colgroup>
<col style="width: 2%">
<col style="width: 97%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th>Misconception</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td>“If <span class="math inline">\(p = .05\)</span>, the null hypothesis has only a 5% chance of being true.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td>“A nonsignificant difference (e.g., <span class="math inline">\(p \geq .05\)</span>) means there is no difference between groups.”</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td>“A statistically significant finding is clinically important.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td>“Studies with <span class="math inline">\(p\)</span>-values on opposite sides of .05 are conflicting.”</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td>“Studies with the same <span class="math inline">\(p\)</span>-value provide the same evidence against the null hypothesis.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td>“<span class="math inline">\(p\)</span> = .05 means that we have observed data that would occur only 5% of the time under the null hypothesis.”</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td>“<span class="math inline">\(p\)</span> = .05 and <span class="math inline">\(p \leq .05\)</span> mean the same thing.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td>“<span class="math inline">\(p\)</span>-values are properly written as inequalities (e.g., ‘<span class="math inline">\(p \leq .02\)</span>’ when <span class="math inline">\(p = .015\)</span>)”</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9</td>
<td>“<span class="math inline">\(p\)</span> = .05 means that if you reject the null hypothesis, the probability of a false positive error is only 5%.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">10</td>
<td>“With a <span class="math inline">\(p\)</span> = .05 threshold for significance, the chance of a false positive error will be 5%.”</td>
</tr>
<tr class="odd">
<td style="text-align: right;">11</td>
<td>“You should use a one-sided <span class="math inline">\(p\)</span>-value when you don’t care about a result in one direction, or a difference in that direction is impossible.”</td>
</tr>
<tr class="even">
<td style="text-align: right;">12</td>
<td>“A scientific conclusion or treatment policy should be based on whether or not the <span class="math inline">\(p\)</span> value is significant.”</td>
</tr>
</tbody>
</table>
</div>
<p>Beyond these misconceptions, there’s another problem. The <span class="math inline">\(p\)</span>-value is a probability of a certain set of events happening (corresponding to the observed data or any “more extreme” data, that is to say, data further from the null). Since <span class="math inline">\(p\)</span>-values are probabilities, we can combine them together across different events. If we run a “null experiment” – an experiment where the true effect is zero – the probability of a dataset with <span class="math inline">\(p &lt; .05\)</span> is of course .05. But if we run two such experiments, we can get <span class="math inline">\(p &lt; .05\)</span> with probability 0.1. By the time we run 20 experiments, we have an 0.64 chance of getting a positive result.</p>
<div class="page-columns page-full"><p>It would obviously be a major mistake to run 20 experiments and then report only the positive ones (which, by design, are false positives) as though these still were “statistically significant.” The same thing applies to doing 20 different statistical tests within a single experiment. There are many statistical corrections that can be made to adjust for this problem, which is known as the problem of <strong>multiple comparisons</strong>.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> But the the broader issue is one of transparency: unless you <em>know</em> what the appropriate set of experiments or tests is, it’s not possible to implement one of these corrections!<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn18"><p><sup>18</sup>&nbsp;The simplest and most versatile one, the Bonferroni correction, just divides .05 (or technically, whatever your threshold is) by the number of comparisons you are making. Using that correction, if you do 20 null experiments, you would have a 3% chance of a false positive.</p></li><li id="fn19"><p><sup>19</sup>&nbsp;This issue is especially problematic with <span class="math inline">\(p\)</span>-values because they are so often presented as an independent set of tests, but the problem of multiple comparisons comes up when you compute a lot of independent Bayes Factors as well. “Posterior hacking” via selective reporting of Bayes Factors is perfectly possible <span class="citation" data-cites="simonsohn2014">(<a href="#ref-simonsohn2014" role="doc-biblioref">Simonsohn 2014</a>)</span>.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<section id="do-extraordinary-claims-require-extraordinary-evidence" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="do-extraordinary-claims-require-extraordinary-evidence">Do extraordinary claims require extraordinary evidence?</h2>
<p>In a blockbuster paper that may have inadvertently kicked off the replication crisis, <span class="citation" data-cites="bem2011">Bem (<a href="#ref-bem2011" role="doc-biblioref">2011</a>)</span> presented nine experiments he claimed provided evidence for precognition – that participants somehow had foreknowledge of the future. In the first of these experiments, Bem showed each of a group of 100 undergraduates 36 two-alternative forced choice trials in which they had to guess which of two locations on a screen would reveal a picture immediately before the picture was revealed. By chance, participants should choose the correct side 50% of the time of course. Bem found that, specifically for erotic pictures, participants’ guesses were 53.1% correct. This rate of guessing was unexpected under the null hypothesis of chance guessing (<span class="math inline">\(p = .01\)</span>). Eight other studies with a total of more than 1,000 participants yielded apparently supportive evidence, with participants appearing to show a variety of psychological effects even before the stimuli were shown!</p>
<p>Based on this evidence, should we conclude that precognition exists? Probably not. <span class="citation" data-cites="wagenmakers2011">Wagenmakers et al. (<a href="#ref-wagenmakers2011" role="doc-biblioref">2011</a>)</span> presented a critique of Bem’s findings, arguing that 1) Bem’s experiments were exploratory (not pre-registered) in nature, 2) that Bem’s conclusions were <em>a priori</em> unlikely, and 3) that the level of statistical evidence from his experiments was quite low. We find each of these arguments alone compelling; together they present a knockdown case against Bem’s interpretation.</p>
<p>First, we’ve already discussed the need to be skeptical about situations where experimenters have the opportunity for analytic flexibility in their choice of measures, manipulations, samples, and analyses. Flexibility leads to the possibility of cherry-picking those set of decisions from the “garden of forking paths” that lead to a positive outcome for the researcher’s favored hypothesis (for more details, see <a href="011-prereg.html"><span>Chapter&nbsp;11</span></a>). And there is plenty of flexibility on display even in Experiment 1 of Bem’s paper. Although there were 100 participants in the study, they may have been combined post hoc from two distinct samples of 40 and 60, each of which saw different conditions. The 40 made guesses about the location of erotic, negative, and neutral pictures; the 60 saw erotic, positive non-romantic, and positive romantic pictures. The means of each of these conditions was presumably tested against chance (at least 6 comparisons, for a false positive rate of 0.26). Had positive romantic pictures been found significant, Bem certainly could have interpreted this finding the same way he interpreted the erotic ones.</p>
<p>Second, as we discussed, a <span class="math inline">\(p\)</span>-value close to .05 does not necessarily provide strong evidence against the null hypothesis. Wagenmakers et al.&nbsp;computed the Bayes Factor for each of experiments in Bem’s paper and found that, in many cases, the amount of evidence for <span class="math inline">\(H_1\)</span> was quite modest under a default Bayesian <span class="math inline">\(t\)</span>-test. Experiment 1 was no exception: the BF was 1.64, giving only “anecdotal” support for the hypothesis of some non-zero effect, even before the multiple-comparisons problem mentioned above.</p>
<p>Finally, since precognition is not supported by any prior compelling scientific evidence (despite many attempts to obtain such evidence) and defies well-established physical laws, perhaps we should assign a low prior probability to Bem’s <span class="math inline">\(H_1\)</span>, a non-zero precognition effect. Taking a strong Bayesian position, Wagenmakers et al.&nbsp;suggest that we might do well to adopt a prior reflecting how unlikely precognition is, say <span class="math inline">\(p(H_1) = 10^{-20}\)</span>. And if we adopt this prior, even a very well-designed, highly informative experiment (with a Bayes factor conveying substantial or even decisive evidence) would still lead to a very low posterior probability of precognition.</p>
<p>Wagenmakers et al.&nbsp;concluded that, rather than supporting precognition, the conclusion from Bem’s paper should be psychologists should revise how they think about analyzing their data (and avoid <span class="math inline">\(p\)</span>-hacking)!</p>
</section>
</div>
</div>
</section>
<section id="philosophical-and-empirical-views-of-probability" class="level3 page-columns page-full" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="philosophical-and-empirical-views-of-probability"><span class="header-section-number">6.4.2</span> Philosophical (and empirical) views of probability</h3>
<div class="page-columns page-full"><p>Up until now we’ve presented Bayesian and frequentist tools as two different sets of computations. But in fact, these different tools derive from fundamentally different philosophical perspectives on what a probability even is. Very roughly, frequentist approaches tend to believe that probabilities quantify the long-run frequencies of certain events. So, if we say that some outcome of an event has probability .5, we’re saying that if that event happened thousands of times, the long run frequency of the outcome would be 50% of the total events. In contrast, the Bayesian viewpoint doesn’t depend on this sense that events could be exactly repeated. Instead, the <strong>subjective Bayesian</strong> interpretation of probability is that it quantifies a person’s degree of belief in a particular outcome.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn20"><p><sup>20</sup>&nbsp;This is really a very rough description. If you’re interested in learning more about this philosophical background, we recommend the Stanford Encyclopedia of Philosophy entry, “Interpretations of Probability” (<a href="https://plato.stanford.edu/entries/probability-interpret/" class="uri">https://plato.stanford.edu/entries/probability-interpret/</a>).</p></li></div></div>
<p>You don’t have to take sides in this deep philosophical debate about what probability is. But it’s helpful to know that people actually seem to reason about the world in ways that are well described by the subjective Bayesian view of probability. Recent cognitive science research has made a lot of headway in describing reasoning as a process of Bayesian inference where probabilities describe degrees of belief in different hypotheses <span class="citation" data-cites="probmods2">(for a textbook review of this approach, see <a href="#ref-probmods2" role="doc-biblioref">N. D. Goodman, Tenenbaum, and Contributors 2016</a>)</span>. These hypotheses in turn are a lot like the theories we described in <a href="002-theories.html"><span>Chapter&nbsp;2</span></a>: they describe the relationships between different abstract entities <span class="citation" data-cites="tenenbaum2011">(<a href="#ref-tenenbaum2011" role="doc-biblioref">Tenenbaum et al. 2011</a>)</span>. You might think that scientists are different from lay-people in this regard, but one of the striking findings from research on probabilistic reasoning and judgment is that expertise doesn’t matter that much. Statistically-trained scientists – and even statisticians – make many of the same reasoning mistakes as their un-trained students <span class="citation" data-cites="kahneman1979">(<a href="#ref-kahneman1979" role="doc-biblioref">Kahneman and Tversky 1979</a>)</span>. Even children seem to reason intuitively in a way that looks a bit like Bayesian inference <span class="citation" data-cites="gopnik2012">(<a href="#ref-gopnik2012" role="doc-biblioref">Gopnik 2012</a>)</span>.</p>
<div class="page-columns page-full"><p>These cognitive science findings help to explain some of the problems that people (scientists included) have in reasoning about <span class="math inline">\(p\)</span>-values. If you are an intuitively Bayesian reasoner, the quantity that you’re probably tracking is how much you believe in your hypothesis (its posterior probability). So, many people treat the <span class="math inline">\(p\)</span>-value as the posterior probability of the null hypothesis.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> That’s exactly what fallacy #1 in <a href="#tbl-dirty-dozen">Table&nbsp;<span>6.3</span></a> states – “If <em>p</em> = .05, the null hypothesis has only a 5% chance of being true.” But this equivalence is incorrect! Written in math, <span class="math inline">\(p(\text{data} | H_0)\)</span> (the likelihood that lets us compute the p-value) is not the same thing as <span class="math inline">\(p(H_0 | \text{data})\)</span> (the posterior that we want). Pulling from our accident report above, even if the <em>probability of the observed ESP data given the null hypothesis</em> is low, that doesn’t mean that the <em>probability of ESP</em> is high.</p><div class="no-row-height column-margin column-container"><li id="fn21"><p><sup>21</sup>&nbsp;<span class="citation" data-cites="cohen1994">Cohen (<a href="#ref-cohen1994" role="doc-biblioref">1994</a>)</span> is a great treatment of this issue.</p></li></div></div>
</section>
<section id="what-framework-to-use" class="level3 page-columns page-full" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="what-framework-to-use"><span class="header-section-number">6.4.3</span> What framework to use?</h3>
<div class="page-columns page-full"><p>The problem with binary inferences is that they enable behaviors that can introduce bias into the scientific ecosystem. By the logic of statistical significance, either an experiment “worked” or it didn’t. Because everyone would usually rather have an experiment that worked than one that didn’t, inference criteria like <span class="math inline">\(p\)</span>-values often become a target for selection, as we discussed in <a href="003-replication.html"><span>Chapter&nbsp;3</span></a>.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn22"><p><sup>22</sup>&nbsp;More generally, this pattern is probably an example of Goodhart’s law, which states that when a measure becomes a target, it ceases to be a good measure <span class="citation" data-cites="strathern1997">(<a href="#ref-strathern1997" role="doc-biblioref">Strathern 1997</a>)</span>. Once the outcomes of statistical inference procedures become targets for publication, they are subject to selection biases – <span class="math inline">\(p\)</span>-hacking, for example – that make them less meaningful.</p></li></div></div>
<p>If you want to quantify evidence for or against a hypothesis, it’s worth considering whether Bayes Factors address your question better than <span class="math inline">\(p\)</span>-values. In practice, <span class="math inline">\(p\)</span>-values are hard to understand and many people misuse them – though to be fair, BFs are misused plenty too. These issues may be rooted in basic facts about how human beings reason about probability.</p>
<p>Despite the reasons to be worried about <span class="math inline">\(p\)</span>-values, for many practicing scientists (at least at time of writing) there is no one right answer about whether to use them or not. Even if we’d like to be Bayesian all the time, there are a number of obstacles. First, though new computational tools make fitting Bayesian models and extracting Bayes Factors much easier than before, it’s still on average quite a bit harder to fit a Bayesian model than it is a frequentist one. Second, because Bayesian analyses are less familiar, it may be an uphill battle to convince advisors, reviewers, and funders to use them.</p>
<p>As a group of authors, some of us are more Bayesian than frequentist, while others are more frequentist than Bayesian – but all of us recognize the need to move between statistical paradigms depending on the problem we’re working on. Furthermore, a lot of the time we’re not so worried about which paradigm we’re using. The paradigms are at their most divergent when making binary inferences, and they often look much more similar when they are used in the context of quantifying measurement precision.</p>
</section>
</section>
<section id="computing-precision" class="level2 page-columns page-full" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="computing-precision"><span class="header-section-number">6.5</span> Computing precision</h2>
<p>Our last section presented an argument against using <span class="math inline">\(p\)</span>-values for making <em>dichotomous</em> inferences. But we still want to move from what we know about our own limited sample to some inference about the population. How should we do this?</p>
<section id="confidence-intervals" class="level3 page-columns page-full" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="confidence-intervals"><span class="header-section-number">6.5.1</span> Confidence intervals</h3>
<p>One alternative to binary hypothesis testing is to ask about the precision of our estiamates, in particular how similar an estimate from a particular sample is to the population parameter of interest. For example, how close is our tea-tasting effect estimate to the true effect in the population? We don’t know what the true effect is, but our knowledge of sampling distributions lets us make some guesses about how precise our estimate is.</p>
<p>The <strong>confidence interval</strong> is a convenient frequentist way to summarize the variability of the sampling distribution – and hence how precise our point estimate is. The confidence interval represents the range of possible values for the parameter of interest that are plausible given the data. More formally, a 95% confidence interval for some estimate (call it <span class="math inline">\(\widehat{\beta}\)</span>, as in our example) is defined as a range of possible values for <span class="math inline">\(\beta\)</span> such that, if we did repeated sampling, 95% of the intervals generated by those samples would contain the true parameter, <span class="math inline">\(\beta\)</span>.</p>
<!-- ```{r inference-tea-se} -->
<!-- tea_ratings <- tea_data$rating[tea_data$condition == "tea first"] -->
<!-- milk_ratings <- tea_data$rating[tea_data$condition == "milk first"] -->
<!-- n_tea <- length(tea_ratings) -->
<!-- n_milk <- length(milk_ratings) -->
<!-- sd_tea <- sd(tea_ratings) -->
<!-- sd_milk <- sd(milk_ratings) -->
<!-- tea_sd_pooled <- sqrt(((n_tea-1)*sd_tea^2 + (n_milk-1)*sd_milk^2) / -->
<!--                         (n_tea + n_milk - 2)) -->
<!-- tea_se <- tea_sd_pooled * sqrt((1/n_tea) + (1/n_milk)) -->
<!-- delta_hat <- mean(milk_ratings) - mean(tea_ratings) -->
<!-- tea_ci_lower <- delta_hat - tea_se *1.96 -->
<!-- tea_ci_upper <- delta_hat + tea_se *1.96 -->
<!-- ``` -->
<div class="page-columns page-full"><p>Confidence intervals are constructed by estimating the middle 95% of the sampling distribution of <span class="math inline">\(\widehat{\beta}\)</span>. Because of our hero, the Central Limit Theorem, we can treat the sampling distribution as normal for reasonably large samples. Given this, it’s common to construct a 95% confidence intervals <span class="math inline">\(\widehat{\beta} \pm 1.96 \; \widehat{SE}\)</span>.<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> If we were to conduct the experiment 100 times and calculate a confidence interval each time, we should expect 95 of the intervals to contain the true <span class="math inline">\(\beta\)</span>, whereas we would expect the remaining 5 to not contain <span class="math inline">\(\beta\)</span>.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn23"><p><sup>23</sup>&nbsp;This type of CI is called a “Wald” confidence interval.</p></li><li id="fn24"><p><sup>24</sup>&nbsp;In case you don’t have enough tea to do the experiment 100 times to confirm this, you can do it virtually using this nice simulation tool: <a href="https://istats.shinyapps.io/ExploreCoverage" class="uri">https://istats.shinyapps.io/ExploreCoverage</a>.</p></li></div></div>
<p>Confidence intervals are like betting on the inferences drawn from your sample. The sample you drew is like one pull of a slot machine that will pay off (i.e., have the confidence interval contain the true parameter) 95% of the time. Put more concisely: 95% of 95% confidence intervals contain the true value of the population parameter.</p>
<div id="fig-inference-condition-cis" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/inference/cis.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;6.11: Confidence intervals on each of the two condition estimates, as well as on the difference between conditions.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Computing confidence intervals analytically is pretty easy. Here we first compute the standard error for the difference between conditions. The only tricky bit here is that we need to compute a pooled standard deviation.</p>
<div class="cell" data-opts.label="code" data-hash="006-inference_cache/html/unnamed-chunk-12_bdc037da8327052f2611cd5214031393">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>tea_ratings <span class="ot">&lt;-</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"tea first"</span>)<span class="sc">$</span>rating</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>milk_ratings <span class="ot">&lt;-</span> <span class="fu">filter</span>(tea_data, condition <span class="sc">==</span> <span class="st">"milk first"</span>)<span class="sc">$</span>rating</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>n_tea <span class="ot">&lt;-</span> <span class="fu">length</span>(tea_ratings)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>n_milk <span class="ot">&lt;-</span> <span class="fu">length</span>(milk_ratings)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>sd_tea <span class="ot">&lt;-</span> <span class="fu">sd</span>(tea_ratings)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>sd_milk <span class="ot">&lt;-</span> <span class="fu">sd</span>(milk_ratings)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>tea_sd_pooled <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(((n_tea <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> sd_tea <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> (n_milk <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> sd_milk <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                        (n_tea <span class="sc">+</span> n_milk <span class="sc">-</span> <span class="dv">2</span>))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>tea_se <span class="ot">&lt;-</span> tea_sd_pooled <span class="sc">*</span> <span class="fu">sqrt</span>((<span class="dv">1</span> <span class="sc">/</span> n_tea) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">/</span> n_milk))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have the standard error, we can get the estimated difference between conditions and compute the confidence intervals by multiplying the standard error by 1.96.</p>
<div class="cell" data-hash="006-inference_cache/html/unnamed-chunk-13_c6e6492eeef1dd02955f3acbf01cfd79">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>delta_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(milk_ratings) <span class="sc">-</span> <span class="fu">mean</span>(tea_ratings)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>tea_ci_lower <span class="ot">&lt;-</span> delta_hat <span class="sc">-</span> tea_se <span class="sc">*</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>tea_ci_upper <span class="ot">&lt;-</span> delta_hat <span class="sc">+</span> tea_se <span class="sc">*</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>For visualization purposes, we can show the confidence intervals on individual estimates (left side of <a href="#fig-inference-condition-cis">Figure&nbsp;<span>6.11</span></a>). These tell us about the precision of our estimates of each quantity relative to the population estimate. But we’ve been talking primarily about the CI on the treatment effect <span class="math inline">\(\widehat{\beta}\)</span> (right side of <a href="#fig-inference-condition-cis">Figure&nbsp;<span>6.11</span></a>). This CI allows us to make an inference about whether or not it overlaps with zero – which is actually equivalent in this case to whether or not the <span class="math inline">\(t\)</span>-test is statistically significant.</p>
</section>
<section id="confidence-in-confidence-intervals" class="level3 page-columns page-full" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="confidence-in-confidence-intervals"><span class="header-section-number">6.5.2</span> Confidence in confidence intervals?</h3>
<p>Confidence intervals are often misinterpreted by students and researchers alike <span class="citation" data-cites="hoekstra2014">(<a href="#ref-hoekstra2014" role="doc-biblioref">Hoekstra et al. 2014</a>)</span>. Imagine a researcher conducts an experiment and reports that “the 95% confidence interval for the mean ranges from 0.1 to 0.4.” All of the statements in <a href="#tbl-inference-ci-false">Table&nbsp;<span>6.4</span></a>, though tempting to make about this situation, are <em>technically false</em>.</p>
<div id="tbl-inference-ci-false" class="anchored">
<table class="table">
<caption>Table&nbsp;6.4: Confidence interval misconceptions for a confidence interval [0.1,0.4]. Adapted from <span class="citation" data-cites="hoekstra2014">Hoekstra et al. (<a href="#ref-hoekstra2014" role="doc-biblioref">2014</a>)</span>.</caption>
<colgroup>
<col style="width: 2%">
<col style="width: 97%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Misconception</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>“The probability that the true mean is greater than 0 is at least 95%.”,</td>
</tr>
<tr class="even">
<td>2</td>
<td>“The probability that the true mean equals 0 is smaller than 5%.”,</td>
</tr>
<tr class="odd">
<td>3</td>
<td>“The ‘null hypothesis’ that the true mean equals 0 is likely to be incorrect.”,</td>
</tr>
<tr class="even">
<td>4</td>
<td>“There is a 95% probability that the true mean lies between 0.1 and 0.4.”,</td>
</tr>
<tr class="odd">
<td>5</td>
<td>“We can be 95% confident that the true mean lies between 0.1 and 0.4.”,</td>
</tr>
<tr class="even">
<td>6</td>
<td>“If we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.4.”</td>
</tr>
</tbody>
</table>
</div>
<div class="page-columns page-full"><p>The problem with all of these statements is that, in the frequentist framework, there is only one true value of the population parameter, and the variability captured in confidence intervals is about the <em>samples</em>, not the parameter itself.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> For this reason, we can’t make any statements about the probability of the value of the parameter or of our confidence in specific numbers. To reiterate, what we <em>can</em> say is: if we were to repeat the procedure of conducting the experiment and calculating a confidence interval many times, in the long run, 95% of those confidence intervals would contain the true parameter.</p><div class="no-row-height column-margin column-container"><li id="fn25"><p><sup>25</sup>&nbsp;In contrast, Bayesians think of parameters themselves as variable rather than fixed.</p></li></div></div>
<p>The Bayesian analog to a confidence interval is a <strong>credible interval</strong>. Recall that for Bayesians, parameters themselves are considered probabilistic (i.e., subject to random variation), not fixed. A 95% credible interval for an estimate, <span class="math inline">\(\widehat{\beta}\)</span>, represents a range of possible values for <span class="math inline">\(\beta\)</span> such that there is a 95% probability that <span class="math inline">\(\beta\)</span> falls inside the interval. Because we are now wearing our Bayesian hats, we are “allowed” to talk about <span class="math inline">\(\beta\)</span> as if it were probabilistic rather than fixed. In practice, credible intervals are constructed by finding the posterior distribution of <span class="math inline">\(\beta\)</span>, as in <a href="005-estimation.html"><span>Chapter&nbsp;5</span></a>, and then taking the middle 95%, for example.</p>
<div class="page-columns page-full"><p>Credible intervals are nice because they don’t give rise to many of the inference fallacies surrounding confidence intervals. They actually <em>do</em> represent our beliefs about where <span class="math inline">\(\beta\)</span> is likely to be, for example. Despite the technical differences between credible intervals and confidence intervals, in practice – with larger sample sizes and weaker priors – they turn out to be quite similar to one another in many cases.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn26"><p><sup>26</sup>&nbsp;They can diverge sharply in cases with less data or stronger priors <span class="citation" data-cites="morey2016">(<a href="#ref-morey2016" role="doc-biblioref">Morey et al. 2016</a>)</span>, but in our experience this is relatively rare.</p></li></div></div>
</section>
</section>
<section id="chapter-summary-inference" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="chapter-summary-inference"><span class="header-section-number">6.6</span> Chapter summary: Inference</h2>
<p>Inference tools help you move from characteristics of the sample to characteristics of the population. This move is a critical part of generalization from research data. But we hope we’ve convinced you that inference doesn’t have to mean making a binary decision about the presence or absence of an effect. A strategy that seeks to estimate an effect and its associated precision is often much more helpful as a building block for theory. As we move towards estimating causal effects in more complex experimental designs, the process will require more sophisticated models. Towards that goal, the next chapter provides some guidance for how to build such models.</p>
<div class="callout callout-style-default callout-note callout-titled" title="discussion questions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
discussion questions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Can you write the definition of a <span class="math inline">\(p\)</span>-value and a Bayes Factor without looking them up? Try this out – what parts of the definitions did you get wrong?</p></li>
<li><p>Take three of Goodman’s (2008) “dirty dozen” in <a href="#tbl-dirty-dozen">Table&nbsp;<span>6.3</span></a>) and write a description of why each is a misconception. (These can be checked against the original article, which gives a nice discussion of each.</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="readings">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p>Many of the concepts described here are illustrated beautifully via interactive visualizations. We recommend <a href="https://seeing-theory.brown.edu/" class="uri">https://seeing-theory.brown.edu/</a> for a broad overview of statistical concepts and <a href="https://rpsychologist.com/viz" class="uri">https://rpsychologist.com/viz</a> for a number of interactives that specifically illustrate concepts discussed in this chapter and the previous one, including <span class="math inline">\(p\)</span>-values, effect sizes, maximum likelihood estimation, confidence intervals, and Bayesian inference.</p></li>
<li><p>A fun, polemical critique of NHST: Cohen, J. (1994). The earth is round (p &lt; .05). American Psychologist, 49, 997–1003. <a href="https://doi.org/10.1037/0003-066X.49.12.997" class="uri">https://doi.org/10.1037/0003-066X.49.12.997</a>.</p></li>
<li><p>A nice introduction to Bayesian data analysis: Kruschke, J. K., &amp; Liddell, T. M. (2018). Bayesian data analysis for newcomers. Psychonomic bulletin &amp; review, 25(1), 155-177. <a href="https://doi.org/10.3758/s13423-017-1272-1" class="uri">https://doi.org/10.3758/s13423-017-1272-1</a>.</p></li>
</ul>
</div>
</div>
</div>
<!-- \refs -->


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-aczel2018" class="csl-entry" role="listitem">
Aczel, Balazs, Bence Palfi, Aba Szollosi, Marton Kovacs, Barnabas Szaszi, Peter Szecsi, Mark Zrubka, Quentin F Gronau, Don van den Bergh, and Eric-Jan Wagenmakers. 2018. <span>“Quantifying Support for the Null Hypothesis in Psychology: An Empirical Investigation.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (3): 357–66.
</div>
<div id="ref-bem2011" class="csl-entry" role="listitem">
Bem, Daryl J. 2011. <span>“Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect.”</span> <em>Journal of Personality and Social Psychology</em> 100 (3): 407.
</div>
<div id="ref-benjamin2018" class="csl-entry" role="listitem">
Benjamin, Daniel J, James O Berger, Magnus Johannesson, Brian A Nosek, E-J Wagenmakers, Richard Berk, Kenneth A Bollen, et al. 2018. <span>“Redefine Statistical Significance.”</span> <em>Nature Human Behaviour</em> 2 (1): 6–10.
</div>
<div id="ref-cohen1990" class="csl-entry" role="listitem">
Cohen, Jacob. 1990. <span>“Things i Have Learned (so Far).”</span> <em>American Psychologist</em> 45: 1304–12.
</div>
<div id="ref-cohen1994" class="csl-entry" role="listitem">
———. 1994. <span>“The Earth Is Round (p&lt;. 05).”</span> <em>American Psychologist</em> 49 (12): 997.
</div>
<div id="ref-cumming2014" class="csl-entry" role="listitem">
Cumming, Geoff. 2014. <span>“The New Statistics: Why and How.”</span> <em>Psychol. Sci.</em> 25 (1): 7–29.
</div>
<div id="ref-fisher1949" class="csl-entry" role="listitem">
Fisher, Ronald A. 1949. <em>The Design of Experiments</em>. 5th ed. Oliver &amp; Boyd.
</div>
<div id="ref-gelman1995" class="csl-entry" role="listitem">
Gelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 1995. <em>Bayesian Data Analysis</em>. Chapman; Hall/CRC.
</div>
<div id="ref-gelman2006b" class="csl-entry" role="listitem">
Gelman, Andrew, and Jennifer Hill. 2006. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge university press.
</div>
<div id="ref-gelman2020" class="csl-entry" role="listitem">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-gigerenzer1989" class="csl-entry" role="listitem">
Gigerenzer, Gerd. 1989. <em>The Empire of Chance: How Probability Changed Science and Everyday Life</em>. 12. Cambridge University Press.
</div>
<div id="ref-probmods2" class="csl-entry" role="listitem">
Goodman, Noah D, Joshua B. Tenenbaum, and The ProbMods Contributors. 2016. <span>“<span class="nocase">Probabilistic Models of Cognition</span>.”</span> <a href="https://probmods.org/" class="uri">https://probmods.org/</a>.
</div>
<div id="ref-goodman1999" class="csl-entry" role="listitem">
Goodman, Steven N. 1999. <span>“Toward Evidence-Based Medical Statistics. 2: The Bayes Factor.”</span> <em>Annals of Internal Medicine</em> 130 (12): 1005–13.
</div>
<div id="ref-goodman2008" class="csl-entry" role="listitem">
———. 2008. <span>“A Dirty Dozen: Twelve p-Value Misconceptions.”</span> In <em>Seminars in Hematology</em>, 45:135–40. 3. Elsevier.
</div>
<div id="ref-gopnik2012" class="csl-entry" role="listitem">
Gopnik, Alison. 2012. <span>“Scientific Thinking in Young Children: Theoretical Advances, Empirical Research, and Policy Implications.”</span> <em>Science</em> 337 (6102): 1623–27.
</div>
<div id="ref-hoekstra2014" class="csl-entry" role="listitem">
Hoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan Wagenmakers. 2014. <span>“Robust Misinterpretation of Confidence Intervals.”</span> <em>Psychonomic Bulletin &amp; Review</em> 21 (5): 1157–64.
</div>
<div id="ref-jeffreys1998" class="csl-entry" role="listitem">
Jeffreys, Harold. 1961. <em>The Theory of Probability</em>. 3rd ed. OUP Oxford.
</div>
<div id="ref-kahneman1979" class="csl-entry" role="listitem">
Kahneman, Daniel, and Amos Tversky. 1979. <span>“Prospect Theory: An Analysis of Decision Under Risk.”</span> <em>Econometrica</em> 47 (2): 363–91.
</div>
<div id="ref-kruschke2014" class="csl-entry" role="listitem">
Kruschke, John K. 2014. <em>Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan</em>. Academic Press.
</div>
<div id="ref-kruschke2018" class="csl-entry" role="listitem">
Kruschke, John K, and Torrin M Liddell. 2018. <span>“The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.”</span> <em>Psychon. Bull. Rev.</em> 25 (1): 178–206.
</div>
<div id="ref-mcelreath2018" class="csl-entry" role="listitem">
McElreath, Richard. 2018. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. Chapman; Hall/CRC.
</div>
<div id="ref-morey2016" class="csl-entry" role="listitem">
Morey, Richard D, Rink Hoekstra, Jeffrey N Rouder, Michael D Lee, and Eric-Jan Wagenmakers. 2016. <span>“The Fallacy of Placing Confidence in Confidence Intervals.”</span> <em>Psychonomic Bulletin &amp; Review</em> 23 (1): 103–23.
</div>
<div id="ref-morey2011" class="csl-entry" role="listitem">
Morey, Richard D, and Jeffrey N Rouder. 2011. <span>“Bayes Factor Approaches for Testing Interval Null Hypotheses.”</span> <em>Psychological Methods</em> 16 (4): 406.
</div>
<div id="ref-pratt1995" class="csl-entry" role="listitem">
Pratt, John Winsor, Howard Raiffa, Robert Schlaifer, et al. 1995. <em>Introduction to Statistical Decision Theory</em>. MIT press.
</div>
<div id="ref-simonsohn2014" class="csl-entry" role="listitem">
Simonsohn, Uri. 2014. <span>“Posterior-Hacking: Selective Reporting Invalidates Bayesian Results Also.”</span> https://doi.org/<a href="https://dx.doi.org/10.2139/ssrn.2374040">https://dx.doi.org/10.2139/ssrn.2374040</a>.
</div>
<div id="ref-strathern1997" class="csl-entry" role="listitem">
Strathern, Marilyn. 1997. <span>“<span>‘Improving Ratings’</span>: Audit in the British University System.”</span> <em>European Review</em> 5 (3): 305–21.
</div>
<div id="ref-tenenbaum2011" class="csl-entry" role="listitem">
Tenenbaum, Joshua B, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. 2011. <span>“How to Grow a Mind: Statistics, Structure, and Abstraction.”</span> <em>Science</em> 331 (6022): 1279–85.
</div>
<div id="ref-wagenmakers2011" class="csl-entry" role="listitem">
Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, and Han LJ Van Der Maas. 2011. <span>“Why Psychologists Must Change the Way They Analyze Their Data: The Case of Psi: Comment on Bem (2011).”</span> <em>Journal of Personality and Social Psychology</em> 100 (3): 426–32. https://doi.org/<a href="https://doi.org/10.1037/a0022790">https://doi.org/10.1037/a0022790</a>.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./005-estimation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Estimation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./007-models.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>