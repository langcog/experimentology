<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Experimentology - 12&nbsp; Data collection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./013-management.html" rel="next">
<link href="./011-prereg.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-659MTW4XZ4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-659MTW4XZ4', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./011-prereg.html">Execution</a></li><li class="breadcrumb-item"><a href="./012-collection.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Experimentology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/langcog/experimentology" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Experimentology.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-theories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Theories</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-replication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Planning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008-measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Measurement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010-sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Execution</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Preregistration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012-collection.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013-management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Project management</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reporting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014-writing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Writing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100-instructors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Instructor’s guide</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./101-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">GitHub</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./102-rmarkdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">R Markdown</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./103-tidyverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tidyverse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./104-ggplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">ggplot</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#informed-consent-and-debriefing" id="toc-informed-consent-and-debriefing" class="nav-link active" data-scroll-target="#informed-consent-and-debriefing"><span class="header-section-number">12.1</span> Informed consent and debriefing</a>
  <ul class="collapse">
  <li><a href="#getting-consent" id="toc-getting-consent" class="nav-link" data-scroll-target="#getting-consent"><span class="header-section-number">12.1.1</span> Getting consent</a></li>
  <li><a href="#prerequisites-of-consent" id="toc-prerequisites-of-consent" class="nav-link" data-scroll-target="#prerequisites-of-consent"><span class="header-section-number">12.1.2</span> Prerequisites of consent</a></li>
  <li><a href="#debriefing-participants" id="toc-debriefing-participants" class="nav-link" data-scroll-target="#debriefing-participants"><span class="header-section-number">12.1.3</span> Debriefing participants</a></li>
  <li><a href="#special-considerations-for-vulnerable-populations" id="toc-special-considerations-for-vulnerable-populations" class="nav-link" data-scroll-target="#special-considerations-for-vulnerable-populations"><span class="header-section-number">12.1.4</span> Special considerations for vulnerable populations</a></li>
  </ul></li>
  <li><a href="#designing-the-research-experience" id="toc-designing-the-research-experience" class="nav-link" data-scroll-target="#designing-the-research-experience"><span class="header-section-number">12.2</span> Designing the “research experience”</a>
  <ul class="collapse">
  <li><a href="#ensuring-good-experiences-for-in-lab-participants" id="toc-ensuring-good-experiences-for-in-lab-participants" class="nav-link" data-scroll-target="#ensuring-good-experiences-for-in-lab-participants"><span class="header-section-number">12.2.1</span> Ensuring good experiences for in-lab participants</a></li>
  <li><a href="#ensuring-good-experiences-for-online-participants" id="toc-ensuring-good-experiences-for-online-participants" class="nav-link" data-scroll-target="#ensuring-good-experiences-for-online-participants"><span class="header-section-number">12.2.2</span> Ensuring good experiences for online participants</a></li>
  <li><a href="#when-to-collect-data-online" id="toc-when-to-collect-data-online" class="nav-link" data-scroll-target="#when-to-collect-data-online"><span class="header-section-number">12.2.3</span> When to collect data online?</a></li>
  </ul></li>
  <li><a href="#ensuring-high-quality-data" id="toc-ensuring-high-quality-data" class="nav-link" data-scroll-target="#ensuring-high-quality-data"><span class="header-section-number">12.3</span> Ensuring high quality data</a>
  <ul class="collapse">
  <li><a href="#conduct-effective-pilot-studies" id="toc-conduct-effective-pilot-studies" class="nav-link" data-scroll-target="#conduct-effective-pilot-studies"><span class="header-section-number">12.3.1</span> Conduct effective pilot studies</a></li>
  <li><a href="#measure-participant-compliance" id="toc-measure-participant-compliance" class="nav-link" data-scroll-target="#measure-participant-compliance"><span class="header-section-number">12.3.2</span> Measure participant compliance</a></li>
  <li><a href="#keep-consistent-data-collection-records" id="toc-keep-consistent-data-collection-records" class="nav-link" data-scroll-target="#keep-consistent-data-collection-records"><span class="header-section-number">12.3.3</span> Keep consistent data collection records</a></li>
  </ul></li>
  <li><a href="#chapter-summary-data-collection" id="toc-chapter-summary-data-collection" class="nav-link" data-scroll-target="#chapter-summary-data-collection"><span class="header-section-number">12.4</span> Chapter summary: Data collection</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/langcog/experimentology/blob/main/012-collection.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/langcog/experimentology/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-collection" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled" title="learning goals">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
learning goals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Outline key features of informed consent and participant debriefing</li>
<li>Identify additional protections necessary for working with vulnerable populations</li>
<li>Review best practices for online and in-person data collection</li>
<li>Implement data integrity checks, manipulation checks, and pilot testing</li>
</ul>
</div>
</div>
</div>
<div class="page-columns page-full"><p>You have selected your measure and manipulation and planned your sample. Your preregistration is set. Now it’s time to think about the nuts and bolts of collecting data. Though the details may vary between contexts, this chapter will describe some general best practices for data collection.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We organize our discussion of these practices around two perspectives: the participant and the researcher.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;The metaphor of “collection” implies to some researchers that the data exist independent of the researcher’s own perspective and actions, so they reject it in favor of the term “data generation.” Unfortunately, this alternative label doesn’t distinguish generating data via interactions with participants on the one hand and generating data from scratch via statistical simulations on the other. We worry that “data generation” sounds too much like the kinds of fraudulent data generation that we talked about in <a href="004-ethics.html"><span>Chapter&nbsp;4</span></a>, so we have opted to keep the more conventional “data collection” label.</p></li></div></div>
<p>The first section takes the perspective of a participant. We begin by reviewing the importance of informed consent. A key principle of running experiments with human participants is that we respect their autonomy, which includes their right to understand the study and choose whether to take part. When we neglect the impact of our research on the people we study, we not only violate regulations governing research, we also create distrust that undermines the moral basis of scientific research.</p>
<p>In the second section, we begin to shift perspectives, discussing the choice of online vs.&nbsp;in-person data collection and some of the advantages of online data collection for <em>transparency</em>. We consider how to optimize the experimental experience for participants in both settings. We then end by taking the experimenter’s perspective more fully, discussing how we can maximize data quality (contributing to <em>measurement precision</em>) using pilot testing, manipulation checks, and attention checks, while still being cognizant of both changes to the participant’s experience and the integrity of statistical inferences (both contributing to <em>bias reduction</em>).</p>
<div class="callout callout-style-default callout-note callout-titled" title="case study">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
case study
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<section id="the-rise-of-online-data-collection" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="the-rise-of-online-data-collection">The rise of online data collection</h2>
<p>Since the rise of experimental psychology laboratories in university settings during the period after World War 2 <span class="citation" data-cites="benjamin2000">(<a href="#ref-benjamin2000" role="doc-biblioref">Benjamin 2000</a>)</span>, experiments have typically been conducted by recruiting participants from what has been referred to as the “subject pool.” This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses <span class="citation" data-cites="sieber1989">(<a href="#ref-sieber1989" role="doc-biblioref">Sieber and Saks 1989</a>)</span> who are required to complete a certain number of experiments as part of their course work. <!-- ^[At various times, students have raised ethical concerns about these requirements, pointing out they are coercive in precisely the way that should be off limits for psychology experiments (see @sec-ethics). As a result, most programs now offer students an alternative option if they do not wish to participate.] --> The ready availability of this convenient population inevitably led to the massive over-representation of undergraduates in published psychology research, undermining its generalizability <span class="citation" data-cites="sears1986 henrich2010">(<a href="#ref-sears1986" role="doc-biblioref">Sears 1986</a>; <a href="#ref-henrich2010" role="doc-biblioref">Henrich, Heine, and Norenzayan 2010</a>)</span>.</p>
<p>Yet over the last couple of decades, there has been a revolution in data collection. Instead of focusing on university undergraduates, increasingly, researchers recruit individuals from crowdsourcing websites like Amazon Mechanical Turk (AMT) and Prolific Academic. Crowdsourcing services were originally designed to recruit and pay workers for ad-hoc business tasks like retyping receipts, but they have also become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments <span class="citation" data-cites="litman2017">(<a href="#ref-litman2017" role="doc-biblioref">Litman, Robinson, and Abberbock 2017</a>)</span>. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates) and this proportion is likely continuing to grow <span class="citation" data-cites="anderson2019">(<a href="#ref-anderson2019" role="doc-biblioref">Anderson et al. 2019</a>)</span>.</p>
<p>Initially, many researchers worried that crowdsourced data from online convenience samples would lead to a decrease in data quality. However, several studies suggest that data quality from online convenience samples is typically comparable to in-lab convenience samples <span class="citation" data-cites="mason2012 buhrmester2016">(<a href="#ref-mason2012" role="doc-biblioref">Mason and Suri 2012</a>; <a href="#ref-buhrmester2016" role="doc-biblioref">Buhrmester, Kwang, and Gosling 2016</a>)</span>. In one particularly compelling demonstration, a set of online experiments were used to replicate a group of classic phenomena in cognitive psychology, with clear successes on every experiment except those requiring sub-50 millisecond stimulus presentation <span class="citation" data-cites="crump2013">(<a href="#ref-crump2013" role="doc-biblioref">Crump, McDonnell, and Gureckis 2013</a>)</span>. Further, as we discuss below, researchers have developed a suite of tools to ensure that online participants understand and comply with the instructions in complex experimental tasks.</p>
<p>Since these initial successes, however, attention has moved away from the validity of online experiments to the ethical challenges of engaging with crowdworkers. In 2020, nearly 130,000 people completed MTurk studies <span class="citation" data-cites="moss2020">(<a href="#ref-moss2020" role="doc-biblioref">Moss et al. 2020</a>)</span>. Of those, an estimated 70% identified as White, 56% identified as women, and 48% had an annual household income below $50,000. A sampling of crowd work determined that the average wage earned was just $2.00 per hour, and less than 5% of workers were paid at least the federal minimum wage <span class="citation" data-cites="hara2018">(<a href="#ref-hara2018" role="doc-biblioref">Hara et al. 2018</a>)</span>. Further, many experimenters routinely withheld payment from workers based on their performance in experiments. These practices clearly violate ethical guidelines for research with human participants, but are often overlooked by institutional review boards who may be unfamiliar with online recruitment platforms or consider that platforms are offering a “service” rather than simply being alternative routes for paying individuals.</p>
<p>With greater attention to the conditions of workers <span class="citation" data-cites="salehi2015">(e.g., <a href="#ref-salehi2015" role="doc-biblioref">Salehi et al. 2015</a>)</span>, best practices for online research have progressed considerably. As we describe below, working with online populations requires attention to both standard ethical issues of consent and compensation, as well as new issues around the “user experience” of participating in research. The availability of online convenience samples can be transformative for the pace of research, for example by enabling large studies to be run in a single day rather than over many months. But online participants are vulnerable in different ways than university convenience samples, and we must take care to ensure that research online is conducted ethically.</p>
</section>
</div>
</div>
<section id="informed-consent-and-debriefing" class="level2 page-columns page-full" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="informed-consent-and-debriefing"><span class="header-section-number">12.1</span> Informed consent and debriefing</h2>
<p>As we discussed in <a href="004-ethics.html"><span>Chapter&nbsp;4</span></a>, experimenters must respect the autonomy of their participants: they must be informed about the risks and benefits of participation before they agree to participate. Researchers must also discuss and contextualize the research by debriefing participants after they have completed the study. Here we look at the nuts and bolts of each of these processes, ending with guidance on the special protections that are required to protect the autonomy of especially vulnerable populations.</p>
<section id="getting-consent" class="level3 page-columns page-full" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="getting-consent"><span class="header-section-number">12.1.1</span> Getting consent</h3>
<p>Experimental participants must give consent. In most regulatory frameworks, there are clear guidelines about what the process of giving consent should look like. Typically participants are expected to read and sign a <strong>consent form</strong>: a document that explains the goals of the research and its procedures, describes potential risks and benefits, and asks for participants’ explicit consent to participate voluntarily. <a href="#tbl-consent-requirements">Table&nbsp;<span>12.1</span></a> gives the full list of consent form requirements from the US Office for Human Research Protections, and <a href="#fig-collection-annotated-consent">Figure&nbsp;<span>12.1</span></a> shows how these individual requirements are reflected in a real consent form used in our research.</p>
<div id="tbl-consent-requirements" class="anchored">
<table class="table">
<caption>Table&nbsp;12.1: US Office of Human Research Protections requirements for a consent form (edited for length).</caption>
<colgroup>
<col style="width: 2%">
<col style="width: 97%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Requirement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>A statement that the study involves research</td>
</tr>
<tr class="even">
<td>2</td>
<td>An explanation of the purposes of the research</td>
</tr>
<tr class="odd">
<td>3</td>
<td>The expected duration of the subject’s participation</td>
</tr>
<tr class="even">
<td>4</td>
<td>A description of the procedures to be followed</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Identification of any procedures which are experimental</td>
</tr>
<tr class="even">
<td>6</td>
<td>A description of any reasonably foreseeable risks or discomforts to the subject</td>
</tr>
<tr class="odd">
<td>7</td>
<td>A description of any benefits to the subject or to others which may reasonably be expected from the research</td>
</tr>
<tr class="even">
<td>8</td>
<td>A disclosure of appropriate alternative procedures or courses of treatment, if any, that might be advantageous to the subject</td>
</tr>
<tr class="odd">
<td>9</td>
<td>A statement describing the extent, if any, to which confidentiality of records identifying the subject will be maintained</td>
</tr>
<tr class="even">
<td>10</td>
<td>For research involving more than minimal risk, an explanation as to whether any compensation or medical treatments are available if injury occurs</td>
</tr>
<tr class="odd">
<td>11</td>
<td>An explanation of whom to contact for answers to pertinent questions about the research and research subjects’ rights</td>
</tr>
<tr class="even">
<td>12</td>
<td>A statement that participation is voluntary, refusal to participate will involve no penalty, and that subject may discontinue participation at any time without penalty</td>
</tr>
</tbody>
</table>
</div>
<div id="fig-collection-annotated-consent" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/collection/annotated_consent.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;12.1: Consent form annotated to show how specific text fulfills the requirements in <a href="#tbl-consent-requirements">Table&nbsp;<span>12.1</span></a>. Categories 5, 8, and 10 were not required for this minimal risk psychology experiment.</figcaption>
</figure>
</div>
<p>These are just samples. Since ethics regulation is almost always managed at the institutional level, your local ethics board will often provide guidance on the specific information you should include in the consent form and they will almost always need to approve the form before you are allowed to begin recruiting participants.</p>
<div class="page-columns page-full"><p>When providing consent information, researchers should focus on what someone might think or feel as a result of participating in the study. Are there any physical or emotional risks associated? What should someone know about the study that may give them pause about agreeing to participate in the first place? Our advice is to center the <em>participant</em> in the consent process rather than the research question. Information about specific research goals can typically be provided during debriefing.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Some experimenters worry that informing participants about the study that they are about to participate in may influence their behavior in the study via so-called “demand characteristics”, discussed in <a href="009-design.html"><span>Chapter&nbsp;9</span></a>. But the goal of a consent form is not to explain the specific psychological construct being manipulated. Instead, a consent form typically focuses on the experience of being in the study (for example, that a participant would be asked to provide quick verbal responses to pictures). This sort of general explanation should not create demand characteristics.</p></li></div></div>
<p>If there are specific pieces of information that about study goals or procedures that <em>must</em> be withheld from participants during consent, <strong>deception</strong> of participants may be warranted. Deception can be approved by ethics boards as long as it poses little risk and is effectively addressed via more extensive debriefing. But an experimental protocol that includes deception will likely undergo greater scrutiny during ethics review, as it must be justified by a specific experimental need.</p>
<div class="page-columns page-full"><p>During the consent process, researchers should explain to participants what will be done with their data. Requirement 9 in <a href="#tbl-consent-requirements">Table&nbsp;<span>12.1</span></a>) merely asks for a statement about data confidentiality, but such a statement is a mere minimum. Some modern consent forms explicitly describe different uses of the data and ask for consent for each. For example, the form in <a href="#fig-collection-annotated-consent">Figure&nbsp;<span>12.1</span></a> asks permission for showing recordings as part of presentations.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Some ethics boards will ask for consent for sharing even anonymized data files. As we discuss in <a href="013-management.html"><span>Chapter&nbsp;13</span></a>, fully anonymized data can often be shared without explicit consent. You may still choose to ask participants’ permission, but this practice may lead to an awkward situation, for example, a dataset with heterogeneous sharing permissions such that most but not all data can be shared publicly.</p></li></div></div>
</section>
<section id="prerequisites-of-consent" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="prerequisites-of-consent"><span class="header-section-number">12.1.2</span> Prerequisites of consent</h3>
<p>In order to give consent, participants must have the cognitive capacity to make decisions (competence), understand what they are being asked to do (comprehension), and know that they have the right to withdraw consent at any time (voluntariness) <span class="citation" data-cites="kadam2017">(<a href="#ref-kadam2017" role="doc-biblioref">Kadam 2017</a>)</span>.</p>
<p>Typically, we assume competence for adult volunteers in our experiments, but if we are working with children or other vulnerable populations (see below), we may need to consider whether they are legally competent to provide consent. Participants who cannot consent on their own should still be informed about participation in an experiment and, if possible, you should still obtain their <strong>assent</strong> (informal agreement) to participate. When a person has no legal ability to consent, you must obtain consent from their legal guardian. But if they do not assent, you should also respect their decision not to participate – even if you previously obtained consent from their guardian.</p>
<p>The second prerequisite is comprehension. It is good practice to discuss consent forms verbally with participants, especially if the study is involved and takes place in person. If the study is online, ensure that participants know how to contact you if they have questions about the study. The consent form itself must be readable for a broad audience, meaning care should be taken to use accessible language and clear formatting. Consider giving participants a copy of the consent form in advance so they can read at their own pace, think of any questions they might have, and decide how to proceed without any chance of feeling coerced <span class="citation" data-cites="young1990">(<a href="#ref-young1990" role="doc-biblioref">Young, Hooker, and Freeberg 1990</a>)</span>.</p>
<p>Finally, participants must understand that their involvement is voluntary, meaning that they are under no obligation to be involved in a study and always have the right to withdraw at any time. Experimenters should not only state that participation is voluntary, they should also pay attention to other features of the study environment that might lead to <strong>structural coercion</strong> <span class="citation" data-cites="fisher2013">(<a href="#ref-fisher2013" role="doc-biblioref">Fisher 2013</a>)</span>. For example, high levels of compensation can make it difficult for lower-income participants to withdraw from research. Similarly, factors like race, gender, and social class can lead participants to feel discomfort around discontinuing a study. It is incumbent on experimenters to provide a comfortable study environment and to avoid such coercive factors wherever possible.</p>
</section>
<section id="debriefing-participants" class="level3 page-columns page-full" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="debriefing-participants"><span class="header-section-number">12.1.3</span> Debriefing participants</h3>
<p>Once a study is completed, researchers should always debrief participants. A debriefing is composed of sevearl parts: (1) gratitude, (2) discussion of goals, (3) explanation of deception (if relevant), and (4) questions and clarification <span class="citation" data-cites="allen2017">(<a href="#ref-allen2017" role="doc-biblioref">Allen 2017</a>)</span>. Together these serve to contextualize the experience for the participant and to mitigate any potential harms from the study.</p>
<ol type="1">
<li><p><strong>Gratitude.</strong> Thank participants for their contribution! Sometimes thanks is enough (for a short experiment), but many studies also include monetary compensation or course credit. Compensation should be commensurate with the amount of time and effort required for participation. Compensation structures vary widely from place to place; typically local ethics boards will have specific guidelines.</p></li>
<li><p><strong>Discussion of goals.</strong> Researchers should share the purpose of the research with participants in, aiming for a short and accessible statement that avoids technical jargon. Sharing goals is especially important when some aspect of the study appears evaluative – participants will often be interested in knowing how well they performed against their peers. For example, a parent whose child completed a word-recognition task may request information about their child’s performance. It can assuage parents’ worries to highlight that the goals of the study are about measuring a particular experimental effect, not about individual evaluation and ranking.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p></li>
<li><p><strong>Explanation of deception.</strong> Researchers must reveal any deception during debriefing, regardless of how minor the deception seems to the researcher. This component of the debriefing process can be thought of as “dehoaxing” because it is meant to illuminate any aspects of the study that were previously misleading or inaccurate <span class="citation" data-cites="holmes1976">(<a href="#ref-holmes1976" role="doc-biblioref">Holmes 1976</a>)</span>. The goal is both to reveal the true intent of the study and to alleviate any potential anxiety associated with the deception. Experimenters should make clear both where in the study the deception occurred and why the deception was necessary for the study’s success.</p></li>
<li><p><strong>Questions and clarification.</strong> Finally, researchers should answer any questions or address any concerns raised by participants. Many researchers use this opportunity to ask participants about their own ideas about the study goals. This practice not only illuminates aspects of the study design that may have been unclear to or hidden from participants, it also begins a discussion where both researchers and participants can communicate about this joint experience. This step is also helpful in identifying negative emotions or feelings resulting from the study <span class="citation" data-cites="allen2017">(<a href="#ref-allen2017" role="doc-biblioref">Allen 2017</a>)</span>. When participants do express negative emotions, researchers are responsible for sharing resources participants can use to help them.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;At the study’s conclusion, you might also consider sharing any findings with participants – many participants appreciate learning about research findings that they contributed to, even months or years after participation.</p></li><li id="fn5"><p><sup>5</sup>&nbsp;In the case that participants report substantial concerns or negative reactions to an experiment – during debriefing or otherwise – researchers will typically have an obligation to report these to their ethics board.</p></li></div></section>
<section id="special-considerations-for-vulnerable-populations" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="special-considerations-for-vulnerable-populations"><span class="header-section-number">12.1.4</span> Special considerations for vulnerable populations</h3>
<p>Regardless of who is participating in research, investigators have an obligation to protect the rights and well-being of all participants. Some populations are considered especially <strong>vulnerable</strong> because of their decreased agency – either in general or in the face of potentially coercive situations. Research with these populations receives additional regulatory oversight. In this section, we will consider several vulnerable populations.</p>
<p><strong>Children.</strong> Children are some of the most commonly used vulnerable populations in research because the study of development can contribute both to children’s welfare and to our understanding of the human mind. In the US, children under the age of 18 may only participate in research with written consent from a parent or guardian. Unless they are pre-verbal, children should additionally be asked for their assent. The risks associated with a research study focusing on children also must be no greater than minimal unless participants may receive some direct benefit from participating or participating in the study may improve a disorder or condition the participant was formally diagnosed with.</p>
<p><strong>People with disabilities.</strong> There are thousands of disabilities that affect cognition, development, motor ability, communication, and decision-making with varying degrees of interference, so it is first important to remember that considerations for this population will be just as diverse as its members. No laws preclude people with disabilities from participating in research. However, those with cognitive disabilities who are unable to make their own decisions may only participant with written consent from a legal guardian and with their individual assent (if applicable). Those retaining full cognitive capacity but who have other disabilities that make it challenging to participate normally in the study should receive appropriate assistance to access information about the study, including the risks and benefits of participation.</p>
<p><strong>Incarcerated populations.</strong> Nearly 2.1 million people are incarcerated in the United States alone <span class="citation" data-cites="gramlich2021">(<a href="#ref-gramlich2021" role="doc-biblioref">Gramlich 2021</a>)</span>. Due to early (and repugnant) use of prisoners as a convenience population that could not provide consent, the use of prisoners in research has been a key focus of protective efforts. The US Office for Human Research Protections (OHRP) supports their involvement in research under very limited circumstances – typically when the research specifically focuses on issues relevant to incarcerated populations <span class="citation" data-cites="ohrp2003">(<a href="#ref-ohrp2003" role="doc-biblioref"><span>“Prisoner Involvement in Research”</span> 2003</a>)</span>. When researchers propose to study incarcerated individuals, the local ethics board must reconfigure to include at least one active prisoner (or someone who can speak from a prisoner’s perspective) and ensure that less than half of the board has any affiliation to the prison system, public or private. Importantly, researchers must not suggest or promise that participation will have any bearing on an individual’s prison sentence or parole eligibility, and compensation must be otherwise commensurate with their contribution.</p>
<p><strong>Low-income populations.</strong> Participants with fewer resources may be more persuaded to participate by monetary incentives, creating a potentially coercive situation. Researchers should consult with their local ethics board to conform to local standards for non-coercive payment.</p>
<p><strong>Indigenous populations.</strong> There is a long and negative history of the involvement of indigenous populations in research without their consent. In the case that research requires the participation of indigenous individuals – because of potential benefits to their communities, rather than due to convenience – then community leadership must be involved to discuss the appropriateness of the research as well as how the consent process should be structured <span class="citation" data-cites="fitzpatrick2016">(<a href="#ref-fitzpatrick2016" role="doc-biblioref">Fitzpatrick et al. 2016</a>)</span>.</p>
<p><strong>Crowdworkers.</strong> Ethics boards do not usually consider crowdworkers on platforms like Amazon Mechanical Turk to be a specific vulnerable population, but many of the same concerns about diminished autonomy and greater need for protection still arise (see Depth Box below). Without platform or ethics board standards, it is up to individual experimenters to commit to fair pay, which should ideally match or exceed the applicable minimum wage (e.g., the US federal minimum wage). Further, in the context of reputation management systems like those of Amazon Mechanical Turk, participants can be penalized for withdrawing from an experiment – once they have their work “rejected” by an experimenter, it can be harder for them to find new jobs, causing serious long-term harm to their ability to earn on the platform.</p>
</section>
</section>
<section id="designing-the-research-experience" class="level2 page-columns page-full" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="designing-the-research-experience"><span class="header-section-number">12.2</span> Designing the “research experience”</h2>
<div class="page-columns page-full"><p>For the majority of psychology experiments, the biggest factor that governs whether a participant has a positive or negative experience of an experiment is not its risk profile, since for many psychology experiments the quantifiable risk to participants is minimal.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Instead, it is the participants’ experience. Did they feel welcome? Did they understand the instructions? Did the software work as designed? Was their compensation clearly described and promptly delivered? These aspects of “user experience” are critical both for ensuring that participants have a good experience in the study (an ethical imperative) and for gathering good data. An experiment that leaves participants unhappy typically doesn’t satisfy either the ethical or the scientific goals of research. In this section, we’ll discuss how to optimize the research experience for both in-person and online experiments, as well as providing some guidance on how to decide between these two administration contexts.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;There are of course exceptions, including research with more sensitive content. Even in these cases, however, attention to the participant’s experience can be important for ensuring good scientific outcomes.</p></li></div></div>
<section id="ensuring-good-experiences-for-in-lab-participants" class="level3 page-columns page-full" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="ensuring-good-experiences-for-in-lab-participants"><span class="header-section-number">12.2.1</span> Ensuring good experiences for in-lab participants</h3>
<div class="page-columns page-full"><p>A participant’s experience begins even before they arrive at the lab. Negative experiences with the recruitment process (e.g., unclear consent forms, poor communication, complicated scheduling) or transit to the lab (e.g., difficulty navigating or finding parking) can lead to frustrated participants with a negative view of your research. Anything you can do to make these experiences smoother and more predicable – prompt communication, well-tested directions, reserved parking slots, etc. – will make your participants happier and increase the quality of your data.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;For some reason, the Stanford Psychology Department building is notoriously difficult to navigate. This seemingly minor issue has resulted in a substantial number of late, frustrated, and flustered participants over the years.</p></li></div></div>
<p>Once a participant enters the lab, every aspect of the interaction with the experimenter can have an effect on their measured behavior <span class="citation" data-cites="gass2018">(<a href="#ref-gass2018" role="doc-biblioref">Gass and Seiter 2018</a>)</span>! For example, a likable and authoritative experimenter who clearly describes the benefits of participation is following general principles for persuasion <span class="citation" data-cites="cialdini2004">(<a href="#ref-cialdini2004" role="doc-biblioref">Cialdini and Goldstein 2004</a>)</span>. This interaction should lead to better compliance with experimental instructions, and hence better data, than an interaction with an unclear or indifferent experimenter.</p>
<p>Any interaction with participants must be scripted and standardized so that all participants have as similar an experience as possible. A lack of standardization can result in differential treatment for participants with different characteristics, which could result in data with greater variability or even specific sociodemographic biases. An experimenter that was kinder and more welcoming to one demographic group would be acting unethically, and they also might find a very different result than they intended.</p>
<div class="page-columns page-full"><p>Even more importantly, experimenters who interact with participants should ideally be unaware of the experimental condition each participant is assigned to. This practice is often called “blinding” or “masking”. Otherwise it is easy for experimenter knowledge to result in small differences in interaction across conditions, which in turn can influence participants’ behavior, resulting in experimenter expectancy effects (see <a href="009-design.html"><span>Chapter&nbsp;9</span></a>)! Even if the experimenter must know a participant’s condition assignment – as is sometimes the case – this information should be revealed at the last possible moment to avoid contamination of other aspects of the experimental session.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;In some experiments, an experimenter delivers a manipulation and hence it cannot be masked from them. In such cases, it’s common to have two experimenters such that one delivers the manipulation and another (masked to condition) collects the measurements. This situation often comes up with studies of infancy, since stimuli are often delivered via an in-person puppet show; at a minimum, behavior should be coded by someone other than the puppeteer.</p></li></div></div>
</section>
<section id="ensuring-good-experiences-for-online-participants" class="level3 page-columns page-full" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored"><span class="header-section-number">12.2.2</span> Ensuring good experiences for online participants</h3>
<p>The design challenges for online experiments are very different than for in-lab experiments. As the experimental procedure is delivered through a web browser, experimenter variability and potential expectancy effects are almost completely eliminated. On the other hand, some online participants do many hours of online tasks a day and many are multi-tasking in other windows or on other devices. It can be much harder to induce interest and engagement in your research when your manipulation is one of dozens the participant has experienced that day and when your interactions are mediated by a small window on a computer screen.</p>
<div class="page-columns page-full"><p>When creating an online experimental experience, we consider four issues: (1) design, (2) communication, (3) payment policies, and (4) effective consent and debriefing:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;For extensive further guidance on this topic, see <span class="citation" data-cites="litman2020">Litman and Robinson (<a href="#ref-litman2020" role="doc-biblioref">2020</a>)</span>.</p></li></div></div>
<p><strong>Basic UX design</strong>. Good experiment design online is a subset of good web <strong>user experience</strong> (UX) design more generally. If your web experiment is unpleasant to interact with, participants will likely become confused and frustrated. They will either drop out or provide data that are lower quality. A good interface should be clean and well-tested and should offer clear places where the participant must type or click to interact. If a participant presses a key at an appropriate time, the experiment should offer a response – otherwise the participant will likely press it again. If the participant is uncertain how many trials are left, they may be more likely to drop out of the experiment so it is also helpful to provide an indication of their progress. And if they are performing a speeded paradigm, they should receive practice trials to ensure that they understand the experiment prior to beginning the critical blocks of trials.</p>
<p><strong>Communication</strong>. Many online studies involve almost no direct contact with participants. When participants do communicate with you it is very important to be responsive and polite (as it is with in-lab participants, of course). Unlike the typical undergraduate participant, the work that a crowdworker is doing for your study may be part of how they earn their livelihood, and a small issue in the study for you may feel very important for them. For that reason, rapid resolution of issues with studies – typically through appropriate compensation – is very important. Crowdworkers often track the reputation of specific labs and experimenters [sometimes through forums or specialized software; <span class="citation" data-cites="irani2013">Irani and Silberman (<a href="#ref-irani2013" role="doc-biblioref">2013</a>)</span>]. A quick and generous response to an issue will ensure that future crowdworkers do not avoid your studies.</p>
<p><strong>Payment policies</strong>. Unclear or punitive payment policies can have a major impact on crowdworkers. We strongly recommend <em>always</em> paying workers if they complete your experiment, regardless of result. This policy is comparable to standard payment policies for in-lab work. We assume good faith in our participants: if someone comes to the lab, they are paid for the experiment, even if it turns out that they did not perform correctly. The major counterargument to this policy is that some online marketplaces have a population of workers who are looking to cheat by being non-compliant with the experiment (e.g., entering gibberish or even using scripts or artificial intelligence tools to progress quickly through studies). Our recommendation is to address this issue through the thoughtful use of “check” trials (see below) – not through punitive non-payment. The easiest way for a participant to complete your experiment should be by complying with your instructions.</p>
<hr>
<div id="tbl-online-consent" class="anchored">
<table>
<caption>Table&nbsp;12.2: Sample online consent statement from our course.</caption>
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th>By answering the following questions, you are participating in a study being performed by cognitive scientists in the Stanford Department of Psychology. If you have questions about this research, please contact us at stanfordpsych251@gmail.com. You must be at least 18 years old to participate. Your participation in this research is voluntary. You may decline to answer any or all of the following questions. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
<hr>
<p><strong>Consent and debriefing</strong>. Because online studies are typically fully automated, participants do not have a chance to interact with researchers around consent and debriefing. Further, engagement with long consent forms may be minimal. In our work we have typically relied on short consent statements such as the one from our class that is shown in <a href="#tbl-online-consent">Table&nbsp;<span>12.2</span></a>. Similarly, debriefing often occurs through a set of pages that summarize all components of the debriefing process (participation gratitude, discussion of goals, explanation of deception if relevant, and questions and clarification). Because these interactions are so short, it is especially important to include contact information prominently so that participants can follow up.</p>
<div class="callout callout-style-default callout-note callout-titled" title="depth">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
depth
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="the-rise-of-online-data-collection-1" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="the-rise-of-online-data-collection-1">The rise of online data collection</h2>
<p>Since the rise of experimental psychology laboratories in university settings during the period after World War 2 <span class="citation" data-cites="benjamin2000">(<a href="#ref-benjamin2000" role="doc-biblioref">Benjamin 2000</a>)</span>, experiments have typically been conducted by recruiting participants from what has been referred to as the “subject pool.” This term denotes a group of people who can be recruited for experiments, typically students from introductory psychology courses <span class="citation" data-cites="sieber1989">(<a href="#ref-sieber1989" role="doc-biblioref">Sieber and Saks 1989</a>)</span> who are required to complete a certain number of experiments as part of their course work. <!-- At various times, students have raised ethical concerns about these requirements, pointing out they are coercive in precisely the way that should be off limits for psychology experiments (see @sec-ethics). As a result, most programs now offer students an alternative option if they do not wish to participate. --> The ready availability of this convenient population inevitably led to a vast over-representation of undergraduates in published psychology research, undermining its generalizability <span class="citation" data-cites="sears1986 henrich2010">(<a href="#ref-sears1986" role="doc-biblioref">Sears 1986</a>; <a href="#ref-henrich2010" role="doc-biblioref">Henrich, Heine, and Norenzayan 2010</a>)</span>.</p>
<p>Yet over the last couple of decades, there has been a revolution in data collection. Instead of focusing on university undergraduates, increasingly, researchers recruit individuals from crowdsourcing websites like Amazon Mechanical Turk (AMT) and Prolific Academic. Crowdsourcing services were originally designed to recruit and pay workers for ad-hoc business tasks such as retyping receipts, but they have become marketplaces to connect researchers with research participants who are willing to complete surveys and experimental tasks for small payments <span class="citation" data-cites="litman2017">(<a href="#ref-litman2017" role="doc-biblioref">Litman, Robinson, and Abberbock 2017</a>)</span>. As of 2015, more than a third of studies in top social and personality psychology journals were conducted on crowdsourcing platforms (another third were still conducted with college undergraduates); this proportion has likely continued to grow over the years since the last systematic surveys were done <span class="citation" data-cites="anderson2019">(<a href="#ref-anderson2019" role="doc-biblioref">Anderson et al. 2019</a>)</span>.</p>
<p>Initially, many researchers worried that crowdsourced data from online convenience samples would lead to a decrease in data quality. However, several studies suggest that data quality from online convenience samples is typically comparable to in-lab convenience samples <span class="citation" data-cites="mason2012 buhrmester2016">(<a href="#ref-mason2012" role="doc-biblioref">Mason and Suri 2012</a>; <a href="#ref-buhrmester2016" role="doc-biblioref">Buhrmester, Kwang, and Gosling 2016</a>)</span>. In one particularly compelling demonstration, <span class="citation" data-cites="crump2013">Crump, McDonnell, and Gureckis (<a href="#ref-crump2013" role="doc-biblioref">2013</a>)</span> repeated a set of classic experiments in cognitive psychology using online participants, successfully replicating all except those requiring sub-50 millisecond stimulus presentation. Further, as we discuss below, researchers have developed a suite of tools to ensure that online participants understand and comply with the instructions in complex experimental tasks.</p>
<p>As the use of online data collection rises, it is important to engage with the ethical challenges of working with crowdworkers to collect psychological data. In 2020, nearly 130,000 people completed MTurk studies <span class="citation" data-cites="moss2020">(<a href="#ref-moss2020" role="doc-biblioref">Moss et al. 2020</a>)</span>. Of those, an estimated 70% identified as White, 56% identified as women, and 48% had an annual household income below $50,000. A sampling of crowd work determined that the average wage earned was just $2.00 per hour, and less than 5% of workers were paid at least the federal minimum wage <span class="citation" data-cites="hara2018">(<a href="#ref-hara2018" role="doc-biblioref">Hara et al. 2018</a>)</span>. Further, many experimenters routinely withheld payment from workers based on their performance in experiments. These practices – low compensation and base compensation only contingent on performance – clearly violate ethical guidelines for research with human participants, but are often overlooked by institutional review boards who may be unfamiliar with online recruitment platforms.</p>
<p>Working with online populations requires attention to both standard ethical issues of consent and compensation, as well as new issues around the “user experience” of participating in research <span class="citation" data-cites="salehi2015">(<a href="#ref-salehi2015" role="doc-biblioref">Salehi et al. 2015</a>)</span>. The availability of online convenience samples can be transformative for the pace of research, for example by enabling large studies to be run in a single day rather than over many months. But online participants are vulnerable in different ways than university convenience samples, and we must take care to ensure that research online is conducted ethically.</p>
</section>
</div>
</div>
</section>
<section id="when-to-collect-data-online" class="level3 page-columns page-full" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="when-to-collect-data-online"><span class="header-section-number">12.2.3</span> When to collect data online?</h3>
<p>Online data collection is increasingly ubiquitous in the behavioral sciences. Further, the web browser – alongside survey software like Qualtrics or packages like jsPsych <span class="citation" data-cites="de-leeuw2015">(<a href="#ref-de-leeuw2015" role="doc-biblioref">De Leeuw 2015</a>)</span> – can be a major aid to transparency in sharing experimental materials. Replication and reuse of experimental materials is vastly simpler if readers and reviewers can click on a link and share the same experience as a participant in your experiment. By and large, well-designed studies yield data that are as reliable as in-lab data [see Depth Box above; <span class="citation" data-cites="buhrmester2016">Buhrmester, Kwang, and Gosling (<a href="#ref-buhrmester2016" role="doc-biblioref">2016</a>)</span>;<span class="citation" data-cites="mason2012">Mason and Suri (<a href="#ref-mason2012" role="doc-biblioref">2012</a>)</span>;<span class="citation" data-cites="crump2013">Crump, McDonnell, and Gureckis (<a href="#ref-crump2013" role="doc-biblioref">2013</a>)</span>].</p>
<p>Still, online data collection is not right for every experiment. Studies that have substantial deception or that induce negative emotions may require an experimenter present to alleviate ethical concerns or provide detailed debriefing. Beyond ethical issues, we discuss four broader concerns to consider when deciding whether to conduct data collection online: (1) population availability, (2) the availability of particular measures, (3) the feasibility of particular manipulations, and (4) the length of experiments.</p>
<div class="page-columns page-full"><p><strong>Population</strong>. Not every target population can be tested online. Indeed, initially, convenience samples from Amazon Mechanical Turk were the only group easily available for online studies. More recently, new tools have emerged to allow pre-screening of crowd participants, including sites like Cloud Research and Prolific <span class="citation" data-cites="eyal2021 peer2021">(<a href="#ref-eyal2021" role="doc-biblioref">Eyal et al. 2021</a>; <a href="#ref-peer2021" role="doc-biblioref">Peer et al. 2021</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> And it may initially have seemed implausible that children could be recruited online, but during the COVID-19 pandemic a substantial amount of developmental data collection moved online, with many studies yielding comparable results to in-lab studies <span class="citation" data-cites="chuey2021">(e.g., <a href="#ref-chuey2021" role="doc-biblioref">Chuey et al. 2021</a>)</span>.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> Finally, new, non-US crowdsourcing platforms continue to grow in popularity, leading to greater global diversity in the available online populations.</p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;These tools still have significant weaknesses for accessing socio-demographically diverse populations within and outside the US, however – screening tools can remove participants, but if the underlying population does not contain many participants from a particular demographic, it can be hard to gather large enough samples. For an example of using crowdsourcing and social media sites to gather diverse participants, see <span class="citation" data-cites="demayo2021">DeMayo et al. (<a href="#ref-demayo2021" role="doc-biblioref">2021</a>)</span>.</p></li><li id="fn11"><p><sup>11</sup>&nbsp;Sites like LookIt (<a href="https://lookit.mit.edu" class="uri">https://lookit.mit.edu</a>) now offer sophisticated platforms for hosting studies for children and families <span class="citation" data-cites="scott2017">(<a href="#ref-scott2017" role="doc-biblioref">Scott and Schulz 2017</a>)</span>.</p></li></div></div>
<p><strong>Online measures</strong>. Not all measures are available online, though more and more are. Although online data collection was initially restricted to the use of survey measures – including ratings and text responses – measurement options have rapidly expanded. The widespread use of libraries like jsPsych <span class="citation" data-cites="de-leeuw2015">(<a href="#ref-de-leeuw2015" role="doc-biblioref">De Leeuw 2015</a>)</span> has meant that millisecond accuracy in capturing response times is now possible within web-browsers; thus, most reaction time tasks are quite feasible <span class="citation" data-cites="crump2013">(<a href="#ref-crump2013" role="doc-biblioref">Crump, McDonnell, and Gureckis 2013</a>)</span>. The capture of sound and video is possible with modern browser frameworks <span class="citation" data-cites="scott2017">(<a href="#ref-scott2017" role="doc-biblioref">Scott and Schulz 2017</a>)</span>. Further, even measures like mouse- and eye-tracking are beginning to become available <span class="citation" data-cites="maldonado2019 slim2021">(<a href="#ref-maldonado2019" role="doc-biblioref">Maldonado, Dunbar, and Chemla 2019</a>; <a href="#ref-slim2021" role="doc-biblioref">Slim and Hartsuiker 2021</a>)</span>. In general, almost any variable that can be measured in the lab without specialized apparatus can also be collected online. On the other hand, studies that measure a broader range of physiological variables (e.g., heart rate or skin conductance) or a larger range of physical behaviors (e.g., walking speed or pose) are still likely difficult to implement online.</p>
<div class="page-columns page-full"><p><strong>Online manipulations</strong>. Online experiments are limited to the set of manipulations that can be created within a browser window – but this restriction excludes many different manipulations that involve real-time social interactions with a human being.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> Synchronous chat sessions can be a useful substitute <span class="citation" data-cites="hawkins2020">(<a href="#ref-hawkins2020" role="doc-biblioref">Hawkins, Frank, and Goodman 2020</a>)</span>, but these focus the experiment on the content of what is said and exclude the broader set of non-verbal cues available to participants in a live interaction (e.g., gaze, race, appearance, accent, etc.). Creative experimenters can circumvent these limitations by using pictures, videos, and other methods. But more broadly, an experimenter interested in implementing a particular manipulation online should ask how compelling the online implementation is compared with an in-lab implementation. If the intention is to induce some psychological state – say stress, fear, or disgust – experimenters must trade off the greater ease of recruitment and larger scale of online studies with the more compelling experience they may be able to offer in a controlled lab context.</p><div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;So-called “moderated” experiments – in which the experimental session is administered through a synchronous video chat have been used widely in online experiments for children but these designs are less common in experiments with adults because they are expensive and time-consuming to administer <span class="citation" data-cites="chuey2021">(<a href="#ref-chuey2021" role="doc-biblioref">Chuey et al. 2021</a>)</span>.</p></li></div></div>
<p><strong>The length of online studies</strong>. One last concern is about attention and focus in online studies. Early guidance around online studies tended to focus on making studies short and easy, with the rationale that crowdsourcing workers were used to short jobs. Our sense is that this guidance no longer holds. Increasingly, researchers are deploying long and complex batteries of tasks to relatively good effect <span class="citation" data-cites="enkavi2019">(e.g., <a href="#ref-enkavi2019" role="doc-biblioref">Enkavi et al. 2019</a>)</span> and conducting repeated longitudinal sampling protocols <span class="citation" data-cites="litman2020">(discussed in depth in <a href="#ref-litman2020" role="doc-biblioref">Litman and Robinson 2020</a>)</span>. Rather than relying on hard and fast rules about study length, a better approach for online testing is to ensure that participants’ experience is as smooth and compelling as possible. Under these conditions, if an experiment is viable in the lab, it is likely viable online.</p>
<div class="page-columns page-full"><p>Online testing tools continue to grow and change but they are already mature enough that using them should be part of most behavioral researchers’ basic toolkit.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn13"><p><sup>13</sup>&nbsp;It is of course import to keep in mind that if a person works part- or full-time on a crowdsourcing platform, they are not a representative sample of the broader national population. Unfortunately, similar caveats hold true for in-person convenience samples (see <a href="010-sampling.html"><span>Chapter&nbsp;10</span></a>). Ultimately, researchers must reason about what their generalization goal is and whether that goal is consistent with the samples they can access (online or otherwise).</p></li></div></div>
</section>
</section>
<section id="ensuring-high-quality-data" class="level2 page-columns page-full" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="ensuring-high-quality-data"><span class="header-section-number">12.3</span> Ensuring high quality data</h2>
<p>In the final section of this chapter, we review some key data collection practices that can help researchers collect high quality data while respecting our ethical obligations to participants. By “high quality,” here we especially mean datasets that are uncontaminated by responses generated by misunderstanding of instructions, fatigue, incomprehension, or intentional neglect of the experimental task.</p>
<p>We’ll begin by discussing the issue of pilot testing; we recommend a systematic procedure for piloting that can maximize the chance of collecting high quality data. Next, we’ll discuss the practice of checking participants’ comprehension and attention and what such checks should and shouldn’t be used for. Finally, we’ll discuss the importance of maintaining consistent data collection records.</p>
<section id="conduct-effective-pilot-studies" class="level3 page-columns page-full" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored"><span class="header-section-number">12.3.1</span> Conduct effective pilot studies</h3>
<p>A <strong>pilot study</strong> is a small study conducted before you collect your main sample. The goal is to ensure smooth and successful data collection by first checking if your experimental procedures and data collection workflow are working correctly. Pilot studies are also an opportunity to get feedback from participants about their experience of the experimental task, for example, is it too easy, too difficult, or too boring.</p>
<p>Because pilot studies usually involve a small number of participants, they are not a reliable indicator of the study results, such as the expected effect size or statistical significance (as we discussed in <a href="010-sampling.html"><span>Chapter&nbsp;10</span></a>). <em>Don’t</em> use pilots to check if your effect is present or to estimate an effect size for power analysis.</p>
<p>What pilots <em>can</em> do is tell you about whether your experimental procedure is viable. For example, pilots studies can reveal:</p>
<ul>
<li>if your code crashes under certain circumstances</li>
<li>if your instructions confuse a substantial portion of your participants</li>
<li>if you have a very high dropout rate</li>
<li>if your data collection procedure fails to log variables of interest, or</li>
<li>if participants are disgruntled by the end of the experiment.</li>
</ul>
<div class="page-columns page-full"><p>We recommend that all experimenters perform – at the very minimum – two pilot studies before they launch a new experiment.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn14"><p><sup>14</sup>&nbsp;We mean especially when deploying a new experimental paradigm or when collecting data from a new population. Once you have run many studies with a similar procedure and similar sample, extensive piloting is less important. Any time you change something, it’s always good to run one or two pilots, though, just to check that you didn’t inadvertently mess up your experiment.</p></li></div></div>
<div class="page-columns page-full"><p>The first pilot, which we call your <strong>non-naïve participant pilot</strong>, can make use of participants who know the goals of the experiment and understand the experimental manipulation – this could be a friend, collaborator, colleague, or family member.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> The goal of this pilot study is to ensure that your experiment is comprehensible, that participants can complete it, and that the data are logged appropriately. You must <em>analyze</em> the data from the non-naive pilot, at least to the point of checking that the relevant data about each trial is logged.</p><div class="no-row-height column-margin column-container"><li id="fn15"><p><sup>15</sup>&nbsp;In a pinch you can even run yourself through the experiment a bunch of times (though this isn’t preferable because you’re likely to miss a lot of aspects of the experience that you are habituated to, especially if you’ve been debugging the experiment already).</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="data-logging-much" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="data-logging-much">Data logging much?</h2>
<p>When Mike was in graduate school, his lab got a contract to test a very large group of participants in a battery of experiments, bringing them into the lab over the course of a series of intense bursts of participant testing. He got the opportunity to add an experiment to the battery, allowing him to test a much larger sample than resources would otherwise allow. He quickly coded up a new experiment as part of a series of ongoing studies and began deploying it, coming to the lab every weekend for several months to help move participants through the testing protocol. Eagerly opening up the data file to reap the reward of this hard work, he found that the condition variable was missing from the data files. Although the experimental manipulation had been deployed properly, there was no record of which condition each participant had been run in, and so the data were essentially worthless. Had he run a quick pilot (even with non-naive participants) and attempted to analyze the data, this error would have been detected, and many hours of participant and experimenter effort would not have been lost.</p>
</section>
</div>
</div>
<div class="page-columns page-full"><p>The second pilot, your <strong>naïve participant pilot</strong>, should consist of a test of a small set of participants recruited via the channel you plan to use for your main study. The number of participants you should pilot depends on the cost of the experiment in time, money, and opportunity as well as its novelty. A brand new paradigm is likely more prone to error than a tried and tested paradigm. For a short online survey-style experiment, a pilot of 10–20 people is reasonable. A more time-consuming laboratory study might require piloting just two or three people.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn16"><p><sup>16</sup>&nbsp;In the case of especially expensive experiments, it can be a dilemma whether to run a larger pilot to identify difficulties since such a pilot will be costly. In these cases, one possibility is to plan to include the pilot participants in the main dataset if no major procedural changes are required. In this case, it is helpful to preregister a contingent testing strategy to avoid introducing data-dependent bias (see <a href="011-prereg.html"><span>Chapter&nbsp;11</span></a>). For example, in a planned sample of 100 participants, you could preregister running 20 as a pilot sample with the stipulation that you will look only at their dropout rate – and not at any condition differences. Then the preregistration can state that, if the dropout rate is lower than 25%, you will collect the next 80 participants and analyze the whole dataset, including the initial pilot, but if dropout rate is higher than 25%, you will discard the pilot sample and make changes. This kind of strategy can help you split the difference between cautious piloting and conservation of rare or costly data.</p></li></div></div>
<p>The goal of the naïve pilot study is to understand properties of the participant experience. Were participants confused? Did they withdraw before the study finished? Even a small number of pilots can tell you that your dropout rate is likely too high: for example, if 5 of 10 pilot participants withdraw you likely need to reconsider aspects of your design. It’s critical for your naïve participant pilot that you debrief more extensively with your participants. This debriefing often takes the form of an interview questionnaire after the study is over. “What did you think the study was about?” and “is there any way we could improve the experience of being in the study?” can be helpful questions. Often this debriefing is more effective if it is interactive, so even if you are running an online study you may want to find some way to chat with your participants.</p>
<p>Piloting – especially piloting with naïve participants to optimize the participant experience – is typically an iterative process. We frequently launch an experiment for a naive pilot, then recognize from the data or from participant feedback that the experience can be improved. We make tweaks and pilot again. Be careful not to over-fit to small differences in pilot data, however. Piloting should be more like workshopping a manuscript to remove typos than doing statistical analysis. If someone has trouble understanding a particular sentence – whether in your manuscript or in your experiment instructions – you should edit to make it clearer!</p>
</section>
<section id="measure-participant-compliance" class="level3 page-columns page-full" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored"><span class="header-section-number">12.3.2</span> Measure participant compliance</h3>
<p>You’ve constructed your experiment and piloted it. You are almost ready to go – but there is one more family of tricks for helping to achieve high quality data: integrating measures of participant compliance into your paradigm. Collecting data on compliance (whether participants followed the experimental procedures as expected) can help you quantify whether participants understood your task, engaged with your manipulation, and paid attention to the full experimental experience. These measures in turn can be used both to modify your experimental paradigm and to exclude specific participants that were especially non-compliant <span class="citation" data-cites="hauser2018 ejelov2020">(<a href="#ref-hauser2018" role="doc-biblioref">Hauser, Ellsworth, and Gonzalez 2018</a>; <a href="#ref-ejelov2020" role="doc-biblioref">Ejelöv and Luke 2020</a>)</span>.</p>
<p>Below we discuss four types of compliance checks: (1) passive measures, (2) comprehension checks, (3) manipulation checks, and (4) attention checks. Passive measures and comprehension checks are very helpful for enhancing data quality. Manipulation checks also often have a role to play. In contrast, we typically caution in the use of attention checks.</p>
<ol type="1">
<li><p><strong>Passive measures of compliance</strong>. Even if you do not ask participants anything extra in an experiment, it is often possible to tell if they have engaged with the experimental procedure simply by how long it takes them to complete the experiment. If you see participants with completion times substantially above or below the median, there is a good chance that they are either multi-tasking or rushing through the experiment without engaging.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> Passive measures cost little to implement and should be inserted whenever possible in experiments.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p></li>
<li><p><strong>Comprehension checks</strong>. For tasks with complex instructions or experimental materials (say a passage that must be understood for a judgment to be made about it), it can be very helpful to get a signal that participants have understood what they have read or viewed. Comprehension checks, which ask about the content of the experimental instructions or materials, are often included for this purpose. For the comprehension of instructions, the best kinds of questions simply query the knowledge necessary to succeed in the experiment, for example, “what are you supposed to do when you see a red circle flash on the screen?” In many platforms, it is possible to make participants reread the instructions again until they can answer these correctly. This kind of repetition is nice because it corrects participants’ misconceptions rather than allowing them to continue in the experiment when they do not understand.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p></li>
<li><p><strong>Manipulation checks</strong>. If your experiment involves more than a very transient manipulation – for example, if you plan to induce some state in participants or have them learn some content – then you can include a measure in your experiment that confirms that your manipulation succeeded <span class="citation" data-cites="ejelov2020">(<a href="#ref-ejelov2020" role="doc-biblioref">Ejelöv and Luke 2020</a>)</span>. This measure is known as a manipulation check because it measures some prerequisite difference between conditions that is not the key causal effect of interest but is causally prerequisite to this effect. For example, if you want to see if anger affects moral judgment, then it makes sense to measure whether participants in your anger induction condition rate themselves as angrier than participants in your control condition. Manipulation checks are useful in the interpretation of experimental findings because they can decouple the failure of a manipulation from the failure of a manipulation to affect your specific measure of interest.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p></li>
<li><p><strong>Attention checks</strong>. A final type of compliance check is a check that participants are paying attention to the experiment at all. One simple technique is to add questions that have a known and fairly obvious right answer (e.g., “what’s the capital of the United States.”). These trials can catch participants that are simply ignoring all text and “mashing buttons”, but they will not find participants who are mildly inattentive. Sometimes experimenters also use trickier compliance checks, such as putting an instruction for participants to click a particular answer deep within a question text that otherwise would have a different answer <a href="#fig-collection-attention-check">Figure&nbsp;<span>12.2</span></a>. Such compliance checks decrease so-called “satisficing” behavior, in which participants read as quickly as they can get away with (doing only the minimum. On the other hand, participants may see such trials as indications that the experimenter is trying to trick them, and adopt a more adversarial stance towards the experiment, which may result in less compliance with other aspects of the design [unless they are at the end of the experiment; <span class="citation" data-cites="hauser2018">Hauser, Ellsworth, and Gonzalez (<a href="#ref-hauser2018" role="doc-biblioref">2018</a>)</span>]. If you choose to include attention checks like these, be aware that you are likely reducing variability in your sample – trading off representativeness for compliance.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn17"><p><sup>17</sup>&nbsp;Measurements of per-page or per-element completion times can be even more specific since they can, for example, identify participants that simply did not read an assigned passage.</p></li><li id="fn18"><p><sup>18</sup>&nbsp;One variation that we endorse in certain cases is to force participants to engage with particular pages for a certain amount of time through the use of timers. Though, beware, this kind of feature can lead to an adversarial relationship with participants – in the face of this kind of coercion, many will opt to pull out their phone and multi-task until the timer runs down.</p></li><li id="fn19"><p><sup>19</sup>&nbsp;If you are querying comprehension of experimental materials rather than instructions, you may not want to re-expose participants to the same passage again in order to avoid confounding a participants’ initial comprehension and the amount of exposure that they receive.</p></li><li id="fn20"><p><sup>20</sup>&nbsp;<span class="citation" data-cites="hauser2018">Hauser, Ellsworth, and Gonzalez (<a href="#ref-hauser2018" role="doc-biblioref">2018</a>)</span> worry that manipulation checks can themselves change the effect of a manipulation – this worry strikes us as sensible, especially for some types of manipulations like emotion inductions. Their recommendation is to test the efficacy of the manipulation in a separate study, rather than trying to nest the manipulation check within the main study.</p></li></div><div id="fig-collection-attention-check" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/collection/instructional-manip.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;12.2: An attention check trial from <span class="citation" data-cites="oppenheimer2009">Oppenheimer, Meyvis, and Davidenko (<a href="#ref-oppenheimer2009" role="doc-biblioref">2009</a>)</span>. These trials can decrease variability in participant attention, but at the cost of selecting a subsample of participants, so they should be used cautiously.</figcaption>
</figure>
</div>
<p>Data from all of these types of checks are used in many different – often inconsistent – ways in the literature. We recommend that you:</p>
<ol type="1">
<li>Use passive measures and comprehension checks as pre-registered exclusion criteria to eliminate a (hopefully small) group of participants who might be non-compliant with your experiment.</li>
<li>Check that exclusions are low and that they are uniform across conditions. If exclusion rates are high, your design may have deeper issues. If exclusions are asymmetric across conditions, you may be compromising your randomization by creating a situation in which (on average) different kinds of participants are included in one condition compared with the other. Both of these situations substantially compromise any estimate of the causal effect of interest.</li>
<li>Deploy manipulation checks if you are concerned about whether your manipulation effectively induces a difference between groups. Analyze the manipulation check separately from the dependent variable to test whether the manipulation was causally effective <span class="citation" data-cites="ejelov2020">(<a href="#ref-ejelov2020" role="doc-biblioref">Ejelöv and Luke 2020</a>)</span>.</li>
<li>Make sure that your attention checks are not confounded in any way with condition – remember our cautionary tale from <a href="009-design.html"><span>Chapter&nbsp;9</span></a>, in which an attention check that was different across conditions actually created an experimental effect.</li>
<li><em>Do not</em> include any of these checks in your analytic models as a covariate, as including this information in your analysis compromises the causal inference from randomization and introduces bias in your analysis <span class="citation" data-cites="montgomery2018">(<a href="#ref-montgomery2018" role="doc-biblioref">Montgomery, Nyhan, and Torres 2018</a>)</span>.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn21"><p><sup>21</sup>&nbsp;Including this information means you are “conditioning on a post-treatment variable,” as we described in <a href="007-models.html"><span>Chapter&nbsp;7</span></a>. In medicine, analysts distinguish “intent-to-treat” analysis, where you analyze data from everyone you gave a drug, and “as treated” analysis, where you analyze data depending on how much of the drug people actually took. In general, intent-to-treat gives you the generalizable causal estimate. In our current situation, if you include compliance as a covariate, you are essential doing an “as treated” analysis and your estimate can be biased as a result. Although there is a place for such analyses, in general you probably want to avoid these analyses.</p></li></div><p>Used appropriately, compliance checks can provide both a useful set of exclusion criteria and a powerful tool for diagnosing potential issues with your experiment during data analysis and correcting them down the road.</p>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="does-data-quality-vary-throughout-the-semester" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="does-data-quality-vary-throughout-the-semester">Does data quality vary throughout the semester?</h2>
<p>Every lab that collects empirical data repeatedly using the same population builds up lore about how that population varies in different contexts. Many researchers who conducted experiments with college undergraduates were taught never to run their studies at the end of the semester. Exhausted and stressed students would likely yield low-quality data, or so the argument went. Until the rise of multi-lab collaborative projects like ManyLabs (see <a href="003-replication.html"><span>Chapter&nbsp;3</span></a>), such beliefs were almost impossible to test.</p>
<p>ManyLabs 3 aimed specifically to evaluate data quality variation across the academic calendar <span class="citation" data-cites="ebersole2016">(<a href="#ref-ebersole2016" role="doc-biblioref">Ebersole et al. 2016</a>)</span>. With 2,696 participants at 20 sites, the study conducted replications of 13 previously published findings. Although only six of these findings showed strong evidence of replicating across sites, none of the six effects was substantially moderated by being collected later in the semester. The biggest effect they observed was a change in the Stroop effect from <span class="math inline">\(d=.89\)</span> during the beginning and middle of the semester to <span class="math inline">\(d=.92\)</span> at the end. There was some evidence that participants <em>reported</em> being less attentive at the end of the semester, but this trend wasn’t accompanied by a moderation of experimental effects.</p>
<p>Researchers are subject to the same cognitive illusions and biases as any human. One of these biases is the search to find meaning in the random fluctuations they sometimes observe in their experiments. The intuitions formed through this process can be helpful prompts for generating hypotheses – but beware of adopting them into your “standard operating procedures” without further examination. Labs that avoided data collection during the end of the semester might have sacrificed 10–20% of their data collection capacity for no reason!</p>
</section>
</div>
</div>
</section>
<section id="keep-consistent-data-collection-records" class="level3 page-columns page-full" data-number="12.3.3">
<h3 data-number="12.3.3" class="anchored" data-anchor-id="keep-consistent-data-collection-records"><span class="header-section-number">12.3.3</span> Keep consistent data collection records</h3>
<div class="page-columns page-full"><p>As an experimentalist, one of the worst feelings is to come back to your data directory and see a group of data files, <code>run1.csv</code>, <code>run2.csv</code>, <code>run3.csv</code> and not know what experimental protocol was run for each. Was <code>run1</code> the pilot? Maybe a little bit of personal archaeology with timestamps and version history can tell you the answer, but there is no guarantee.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn22"><p><sup>22</sup>&nbsp;We’ll have a lot to say about this issue in <a href="013-management.html"><span>Chapter&nbsp;13</span></a>.</p></li></div></div>
<div id="fig-collection-runsheet" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/collection/runsheet.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;12.3: Part of a run sheet for a developmental study.</figcaption>
</figure>
</div>
<p>As well as collecting the actual data in whatever form they take (e.g., paper surveys, videos, or files on a computer), it is important to log <strong>metadata</strong> – data about your data – including relevant information like the date of data collection, the sample that was collected, the experiment version, the research assistants who were present, etc. The relevant meta-data will vary substantially from study to study – the important part is that you keep detailed records. <a href="#fig-collection-runsheet">Figure&nbsp;<span>12.3</span></a> and <a href="#fig-collection-log">Figure&nbsp;<span>12.4</span></a> give two examples from our own research. The key feature is that they provide some persistent metadata about how the experiments were conducted.</p>
<div id="fig-collection-log" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<hr>
<pre><code>%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Added a simple familiarization slide substitute that presents Bob and
shows that the experiment is about a person talking to you. Before
that, the familiarization slide was simply skipped.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
----------------------------
November 18 2013
50 subjects | Betting | No familiarization | Friend
var participant_response_type = 1;
var participant_feature_count = 1;
var linguistic_framing = 0;
var question_type = 0;
----------------------------
November 18 2013
50 subjects | Likert | No familiarization | Friend
var participant_response_type = 2;
var participant_feature_count = 1;
var linguistic_framing = 0;
var question_type = 2;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The experiment now asked the subjects the referent of Bobs statement
at the bottom of the page. The previous experiments always had the
input field just below the stimuli or, in the case of 3fc hoovering
over the images did highlighted possible ones.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
----------------------------
November 30 2013 ~ 7 pm:
50 subjects | 3 forced choice condition | No familiarization | Friend
var participant_response_type = 0;
var participant_feature_count = 1;
var linguistic_framing = 0;
var question_type = 0;</code></pre>
<hr>
<figcaption class="figure-caption margin-caption">Figure&nbsp;12.4: Excerpt of a log for an iterative run of online experiments.</figcaption>
</figure>
</div>
</section>
</section>
<section id="chapter-summary-data-collection" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="chapter-summary-data-collection"><span class="header-section-number">12.4</span> Chapter summary: Data collection</h2>
<p>In this chapter, we took the perspective of both the participant and the researcher. Our goal was to discuss how to achieve a good research outcome for both. On the side of the participant, we highlighted the responsibility of the experimenter to ensure a robust consent and debriefing process. We also discussed the importance of a good experimental experience in the lab and online – ensuring that the experiment is not only conducted ethically but is also pleasant to participate in. Finally, we discussed how to address some concerns about data quality from the researcher perspective, recommending both the extensive use of non-naive and naive pilot participants and the use of comprehension and manipulation checks.</p>
<div class="callout callout-style-default callout-note callout-titled" title="discussion questions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
discussion questions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>“Citizen science” is a movement to have a broader base of individuals participate in research because they are interested in discoveries and want to help. In practice, citizen science projects in psychology like Project Implicit (<a href="https://implicit.harvard.edu/implicit/" class="uri">https://implicit.harvard.edu/implicit/</a>), Children Helping Science (<a href="https://lookit.mit.edu" class="uri">https://lookit.mit.edu</a>), and TheMusicLab.org (<a href="https://themusiclab.org" class="uri">https://themusiclab.org</a>) have all succeeded by offering participants a compelling experience. Check one of these out, participate in a study, and make a list the features that make it fun and easy to contribute data.</p></li>
<li><p>Be a Turker! Sign up for an account as an Amazon Mechanical Turk or Prolific Academic worker and complete a couple of tasks. How did you feel about browsing the list of tasks looking for work? What features of tasks attracted your interest? How hard was it to figure out how to participate in each task? And how long did it take to get paid?</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="readings">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>An introduction to online research: Buhrmester, M. D., Talaifar, S., &amp; Gosling, S. D. (2018). An evaluation of Amazon’s Mechanical Turk, its rapid rise, and its effective use. Perspectives on Psychological Science, 13(2), 149-154. <a href="https://doi.org/10.1177/1745691617706516" class="uri">https://doi.org/10.1177/1745691617706516</a>.</li>
</ul>
</div>
</div>
</div>
<!-- \refs -->


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-allen2017" class="csl-entry" role="listitem">
Allen, Michael. 2017. <span>“Debriefing of Participants.”</span> In <em>The SAGE Encyclopedia of Communication Research Methods</em>. Vol. 1–4. Thousand Oaks, CA: Sage Publications.
</div>
<div id="ref-anderson2019" class="csl-entry" role="listitem">
Anderson, Craig A, Johnie J Allen, Courtney Plante, Adele Quigley-McBride, Alison Lovett, and Jeffrey N Rokkum. 2019. <span>“The MTurkification of Social and Personality Psychology.”</span> <em>Personality and Social Psychology Bulletin</em> 45 (6): 842–50.
</div>
<div id="ref-benjamin2000" class="csl-entry" role="listitem">
Benjamin, Ludy T. 2000. <span>“The Psychology Laboratory at the Turn of the 20th Century.”</span> <em>American Psychologist</em> 55 (3): 318.
</div>
<div id="ref-buhrmester2016" class="csl-entry" role="listitem">
Buhrmester, Michael, Tracy Kwang, and Samuel D Gosling. 2016. <span>“Amazon’s Mechanical Turk: A New Source of Inexpensive, yet High-Quality Data?”</span>
</div>
<div id="ref-chuey2021" class="csl-entry" role="listitem">
Chuey, Aaron, Mika Asaba, Sophie Bridgers, Brandon Carrillo, Griffin Dietz, Teresa Garcia, Julia A Leonard, et al. 2021. <span>“Moderated Online Data-Collection for Developmental Research: Methods and Replications.”</span> <em>Frontiers in Psychology</em>, 4968.
</div>
<div id="ref-cialdini2004" class="csl-entry" role="listitem">
Cialdini, Robert B, and Noah J Goldstein. 2004. <span>“Social Influence: Compliance and Conformity.”</span> <em>Annual Review of Psychology</em> 55 (1): 591–621.
</div>
<div id="ref-crump2013" class="csl-entry" role="listitem">
Crump, Matthew J C, John V McDonnell, and Todd M Gureckis. 2013. <span>“Evaluating Amazon’s Mechanical Turk as a Tool for Experimental Behavioral Research.”</span> <em>PLoS One</em> 8 (3): e57410.
</div>
<div id="ref-de-leeuw2015" class="csl-entry" role="listitem">
De Leeuw, Joshua R. 2015. <span>“jsPsych: A JavaScript Library for Creating Behavioral Experiments in a Web Browser.”</span> <em>Behavior Research Methods</em> 47 (1): 1–12.
</div>
<div id="ref-demayo2021" class="csl-entry" role="listitem">
DeMayo, Benjamin, Danielle Kellier, Mika Braginsky, Christina Bergmann, Cielke Hendriks, Caroline F Rowland, Michael Frank, and Virginia Marchman. 2021. <span>“Web-CDI: A System for Online Administration of the MacArthur-Bates Communicative Development Inventories.”</span> <em>Language Development Research</em>.
</div>
<div id="ref-ebersole2016" class="csl-entry" role="listitem">
Ebersole, Charles R, Olivia E Atherton, Aimee L Belanger, Hayley M Skulborstad, Jill M Allen, Jonathan B Banks, Erica Baranski, et al. 2016. <span>“Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.”</span> <em>J. Exp. Soc. Psychol.</em> 67 (November): 68–82.
</div>
<div id="ref-ejelov2020" class="csl-entry" role="listitem">
Ejelöv, Emma, and Timothy J Luke. 2020. <span>“<span>‘Rarely Safe to Assume’</span>: Evaluating the Use and Interpretation of Manipulation Checks in Experimental Social Psychology.”</span> <em>Journal of Experimental Social Psychology</em> 87: 103937.
</div>
<div id="ref-enkavi2019" class="csl-entry" role="listitem">
Enkavi, A Zeynep, Ian W Eisenberg, Patrick G Bissett, Gina L Mazza, David P MacKinnon, Lisa A Marsch, and Russell A Poldrack. 2019. <span>“Large-Scale Analysis of Test–Retest Reliabilities of Self-Regulation Measures.”</span> <em>Proceedings of the National Academy of Sciences</em> 116 (12): 5472–77.
</div>
<div id="ref-eyal2021" class="csl-entry" role="listitem">
Eyal, Peer, Rothschild David, Gordon Andrew, Evernden Zak, and Damer Ekaterina. 2021. <span>“Data Quality of Platforms and Panels for Online Behavioral Research.”</span> <em>Behavior Research Methods</em>, 1–20.
</div>
<div id="ref-fisher2013" class="csl-entry" role="listitem">
Fisher, Jill A. 2013. <span>“Expanding the Frame of" Voluntariness" in Informed Consent: Structural Coercion and the Power of Social and Economic Context.”</span> <em>Kennedy Institute of Ethics Journal</em> 23 (4): 355–79.
</div>
<div id="ref-fitzpatrick2016" class="csl-entry" role="listitem">
Fitzpatrick, Emily FM, Alexandra LC Martiniuk, Heather D’Antoine, June Oscar, Maureen Carter, and Elizabeth J Elliott. 2016. <span>“Seeking Consent for Research with Indigenous Communities: A Systematic Review.”</span> <em>BMC Medical Ethics</em> 17 (1): 1–18.
</div>
<div id="ref-gass2018" class="csl-entry" role="listitem">
Gass, Robert H, and John S Seiter. 2018. <em>Persuasion: Social Influence and Compliance Gaining</em>. Routledge.
</div>
<div id="ref-gramlich2021" class="csl-entry" role="listitem">
Gramlich, John. 2021. <span>“America’s Incarceration Rate Falls to Lowest Level Since 1995.”</span> <a href="https://www.pewresearch.org/fact-tank/2021/08/16/americas-incarceration-rate-lowest-since-1995/">https://www.pewresearch.org/fact-tank/2021/08/16/americas-incarceration-rate-lowest-since-1995/</a>.
</div>
<div id="ref-hara2018" class="csl-entry" role="listitem">
Hara, Kotaro, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey P Bigham. 2018. <span>“A Data-Driven Analysis of Workers’ Earnings on Amazon Mechanical Turk.”</span> In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>, 1–14.
</div>
<div id="ref-hauser2018" class="csl-entry" role="listitem">
Hauser, David J, Phoebe C Ellsworth, and Richard Gonzalez. 2018. <span>“Are Manipulation Checks Necessary?”</span> <em>Frontiers in Psychology</em> 9: 998.
</div>
<div id="ref-hawkins2020" class="csl-entry" role="listitem">
Hawkins, Robert D, Michael C Frank, and Noah D Goodman. 2020. <span>“Characterizing the Dynamics of Learning in Repeated Reference Games.”</span> <em>Cognitive Science</em> 44 (6): e12845.
</div>
<div id="ref-henrich2010" class="csl-entry" role="listitem">
Henrich, Joseph, Steven J Heine, and Ara Norenzayan. 2010. <span>“The Weirdest People in the World?”</span> <em>Behavioral and Brain Sciences</em> 33 (2-3): 61–83.
</div>
<div id="ref-holmes1976" class="csl-entry" role="listitem">
Holmes, David S. 1976. <span>“Debriefing After Psychological Experiments: I. Effectiveness of Postdeception Dehoaxing.”</span> <em>American Psychologist</em> 31 (12): 858.
</div>
<div id="ref-irani2013" class="csl-entry" role="listitem">
Irani, Lilly C, and M Six Silberman. 2013. <span>“Turkopticon: Interrupting Worker Invisibility in Amazon Mechanical Turk.”</span> In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>, 611–20.
</div>
<div id="ref-kadam2017" class="csl-entry" role="listitem">
Kadam, Rashmi Ashish. 2017. <span>“Informed Consent Process: A Step Further Towards Making It Meaningful!”</span> <em>Perspectives in Clinical Research</em> 8 (3): 107.
</div>
<div id="ref-litman2020" class="csl-entry" role="listitem">
Litman, Leib, and Jonathan Robinson. 2020. <em>Conducting Online Research on Amazon Mechanical Turk and Beyond</em>. Sage Publications.
</div>
<div id="ref-litman2017" class="csl-entry" role="listitem">
Litman, Leib, Jonathan Robinson, and Tzvi Abberbock. 2017. <span>“TurkPrime. Com: A Versatile Crowdsourcing Data Acquisition Platform for the Behavioral Sciences.”</span> <em>Behavior Research Methods</em> 49 (2): 433–42.
</div>
<div id="ref-maldonado2019" class="csl-entry" role="listitem">
Maldonado, Mora, Ewan Dunbar, and Emmanuel Chemla. 2019. <span>“Mouse Tracking as a Window into Decision Making.”</span> <em>Behavior Research Methods</em> 51 (3): 1085–1101.
</div>
<div id="ref-mason2012" class="csl-entry" role="listitem">
Mason, Winter, and Siddharth Suri. 2012. <span>“Conducting Behavioral Research on Amazon’s Mechanical Turk.”</span> <em>Behavior Research Methods</em> 44 (1): 1–23.
</div>
<div id="ref-montgomery2018" class="csl-entry" role="listitem">
Montgomery, Jacob M, Brendan Nyhan, and Michelle Torres. 2018. <span>“How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.”</span> <em>Am. J. Pol. Sci.</em> 62 (3): 760–75.
</div>
<div id="ref-moss2020" class="csl-entry" role="listitem">
Moss, Aaron J, Cheskie Rosenzweig, Jonathan Robinson, and Leib Litman. 2020. <span>“Demographic Stability on Mechanical Turk Despite COVID-19.”</span> <em>Trends in Cognitive Sciences</em> 24 (9): 678–80.
</div>
<div id="ref-oppenheimer2009" class="csl-entry" role="listitem">
Oppenheimer, Daniel M, Tom Meyvis, and Nicolas Davidenko. 2009. <span>“Instructional Manipulation Checks: Detecting Satisficing to Increase Statistical Power.”</span> <em>Journal of Experimental Social Psychology</em> 45 (4): 867–72.
</div>
<div id="ref-peer2021" class="csl-entry" role="listitem">
Peer, Eyal, David M Rothschild, Zak Evernden, Andrew Gordon, and Ekaterina Damer. 2021. <span>“MTurk, Prolific or Panels? Choosing the Right Audience for Online Research.”</span> <em>Choosing the Right Audience for Online Research (January 10, 2021)</em>.
</div>
<div id="ref-ohrp2003" class="csl-entry" role="listitem">
<span>“Prisoner Involvement in Research.”</span> 2003. <a href="https://www.hhs.gov/ohrp/regulations-and-policy/guidance/prisoner-research-ohrp-guidance-2003/index.html">https://www.hhs.gov/ohrp/regulations-and-policy/guidance/prisoner-research-ohrp-guidance-2003/index.html</a>.
</div>
<div id="ref-salehi2015" class="csl-entry" role="listitem">
Salehi, Niloufar, Lilly C Irani, Michael S Bernstein, Ali Alkhatib, Eva Ogbe, and Kristy Milland. 2015. <span>“We Are Dynamo: Overcoming Stalling and Friction in Collective Action for Crowd Workers.”</span> In <em>Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>, 1621–30.
</div>
<div id="ref-scott2017" class="csl-entry" role="listitem">
Scott, Kimberly, and Laura Schulz. 2017. <span>“Lookit (Part 1): A New Online Platform for Developmental Research.”</span> <em>Open Mind</em> 1 (1): 4–14.
</div>
<div id="ref-sears1986" class="csl-entry" role="listitem">
Sears, David O. 1986. <span>“College Sophomores in the Laboratory: Influences of a Narrow Data Base on Social Psychology’s View of Human Nature.”</span> <em>Journal of Personality and Social Psychology</em> 51 (3): 515.
</div>
<div id="ref-sieber1989" class="csl-entry" role="listitem">
Sieber, Joan E, and Michael J Saks. 1989. <span>“A Census of Subject Pool Characteristics and Policies.”</span> <em>American Psychologist</em> 44 (7): 1053.
</div>
<div id="ref-slim2021" class="csl-entry" role="listitem">
Slim, Mieke Sarah, and Robert Hartsuiker. 2021. <span>“Visual World Eyetracking Using WebGazer. Js.”</span>
</div>
<div id="ref-young1990" class="csl-entry" role="listitem">
Young, Daniel R, Donald T Hooker, and Fred E Freeberg. 1990. <span>“Informed Consent Documents: Increasing Comprehension by Reducing Reading Level.”</span> <em>IRB: Ethics &amp; Human Research</em> 12 (3): 1–5.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./011-prereg.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Preregistration</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./013-management.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Project management</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>