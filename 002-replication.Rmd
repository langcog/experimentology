# Replication, reproducibility, and transparency  {#replication}

::: {.learning-goals}
üçé Learning goals: 

* Understand the "crisis" narrative in psychology and the empirical evidence supporting it
* Define the distinction between reproducibility and replicability
* Consider types of replication 
* Reason about the relation of replication to theory building
:::

In the previous chapter, we gave a sober and considered introduction to the topic of experiments, their connection with causal inference, and their role in building psychological theory. In this chapter we're going to change gears a little bit and tell the story of the period from 2011 -- 2021 and how it has given rise to a number of "crisis" narratives in psychology. 


In order to set the terms of discussion, we need to describe some of the outcomes we are interested in. Figure \@ref(fig:replication-terms) gives us a basic starting point for our definitions.^[These terms have been a bit of a problem for the field, at least at first, but it seems like people have been agreeing on them recently.] For some claim in a paper, if we can take the same data that were analyzed in that paper, do the same analysis, and get the same result, we call that result **reproducible** (sometimes, **computationally reproducible**). If we can collect new data in the same experiment, do the same analysis, and get the same result, we call that a **replication** and say that the experiment is **replicable**. If we can do a different analysis with the original dataset, we call this a **robustness check** and so if a claim passes it is **robust**.^[These are less common in experimental psychology, but are very common in fields that work with large, complex observational datasets like sociology.] And if the same finding obtains in a different population, perhaps with a different analysis, the result is likely to be more **generalizable** beyond the initial conditions in which it was observed.^[You might have observed that a lot of work is being done here by the word "same." How do we operationalize same-ness for experimental procedures, statistical analyses, or samples? These are difficult questions that we'll address in part below, but there's no single answer and so these terms are always going to helpful guides rather than 



```{r replication-terms, fig.cap="A terminological framework for meta-science discussions. Based on [https://figshare.com/articles/Publishing_a_reproducible_paper/5440621]().", fig.margin=TRUE}
knitr::include_graphics("images/replication/terms.png")
```

We're also going to abandon the sober tone of the introduction and try to get you a bit worked up. From an empirical perspective, things have been far from ideal in the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests are probably not trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are "real" they may not reflect deep psychological generalizations. And even if they do, they likely don't reflect generalizations that are true about people in general, only some very specific groups of people. Your hair should be on fire, at least a little bit. If you by the end of this chapter, you don't feel a little bit of despair about the published psychological literature, then we haven't done our job.

On the other hand, you might be thinking, how do you know that all this bad stuff is true? Claims about a literature or field as a whole go beyond the kind of standard paradigmatic science that we were talking about in the previous chapter -- instead they are part of a new field called **meta-science**. Meta-science research is research *about research*, for example investigating 


::: {.case-study}
üî¨ Case study: The Open Science Collaboration

Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebeccca Saxe [@frank2012]. The idea was to have a replication-based course that introduced students to the nuts and bolts of research.^[One of the current author team was a student in the course that year!] A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies from top psychology journals in 2008.


```{r replication-osc-2015, fig.cap="Results from the Open Science Collaboration (2015). Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size shows estimated statistical power. The dotted line represents a perfect replication."}
knitr::include_graphics("images/replication/osc-2015.png")
```

:::



## The meta-science of replicability and reproducibility

We have to start with some definitions. 

```{r}

```


### Reproducibility

Critical notion of the "provenance chain" for specific numbers - that they can be traced back to analytic computations. 

### Replication

## A framework for replication 

- We describe the framework for replication in Zwaan et al. (2018), highlighting the idea of multiple dimensions on which a replication can differ from the original study and the ways that this complicates inferences about replication ‚Äúsuccess‚Äù (Mathur and VanderWeele 2019, 2020).
- We can get different epistemic value from doing direct replications (attempt to copy original study with as much fidelity as possible) vs. conceptual replications (attempt to somewhat perturb operationalization of original study).

::: {.accident-report}
‚ö†Ô∏è Accident report: The "small telescopes" case study of Simonsohn et al. (2015): what if weather really does affect mood, but the effect is too small for the original study to ever detect?

> Imagine I claimed our next-door neighbor was a billionaire oil sheik who kept thousands of boxes of gold and diamonds hidden in his basement. Later we meet the neighbor, and he is the manager of a small bookstore and has a salary 10% above the US average... Should we describe this as ‚Äúwe have confirmed the Wealthy Neighbor Hypothesis, though the effect size was smaller than expected‚Äù? Or as ‚ÄúI made up a completely crazy story, and in unrelated news there was an irrelevant deviation from literally-zero in the same space‚Äù?
:::

## Relation to theory building

How do reproducibility and replicability contribute to theory building? We draw on Hardwicke et al. (2018), considering these factors in the informativeness of an experiment.


Introduce p-hacking and publication biases as major sources of irreproducibility. These are biases - and we highlight bias reduction as a key goal.


::: {.accident-report}
‚ö†Ô∏è Accident report: When I'm 64?

Simmons, Simonsohn, Nelson introduce p-hacking into our lexicon. 

:::




::: {.case-study}
üî¨ Case study: Hardwicke et al. (2018; 2021) meta-studies of the analytic reproducibility of specific analytic findings in the empirical literature.
:::
