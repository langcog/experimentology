<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Experimentology - 3&nbsp; Replication</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./004-ethics.html" rel="next">
<link href="./002-theories.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-659MTW4XZ4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-659MTW4XZ4', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./001-experiments.html">Foundations</a></li><li class="breadcrumb-item"><a href="./003-replication.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Experimentology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/langcog/experimentology" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Experimentology.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-theories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Theories</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-replication.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Planning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008-measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Measurement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010-sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Execution</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Preregistration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013-management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Project management</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reporting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014-writing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Writing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100-instructors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Instructor’s guide</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./101-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">GitHub</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./102-rmarkdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">R Markdown</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./103-tidyverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tidyverse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./104-ggplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">ggplot</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#reproducibility" id="toc-reproducibility" class="nav-link active" data-scroll-target="#reproducibility"><span class="header-section-number">3.1</span> Reproducibility</a></li>
  <li><a href="#replication" id="toc-replication" class="nav-link" data-scroll-target="#replication"><span class="header-section-number">3.2</span> Replication</a>
  <ul class="collapse">
  <li><a href="#conceptual-frameworks-for-replication" id="toc-conceptual-frameworks-for-replication" class="nav-link" data-scroll-target="#conceptual-frameworks-for-replication"><span class="header-section-number">3.2.1</span> Conceptual frameworks for replication</a></li>
  <li><a href="#the-meta-science-of-replication" id="toc-the-meta-science-of-replication" class="nav-link" data-scroll-target="#the-meta-science-of-replication"><span class="header-section-number">3.2.2</span> The meta-science of replication</a></li>
  </ul></li>
  <li><a href="#causes-of-replication-failure" id="toc-causes-of-replication-failure" class="nav-link" data-scroll-target="#causes-of-replication-failure"><span class="header-section-number">3.3</span> Causes of replication failure</a></li>
  <li><a href="#replication-reproducibility-theory-building-and-open-science" id="toc-replication-reproducibility-theory-building-and-open-science" class="nav-link" data-scroll-target="#replication-reproducibility-theory-building-and-open-science"><span class="header-section-number">3.4</span> Replication, reproducibility, theory building, and open science</a>
  <ul class="collapse">
  <li><a href="#reciprocity-between-replication-and-theory" id="toc-reciprocity-between-replication-and-theory" class="nav-link" data-scroll-target="#reciprocity-between-replication-and-theory"><span class="header-section-number">3.4.1</span> Reciprocity between replication and theory</a></li>
  <li><a href="#deciding-when-to-replicate-to-maximize-epistemic-value" id="toc-deciding-when-to-replicate-to-maximize-epistemic-value" class="nav-link" data-scroll-target="#deciding-when-to-replicate-to-maximize-epistemic-value"><span class="header-section-number">3.4.2</span> Deciding when to replicate to maximize epistemic value</a></li>
  <li><a href="#open-science" id="toc-open-science" class="nav-link" data-scroll-target="#open-science"><span class="header-section-number">3.4.3</span> Open science</a></li>
  <li><a href="#a-crisis" id="toc-a-crisis" class="nav-link" data-scroll-target="#a-crisis"><span class="header-section-number">3.4.4</span> A crisis?</a></li>
  </ul></li>
  <li><a href="#chapter-summary-replication" id="toc-chapter-summary-replication" class="nav-link" data-scroll-target="#chapter-summary-replication"><span class="header-section-number">3.5</span> Chapter summary: Replication</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/langcog/experimentology/blob/main/003-replication.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/langcog/experimentology/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-replication" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled" title="learning goals">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
learning goals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Define and distinguish reproducibility and replicability</li>
<li>Synthesize the meta-scientific literature on replication and the causes of replication failures</li>
<li>Reason about the relation of replication to theory building</li>
</ul>
</div>
</div>
</div>
<p>In the previous chapters, we introduced experiments, their connection with causal inference, and their role in building psychological theory. In principle, repeated experimental work combined with theory building should yield strong research programs that explain and predict phenomena with increasing scope.</p>
<p>Yet in recent years there has been an increasing recognition that this idealized view of science might not be a good description of what we actually see when we look at the psychology literature. Many classic findings may be wrong, or at least overstated. Their statistical tests might not be trustworthy. The actual numbers are even wrong in many papers! And even when experimental findings are “real”, they may not generalise broadly to different people and different situations.</p>
<p>How do we know about these problems? A burgeoning field called <strong>meta-science</strong> is providing the evidence. Meta-science is research <em>about research</em>, for example investigating how often findings in a literature can be successfully built on, or trying to figure out how widespread some negative practice is. Meta-science allows us to go beyond one-off anecdotes about a particular set of flawed results or rumors about bad practices. Perhaps the most obvious sign that something is wrong is that when independent scientists team up in meta-science projects and try to repeat previous studies, they often do not get the same results.</p>

<div class="no-row-height column-margin column-container"><div id="fig-replication-terms" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/replication/terms.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3.1: A framework for understanding different terms related to the repeatabilty of scientific findings.</figcaption>
</figure>
</div></div><div class="page-columns page-full"><p>Before we begin reviewing this evidence, let’s discuss the different ways in which a scientific finding can be repeated. <a href="#fig-replication-terms">Figure&nbsp;<span>3.1</span></a> gives us a basic starting point for our definitions (based on <a href="https://figshare.com/articles/Publishing_a_reproducible_paper/5440621">"Publishing a reproducible paper" by Kirstie Whitaker</a>). For a particular finding in a paper, if we take the same data, do the same analysis, and get the same result, we call that finding <strong>reproducible</strong> (sometimes, <strong>analytically</strong> or <strong>computationally reproducible</strong>. If we collect <em>new</em> data using the same methods, do the same analysis, and get the same result, we call that a <strong>replication</strong> and say that the finding is <strong>replicable</strong>. If we do a different analysis with the same data, we call this a <strong>robustness check</strong> and if we get a similar result, we say that the finding is <strong>robust</strong>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We leave the last quadrant empty because there’s no specific term for it in the literature – the eventual goal is to draw <strong>generalizable</strong> conclusions but this term means more than just having a finding that is reproducible and replicable.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;You might have observed that a lot of work is being done here by the word “same.” How do we operationalize same-ness for experimental procedures, statistical analyses, samples, or results? These are difficult questions that we’ll touch on below. Keep in mind that there’s no single answer and so these terms are always going to helpful guides rather than exact labels.</p></li></div></div>
<p>In this chapter, we’ll primarily discuss reproducibility and replicability; discussions of robustness and generalizability will be taken up in Chapters <a href="011-prereg.html"><span>11</span></a> and <a href="010-sampling.html"><span>10</span></a> respectively. We’ll start out by reviewing key concepts around reproducibility and replicability as well as some important meta-science findings. This literature suggests that when you read an average psychology paper, your default expectation should be that it might not replicate!</p>
<p>We’ll then discuss some of the main reasons <em>why</em> findings might not replicate – especially <strong>analytic flexibility</strong> and <strong>publication bias</strong>. We end by taking up the issue of how reproducibility and replicability relate to theory building in psychology, and the role of <strong>open science</strong> in this discussion. This discussion focuses on the key role of <em>transparency</em> (one of our major book themes) in providing supports for theory building.</p>
<div class="callout callout-style-default callout-note callout-titled" title="case study">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
case study
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<section id="the-open-science-collaboration" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="the-open-science-collaboration">The Open Science Collaboration</h2>
<p>Around 2011, we were teaching our Experimental Methods course for the first time, based on a course model that we had worked on with Rebecca Saxe <span class="citation" data-cites="frank2012">(<a href="#ref-frank2012" role="doc-biblioref">Frank and Saxe 2012</a>)</span>. The idea was to introduce students to the nuts and bolts of research by having them run replications. A guy named Brian Nosek was on sabbatical nearby, and over coffee we learned that he was starting up an ambitious project to replicate a large sample of studies published in top psychology journals in 2008.</p>
<p>In the course that year we chose replication projects from the sample that Nosek had told us about. Four of these projects were executed very well and were nominated by the course TAs for inclusion in the broader project. A few years later, when the final group of 100 replication studies was completed, we got a look at the results, shown in <a href="#fig-replication-osc-2015">Figure&nbsp;<span>3.2</span></a>.</p>
<div id="fig-replication-osc-2015" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/replication/osc-2015.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3.2: Results from the <span class="citation" data-cites="osc2015">Open Science Collaboration (<a href="#ref-osc2015" role="doc-biblioref">2015</a>)</span>. Each point represents one of the studies in the sample, with the horizontal position giving the original effect size and the vertical position giving the replication effect size. Dot size represents estimated statistical power. The dotted line represents a perfect replication.</figcaption>
</figure>
</div>
<p>The resulting meta-science paper, which we and others refer to as the “replication project in psychology” (RPP), made a substantial impression on both psychologists and the broader research community, defining both a field of psychology meta-science studies and providing a template for many-author collaborative projects <span class="citation" data-cites="osc2015">(<a href="#ref-osc2015" role="doc-biblioref">Open Science Collaboration 2015</a>)</span>. But the most striking thing was the result: disappointingly, only around a third of the replications had similar findings to the original studies. The others yielded smaller effects that were not statistically significant in the replication sample (almost all of the original studies were significant). RPP provided the first large-scale evidence that there were systematic issues with replicability in the psychology literature.</p>
<p>RPP’s results – and their interpretation – were controversial, however, and much ink was spilled on what these data showed. In particular, critics pointed to different degrees of fidelity between the original studies and the replications; insufficient levels of statistical power and low evidential value in the replications; non-representative sampling of the literature; and difficulties identifying specific statistical outcomes for replication success <span class="citation" data-cites="gilbert2016 anderson2016 etz2016">(<a href="#ref-gilbert2016" role="doc-biblioref">Gilbert et al. 2016</a>; <a href="#ref-anderson2016" role="doc-biblioref">Anderson et al. 2016</a>; <a href="#ref-etz2016" role="doc-biblioref">Etz and Vandekerckhove 2016</a>)</span>. In our view, many of these critiques have merit, and you can’t simply interpret the results of RPP as an unbiased estimate of the replicability of results in the literature, contra the title. <!-- Confusingly, the title of the paper is "Estimating the reproducibility of psychological science", not "the replicability of psychological science". This caused terminological confusion for several years; it seems like at this point people have just decided it was a mistake. --></p>
<p>And yet, RPP’s results are still important and compelling, and they undeniably changed the direction of the field of psychology. Many good studies are like this – they have flaws but they inspire follow up studies that can address those problems. For several of us personally, working on this project was also transformative in that it showed us the power of collaborative work. Together we could do a study that no one of us had any hope of completing on our own, and potentially make a difference in our field.</p>
</section>
</div>
</div>
<section id="reproducibility" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="reproducibility"><span class="header-section-number">3.1</span> Reproducibility</h2>
<div class="page-columns page-full"><p>Scientific papers are full of numbers: sample sizes, measurements, statistical results, and visualizations. For those numbers to have meaning, and for other scientists to be able to verify them, we need to know where they came from (their <strong>provenance</strong>). The chain of actions that scientists perform on the raw data, all the way through to reporting numbers in their papers, is sometimes called the <em>analysis pipeline</em>. For much of history, scientific papers have only provided a verbal, description of the analysis pipeline, usually with little detail.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;The situation is nicely summed up by a prescient quote from <span class="citation" data-cites="buckheit1995">Buckheit and Donoho (<a href="#ref-buckheit1995" role="doc-biblioref">1995</a>)</span>: “… a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.”</p></li></div></div>
<div class="page-columns page-full"><p>Moreover, researchers typically do not share key research objects from this pipeline, such as the analysis scripts or the raw data <span class="citation" data-cites="hardwicke2021c">(<a href="#ref-hardwicke2021c" role="doc-biblioref">Hardwicke, Thibault, et al. 2021</a>)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Without code and data, the numbers reported in scientific papers are often not reproducible – an independent scientist cannot repeat all of the steps in the analysis pipeline and get the same results as the original scientists.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;For many years, professional societies, like the American Psychological Association, have mandated data sharing (<a href="https://www.apa.org/ethics/code" class="uri">https://www.apa.org/ethics/code</a>), but only for purposes of verification, and only “on request” – in other words, scientists could keep data hidden by default and it was their responsibility to share if another scientist requested access. In practice, this kind of policy does not work; data are rarely made available on request <span class="citation" data-cites="wicherts2006">(<a href="#ref-wicherts2006" role="doc-biblioref">Wicherts et al. 2006</a>)</span>. We believe this situation is untenable. We provide a longer argument justifying data sharing in <a href="004-ethics.html"><span>Chapter&nbsp;4</span></a> and discuss some of the practicalities of sharing in <a href="013-management.html"><span>Chapter&nbsp;13</span></a>.</p></li></div></div>
<p>Reproducibility is desirable for a number of reasons. Without it:</p>
<ul>
<li>Errors in calculation or reporting could lead to disparities between the reported result and the actual result,</li>
<li>Vague verbal descriptions of analytic computations could keep readers from understanding the computations that were actually performed,</li>
<li>The robustness of data analyses to alternative model specifications cannot be checked, and</li>
<li>Synthesizing evidence across studies, a key part of building a cumulative body of scientific knowledge (<a href="016-meta.html"><span>Chapter&nbsp;16</span></a>), is much more difficult.</li>
</ul>
<div class="page-columns page-full"><p>From this list, error detection and correction is probably the most pressing issue. But are errors common? There are plenty of individual instances of errors that are corrected in the published literature <span class="citation" data-cites="cesana-arlotti2018">(e.g., some of us found an error in <a href="#ref-cesana-arlotti2018" role="doc-biblioref">Cesana-Arlotti et al. 2018</a>)</span>, and we ourselves have made significant analytic errors <span class="citation" data-cites="frank2013">(e.g., <a href="#ref-frank2013" role="doc-biblioref">Frank et al. 2013</a>)</span>. But these kinds of experiences don’t tell us about the frequency of errors more generally (or the consequences of error for the conclusions that researchers draw).<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;There is a very interesting discussion of the pernicious role of scientific error on theory building in <span class="citation" data-cites="gould1996">Gould, Gold, et al. (<a href="#ref-gould1996" role="doc-biblioref">1996</a>)</span>’s “The Mismeasure of Man.” Gould examines research on racial differences in intelligence and documents how scientific errors that supported racial differences were often overlooked. Errors are often caught asymmetrically; we are more motivated to double-check a result that contradicts our biases.</p></li></div></div>
<p>Estimating the frequency of errors is a meta-scientific issue that researchers have attempted to answer over the years. If errors are frequent, that would suggest a need for changes in our policies and practices to reduce their frequency! Unfortunately, the lack of data availability creates a problem: it’s hard to figure out if calculations are wrong if you can’t check them in the first place. Here’s one clever approach to this issue. In standard American Psychological Association (APA) reporting format, inferential statistics must be reported with three pieces of information: the test statistic, the degrees of freedom for the test, and the <span class="math inline">\(p\)</span>-value (e.g., <span class="math inline">\(t(18) = -0.74\)</span>, <span class="math inline">\(p = 0.47\)</span>). Yet these pieces of information are redundant with one another. Thus, reported statistics can be checked for consistency simply by evaluating whether they line up with one another – that is, whether the <span class="math inline">\(p\)</span>-value recomputed from the <span class="math inline">\(t\)</span> and degrees of freedom matches the reported value.</p>
<div class="page-columns page-full"><p><span class="citation" data-cites="bakker2011">Bakker and Wicherts (<a href="#ref-bakker2011" role="doc-biblioref">2011</a>)</span> performed this kind of statistical consistency analysis on a sample of 281 papers, and found that around 18% of statistical results were incorrectly reported. Even more worrisome, around 15% of articles contained at least one decision error – that is, a case where the error changed the direction of the inference that was made (e.g., from significant to insignificant).<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="citation" data-cites="nuijten2016">Nuijten et al. (<a href="#ref-nuijten2016" role="doc-biblioref">2016</a>)</span> used an automated method called “statcheck”<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> to confirm and extend this analysis. They checked <span class="math inline">\(p\)</span>-values for more than 250,000 psychology papers in the period 1985–2013 and found that around half of all papers contained at least one incorrect <span class="math inline">\(p\)</span>-value!</p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;Confirming Gould’s speculation (see note above), most of the reporting errors that led to decision errors were in line with the researchers’ own hypotheses.</p></li><li id="fn6"><p><sup>6</sup>&nbsp;Statcheck is now available as a web app (<a href="http://statcheck.io" class="uri">http://statcheck.io</a>) and an R package so that you can check your own manuscripts!</p></li></div></div>
<p>These findings provide a lower bound on the number of errors in the literature and suggest that reproducibility of analyses is likely very important. However, they only address the consistency of statistical reporting. What would happen if we tried to repeat the entire analysis pipeline from start to finish? It’s rather difficult to answer this question at a large scale: firstly, it takes a long time to run a reproducibility check; and secondly, the lack of access to raw data means that for most scientific papers, checking reproducibility is impossible.</p>
<p>Nevertheless, a few years ago a group of us spotted an opportunity to check reproducibility by examining studies published in two journals that either required or encouraged data sharing. <span class="citation" data-cites="hardwicke2018b">Hardwicke et al. (<a href="#ref-hardwicke2018b" role="doc-biblioref">2018</a>)</span> and <span class="citation" data-cites="hardwicke2021a">Hardwicke, Bohn, et al. (<a href="#ref-hardwicke2021a" role="doc-biblioref">2021</a>)</span> first identified studies that shared data, then narrowed those down to studies that shared <em>reusable</em> data (the data were accessible, complete, and comprehensible). For 60 of these articles, we then attempted to reproduce numerical values related to a particular statistical result in the paper. The process was incredibly labor-intensive, with articles typically requiring 5–10 hours of work each. And the results were concerning: the targeted values in only about a third of articles were completely reproducible without help from the original authors! In many cases, after – sometimes extensive – correspondence with the original authors, they provided additional information that was not reported in the original paper. After author contact, the reproducibility success rate improved to 62% (<a href="#fig-replication-hardwicke">Figure&nbsp;<span>3.3</span></a>). The remaining papers appeared to have some values that neither we, nor the original authors, could reproduce. Importantly, we didn’t identify any patterns of non-reproducibility that seriously undermined the conclusions drawn in the original articles; however, other reproducibility studies have found a distressingly high number of decision errors <span class="citation" data-cites="artner2020">(<a href="#ref-artner2020" role="doc-biblioref">Artner et al. 2020</a>)</span>, albeit with a slightly higher success rate overall.</p>
<div id="fig-replication-hardwicke" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/replication/hardwicke2021.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;3.3: Analytic reproducibility of results from open-data articles in <em>Cognition</em> and <em>Psychological Science</em>. From <span class="citation" data-cites="hardwicke2021a">Hardwicke, Bohn, et al. (<a href="#ref-hardwicke2021a" role="doc-biblioref">2021</a>)</span>.</figcaption>
</figure>
</div>
<p>In sum: transparency is a critical imperative for decreasing the frequency of errors in the published literature. Reporting and computation errors are frequent in the published literature, and the identification of these errors depends on the findings being reproducible. If data are not available, then errors usually cannot be found.</p>
</section>
<section id="replication" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="replication"><span class="header-section-number">3.2</span> Replication</h2>
<p>Beyond verifying a paper’s original analysis pipeline, we are often interested in understanding whether the study can be replicated – if we repeat the study methods and obtain new data, do we get similar results? To quote from <span class="citation" data-cites="popper2005">Popper (<a href="#ref-popper2005" role="doc-biblioref">2005</a>)</span>, “the scientifically significant… effect may be defined as that which can be regularly [replicated] by anyone who carries out the appropriate experiment in the way prescribed.”</p>
<p>Replications can be conducted for many reasons <span class="citation" data-cites="schmidt2009">(<a href="#ref-schmidt2009" role="doc-biblioref">Schmidt 2009</a>)</span>. One goal can be to verify that the results of an existing study can be obtained again if the study is conducted again in exactly the same way, to the best of our abilities. A second goal can be to gain a more precise estimate of the effect of interest by conducting a larger replication study, or combining the results of a replication study with the existing study. A third goal can be to investigate whether an effect will persist when, for example, the experimental manipulation is done in a different, but still theory-consistent, manner. Alternatively, we might want to investigate whether the effect persists in a different population. Such replications are often efforts to “replicate and extend,” and are common both when the same research team wants to conduct a sequence of experiments that each build on one another or when a new team wants to build on a result from a paper they have read <span class="citation" data-cites="rosenthal1990">(<a href="#ref-rosenthal1990" role="doc-biblioref">Rosenthal 1990</a>)</span>.</p>
<p>Much of the meta-science literature (and attendant debate and discussion) has focused on the first goal – simple verification. This focus has been so intense that the term “replication” has become associated with skepticism or even attacks on the foundations of the field. This dynamic is at odds with the role that replication is given in a lot of philosophy of science, where it is assumed to be a typical part of “normal science.”</p>
<section id="conceptual-frameworks-for-replication" class="level3 page-columns page-full" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored"><span class="header-section-number">3.2.1</span> Conceptual frameworks for replication</h3>
<p>The key challenge of replication is <strong>invariance</strong> – Popper’s stipulation that a replication be conducted “in the way prescribed” in the quote above. That is, what are the features of the world over which a particular observation should be relatively constant, and what are those that are specified as the key ingredients for the effect? Replication is relatively straightforward in the physical and biological sciences, in part because of presupposed theoretical background that allows us to make strong inferences about invariance. If a biologist reports an observation about a particular cell type from an organism, the color of the microscope is presumed not to matter to the observation.</p>
<div class="page-columns page-full"><p>These invariances are far harder to state in psychology, for both the procedure of an experiment and its sample. Procedurally, should the color of the experimental stimulus matter to the measured effect? In some cases yes, in some cases no.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Yet the task of postulating how a scientific effect should be invariant to lab procedures pales in comparison to the task of postulating how the effect should be invariant across different human populations!<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;A fascinating study by <span class="citation" data-cites="baribault2018">Baribault et al. (<a href="#ref-baribault2018" role="doc-biblioref">2018</a>)</span> proposes a method for empirically understanding psychological invariances. Treating a subliminal priming effect as their model system, they sampled thousands of “micro-experiments” in which small parameters of their experimental procedure were randomly sampled. These parameters allowed for measurement of their effect of interest, averaging across this irrelevant variation. In their case, it turned out that color did not matter.</p></li><li id="fn8"><p><sup>8</sup>&nbsp;In some sense, the research program of some branches of the social sciences amounts to an understanding of invariances across human cognition.</p></li></div></div>
<!-- If color does not matter, how about the context of presentation for an experiment -- should presentation to a participant at home on a web browser produce the same effect as presentation in a laboratory setting using custom software [@crump2013]?  -->
<div class="page-columns page-full"><p>A lot is at stake in this discussion. If Dr.&nbsp;Frog publishes a finding with US undergraduates and Dr.&nbsp;Toad then “replicates” the procedure in Germany, to what extent should we be perturbed if the effect is different in magnitude or absent?<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Meta-researchers have made a number of replication taxonomies to try and quantify the degree of methodological consistency between two experiments.</p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;Presumably not very much if Dr.&nbsp;Toad gave the original instructions in English instead of in German – that’s another one of these pesky invariances that we are always worrying about!</p></li></div></div>
<div class="page-columns page-full"><p>One influential taxonomy distinguishes between <strong>direct replications</strong><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> and <strong>conceptual replications</strong> <span class="citation" data-cites="zwaan2018">(<a href="#ref-zwaan2018" role="doc-biblioref">Zwaan et al. 2018</a>)</span>. Direct replications are those that attempt to reproduce all of the salient features of the prior study, up to whatever invariances the experimenters believe are present (e.g., color of the paint, gender of the experimenter, etc.). In contrast, conceptual replications are typically paradigms that attempt to test the same hypothesis via different operationalizations of the manipulation and/or the measure. We agree with Zwaan et al.&nbsp;(2018): labeling this second type of experiment as a “replication” is a little misleading. Rather, so-called “conceptual replications” are alternative tests of the same part of your theory. Such tests can be extremely valuable, but they serve a different goal than replication.</p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;These also get called <strong>exact replications</strong> sometimes. We think this term is misleading because similarity between two different experiments is always going to be on a gradient, and where you cut this continuum is always going to be a theory-laden decision. One person’s “exact” is another’s “inexact.”</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="small-telescopes" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="small-telescopes">“Small Telescopes”</h2>
<p>We’ve been discussing the question of invariance with respect to procedure and sample, but we haven’t really discussed invariance with respect to the studies’ statistical results. To what extent can we consider two statistical results to be “the same”? Several obvious metrics, including those used by RPP, have important limitations <span class="citation" data-cites="simonsohn2015">(<a href="#ref-simonsohn2015" role="doc-biblioref">Simonsohn 2015</a>)</span>. For example, if one finding is statistically significant and the other isn’t, they still could have effect sizes that are actually quite close to one another, in part because one might have a larger sample size than the other. Or you could have two significant findings that nevertheless have very different effect sizes.</p>
<div id="fig-replication-telescopes" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/replication/telescopes.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3.4: The original finding by <span class="citation" data-cites="schwarz1983">Schwarz and Clore (<a href="#ref-schwarz1983" role="doc-biblioref">1983</a>)</span> and two replications with much larger samples. All three estimates include a 95% confidence interval, but the confidence intervals are very small for the two replication studies. The green dashed line shows the smallest effect that the origial study could reasonably have detected. From <span class="citation" data-cites="simonsohn2015">Simonsohn (<a href="#ref-simonsohn2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
</div>
<p>In a classic study, <span class="citation" data-cites="schwarz1983">Schwarz and Clore (<a href="#ref-schwarz1983" role="doc-biblioref">1983</a>)</span> reported that participants (N=28) rated their life satisfaction as higher on sunny days than rainy days, suggesting that they mis-attributed temporary happiness about the weather to longer-term life satisfaction. However, when two more recent studies examined very large samples of survey responses, they yielded estimates of the effect that were much smaller. (All of these effects have been standardized so they are on the same scale using a metric called Cohen’s <span class="math inline">\(d\)</span> that we will introduce more formally in <a href="005-estimation.html"><span>Chapter&nbsp;5</span></a>). In one survey, the effect was statistically significant but extremely small; in the other it was essentially zero (<a href="#fig-replication-telescopes">Figure&nbsp;<span>3.4</span></a>). Using statistical significance as the metric of replication success, you might be tempted to say that the first of these studies was a successful replication and the second was a failed replication.</p>
<p>Simonsohn points out that this interpretation doesn’t make sense, using the analogy of a study’s sample size as a telescope. Following this analogy, Schwarz and Clore had a very small telescope (i.e., a small sample size), and they pointed it in a particular direction and claimed to have observed a planet (i.e., a nonzero effect). Now it might turn out that there <em>was</em> a planet at that location when you look with a much larger telescope (first replication), and it might turn out that there <em>wasn’t</em> (second replication). Regardless, however, the original small telescope was simply not powerful enough to have seen whatever was there. Both studies fail to replicate the original observation, regardless of whether their observed effect was in the same direction.</p>
<p>Following Simonsohn’s example, numerous metrics for replication success have been proposed <span class="citation" data-cites="mathur2020">(<a href="#ref-mathur2020" role="doc-biblioref">Mathur and VanderWeele 2020</a>)</span>. The best of these move away from the idea that there is a binary test of whether an individual replication was successful and towards a comparison of the two effects and whether they appear consistent with the same theory. <span class="citation" data-cites="gelman2018">Gelman (<a href="#ref-gelman2018" role="doc-biblioref">2018</a>)</span> suggests the “time reversal” heuristic – rather than thinking of a replication as a success or a failure, consider the alternative world in which the replication study had been performed first and the original study followed it. What would we say then? If we leave behind the idea that the original study has precedence, it makes much more sense to consider the sum total of the evidence across the two. Do they agree or disagree? Taken together, do they support the presence of the effect, or do they present a strong case that it’s present only under certain conditions? Using this approach, it seems pretty clear that the weather mis-attribution effect is, at best, a tiny factor in people’s overall judgments of their life satisfaction.</p>
</section>
</div>
</div>
</section>
<section id="the-meta-science-of-replication" class="level3 page-columns page-full" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored"><span class="header-section-number">3.2.2</span> The meta-science of replication</h3>
<p>In RPP, replication teams reported subjectively that 39% of replications were successful, with 36% reporting a significant effect in the same direction as the original. How generalizable is this estimate – and how replicable <em>is</em> psychological research more broadly? Based on the discussion above, we hope we’ve made you skeptical that this is a well-posed question, at least without a lot of additional qualifiers. Any answer is going to have to provide details about the scope of this claim, the definition of replication being used, and the metric for replication success. On the other hand, <em>versions</em> of this question have led to a number of empirical studies that help us better understand the scope of replication issues.</p>
<!-- We'll review these briefly here because we think a good understanding of the meta-science literature on replication can help us decide how worried we should be about the state of the psychology literature.  -->
<div class="page-columns page-full"><p>Many subsequent empirical studies of replication have focused on particular subfields or journals, with the goal of informing particular field-specific practices or questions. For example, <span class="citation" data-cites="camerer2016">Camerer et al. (<a href="#ref-camerer2016" role="doc-biblioref">2016</a>)</span> replicated all of the between-subject laboratory articles published in two top economics journals in the period 2011–2014. They found a replication rate of 61% of significant effects in the same direction of the original, higher than the rate in RPP but lower than the naive expectation based on their level of statistical power. Another study attempted to replicate all 21 behavioral experiments published in the journals <em>Science</em> and <em>Nature</em> from 2010–2015, finding a replication rate of 62% significant effects <span class="citation" data-cites="camerer2018">(<a href="#ref-camerer2018" role="doc-biblioref">Camerer et al. 2018</a>)</span>.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> While these types of studies do not answer all the questions that were raised about RPP, they suggest that replication rates for top experiments are not as high as we’d like them to be, even when care is taken with the sampling and individual study protocols.</p><div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;This study was notable because they followed a two-step procedure – after an initial round of replications, they followed up on the failures by consulting with the original authors and pursuing extremely large sample sizes. The resulting estimate thus is less subject to many of the critiques of the original RPP paper.</p></li></div></div>
<p>Other scientists working in the same field can often predict when an experiment will fail to replicate. <span class="citation" data-cites="dreber2015">Dreber et al. (<a href="#ref-dreber2015" role="doc-biblioref">2015</a>)</span> showed that prediction markets (where participants bet small sums of real money on replication outcomes) made fairly accurate estimates of replication success in the aggregate. This result has itself now been replicated several times (e.g., in the Camerer et al., 2018 study described earlier). Maybe even more surprisingly, there’s some evidence that machine learning models trained on the text of papers can predict replication success <span class="citation" data-cites="yang2020 youyou2023">(<a href="#ref-yang2020" role="doc-biblioref">Yang, Youyou, and Uzzi 2020</a>; <a href="#ref-youyou2023" role="doc-biblioref">Youyou, Yang, and Uzzi 2023</a>)</span>, though more work still needs to be done to validate these models and understand the features they use. More generally, these two lines of research suggest the possibility of isolating consistent factors that lead to replication success or failure. (In the next section we consider what these factors are in more depth.)</p>
<p>Although more work still needs to be done to get generalizable estimates of replicability, taken together, the meta-science literature does provide some clarity on what we should expect. Altogether, the chance of a significant finding in a (well-powered) replication study of a generic experiment in social and cognitive psychology is likely somewhere around 56%. Furthermore, the replication effect will likely be on average 53% as large <span class="citation" data-cites="nosek2021">(<a href="#ref-nosek2021" role="doc-biblioref">Nosek et al. 2021</a>)</span>.</p>
<p>On the other hand, these large-scale replication studies have substantial limitations as well. With relatively few exceptions, the studies chosen for replication used short, computerized tasks that mostly would fall into the categories of social and cognitive psychology. Further, and perhaps most troubling from the perspective of theory development, they tell us only whether a particular experimental effect can be replicated. They tell us much less about whether the construct that the effect was meant to operationalize is in fact real! We’ll return to the difficult issue of how replication and theory construction relate to one another in the final section of this chapter.</p>
<p>Some have called the narrative that emerges from the sum of these meta-science studies the “replication crisis.” We think of it as a major tempering of expectations with respect to the published literature. Your naive expectation might reasonably be that you could read a typical journal article, select an experiment from it, and replicate that experiment in your own research. The upshot of this literature is, unfortunately, if you try selecting and replicating an exeriment, you might well be disappointed by the result.</p>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="consequences-for-the-study-consequences-for-the-person" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="consequences-for-the-study-consequences-for-the-person">Consequences for the study, consequences for the person</h2>
<p>“Power posing” is the idea that adopting a more open and expansive physical posture might also change your confidence. <span class="citation" data-cites="carney2010">Carney, Cuddy, and Yap (<a href="#ref-carney2010" role="doc-biblioref">2010</a>)</span> told 42 participants that they were taking part in a study of physiological recording. They then held two poses, each for a minute. In one condition, the poses were expansive (e.g., legs out, hands on head); in another condition, the poses were contractive (e.g., arms and legs crossed). Participants in the expansive pose condition showed increases in testosterone and decreases in salivary cortisol (a stress marker), they took a greater number of risk in a gambling task, and they reported that they were more “in charge” in a survey. This result suggested that a two-minute manipulation could lead to striking physiological and psychological changes – in turn leading to power posing becoming firmly enshrined as part of the set of recommended strategies in business and elsewhere. The original publication contributed to the rise of the researchers’ careers, including becoming a principal piece of evidence in a hugely-popular TED talk by Amy Cuddy, one of the authors.</p>
<p>Followup work has questioned these findings, however. A replication study with a larger number of participants (N=200) failed to find evidence for physiological effects of power-posing, even as it did find some effects on participants’ own beliefs <span class="citation" data-cites="ranehill2015">(<a href="#ref-ranehill2015" role="doc-biblioref">Ranehill et al. 2015</a>)</span>. And a review of the published literature suggested that many findings appeared to be the result of some sort of publication bias, as far too many of them had <em>p</em>-values very close to the .05 threshold <span class="citation" data-cites="simmons2017">(<a href="#ref-simmons2017" role="doc-biblioref">Simmons and Simonsohn 2017</a>)</span>. In light of this evidence, the first author of the replication study bravely made a public statement that she does not believe that “power pose” effects are real <span class="citation" data-cites="carney2016">(<a href="#ref-carney2016" role="doc-biblioref">Carney 2016</a>)</span>.</p>
<div class="cell" data-hash="003-replication_cache/html/fig-replication-powerpose_9032ac823b7a5fb4ad5289935bd415d2">
<div class="cell-output-display">
<div id="fig-replication-powerpose" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="003-replication_files/figure-html/fig-replication-powerpose-1.png" class="img-fluid figure-img" width="2100"></p>
<figcaption class="figure-caption">Figure&nbsp;3.5: Google trends time series for “power pose” from 2007-2023.</figcaption>
</figure>
</div>
</div>
</div>
<p>From the scientific perspective, it’s very tempting to take this example as a case in which the scientific ecosystem corrects itself. Although many people continue to cite the original power posing work, we suspect the issues are well-known throughout the social psychology community, and overall interest from the lay public has gone down (see <a href="#fig-replication-powerpose">Figure&nbsp;<span>3.5</span></a>). But this narrative masks the very real human impacts of the self-correction process, which can raise ethical questions about the best way to address issues in the scientific record.</p>
<p>The process of debate and discussion around individual findings can be bruising and complicated. In the case of power posing, Cuddy herself was tightly associated with the findings and many critiques of the findings became critiques of the individual. Several commentators used Cuddy’s name as a stand-in for low-quality psychological results, likely because of her prominence and perhaps because of her gender and age as well. These comments were harmful to Cuddy personally and her career more generally. <!-- ^[For further reading, see ["When the Revolution Came for Amy Cuddy"](https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html).] --></p>
<p>Scientists should critique, reproduce, and replicate results – these are all parts of the progress of normal science. But it’s important to do this in a way that’s sensitive to the people involved. Here are a few guidelines for courteous and ethical conduct:</p>
<ul>
<li>Always communicate about the work, never the person. Try to use language that is specific to the analysis or design being critiqued, rather than the person who did the analysis or thought up the design.</li>
<li>Avoid using language that assumes negative intentions, e.g.&nbsp;“the authors misleadingly state that …”</li>
<li>Ask someone to read your paper, email, blogpost, or tweet before you hit send. It can be very difficult to predict how someone else will experience the tone of your writing; a reader can help you make this judgement.</li>
<li>Consider communicating personally before communicating publicly. As Joe Simmons, one critic in the power-posing debate said, “I wish I’d had the presence of mind to pick up the phone and call [before publishing my critique]” <span class="citation" data-cites="dominus2017">(<a href="#ref-dominus2017" role="doc-biblioref">Dominus 2017</a>)</span>. Personal communication isn’t always necessary (and can be difficult due to asymmetries of power or status), but it can be helpful.</li>
</ul>
<p>As we will argue in the next chapter, we have an ethical duty as scientists to promote good science and critique low quality science. But we also have a duty to our colleagues and communities to be good to one another.</p>
</section>
</div>
</div>
</section>
</section>
<section id="causes-of-replication-failure" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="causes-of-replication-failure"><span class="header-section-number">3.3</span> Causes of replication failure</h2>
<div class="page-columns page-full"><p>The general argument of this chapter is that everything is not all right in experimental psychology, and hence that we need to change our methodological practices to avoid negative outcomes like irreproducible papers and unreplicable results. Towards that goal, we have been presenting meta-scientific evidence on reproducibility and replicability. But this evidence has been controversial, to say the least! Do large-scale replication studies like RPP – or for that matter, smaller-scale individual replications of effects like “power posing” – really lead to the conclusion that our methods require changes? Or are there reasons why a lower replication rate is actually consistent with a cumulative, positive vision of psychology?<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;One line of argument addresses this question through the dynamics of scientific change. There are many versions, but one is given by <span class="citation" data-cites="wilson2020">Wilson, Harris, and Wixted (<a href="#ref-wilson2020" role="doc-biblioref">2020</a>)</span>. The idea is that progress in psychology consists of a two-step process by which candidate ideas are “screened” by virtue of small, noisy experiments that reveal promising but tentative ideas that can then be “confirmed” by large-scale replications. On this kind of view, it’s business as usual to find that many randomly-selected findings don’t hold up in large-scale replications and so we shouldn’t be distressed by results like those of RPP. The key to progress is to finding a small set that <em>do</em> hold up, which will lead to new areas of inquiry. We’re not sure this is view is either a good description of current practice or a good normative goal for scientific progress, but we won’t focus on that critique of Wilson et al.’s argument here. Instead, since book is written for experimenters-in-training, we assume that <em>you</em> do not want your experiment to be a false positive from a noisy screening procedure, regardless of your feelings about the rest of the literature!</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="depth">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
depth
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="context-moderators-and-expertise" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="context-moderators-and-expertise">Context, moderators, and expertise</h2>
<p>There are many explanations for failed replications. The wonderful thing about meta-science is that these explanations can be tested empirically!</p>
<p>Let’s start with the idea that specific experimental operationalizations of a theory might be “context sensitive,” especially in subfields, like social psychology, whose theories inherently refer to environmental context <span class="citation" data-cites="van-bavel2016">(<a href="#ref-van-bavel2016" role="doc-biblioref">Van Bavel et al. 2016</a>)</span>. Critics brought this issue up for RPP, where there were several studies in which the original experimental materials were tailored to one cultural context but then were deployed in another context, potentially leading to failure due to mismatch <span class="citation" data-cites="gilbert2016">(<a href="#ref-gilbert2016" role="doc-biblioref">Gilbert et al. 2016</a>)</span>.</p>
<p>Context sensitivity seems like a great explanation because in some sense, it <em>must</em> be right. If the context of an experiment includes the vast network of learned associations, practices, and beliefs that we all hold, then there’s no question that an experiment’s materials tap into this context to one degree or another. For example, if your experiment relies on the association between <em>doctor</em> and <em>nurse</em> concepts, you would expect this experiment to work differently in the past when <em>nurse</em> meant something more like <em>nanny</em> <span class="citation" data-cites="ramscar2016">(<a href="#ref-ramscar2016" role="doc-biblioref">Ramscar 2016</a>)</span>.</p>
<p>On the other hand, as an explanation of specific replication failures, context sensitivity has not fared very well. The “Many Labs” projects were a series of replication projects in which <em>multiple</em> labs independently attempted to replicate several original studies. (In contrast, in RPP and similar studies, a single replication was conducted for each original study.) Some of the Many Labs projects assessed variation in replication success across different labs. In ManyLabs 2, <span class="citation" data-cites="klein2018b">Klein et al. (<a href="#ref-klein2018b" role="doc-biblioref">2018</a>)</span> replicated 28 findings, distributed across 125 different samples and more than 15,000 participants. ManyLabs 2 found almost no support for the context sensitivity hypothesis as an explanation of replication failure. In general, when effects failed to replicate, they did so when conducted in person as well as when conducted online, and these failures were consistent across many cultures and labs. <!-- The size of effects in successful replications were modulated a bit by contextual factors, but heterogeneity in general was not high.  --></p>
<p>On the other hand, a review of several Many Labs-style replication projects indicated, on re-analysis, that population effects differed across replication labs even when the replication protocols were very similar to one another <span class="citation" data-cites="olsson2020heterogeneity errington2021investigating">(<a href="#ref-olsson2020heterogeneity" role="doc-biblioref">Olsson-Collentine, Wicherts, and Assen 2020</a>; <a href="#ref-errington2021investigating" role="doc-biblioref">Errington et al. 2021</a>)</span>. So context sensitivity is almost certainly present – and we’ll return to the broader issues of generalizability, context, and invariance in the next section – but so far we have not identified specific forms of context sensitivity that reliably affect replication success.</p>
<div id="fig-replication-lf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/replication/lewis-stims.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3.6: Stimuli from <span class="citation" data-cites="lewis2016">Lewis and Frank (<a href="#ref-lewis2016" role="doc-biblioref">2016</a>)</span>.</figcaption>
</figure>
</div>
<!-- ![Forest plot showing effect sizes and confidence intervals for all of the studies in the paper as well as the two original studies being replicated and the meta-analytic effect (red diamond).](images/replication/lewis-data.png){#fig-replication-lf2} -->
<p>These observations – that 1) direct replications vary in how successful they are, but 2) we cannot identify specific contextual moderators – together suggest the possible presence of “hidden moderators.” That is, when faced with a successful original study and a failed replication, there may be some unknown factor(s) that moderates the effect. We’ve personally had several experiences that corroborate this. For example, in <span class="citation" data-cites="lewis2016">Lewis and Frank (<a href="#ref-lewis2016" role="doc-biblioref">2016</a>)</span>, we were unsuccessful in replicating a simple categorization experiment. We then made a series of iterative changes to the stimuli and instructions, for example changing the color and pattern of the stimuli (<a href="#fig-replication-lf">Figure&nbsp;<span>3.6</span></a>), eventually resulting in a larger (and statistically significant) effect – though still much smaller than the original. Critically, however, each alteration that we made to the procedure yielded a very small change in the effect, and it would have taken us many thousands of participants to figure exactly which alteration made the difference. (If you’re keeping score, here’s a case where stimulus color <em>did</em> matter to the outcome of the experiment!.</p>
<p>Another explanation for replication failure that is often cited is experimenter expertise <span class="citation" data-cites="schwarz2014">(e.g., <a href="#ref-schwarz2014" role="doc-biblioref">Schwarz and Strack 2014</a>)</span>. On this hypothesis, replications fail because the researchers performing the replication do not have sufficient expertise to execute the study. Like context sensitivity, this explanation is almost certainly true for some replications. In our own work, we have repeatedly performed experiments that failed due to our own incompetence!</p>
<p>Yet as an explanation of the pattern of meta-science findings, the expertise hypothesis hasn’t been supported empirically. First, team expertise was not a predictor of replication success in RPP <span class="citation" data-cites="bench2017">(cf. <a href="#ref-bench2017" role="doc-biblioref">Bench et al. 2017</a>)</span>. More convincingly, Many Labs 5 selected ten findings from RPP with unsuccessful replications and systematically evaluated whether formal expert peer review of the protocols, including by the authors of the original study, would lead to a larger effect sizes. Despite a massive sample size and extremely thorough review process, there was little to no change in the effects for the vetted protocols relative to the original protocol used in RPP <span class="citation" data-cites="ebersole2020">(<a href="#ref-ebersole2020" role="doc-biblioref">Ebersole et al. 2020</a>)</span>.</p>
<p>Context, moderators, and expertise seem like reasonable explanations for individual replication failures. Certainly, we should expect them to be explanatory! But for these hypotheses to be operationalized in such a way that they carry weight in our evaluation of the meta-scientific evidence, they must be evaluated empirically rather than accepted uncritically. When such evaluations have been carried out, they have failed to support a large role for these factors.</p>
</section>
</div>
</div>
<div class="page-columns page-full"><p>In RPP and subsequent meta-science studies, original studies with lower <span class="math inline">\(p\)</span>-values, larger effect sizes, and larger sample sizes were more likely to replicate successfully <span class="citation" data-cites="yang2020">(<a href="#ref-yang2020" role="doc-biblioref">Yang, Youyou, and Uzzi 2020</a>)</span>. From a theoretical perspective, this result is to be expected, because the <span class="math inline">\(p\)</span>-value literally captures the probability of the data (or any “more extreme”) under the null hypothesis of no effect. So a lower <span class="math inline">\(p\)</span>-value should indicate a lower probability of a spurious result.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> In some sense, the fundamental question about the replication meta-science literature is why the <span class="math inline">\(p\)</span>-values <em>aren’t</em> better predictors of replicability! For example, <span class="citation" data-cites="camerer2018">Camerer et al. (<a href="#ref-camerer2018" role="doc-biblioref">2018</a>)</span> computes an expected number of successful replications on the basis of the effects and sample sizes – and their proportion of successful replications is substantially lower than that number.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> <!-- Such heterogeneity is not uncommon, even if it is relatively small, in multi-site replications in which heterogeneity can be directly estimated [@ebersole2020, olsson2020heterogeneity]. As such, the metrics will typically underestimate replication success when there is heterogeneity within pairs [@mathur2020]. --></p><div class="no-row-height column-margin column-container"><li id="fn13"><p><sup>13</sup>&nbsp;In <a href="006-inference.html"><span>Chapter&nbsp;6</span></a> we will have a lot more to say about <span class="math inline">\(p &lt; .05\)</span> but for now we’ll mostly just treat it as a particular research outcome.</p></li><li id="fn14"><p><sup>14</sup>&nbsp;This calculation, as with most other metrics of replication success, assumes that the underlying population effect is exactly the same for the replication and the original. This is a limitation because there could be unmeasured moderators that could produce genuine substantive differences between the two estimates.</p></li></div></div>
<div class="page-columns page-full"><p>One explanation is that the statistical evidence that is presented in papers often dramatically overstates the true evidence from a study. That’s because of two pervasive and critical issues: <strong>analytic flexibility</strong> (also known as <strong>p-hacking</strong> or <strong>questionable research practices</strong>) and <strong>publication bias</strong>.<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn15"><p><sup>15</sup>&nbsp;These terms basically mean the same thing and are not used very precisely in the literature. <span class="math inline">\(p\)</span>-hacking is an informal term that sounds like you know you are doing something bad; sometimes people do, and sometimes they don’t. Questionable research practices is a more formal-sounding term that is in principle vague enough to encompass many ethical failings but in practice gets used to talk about <span class="math inline">\(p\)</span>-hacking. Unless <span class="math inline">\(p\)</span>-hacking intent is crystal clear, we favor two clunkier terms: “data-dependent decision-making” and “undisclosed analytic flexibility” describe the actual practices more precisely: trying many different things after looking at data, typically without reporting all of them.</p></li></div></div>
<div class="page-columns page-full"><p>Publication bias refers to the relative preference (of scientists and other stakeholders, like journals) for experiments that “work” than those that do not, where “work” is typically defined as yielding a significant result at <span class="math inline">\(p&lt;.05\)</span>. Because of this preference, it is typically easier to publish positive (statistically significant) results. The relative absence of negative results leads to biases in the literature. Intuitively, this bias will lead to a literature filled with papers where <span class="math inline">\(p&lt;.05\)</span>. Negative findings will then remain unpublished, living in the proverbial “file drawer” <span class="citation" data-cites="rosenthal1979">(<a href="#ref-rosenthal1979" role="doc-biblioref">Rosenthal 1979</a>)</span>.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> In a literature with a high degree of publication bias, many findings will be spurious because experimenters got lucky and published the study that “worked” even if that success was due to chance variation. In this situation, these spurious findings will not be replicable and so the overall rate of replicability in the literature will be lowered.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn16"><p><sup>16</sup>&nbsp;One estimate is that 96% of (non-preregistered) papers report positive findings <span class="citation" data-cites="scheel2021">(<a href="#ref-scheel2021" role="doc-biblioref">Scheel, Schijen, and Lakens 2021</a>)</span>! We’ll have a lot more to say about analytic flexibility and publication bias in Chapters <a href="011-prereg.html"><span>11</span></a> and <a href="016-meta.html"><span>16</span></a>, respectively.</p></li><li id="fn17"><p><sup>17</sup>&nbsp;The mathematics of the publication bias scenario strikes some observers as implausible: most psychologists don’t run dozens of studies and report only one out of each group <span class="citation" data-cites="nelson2018">(<a href="#ref-nelson2018" role="doc-biblioref">Nelson, Simmons, and Simonsohn 2018</a>)</span>. Instead, a more common scenario is to conduct many different analyses and then report the most successful, creating some of the same effects as publication bias – a promotion of spurious variation – without a file drawer full of failed studies.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<section id="analytic-flexibility-reveals-a-fountain-of-eternal-youth" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="analytic-flexibility-reveals-a-fountain-of-eternal-youth">Analytic flexibility reveals a fountain of eternal youth</h2>
<p>The way they tell it, Joseph Simmons, Leif Nelson, and Uri Simonsohn wrote their paper on “false positive psychology” <span class="citation" data-cites="simmons2011">(<a href="#ref-simmons2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn 2011</a>)</span> as an attempt at catharsis <span class="citation" data-cites="simmons2018">(<a href="#ref-simmons2018" role="doc-biblioref">Simmons, Nelson, and Simonsohn 2018</a>)</span>. They were fed up with work that they felt exploited flexibility in data analysis to produce findings blessed with <em>p</em> &lt; .05 but likely did not reflect replicable effects. They called this practice <strong>p-hacking</strong>: trying different things to get your <em>p</em>-value to be below .05.</p>
<p>Their paper reported on a simple experiment: they played participants either the Beatles song, “when I’m 64,” or a control song and then asked them to report their date of birth <span class="citation" data-cites="simmons2011">(<a href="#ref-simmons2011" role="doc-biblioref">Simmons, Nelson, and Simonsohn 2011</a>)</span>. This manipulation resulted in a significant one and a half year rejuvenation effect. Listening to the Beatles seemed to have made their participants younger!</p>
<p>This result is impossible, of course. But the authors produced a statistically significant difference between the groups that, by definition, was a <strong>false positive</strong> – a case where the statistical test indicated that there was a difference between groups despite no difference existing. In essence, they did so by trying many possible analyses and “cherry-picking” the one that produced a positive result. This practice of course invalidates the inference that the statistical test is supposed to help you make.</p>
<p>Several of the practices they followed included:</p>
<ul>
<li>Selectively reporting dependent measures (e.g., collecting several measures and reporting only one),</li>
<li>Selectively dropping manipulation conditions,</li>
<li>Conducting their statistical test and then testing extra participants in the case that they did not see a significant finding, and</li>
<li>Adjusting for gender as a covariate in their analysis if doing so resulted in a significant effect.</li>
</ul>
<p>Many of the practices that the authors followed in their rejuvenation study were (and maybe still are!) commonplace in the research literature. <span class="citation" data-cites="john2012">John, Loewenstein, and Prelec (<a href="#ref-john2012" role="doc-biblioref">2012</a>)</span> surveyed research psychologists on the prevalence of what they called <strong>questionable research practices</strong>. Most participants admitted to following some of these practices – including exactly the same practices followed by the rejuvenation study.</p>
<p>For many in the field, “false positive psychology” was a galvanizing moment, leading them to recognize how common practices could lead to completely spurious (or even impossible) conclusions. As Simmons, Nelson, and Simonsohn wrote in their 2018 article, “Everyone knew [p-hacking] was wrong, but they thought it was wrong the way it is wrong to jaywalk. We decided to write ‘False-Positive Psychology’ when simulations revealed that it was wrong the way it is wrong to rob a bank.”</p>
</section>
</div>
</div>
<div class="page-columns page-full"><p>It’s our view that publication bias and its even more pervasive cousin, analytic flexibility, are likely to be key drivers of lower replicability. We admit that the meta-scientific evidence for this hypothesis isn’t unambiguous, but that’s because there’s no sure-fire way to diagnose analytic flexibility in a particular paper – since we can almost never reconstruct the precise choices that were made in the data collection and analysis process! On the other hand, it is possible to analyze indicators of publication bias in specific literatures and there are several cases where publication bias diagnostics appear to go hand in hand with replication failure.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn18"><p><sup>18</sup>&nbsp;Here are two examples. First, in the “power posing” example described above, <span class="citation" data-cites="simmons2017">Simmons and Simonsohn (<a href="#ref-simmons2017" role="doc-biblioref">2017</a>)</span> noted strong evidence of analytic flexibility throughout the literature, leading them to conclude that there was no evidential value in the literature. Second, in the case of “money priming” (incidental exposures to images or text about money that were hypothesized to lead to changes in political attitudes), strong evidence of publication bias <span class="citation" data-cites="vadillo2016">(<a href="#ref-vadillo2016" role="doc-biblioref">Vadillo, Hardwicke, and Shanks 2016</a>)</span> was accompanied by a string of failed replications <span class="citation" data-cites="rohrer2015">(<a href="#ref-rohrer2015" role="doc-biblioref">Rohrer, Pashler, and Harris 2015</a>)</span>.</p></li></div></div>
</section>
<section id="replication-reproducibility-theory-building-and-open-science" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="replication-reproducibility-theory-building-and-open-science"><span class="header-section-number">3.4</span> Replication, reproducibility, theory building, and open science</h2>
<p>So, empirical measures of reproducibility and replicability in the experimental psychology literature are low – lower than we might have naively suspected and lower than we want. How do we address these issues? And how do these issues interact with the goal of building theories? In this last section, we discuss the relationship between replication and theory – and the role that open and transparent research practices can play.</p>
<section id="reciprocity-between-replication-and-theory" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="reciprocity-between-replication-and-theory"><span class="header-section-number">3.4.1</span> Reciprocity between replication and theory</h3>
<p>Analytic reproducibility is a prerequisite for theory building because if the twin goals of theories are to explain and to predict experimental measurements, then an error-ridden literature undermines this goal. If some proportion of all numerical values reported in the literature were simple, unintentional typos, this situation would create an extra level of noise – irrelevant random variation – impeding our goal of getting precise enough measurements to distinguish between theories. But in fact, the situation is likely to be worse: errors are much more often in the direction that favors authors’ own hypotheses. Thus, irreproducibility not only decreases our precision, it also increases the bias in the literature, creating obstacles to the fair evaluation of theories with respect to data.</p>
<p>Replicability is also foundational to theory building. Across a wide range of different conceptions of how science works, scientific theories are evaluated with respect to their relationship to the world. They must be supported, or at least fail to be falsified, by specific observations. It may be that some observations are by their nature un-repeatable (e.g., a particular astrophysical event might not be observed again a human lifetime). But for laboratory sciences – and experimental psychology can be counted among these, to a certain extent at least – the independent and skeptical evaluation of theories requires repeatability of measurements.</p>
<p>Some authors have argued (following the philosopher Heraclitus), “you can’t step in the same river twice” <span class="citation" data-cites="mcshane2014">(<a href="#ref-mcshane2014" role="doc-biblioref">McShane and Böckenholt 2014</a>)</span> – meaning, the circumstances and context of psychological experiments are constantly changing and no observation will be identical to another. This is of course technically true from a philosophical perspective. But that’s where theory comes in! As we discussed above, our theories postulate the invariances that allow us to group together similar observations and generalize across them.</p>
<p>In this sense, replication is critical to theory, but theory is also critical to replication. Without a theory of “what matters” to a particular outcome, we really are stepping into an ever-changing river. But a good theory can concentrate our expectations on a much smaller set of causal relationships, allowing us to make strong predictions about what factors should and shouldn’t matter to experimental outcomes. To return to an example we discussed earlier, should stimulus color matter to the outcome of an experiment? Our theory could tell us that it shouldn’t matter for a priming experiment <span class="citation" data-cites="baribault2018">(<a href="#ref-baribault2018" role="doc-biblioref">Baribault et al. 2018</a>)</span> but that it should for a generalization experiment <span class="citation" data-cites="lewis2016">(<a href="#ref-lewis2016" role="doc-biblioref">Lewis and Frank 2016</a>)</span>.</p>
</section>
<section id="deciding-when-to-replicate-to-maximize-epistemic-value" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="deciding-when-to-replicate-to-maximize-epistemic-value"><span class="header-section-number">3.4.2</span> Deciding when to replicate to maximize epistemic value</h3>
<p>As a scientific community, how much emphasis should we place on replication? In the words of <span class="citation" data-cites="newell1973">Newell (<a href="#ref-newell1973" role="doc-biblioref">1973</a>)</span>, “you can’t play 20 questions with nature and win”. A series of well-replicated measurements does not itself constitute a theory. Theory construction is its own important activity. We’ve tried to make the case here that a reproducible and replicable literature is a critical foundation for theory building. That doesn’t necessarily mean you have to do replications all the time.</p>
<p>More generally, any scientific community needs to trade off between exploring new phenomena and confirming previously reported effects. In a thought-provoking analysis, <span class="citation" data-cites="oberauer2019">Oberauer and Lewandowsky (<a href="#ref-oberauer2019" role="doc-biblioref">2019</a>)</span> suggest that perhaps replications also aren’t the best test of theoretical hypotheses. In their analysis, if you don’t have a theory then it makes sense to try and discover new phenomena and then to replicate them. If you <em>do</em> have a theory, you should expend your energy in testing new predictions rather than repeating the same test across multiple replications. Analyses such <span class="citation" data-cites="oberauer2019">Oberauer and Lewandowsky (<a href="#ref-oberauer2019" role="doc-biblioref">2019</a>)</span> can provide a guide to our allocation of scientific effort.</p>
<p>Our goal in this book is somewhat different than the general goal of metascientists considering how science should be conducted. Once <em>you</em> as a researcher decide to do a particular experiment, we think you will want to maximize its scientific value and so you will want it to be replicable. But we aren’t suggesting that you should necessarily do a replication study. There are many concerns that go into whether to replicate – including not only whether you are trying to gather evidence about a particular phenomenon, but also whether you are trying to master techniques and paradigms related to it. As we said at the beginning of this chapter, not all replication is for the purpose of verification, and you as a researcher can make an informed decision about what experimental strategy is best for you.</p>
</section>
<section id="open-science" class="level3 page-columns page-full" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="open-science"><span class="header-section-number">3.4.3</span> Open science</h3>
<div class="page-columns page-full"><p>The <strong>open science movement</strong> is, in part, a response – really a set of responses – to the challenges of reproducibility and replicability. The open science (and now the broader <strong>open scholarship</strong>) movement is a broad umbrella (<a href="#fig-replication-umbrella">Figure&nbsp;<span>3.7</span></a>), but in this book we take open science to be a set of beliefs, research practices, results, and policies that are organized around the central roles of transparency and verifiability in scientific practice.<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> The core of this movement is the idea of “nullius in verba” (the motto of the British Royal Society, which roughly means “take no one’s word for it.”<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn19"><p><sup>19</sup>&nbsp;Another part of the open science umbrella involves a democratization of the scientific process through efforts to open access to science. This process involves both removal of barriers to access the scientific literature but also efforts to remove barriers to scientific training – especially to groups historically underrepresented in the sciences. The hope is that these processes increase both the set of people and the range of perspectives contributing to science. We view these changes as no less critical than the transparency aspects of the open science movement, though more indirectly related to the current discussion of reproducibility and replicability.</p></li><li id="fn20"><p><sup>20</sup>&nbsp;At least that’s a reasonable paraphrase; there’s some interesting discussion about what this quote from Horace really means in a letter by <span class="citation" data-cites="gould1991">Gould (<a href="#ref-gould1991" role="doc-biblioref">1991</a>)</span>.</p></li></div></div>
<!-- (https://laneblog.stanford.edu/2020/09/01/introducing-the-open-science-reading-group/) -->

<div class="no-row-height column-margin column-container"><div id="fig-replication-umbrella" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/replication/umbrella.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3.7: The broad umbrella of open science (adapted from an image created for the Stanford Lane Library Blog).</figcaption>
</figure>
</div></div><p>Transparency initiatives are critical for ensuring reproducibility. As we discussed above, you cannot even evaluate reproducibility in the absence of data sharing. Code sharing can go even further towards helping reproducibility, as code makes the exact computations involved in data analysis much more explicit than the verbal descriptions that are the norm in papers <span class="citation" data-cites="hardwicke2018b">(<a href="#ref-hardwicke2018b" role="doc-biblioref">Hardwicke et al. 2018</a>)</span>. Further, as we will discuss in <a href="013-management.html"><span>Chapter&nbsp;13</span></a>, the set of practices involved in preparing materials for sharing can themselves encourage reproducibility by leading to better organizational practices for research data, materials, and code.</p>
<p>Transparency also plays a major role in advancing replicability. This point may not seem obvious at first – why would sharing things openly lead to more replicable experiments? – but it is one of the major theses of this book, so we’ll unpack it a bit. Here are a couple of routes by which transparent practices lead to greater replication rates.</p>
<ol type="1">
<li><p>Sharing of experimental materials enables replications that closely follow the original study’s methods (<a href="013-management.html"><span>Chapter&nbsp;13</span></a>). One critique of many replications has been that they differ in key respects from the originals. Sometimes those deviations were purposeful, but in other cases they were simply because the replicators could not use the original experimental materials. Sharing materials solves this problem.</p></li>
<li><p>Sharing sampling and analysis plans allows replication of key aspects of design and analysis that may not be clear in verbal descriptions, for example exclusion criteria or details of data pre-processing.</p></li>
<li><p>Sharing of analytic decision-making via preregistration can lead to a decrease in <span class="math inline">\(p\)</span>-hacking and other practices that can introduce bias (<a href="011-prereg.html"><span>Chapter&nbsp;11</span></a>). The strength of statistical evidence in the original study is a predictor of replicability in subsequent studies. If original studies are preregistered, they are more likely to report effects that are not subject to inflation via questionable research practices.</p></li>
<li><p>Preregistration can also clarify the distinction between confirmatory and exploratory findings, helping subsequent experimenters to make a more informed judgment about which effects are likely to be good targets for replication.</p></li>
</ol>
<p>For all of these reasons, we believe that open science practices can play a critical role in increasing reproducibility and replicability.</p>
</section>
<section id="a-crisis" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="a-crisis"><span class="header-section-number">3.4.4</span> A crisis?</h3>
<p>So, is there a “replication crisis”? The common meaning of “crisis” is “a difficult time.” The data we reviewed in this chapter suggest that there are real problems in the reproducibility and replicability of the psychology literature. But there’s no evidence that things have gotten worse. If anything, we are optimistic about the changes in practices that have happened in the last ten years. So in that sense, we are not sure that a crisis narrative is warranted.</p>
<p>On the other hand, for <span class="citation" data-cites="kuhn1962">Kuhn (<a href="#ref-kuhn1962" role="doc-biblioref">1962</a>)</span>, the term “crisis” had a special meaning: it is a period of intense uncertainty in a scientific field brought on by the failure of a particular paradigm (<a href="002-theories.html"><span>Chapter&nbsp;2</span></a>). A crisis typically heralds a shift in paradigm, in which new approaches and phenomena come to the fore.</p>
<p>In this sense, the replication crisis narrative isn’t mutually exclusive with other crisis narratives, including the “generalizability crisis” <span class="citation" data-cites="yarkoni2020">(<a href="#ref-yarkoni2020" role="doc-biblioref">Yarkoni 2020</a>)</span> and the “theory crisis” <span class="citation" data-cites="oberauer2019">(<a href="#ref-oberauer2019" role="doc-biblioref">Oberauer and Lewandowsky 2019</a>)</span>. All of these are symptoms of discontent with the status quo. We share this discontent! We are writing this book to encourage further changes in experimental methods and practices to improve reproducibility and replicability outcomes – many of them driven by the broader set of ideas referred to as “open science.” These changes may not lead to a paradigm shift in the Kuhnian sense, but we hope that they lead to eventual improvements. In that sense, we think agree with those who say that the “replication crisis” has led to a “credibility revolution” <span class="citation" data-cites="vazire2018">(<a href="#ref-vazire2018" role="doc-biblioref">Vazire 2018</a>)</span>.</p>
</section>
</section>
<section id="chapter-summary-replication" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="chapter-summary-replication"><span class="header-section-number">3.5</span> Chapter summary: Replication</h2>
<p>In this chapter we introduce the notions of reproducibility – getting the same numbers from the same analysis – and replicability – getting the same conclusions from a new dataset. Both of these are critical prerequisites of a cumulative scientific literature, yet the meta-science literature has suggested that the rate of both reproducibility and replicability in the published literature is quite a bit lower than we would hope. A strong candidate explanation for low reproducibility is simply that code and data are rarely shared alongside published research. Lowered replicability is more difficult to explain, but our best guess is that analytic flexibility (“<span class="math inline">\(p\)</span>-hacking”) is at least partially to blame. On our account, replication is a meta-scientific tool for understanding the status of the scientific literature rather than an end in itself. Instead, we see the open science movement, a movement focused on the role of transparency in the scientific process, as a promising response to issues of reproducibility and replicability.</p>
<div class="callout callout-style-default callout-note callout-titled" title="discussion questions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
discussion questions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>How would you design a measure of the context sensitivity of an experiment? Think of a measure you could apply <em>post hoc</em> to a description of an experiment (e.g., from reading a paper) so that you could take a group of experiments and annotate how context-sensitive they are on some scale.</p></li>
<li><p>Take the measure you designed above. How would you test that this measure really captured context sensitivity in a way that was not circular? What would be an “objective measure” of context sensitivity?</p></li>
<li><p>What proportion of reproducibility failures do you think are due to questionable practices by experimenters vs.&nbsp;just plain errors? How would you test your hypothesis?</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="readings">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p>Still a very readable and entertaining introduction to the idea of p-hacking: Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359-1366. <a href="https://doi.org/10.1177/0956797611417632" class="uri">https://doi.org/10.1177/0956797611417632</a>.</p></li>
<li><p>A recent review of issues of replication in psychology: Nosek, B. et al.&nbsp;(2022). Replicability, Robustness, and Reproducibility in Psychological Science. Annual Review of Psychology, 73, 719-748. <a href="https://doi.org/10.1146/annurev-psych-020821-114157" class="uri">https://doi.org/10.1146/annurev-psych-020821-114157</a>.</p></li>
</ul>
</div>
</div>
</div>
<!-- \refs -->


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-anderson2016" class="csl-entry" role="listitem">
Anderson, CJ, S Bahnik, M Barnett-Cowan, FA Bosco, J Chandler, CR Chartier, and otherss. 2016. <span>“Response to Comment on <span>‘Estimating the Reproducibility of Psychological Science’</span>.”</span> <em>Science</em> 351 (6277): 1037–37.
</div>
<div id="ref-artner2020" class="csl-entry" role="listitem">
Artner, Richard, Thomas Verliefde, Sara Steegen, Sara Gomes, Frits Traets, Francis Tuerlinckx, and Wolf Vanpaemel. 2020. <span>“The Reproducibility of Statistical Results in Psychological Research: An Investigation Using Unpublished Raw Data.”</span> <em>Psychological Methods</em>.
</div>
<div id="ref-bakker2011" class="csl-entry" role="listitem">
Bakker, Marjan, and Jelte M Wicherts. 2011. <span>“The (Mis) Reporting of Statistical Results in Psychology Journals.”</span> <em>Behavior Research Methods</em> 43 (3): 666–78.
</div>
<div id="ref-baribault2018" class="csl-entry" role="listitem">
Baribault, Beth, Chris Donkin, Daniel R Little, Jennifer S Trueblood, Zita Oravecz, Don Van Ravenzwaaij, Corey N White, Paul De Boeck, and Joachim Vandekerckhove. 2018. <span>“Metastudies for Robust Tests of Theory.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2607–12.
</div>
<div id="ref-bench2017" class="csl-entry" role="listitem">
Bench, Shane W, Grace N Rivera, Rebecca J Schlegel, Joshua A Hicks, and Heather C Lench. 2017. <span>“Does Expertise Matter in Replication? An Examination of the Reproducibility Project: Psychology.”</span> <em>Journal of Experimental Social Psychology</em> 68: 181–84.
</div>
<div id="ref-buckheit1995" class="csl-entry" role="listitem">
Buckheit, Jonathan B, and David L Donoho. 1995. <span>“Wavelab and Reproducible Research.”</span> In <em>Wavelets and Statistics</em>, 55–81. Springer.
</div>
<div id="ref-camerer2016" class="csl-entry" role="listitem">
Camerer, Colin F, Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2016. <span>“Evaluating Replicability of Laboratory Experiments in Economics.”</span> <em>Science</em> 351 (6280): 1433–36.
</div>
<div id="ref-camerer2018" class="csl-entry" role="listitem">
Camerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. <span>“Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015.”</span> <em>Nature Human Behaviour</em> 2 (9): 637–44.
</div>
<div id="ref-carney2016" class="csl-entry" role="listitem">
Carney, Dana R. 2016. <span>“My Position on Power Poses.”</span> <em>Unpublished Manuscript. Haas School of Business, University of California</em>. <a href="https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf">https://faculty.haas.berkeley.edu/dana_carney/pdf_my%20position%20on%20power%20poses.pdf</a>.
</div>
<div id="ref-carney2010" class="csl-entry" role="listitem">
Carney, Dana R, Amy JC Cuddy, and Andy J Yap. 2010. <span>“Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance.”</span> <em>Psychological Science</em> 21 (10): 1363–68.
</div>
<div id="ref-cesana-arlotti2018" class="csl-entry" role="listitem">
Cesana-Arlotti, N, A Martı́n, E Téglás, L Vorobyova, R Cetnarski, and L L Bonatti. 2018. <span>“Erratum for the Report <span>‘Precursors of Logical Reasoning in Preverbal Human Infants’</span>.”</span> <em>Science</em> 361 (6408).
</div>
<div id="ref-dominus2017" class="csl-entry" role="listitem">
Dominus, Susan. 2017. <span>“When the Revolution Came for Amy Cuddy.”</span> <em>When the Revolution Came for Amy Cuddy</em>. <a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html">https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html</a>.
</div>
<div id="ref-dreber2015" class="csl-entry" role="listitem">
Dreber, Anna, Thomas Pfeiffer, Johan Almenberg, Siri Isaksson, Brad Wilson, Yiling Chen, Brian A Nosek, and Magnus Johannesson. 2015. <span>“Using Prediction Markets to Estimate the Reproducibility of Scientific Research.”</span> <em>Proceedings of the National Academy of Sciences</em> 112 (50): 15343–47.
</div>
<div id="ref-ebersole2020" class="csl-entry" role="listitem">
Ebersole, Charles R, Maya B Mathur, Erica Baranski, Diane-Jo Bart-Plange, Nicholas R Buttrick, Christopher R Chartier, Katherine S Corker, et al. 2020. <span>“Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 3 (3): 309–31.
</div>
<div id="ref-errington2021investigating" class="csl-entry" role="listitem">
Errington, Timothy M, Maya Mathur, Courtney K Soderberg, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021. <span>“Investigating the Replicability of Preclinical Cancer Biology.”</span> <em>Elife</em> 10: e71601.
</div>
<div id="ref-etz2016" class="csl-entry" role="listitem">
Etz, Alexander, and Joachim Vandekerckhove. 2016. <span>“A Bayesian Perspective on the Reproducibility Project: Psychology.”</span> <em><span>PLOS</span> <span>ONE</span></em> 11 (2): e0149794. <a href="https://doi.org/10.1371/journal.pone.0149794">https://doi.org/10.1371/journal.pone.0149794</a>.
</div>
<div id="ref-frank2012" class="csl-entry" role="listitem">
Frank, Michael C, and Rebecca Saxe. 2012. <span>“Teaching Replication.”</span> <em>Perspectives on Psychological Science</em> 7: 595–99.
</div>
<div id="ref-frank2013" class="csl-entry" role="listitem">
Frank, Michael C, Jonathan A Slemmer, Gary F Marcus, and Scott P Johnson. 2013. <span>“" Information from Multiple Modalities Helps 5-Month-Olds Learn Abstract Rules": Erratum.”</span>
</div>
<div id="ref-gelman2018" class="csl-entry" role="listitem">
Gelman, Andrew. 2018. <span>“Don’t Characterize Replications as Successes or Failures.”</span> <em>Behavioral and Brain Sciences</em> 41.
</div>
<div id="ref-gilbert2016" class="csl-entry" role="listitem">
Gilbert, Daniel T, Gary King, Stephen Pettigrew, and Timothy D Wilson. 2016. <span>“Comment on <span>‘Estimating the Reproducibility of Psychological Science’</span>.”</span> <em>Science</em> 351 (6277): 1037–37.
</div>
<div id="ref-gould1991" class="csl-entry" role="listitem">
Gould, Stephen Jay. 1991. <span>“Royal Shorthand.”</span> <em>Science</em> 251 (4990): 142–42.
</div>
<div id="ref-gould1996" class="csl-entry" role="listitem">
Gould, Stephen Jay, Steven James Gold, et al. 1996. <em>The Mismeasure of Man</em>. WW Norton &amp; company.
</div>
<div id="ref-hardwicke2021a" class="csl-entry" role="listitem">
Hardwicke, Tom E, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michèle B Nuijten, Benjamin N Peloquin, Benjamin E deMayo, Bria Long, Erica J Yoon, and Michael C Frank. 2021. <span>“Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal Psychological Science : An Observational Study.”</span> <em>Royal Society Open Science</em>.
</div>
<div id="ref-hardwicke2018b" class="csl-entry" role="listitem">
Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne, George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al. 2018. <span>“Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal Cognition.”</span>
</div>
<div id="ref-hardwicke2021c" class="csl-entry" role="listitem">
Hardwicke, Tom E, Robert T. Thibault, Jessica Kosie, Joshua D. Wallach, Mallory C. Kidwell, and John Ioannidis. 2021. <span>“Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psychology (2014-2017).”</span> <em>Perspectives on Psychological Science</em>. <a href="https://doi.org/10.1177/1745691620979806">https://doi.org/10.1177/1745691620979806</a>.
</div>
<div id="ref-john2012" class="csl-entry" role="listitem">
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012. <span>“Measuring the Prevalence of Questionable Research Practices with Incentives for Truth Telling.”</span> <em>Psychological Science</em> 23 (5): 524–32.
</div>
<div id="ref-klein2018b" class="csl-entry" role="listitem">
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams, Reginald B Adams Jr, Sinan Alper, Mark Aveyard, et al. 2018. <span>“Many Labs 2: Investigating Variation in Replicability Across Samples and Settings.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (4): 443–90.
</div>
<div id="ref-kuhn1962" class="csl-entry" role="listitem">
Kuhn, Thomas. 1962. <em>The Structure of Scientific Revolutions</em>. Princeton University Press.
</div>
<div id="ref-lewis2016" class="csl-entry" role="listitem">
Lewis, Molly L, and Michael C Frank. 2016. <span>“Understanding the Effect of Social Context on Learning: A Replication of Xu and Tenenbaum (2007b).”</span> <em>Journal of Experimental Psychology: General</em> 145 (9): e72.
</div>
<div id="ref-mathur2020" class="csl-entry" role="listitem">
Mathur, Maya B, and Tyler J VanderWeele. 2020. <span>“New Statistical Metrics for Multisite Replication Projects.”</span> <em>J. R. Stat. Soc. Ser. A Stat. Soc.</em> 183 (3): 1145–66.
</div>
<div id="ref-mcshane2014" class="csl-entry" role="listitem">
McShane, Blakeley B, and Ulf Böckenholt. 2014. <span>“You Cannot Step into the Same River Twice: When Power Analyses Are Optimistic.”</span> <em>Perspectives on Psychological Science</em> 9 (6): 612–25.
</div>
<div id="ref-nelson2018" class="csl-entry" role="listitem">
Nelson, Leif D, Joseph Simmons, and Uri Simonsohn. 2018. <span>“Psychology’s Renaissance.”</span> <em>Annual Review of Psychology</em> 69: 511–34.
</div>
<div id="ref-newell1973" class="csl-entry" role="listitem">
Newell, Allen. 1973. <span>“You Can’t Play 20 Questions with Nature and Win: Projective Comments on the Papers of This Symposium.”</span>
</div>
<div id="ref-nosek2021" class="csl-entry" role="listitem">
Nosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber Almenberg, Fiona Fidler, et al. 2021. <span>“Replicability, Robustness, and Reproducibility in Psychological Science.”</span> <em>Annual Review of Psychology</em>.
</div>
<div id="ref-nuijten2016" class="csl-entry" role="listitem">
Nuijten, Michèle B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha Epskamp, and Jelte M Wicherts. 2016. <span>“The Prevalence of Statistical Reporting Errors in Psychology (1985–2013).”</span> <em>Behav. Res. Methods</em> 48 (4): 1205–26.
</div>
<div id="ref-oberauer2019" class="csl-entry" role="listitem">
Oberauer, Klaus, and Stephan Lewandowsky. 2019. <span>“Addressing the Theory Crisis in Psychology.”</span> <em>Psychonomic Bulletin &amp; Review</em> 26 (5): 1596–1618.
</div>
<div id="ref-olsson2020heterogeneity" class="csl-entry" role="listitem">
Olsson-Collentine, Anton, Jelte M Wicherts, and Marcel ALM van Assen. 2020. <span>“Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size.”</span> <em>Psychological Bulletin</em> 146 (10): 922.
</div>
<div id="ref-osc2015" class="csl-entry" role="listitem">
Open Science Collaboration. 2015. <span>“Estimating the Reproducibility of Psychological Science.”</span> <em>Science</em> 349 (6251).
</div>
<div id="ref-popper2005" class="csl-entry" role="listitem">
Popper, Karl. 2005. <em>The Logic of Scientific Discovery</em>. Routledge.
</div>
<div id="ref-ramscar2016" class="csl-entry" role="listitem">
Ramscar, Michael. 2016. <span>“Learning and the Replicability of Priming Effects.”</span> <em>Current Opinion in Psychology</em> 12: 80–84.
</div>
<div id="ref-ranehill2015" class="csl-entry" role="listitem">
Ranehill, Eva, Anna Dreber, Magnus Johannesson, Susanne Leiberg, Sunhae Sul, and Roberto A Weber. 2015. <span>“Assessing the Robustness of Power Posing: No Effect on Hormones and Risk Tolerance in a Large Sample of Men and Women.”</span> <em>Psychological Science</em> 26 (5): 653–56.
</div>
<div id="ref-rohrer2015" class="csl-entry" role="listitem">
Rohrer, Doug, Harold Pashler, and Christine R Harris. 2015. <span>“Do Subtle Reminders of Money Change People’s Political Views?”</span> <em>Journal of Experimental Psychology: General</em> 144 (4): e73.
</div>
<div id="ref-rosenthal1979" class="csl-entry" role="listitem">
Rosenthal, Robert. 1979. <span>“The File Drawer Problem and Tolerance for Null Results.”</span> <em>Psychological Bulletin</em> 86 (3): 638.
</div>
<div id="ref-rosenthal1990" class="csl-entry" role="listitem">
———. 1990. <span>“Replication in Behavioral Research.”</span> <em>Journal of Social Behavior &amp; Personality</em> 5: 1–30.
</div>
<div id="ref-scheel2021" class="csl-entry" role="listitem">
Scheel, Anne M, Mitchell RMJ Schijen, and Daniël Lakens. 2021. <span>“An Excess of Positive Results: Comparing the Standard Psychology Literature with Registered Reports.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 4 (2): 25152459211007467.
</div>
<div id="ref-schmidt2009" class="csl-entry" role="listitem">
Schmidt, Stefan. 2009. <span>“Shall We Really Do It Again? The Powerful Concept of Replication Is Neglected in the Social Sciences.”</span> <em>Review of General Psychology</em> 13: 90–100.
</div>
<div id="ref-schwarz1983" class="csl-entry" role="listitem">
Schwarz, Norbert, and Gerald L Clore. 1983. <span>“Mood, Misattribution, and Judgments of Well-Being: Informative and Directive Functions of Affective States.”</span> <em>Journal of Personality and Social Psychology</em> 45 (3): 513.
</div>
<div id="ref-schwarz2014" class="csl-entry" role="listitem">
Schwarz, Norbert, and Fritz Strack. 2014. <span>“Does Merely Going Through the Same Moves Make for a <span>‘Direct’</span> Replication? Concepts, Contexts, and Operationalizations.”</span>
</div>
<div id="ref-simmons2011" class="csl-entry" role="listitem">
Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. <span>“False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.”</span> <em>Psychological Science</em> 22 (11): 1359–66.
</div>
<div id="ref-simmons2018" class="csl-entry" role="listitem">
———. 2018. <span>“False-Positive Citations.”</span> <em>Perspectives on Psychological Science</em> 13 (2): 255–59.
</div>
<div id="ref-simmons2017" class="csl-entry" role="listitem">
Simmons, Joseph P, and Uri Simonsohn. 2017. <span>“Power Posing: P-Curving the Evidence.”</span> <em>Psychological Science</em>.
</div>
<div id="ref-simonsohn2015" class="csl-entry" role="listitem">
Simonsohn, Uri. 2015. <span>“Small Telescopes: Detectability and the Evaluation of Replication Results.”</span> <em>Psychol. Sci.</em> 26 (5): 559–69.
</div>
<div id="ref-vadillo2016" class="csl-entry" role="listitem">
Vadillo, Miguel A, Tom E Hardwicke, and David R Shanks. 2016. <span>“Selection Bias, Vote Counting, and Money-Priming Effects: A Comment on Rohrer, Pashler, and Harris (2015) and Vohs (2015).”</span> <em>Journal of Experimental Psychology: General</em>.
</div>
<div id="ref-van-bavel2016" class="csl-entry" role="listitem">
Van Bavel, Jay J, Peter Mende-Siedlecki, William J Brady, and Diego A Reinero. 2016. <span>“Contextual Sensitivity in Scientific Reproducibility.”</span> <em>Proceedings of the National Academy of Sciences</em> 113 (23): 6454–59.
</div>
<div id="ref-vazire2018" class="csl-entry" role="listitem">
Vazire, Simine. 2018. <span>“Implications of the Credibility Revolution for Productivity, Creativity, and Progress.”</span> <em>Perspectives on Psychological Science</em> 13 (4): 411–17.
</div>
<div id="ref-wicherts2006" class="csl-entry" role="listitem">
Wicherts, Jelte M, Denny Borsboom, Judith Kats, and Dylan Molenaar. 2006. <span>“The Poor Availability of Psychological Research Data for Reanalysis.”</span> <em>American Psychologist</em> 61 (7): 726.
</div>
<div id="ref-wilson2020" class="csl-entry" role="listitem">
Wilson, Brent M, Christine R Harris, and John T Wixted. 2020. <span>“Science Is Not a Signal Detection Problem.”</span> <em>Proceedings of the National Academy of Sciences</em> 117 (11): 5559–67.
</div>
<div id="ref-yang2020" class="csl-entry" role="listitem">
Yang, Yang, Wu Youyou, and Brian Uzzi. 2020. <span>“Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence.”</span> <em>Proceedings of the National Academy of Sciences</em> 117 (20): 10762–68.
</div>
<div id="ref-yarkoni2020" class="csl-entry" role="listitem">
Yarkoni, Tal. 2020. <span>“The Generalizability Crisis.”</span> <em>Behav. Brain Sci.</em> 45: 1–37.
</div>
<div id="ref-youyou2023" class="csl-entry" role="listitem">
Youyou, Wu, Yang Yang, and Brian Uzzi. 2023. <span>“A Discipline-Wide Investigation of the Replicability of Psychology Papers over the Past Two Decades.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (6): e2208863120.
</div>
<div id="ref-zwaan2018" class="csl-entry" role="listitem">
Zwaan, Rolf Antonius, Alexander Etz, Richard E Lucas, and Brent Donnellan. 2018. <span>“Making Replication Mainstream.”</span>
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./002-theories.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Theories</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./004-ethics.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ethics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>