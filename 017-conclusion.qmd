{{< include _setup.qmd >}}

::: {.content-visible when-format="html"}
# Conclusion {#sec-conclusion .unnumbered}
:::

::: {.content-visible when-format="pdf"}
# Conclusion {#sec-conclusion}
:::

::: {.callout-note title="learning goals"}
* Synthesize the viewpoint of this book
* Discuss the outlook for psychology
:::

You've made it to the end of Experimentology, our (sometimes opinionated) guide to how to run psychology experiments.

## Summarizing our approach

In this book we've tried to present a single unified approach to the why and how of running experiments. This approach begins with the goal of doing experiments: 

> Experiments are intended to make maximally unbiased, generalizable, and precise estimates of specific causal effects. 

This formulation isn't exactly how experiments are talked about in the broader field, but we hope you've started to see some of the rationale behind this approach.

- The emphasis on causal effects stems from an acknowledgement of the key role of experiments in establishing causal inferences (@sec-experiments) and the importance of causal relationships to theories (@sec-theories). 

- A desire for specificity informs our preference for simple experimental designs (@sec-design). 

- We select our sample in order to maximize the generalizability of our findings to some target population (@sec-sampling).

- We also try to minimize the possibility of bias through our decisions about data collection (@sec-collection) and data analysis (@sec-prereg). 

- A focus on estimation (@sec-estimation) helps us avoid some of the fallacies that come along with dichotomous inference (@sec-inference), and allows us to think meta-analytically (@sec-meta) about the overall evidence for a particular effect.

- The desire for precision informs our choice of reliable and valid measures (@sec-measurement). 

Woven throughout this narrative is the hope that embracing open science values throughout the experimental process will help you maximize your work. Not only is sharing your work openly an ethical responsibility (@sec-ethics), it's also a great way to minimize errors while creating valuable products that both advance scientific progress and accelerate your own career (@sec-management).

## Forward the field

As we've learned at various points in this book, there's a replication crisis [@osc2015], a theory crisis [@oberauer2019], and a generalizability crisis [@yarkoni2020] in psychology. Based on all these crises, you might think that we are pessimistic about the future of psychology. Not so.

There have been tremendous changes in psychological methods in the ten years since we began teaching our course (2012 -- 2022). When we began, it was common for incoming graduate students to describe the rampant $p$-hacking they had been encouraged to do in their undergraduate labs. Now, students join the class aware of new practices like preregistration and cognizant of problems of generalizability and theory building. It takes a long time for a field to change, but we have seen tremendous progress at every level -- from US government policies requiring transparency in the sciences all the way down to individual researchers' adoption of tools and practices that increase the efficiency of their work while also decreasing the chances of error. 

One of the most exciting trends has been the rise of meta-science, in which researchers use the tools of science to understand how to make science better [@hardwicke2020b]. Reproducibility and replicability projects (reviewed in @sec-replication) can help us measure the robustness of the scientific literature. In addition, studies that evaluate the impacts of new policies [e.g., @hardwicke2018b] -- can help stakeholders like journal editors and funders make informed choices about how to push the field towards more robust science. 

In addition to changes that correct methodological issues, the last ten years have seen the rise of "big team science" efforts that advance the field in new ways [@coles2022]. Collaborations such as the Psychological Science Accelerator [@moshontz2018] and ManyBabies [@frank2017b] allow hundreds of researchers from around the world to come together to run shared projects. These projects are enabled by open science practices like data and code sharing, and they provide a way for researchers to learn best practices via participating. In addition, by including broader and more diverse samples they can help address challenges around generalizability [@klein2018b]. 

Finally, the last ten years have seen huge progress in the use of statistical models both for understanding data [@mcelreath2018] and for describing specific psychological mechanisms [@ma2022]. In our own work we have used these models extensively and we believe that they provide an exciting toolkit for building quantitative theories that allow us to explain and to predict the human mind. 

## Final thoughts

Doing experiments is a craft, one that requires practice and attention. The first experiment you run will have limitations and issues. So will the 100th. But as you refine your skills, the quality of the studies you design will get better. Further, your own ability to judge others' experiments will improve as well, making you a more discerning consumer of empirical results. We hope you enjoy this journey!

<!-- \refs -->
