<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Experimentology - 9&nbsp; Design</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./010-sampling.html" rel="next">
<link href="./008-measurement.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-659MTW4XZ4"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-659MTW4XZ4', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./008-measurement.html">Planning</a></li><li class="breadcrumb-item"><a href="./009-design.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Experimentology</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/langcog/experimentology" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Experimentology.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./001-experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./002-theories.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Theories</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./003-replication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Replication</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./004-ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./005-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./006-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./007-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Planning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./008-measurement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Measurement</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./009-design.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./010-sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Execution</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./011-prereg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Preregistration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./012-collection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Data collection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./013-management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Project management</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reporting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./014-writing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Writing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./015-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./016-meta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Meta-analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./017-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./100-instructors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Instructor’s guide</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./101-github.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Git and GitHub</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./102-rmarkdown.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">R Markdown and Quarto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./103-tidyverse.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tidyverse</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./104-ggplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">ggplot</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#experimental-designs" id="toc-experimental-designs" class="nav-link active" data-scroll-target="#experimental-designs"><span class="header-section-number">9.1</span> Experimental designs</a>
  <ul class="collapse">
  <li><a href="#a-two-factor-experiment" id="toc-a-two-factor-experiment" class="nav-link" data-scroll-target="#a-two-factor-experiment"><span class="header-section-number">9.1.1</span> A two-factor experiment</a></li>
  <li><a href="#generalized-factorial-designs" id="toc-generalized-factorial-designs" class="nav-link" data-scroll-target="#generalized-factorial-designs"><span class="header-section-number">9.1.2</span> Generalized factorial designs</a></li>
  <li><a href="#between--vs.-within-participant-designs" id="toc-between--vs.-within-participant-designs" class="nav-link" data-scroll-target="#between--vs.-within-participant-designs"><span class="header-section-number">9.1.3</span> Between- vs.&nbsp;within-participant designs</a></li>
  <li><a href="#repeated-measurements-and-experimental-items" id="toc-repeated-measurements-and-experimental-items" class="nav-link" data-scroll-target="#repeated-measurements-and-experimental-items"><span class="header-section-number">9.1.4</span> Repeated measurements and experimental items</a></li>
  <li><a href="#discrete-and-continuous-experimental-manipulations" id="toc-discrete-and-continuous-experimental-manipulations" class="nav-link" data-scroll-target="#discrete-and-continuous-experimental-manipulations"><span class="header-section-number">9.1.5</span> Discrete and continuous experimental manipulations</a></li>
  </ul></li>
  <li><a href="#choosing-your-manipulation" id="toc-choosing-your-manipulation" class="nav-link" data-scroll-target="#choosing-your-manipulation"><span class="header-section-number">9.2</span> Choosing your manipulation</a>
  <ul class="collapse">
  <li><a href="#internal-validity-threats-confounding" id="toc-internal-validity-threats-confounding" class="nav-link" data-scroll-target="#internal-validity-threats-confounding"><span class="header-section-number">9.2.1</span> Internal validity threats: Confounding</a></li>
  <li><a href="#internal-validity-threats-placebo-demand-and-expectancy" id="toc-internal-validity-threats-placebo-demand-and-expectancy" class="nav-link" data-scroll-target="#internal-validity-threats-placebo-demand-and-expectancy"><span class="header-section-number">9.2.2</span> Internal validity threats: Placebo, demand, and expectancy</a></li>
  <li><a href="#external-validity-of-manipulations" id="toc-external-validity-of-manipulations" class="nav-link" data-scroll-target="#external-validity-of-manipulations"><span class="header-section-number">9.2.3</span> External validity of manipulations</a></li>
  </ul></li>
  <li><a href="#summary-experimental-design" id="toc-summary-experimental-design" class="nav-link" data-scroll-target="#summary-experimental-design"><span class="header-section-number">9.3</span> Summary: Experimental design</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/langcog/experimentology/blob/main/009-design.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/langcog/experimentology/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-design" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Design</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled" title="learning goals">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
learning goals
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Describe key elements of experimental design</li>
<li>Define randomization and counterbalancing strategies for removing confounds</li>
<li>Discuss strategies to design experiments that are appropriate to the populations of interest</li>
</ul>
</div>
</div>
</div>
<p>The key thesis of our book is that experiments should be designed to yield precise and unbiased measurements of a causal effect. But the causal effect of what? The manipulation! In an experiment we manipulate (intervene on) some aspect of the world and measure the effects of that manipulation. We then compare that measurement to a situation where the intervention has not occurred.</p>
<p>We refer to different intervention states as <strong>conditions</strong> of the experiment. The most common experimental design is the comparison between a <strong>control</strong> condition, in which the intervention is not performed, and an <strong>experimental</strong> (sometimes called <strong>treatment</strong>) condition in which the intervention is performed.</p>
<p>But many other experimental designs are possible. In more complex experiments, manipulations along different dimensions (sometimes called <strong>factors</strong> in this context) can be combined. In the first part of the chapter, we’ll introduce some common experimental designs and the vocabulary for describing them. Our focus here is in identifying designs that maximize <span class="smallcaps">measurement precision</span>.</p>
<div class="page-columns page-full"><p>A good experimental measure must be a valid measure of the construct of interest. The same is true for a manipulation – it must validly relate to the causal effect of interest. In the second part of the chapter, we’ll discuss issues of <strong>manipulation validity</strong>, including both issues of ecological validity and <strong>confounding</strong>. We’ll talk about how practices like <strong>randomization</strong> and <strong>counterbalancing</strong> can help remove nuisance confounds, an important part of <span class="smallcaps">bias reduction</span> for experimental designs.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;This section will draw on our introduction to causal inference in <a href="001-experiments.html"><span>Chapter&nbsp;1</span></a>, so if you haven’t read that, now’s the time.</p></li></div></div>
<p>To preview our general take-home points from this chapter: we think that your default experiment should manipulate one or two factors – usually not more – and should manipulate those factors continuously and within-participants. Although such designs are not always possible, they are typically the most likely to yield precise estimates of a particular effect that can be used to constrain future theorizing. We’ll start by considering a case study in which a subtle confound led to difficulties interpreting an experimental result.</p>
<div class="callout callout-style-default callout-note callout-titled" title="case study">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
case study
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<section id="automatic-theory-of-mind" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="automatic-theory-of-mind">Automatic theory of mind?</h2>
<p>In an early version of our course, student Desmond Ong set out to replicate a thought-provoking finding: both infants and adults seemed to show evidence of tracking other agents’ belief state, even when it was irrelevant to the task at hand <span class="citation" data-cites="kovacs2010">(<a href="#ref-kovacs2010" role="doc-biblioref">Kovács, Téglás, and Endress 2010</a>)</span>. In the paradigm, an animated Smurf character would watch as a self-propelled ball came in and out from behind a screen. At the end of the video, the screen would swing down and the participant had to respond whether the ball was present or absent. Reaction time for this decision was the key dependent variable.</p>
<p>The experimental design investigated two factors: whether the participant believed the ball was present or absent (P+/P-) and whether the animated agent <em>would have believed</em> the ball was present or absent (A+/A-) based on what it saw. The result was four conditions: P+/A+, P+/A-, P-/A+, and P-/A-. (We could call this a <strong>fully crossed</strong> design because each level of one factor was presented with each level of the other).</p>
<div class="cell" data-hash="009-design_cache/html/fig-design-kovacs-original_dbbc3d16fb3bce6bc2b6fcc64032cef8">
<div class="cell-output-display">
<div id="fig-design-kovacs-original" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="009-design_files/figure-html/fig-design-kovacs-original-1.png" class="img-fluid figure-img" style="width:45.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9.1: Original data from <span class="citation" data-cites="kovacs2010">Kovács, Téglás, and Endress (<a href="#ref-kovacs2010" role="doc-biblioref">2010</a>)</span>. Error bars show 95% confidence intervals. Based on <span class="citation" data-cites="phillips2015">Phillips et al. (<a href="#ref-phillips2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>Both the original experiments and the replication that Desmond ran showed a significant effect of the agent’s beliefs on participants’ reaction times, suggesting that what the – totally irrelevant – agent thought about the ball was leading them to react more or less quickly to the presence of the ball. <a href="#fig-design-kovacs-original">Figure&nbsp;<span>9.1</span></a> shows the original data (N=24). But although both studies showed an effect of agent belief, the replication and several variations also showed a crossover <strong>interaction</strong> of participant and agent belief. The participants were slower when the agents <em>and</em> the participants believed that the ball was behind the screen (<a href="#fig-design-kovacs-replication">Figure&nbsp;<span>9.2</span></a>). That finding wasn’t consistent with the theory that tracking inconsistent beliefs slowed down reaction times. If participants were tracking their own beliefs about the ball <em>and</em> the agent’s, they should have been fastest in the P+/A+ condition, not slower.</p>
<div class="cell" data-hash="009-design_cache/html/fig-design-kovacs-replication_df3d7a9f37703c4ff6eed66f3a83e147">
<div class="cell-output-display">
<div id="fig-design-kovacs-replication" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="009-design_files/figure-html/fig-design-kovacs-replication-1.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9.2: Data from a series of replications of <span class="citation" data-cites="kovacs2010">Kovács, Téglás, and Endress (<a href="#ref-kovacs2010" role="doc-biblioref">2010</a>)</span>, including versions on the web (Experiments 1a and 1b) and in lab (Experiment 1c), as well as several variations on the format of responding (Experiments 2 and 3; 2AFC = two alternative forced choice) and an experiment where a large wall kept the agent from seeing the ball at all (Experiment 4). “Hits” and “CRs” panels refer to different subsets of trials where participants responded “present” when the ball was present and “absent” when the ball was absent. Error bars are 95% confidence intervals. Based on <span class="citation" data-cites="phillips2015">Phillips et al. (<a href="#ref-phillips2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>A collaborative team working on this paradigm identified a key issue <span class="citation" data-cites="phillips2015">(<a href="#ref-phillips2015" role="doc-biblioref">Phillips et al. 2015</a>)</span>. There was a <strong>confound</strong> in the experimental design – another factor that varied across conditions besides the target factors. In other words, something was changing between conditions other than the agent’s and participant’s belief states. The confound was an attention check (discussed further in <a href="012-collection.html"><span>Chapter&nbsp;12</span></a>): participants had to press a key when the agent left the scene to show that they were paying attention. This attention check appeared a few seconds later in the videos for the P+/A+ and P-/A- trials – the ones that yielded the slow reaction times – than it did for the other two. When the attention check was removed or when its timing was equalized across conditions, reaction time effects were eliminated, suggesting that the original pattern of findings may have been due to the confound.</p>
<p>If the standard for replication is significance of particular statistical tests at <em>p&lt;.05</em>, then this experiment replicated successfully. But the effect estimates were inconsistent with the proposed theoretical explanation. A finding can be replicable without providing support for the underlying theory!</p>
<p>There’s an important caveat to this story. The followup work <em>only</em> revealed that there was a confound in one particular experimental operationalization, and did not provide evidence against automatic theory of mind in general. Indeed, others have suggested that different versions of this paradigm <em>do</em> reveal evidence for theory of mind processing once the confound is eliminated <span class="citation" data-cites="el-kaddouri2020">(<a href="#ref-el-kaddouri2020" role="doc-biblioref">El Kaddouri et al. 2020</a>)</span>.</p>
</section>
</div>
</div>
<section id="experimental-designs" class="level2 page-columns page-full" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="experimental-designs"><span class="header-section-number">9.1</span> Experimental designs</h2>
<div class="page-columns page-full"><p>Experimental designs are fundamental to many fields; unfortunately the terminology used to describe them can vary, which can get quite confusing! Here we will mostly describe an experiment as a relationship between some manipulation(s), in which participants are randomly assigned to experimental conditions to estimate effects on some measure. Factors are the dimensions along which manipulations vary. For example, in our case study above, the two factors were participant belief and agent belief. Another terminology it’s good to be familiar with is the terms used in Chapters <a href="005-estimation.html"><span>5</span></a>–<a href="007-models.html"><span>7</span></a>, which are often used in econometrics and statistics: <strong>treatment</strong> (manipulation) and <strong>outcome</strong> (measure).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Terminology here is hard. In psychology people sometimes say there’s an <strong>independent variable</strong> (the manipulation, which is causally prior and hence “independent” of other causal influences) and a <strong>dependent variable</strong> (the measure, which causally depends on the manipulation, or so we hypothesize). We find this terminology to be hard to remember because the terms are so different from the actual concepts being described.</p></li></div></div>
<p>In this section, we’ll discuss key dimensions on which experiments vary: 1) how many factors they incorporate and how these factors are crossed, 2) how many conditions and measures are given to each participant, and 3) if manipulations have discrete levels or fall on a continuous scale.</p>
<section id="a-two-factor-experiment" class="level3 page-columns page-full" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="a-two-factor-experiment"><span class="header-section-number">9.1.1</span> A two-factor experiment</h3>
<p>The classical “design of experiments” framework has as its goal to separate observed variability in the dependent measure into 1) variability due to the manipulation(s) and (2) other variability, including measurement error and participant-level variation. This framework maps nicely onto the statistical framework described in Chapters <a href="005-estimation.html"><span>5</span></a>–<a href="007-models.html"><span>7</span></a>. In essence, this framework models the distribution of the measure using the condition structure of our experiment as the predictor.</p>
<p>Different experimental designs will allow us to estimate specific effects more and less effectively. Recall in <a href="005-estimation.html"><span>Chapter&nbsp;5</span></a>, we estimated the effect of our tea/milk order manipulation by a simple subtraction: <span class="math inline">\(\beta = \theta_{T} - \theta_{C}\)</span> (where <span class="math inline">\(\beta\)</span> is the effect estimate, and <span class="math inline">\(\theta\)</span>s indicate the estimates for each condition, treatment <span class="math inline">\(T\)</span> and control <span class="math inline">\(C\)</span>; we called them <span class="math inline">\(\theta_T\)</span> and <span class="math inline">\(\theta_M\)</span> in that chapter to denote tea- and milk-first conditions). This logic works just fine also if there are two distinct treatments in a three condition experiment: each treatment can be compared to control separately. For treatment 1, <span class="math inline">\(\beta_{T_1} = \theta_{T_1} - \theta_{C}\)</span> and <span class="math inline">\(\beta_{T_2} = \theta_{T_2} - \theta_{C}\)</span>.</p>

<div class="no-row-height column-margin column-container"><div id="fig-design-young-design" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/design/young2007-design.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9.3: The 2x2 crossed design used in <span class="citation" data-cites="young2007">Young et al. (<a href="#ref-young2007" role="doc-biblioref">2007</a>)</span></figcaption>
</figure>
</div></div><p>This logic is going to get more complicated if we have more than one distinct factor of interest, though. Let’s look at an example.</p>
<p><span class="citation" data-cites="young2007">Young et al. (<a href="#ref-young2007" role="doc-biblioref">2007</a>)</span> were interested in how moral judgments depend on both the beliefs of actors and the outcomes of their actions. They presented participants with vignettes in which they learned, for example, that Grace visits a chemical factory with her friend and goes to the coffee break room, where she sees a white powder that she puts in her friend’s coffee. They then manipulated both Grace’s <em>beliefs</em> and the <em>outcomes</em> of her action following the schema in <a href="#fig-design-young-design">Figure&nbsp;<span>9.3</span></a>. Participants (N=10) used a four-point Likert scale to rate whether the actions were morally forbidden (1) or permissible (4). <a href="#fig-design-young-data">Figure&nbsp;<span>9.4</span></a> shows the data.</p>
<div id="fig-design-young-data" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/young2007-data.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.4: Moral permissability as a function of belief and outcome. Results from <span class="citation" data-cites="young2007">Young et al. (<a href="#ref-young2007" role="doc-biblioref">2007</a>)</span>, annotated with the estimated effects. Simple effects measure differences between the individual conditions and the neutral belief, neutral outcome condition. The interaction measures the difference between the predicted sum of the two simple effects and the actual observed data for the negative belief, negative outcome condition.</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>Young et al.’s design has two factors – belief and outcome – each with two levels (neutral and negative, noted as <span class="math inline">\(B\)</span> and <span class="math inline">\(-B\)</span> for belief and <span class="math inline">\(O\)</span> and <span class="math inline">\(-O\)</span> for outcome).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> These factors are <strong>fully crossed</strong>: each level of each factor is combined with each level of each other.</p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;Neither of these is necessarily a “control” condition: the goal is simply to compare these two levels of the factor – negative and neutral – to estimate the effect due to the factor.</p></li></div></div>
<!-- That means that we can estimate a number of effects of interest. The experimental data are shown in  -->
<p>This fully-crossed design makes it easy for us to estimate quantities of interest. Let’s say that our <strong>reference</strong> group (equivalent to the control group for now) is neutral belief, neutral outcome. Now it’s easy to use the same kind of subtraction we did before to estimate particular effects we care about. For example, we can look at the effect of negative belief in the case of a neutral outcome: <span class="math inline">\(\beta_{-B,O} = \theta_{-B,O} - \theta_{B,O}\)</span>. This effect is shown on the left side of <a href="#fig-design-young-data">Figure&nbsp;<span>9.4</span></a>. <!-- The effect of a negative outcome is computed similarly as $\beta_{B,-O} = \theta_{B,-O} - \theta_{B,O}$. --></p>
<div class="page-columns page-full"><p>But now there is a complexity: these two <strong>simple effects</strong> (effects of one variable at a particular level of another variable) together suggest that the combined effect <span class="math inline">\(\beta_{-B,-O}\)</span> in the negative belief, negative outcome condition should be equal to the sum of <span class="math inline">\(\beta_{-B,O}\)</span> and <span class="math inline">\(\beta_{B,-O}\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> As we can see from <a href="#fig-design-young-data">Figure&nbsp;<span>9.4</span></a>, that’s not right. If it were, the negative belief, negative outcome condition would be below the minimum possible rating!</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;If you’re interested, you can also compute the <strong>average</strong> or <strong>main</strong> effect of a particular factor via the same subtractive logic. For example, the average effect of negative belief (<span class="math inline">\(-B\)</span>) vs.&nbsp;a neutral belief (<span class="math inline">\(B\)</span>) can be computed as <span class="math inline">\(\beta_{-B} = \frac{(\theta_{-O, -B} + \theta_{O, -B}) - (\theta_{-O, B} + \theta_{O, B})}{2}\)</span>.</p></li></div></div>
<div class="page-columns page-full"><p>Instead, we observe an <strong>interaction</strong> effect (sometimes called a <strong>two-way interaction</strong> when there are two factors): The effect when both factors are present is different than the sum of the two simple effects. To capture this effect, we need an interaction term: <span class="math inline">\(\beta_{-B,-O}\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> In other words, the effect of negative beliefs (intent) on subjective moral permissibility depends on whether the action caused harm. Critically, without a fully-crossed design, we can’t estimate this interaction and we would have made an incorrect prediction about one condition.</p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;If you’re reading carefully, you might be thinking that this all sounds like we’re talking about the analysis of variance (ANOVA), not about experimental design per se. These two topics are actually the same topic! The question is how to design an experiment so that these statistical models can be used to estimate particular effects – and combinations of effects – that we care about. In case you missed it, we discuss modeling interactions in a regression framework in <a href="007-models.html"><span>Chapter&nbsp;7</span></a>.</p></li></div></div>
</section>
<section id="generalized-factorial-designs" class="level3 page-columns page-full" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored"><span class="header-section-number">9.1.2</span> Generalized factorial designs</h3>
<p>Young et al.’s design, in which there are two factors with two levels each, is called a <strong>2x2 design</strong> (pronounced “two by two”). 2x2 designs are incredibly common and useful, but they are only one of an infinite variety of such designs that can be constructed.</p>
<p>Say we added a third factor to Young et al.’s design such that Grace either feels neutral towards her friend or is angry on that day. If we fully crossed this third affective factor with the other two (belief and outcome), we’d have a 2x2x2 design. This design would have eight conditions: <span class="math inline">\((A, B, O)\)</span>, <span class="math inline">\((A, B, -O)\)</span>, <span class="math inline">\((A, -B, O)\)</span>, <span class="math inline">\((A, -B, -O)\)</span>, <span class="math inline">\((-A, B, O)\)</span>, <span class="math inline">\((-A, B, -O)\)</span>, <span class="math inline">\((-A, -B, O)\)</span>, <span class="math inline">\((-A, -B, -O)\)</span>. These conditions would in turn allow us to estimate both two-way and three-way interactions, enumerated in <a href="#tbl-three-way">Table&nbsp;<span>9.1</span></a>.</p>
<div id="tbl-three-way" class="anchored">
<table class="table">
<caption>Table&nbsp;9.1: Effects in a 2x2x2 design with affect, belief, and outcome as factors.</caption>
<thead>
<tr class="header">
<th>Effect</th>
<th>Term Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Affect</td>
<td>Main effect</td>
</tr>
<tr class="even">
<td>Belief</td>
<td>Main effect</td>
</tr>
<tr class="odd">
<td>Outcome</td>
<td>Main effect</td>
</tr>
<tr class="even">
<td>Affect X Belief</td>
<td>2-way interaction</td>
</tr>
<tr class="odd">
<td>Affect X Outcome</td>
<td>2-way interaction</td>
</tr>
<tr class="even">
<td>Belief X Outcome</td>
<td>2-way interaction</td>
</tr>
<tr class="odd">
<td>Affect X Belief X Outcome</td>
<td>3-way interaction</td>
</tr>
</tbody>
</table>
</div>
<p>Three-way interactions are hard to think about! The affect X belief X outcome interaction tells you about the difference in moral permissibility that’s due to all three factors being present as opposed to what you’d predict on the basis of your estimates of the two-way interactions. In addition to being hard to think about, higher order interactions tend to be hard to estimate, because estimating them accurately requires you to have a stable estimate of all of the lower-order interactions <span class="citation" data-cites="mcclelland1993">(<a href="#ref-mcclelland1993" role="doc-biblioref">McClelland and Judd 1993</a>)</span>. For this reason, we recommend against experimental designs that rely on higher-order interactions unless you are in a situation where you both have strong predictions about these interactions and are confident in your ability to estimate them appropriately.</p>
<div class="page-columns page-full"><p>Things can get even more complicated. If you have three factors with two levels each, as in the example above (<a href="#tbl-three-way">Table&nbsp;<span>9.1</span></a>), you can estimate 7 total effects of interest. But if you have <em>four</em> factors with two levels each, you get 15. Four factors with <em>three</em> levels each gets you a horrifying 80 different effects!<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> This way lies madness, at least from the perspective of estimating and interpreting individual effects in a reasonable sample size. Again, we suggest starting with one- and two-factor designs. There is a lot to be learned from simple designs that follow good measurement and sampling practices.</p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;The general formula for <span class="math inline">\(N\)</span> factors with <span class="math inline">\(M\)</span> levels each is <span class="math inline">\(M^N-1\)</span>.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="depth">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
depth
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<section id="estimation-strategies-for-generalized-factorial-designs" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="estimation-strategies-for-generalized-factorial-designs">Estimation strategies for generalized factorial designs</h2>
<p>So what should you do if you really do care about four or more factors – in the sense that you want to estimate their effects and include them in your theory? The simplest strategy is to start your research off by measuring them independently in a series of single-factor experiments. This kind of setup is natural when there is a single reference level for each factor of interest, and such experiments can provide a basis for judging which factors are most important for your outcome and hence which should be prioritized for experiments to estimate interactions.</p>
<p>On the other hand, sometimes there is no reference level for a factor. For example, in the <span class="citation" data-cites="kovacs2010">Kovács, Téglás, and Endress (<a href="#ref-kovacs2010" role="doc-biblioref">2010</a>)</span> paradigm, it’s not clear whether a positive or negative belief is the reference level. That’s not a problem in a fully-crossed design like theirs, but this situation can pose a problem if you have more than two such factors. Ideally you would want to run independent experiments, but you have to choose some level for all of the other variables – you can’t just assume that one level is “neutral.”</p>
<p>One solution that lets you compute main effects but not interactions is called a <strong>Latin square</strong>. Latin squares are a good solution for three-factor designs, which is the level at which a fully-crossed design typically gets overwhelming. <!-- ^[There's a variant called the "Greco-Latin square" for four factors, in case you need that.] --> A Latin square is an <span class="math inline">\(n x n\)</span> matrix in which each number occurs exactly once in each row and column, e.g. <span class="math display">\[\begin{bmatrix}
    1 &amp; 2 &amp; 3 \\
    2 &amp; 3 &amp; 1\\
    3 &amp; 1 &amp; 2 \\
    \end{bmatrix}\]</span> This Latin square for <span class="math inline">\(n=3\)</span> gives the solution for how to balance factors across a 3x3x3 experiment. The row number is one factor, the column number is the second factor, and the number in the cell is the third factor. So one condition would be (1,1,1), the first level of all factors, shown in the upper left cell. Another would be (3,3,2), the lower right cell. Although a fully-crossed design would require 27 cells to be run, the Latin square has only nine. Critically, the combinations of factors are balanced across the nine cells so that the average effect of each level of the three factors can be estimated. <!-- ^[You can check and see that no interactions can be estimated, because no factor co-occurs with two different levels of another factor.]   --></p>
<p>There are also fancier methods available. For example, the literature on <strong>optimal experiment design</strong> contains methods for choosing the most informative sequence of experiments to run in order to estimate the parameters in a model that can include many factors and their interactions <span class="citation" data-cites="myung2009">(<a href="#ref-myung2009" role="doc-biblioref">Myung and Pitt 2009</a>)</span>. Going down this road typically means having an implemented computational theory of your domain, but it can be a very productive strategy for exploring a complex experimental space with many factors.</p>
</section>
</div>
</div>
</section>
<section id="between--vs.-within-participant-designs" class="level3 page-columns page-full" data-number="9.1.3">
<h3 data-number="9.1.3" class="anchored" data-anchor-id="between--vs.-within-participant-designs"><span class="header-section-number">9.1.3</span> Between- vs.&nbsp;within-participant designs</h3>

<div class="no-row-height column-margin column-container"><div id="fig-design-between" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/design/between.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9.5: A between-participants design.</figcaption>
</figure>
</div></div><p>Once you know what factor(s) you would like to manipulate in your experiment, the next step is to consider how these will be presented to participants, and how that presentation will interact with your measurements. The biggest decision to be made is whether each participant will experience one level of a factor – a <strong>between-participants</strong> design – or whether they will experience multiple levels – a <strong>within-participants</strong> design. <a href="#fig-design-between">Figure&nbsp;<span>9.5</span></a> shows a simple example of between-participants design with four participants (two assigned to each condition), while <a href="#fig-design-within">Figure&nbsp;<span>9.6</span></a> shows a within-participants version of the same design.</p>
<div id="fig-design-within" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/within.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.6: A within-participants design, counterbalanced for order (discussion of counterbalancing below).</figcaption>
</figure>
</div>
<p>Because people are very variable, the decision whether to measure a particular factor between- or within-participants is consequential. Imagine we’re estimating our treatment effect as before, simply by computing <span class="math inline">\(\widehat{\beta} = \widehat{\theta}_{T} - \widehat{\theta}_{C}\)</span> with each of these estimates from different populations of participants. In this scenario, our estimate <span class="math inline">\(\widehat{\beta}\)</span> contains three components: 1) the true differences between <span class="math inline">\(\theta_{T}\)</span> and <span class="math inline">\(\theta_{C}\)</span>, 2) sampling-related variation in which participants from the population ended up in the samples for the two conditions, and 3) measurement error. Component #2 is present because any two samples of participants from a population will differ in their average on a measure – this is precisely the kind of sampling variation we saw in the null distributions in <a href="006-inference.html"><span>Chapter&nbsp;6</span></a>.</p>
<div class="page-columns page-full"><p>When our experimental design is within-participants, component #2 is not present, because participants in both conditions are sampled from the <em>same</em> population. If we get unlucky and all of our participants are lower than the population mean on our measure, then that unluckiness affects our conditions equally. The consequences for choosing an appropriate sample size are fairly extreme: Between-participants designs typically require between two and eight times as many participants as within-participants designs!<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;If you want to estimate how big an advantage you get from within-participants data collection, you need to know how correlated (reliable) your observations are. One analysis of this issue <span class="citation" data-cites="lakens2016">(<a href="#ref-lakens2016" role="doc-biblioref">Lakens 2016</a>)</span> suggests that the key relationship is that <span class="math inline">\(N_{within} = N_{between} (1-\rho) /2\)</span> where <span class="math inline">\(\rho\)</span> is the correlation between the measurement of the two conditions within individuals. The more correlated they are, the smaller your within-participants <span class="math inline">\(N\)</span>.</p></li></div></div>
<p>Given these advantages, why would you consider using a between-participants design? A within-participants design is simply not possible for all experiments. For example, consider a medical intervention – say, a new surgical procedure that is being compared to an established one. Patients cannot receive two different procedures, and so no within-participant comparison is possible.</p>
<div class="page-columns page-full"><p>Most manipulations in the behavioral sciences are not so extreme, but it still may be impractical or inadvisable to deliver multiple conditions. <span class="citation" data-cites="greenwald1976">Greenwald (<a href="#ref-greenwald1976" role="doc-biblioref">1976</a>)</span> distinguishes three types of undesirable effects:<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;We tend to think of all of these as being forms of carry-over effect, and sometimes use this label as a catch-all description. Some people also use the picturesque description “poisoning the well” <span class="citation" data-cites="gelman2017">(<a href="#ref-gelman2017" role="doc-biblioref">Gelman 2017</a>)</span> – earlier conditions “ruin” the data for later conditions.</p></li></div></div>
<ul>
<li><strong>Practice effects</strong> occur when administering the measure or the treatment will lead to change. Imagine a curriculum intervention for teaching a math concept – it would be hard to convince a school to teach the same topic to students twice, and the effect of the second round of teaching would likely be quite different than the first!</li>
<li><strong>Sensitization effects</strong> occur when seeing two versions of an intervention mean that you might respond differently to the second than the first because you have compared them and noticed the contrast. Consider a study on room lighting – if the experimenters are constantly changing the lighting, participants may become aware that lighting is the focus of the study!</li>
<li><strong>Carry-over effects</strong> refer to the case where one treatment might have a longer-lasting effect than the measurement period. For example, imagine a study in which one treatment was to make participants frustrated with an impossible puzzle; if a second condition were given after this first one, participants might still be frustrated, leading to spill-over of effects between conditions.</li>
</ul>
<p>All of these issues can lead to real concerns with respect to within-participant designs. But the desire for effect estimates that are completely unbiased by these concerns may lead to the overuse of between-participant designs <span class="citation" data-cites="gelman2017">(<a href="#ref-gelman2017" role="doc-biblioref">Gelman 2017</a>)</span>. As we mentioned above, between-participant designs come at a major cost in terms of power and precision.</p>
<div class="page-columns page-full"><p>An alternative approach is to acknowledge the possibility of carry-over type effects and seek to mitigate them. First, you can make sure that the order of condition is randomized or balanced (see below); and second, you can analyze carryover effects these within your statistical model (for example by estimating the interaction of condition and order).<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;Even when one factor must be varied between participants, it is often still possible to vary others within subjects, leading to a <strong>mixed</strong> design in which some factors are between and others within.</p></li></div></div>
<div class="page-columns page-full"><p>We summarize the state of affairs from our perspective in <a href="#fig-design-between-within">Figure&nbsp;<span>9.7</span></a>. We think that within-participant designs should be preferred whenever possible. This conclusion is also consistent with meta-research we’ve done on replications from our course: across 176 student replications, the use of a within-subjects design was the strongest correlate of a successful replication <span class="citation" data-cites="boyce2023">(<a href="#ref-boyce2023" role="doc-biblioref">Boyce, Mathur, and Frank 2023</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;Caveat: this study used an observational design, so no causal inference is possible.</p></li></div></div>
<div id="fig-design-between-within" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/between-within.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.7: Pros and cons of between- vs.&nbsp;within-participant designs. We recommend within-participant designs when possible.</figcaption>
</figure>
</div>
</section>
<section id="repeated-measurements-and-experimental-items" class="level3 page-columns page-full" data-number="9.1.4">
<h3 data-number="9.1.4" class="anchored"><span class="header-section-number">9.1.4</span> Repeated measurements and experimental items</h3>
<p>We just discussed decision-making about whether to administer multiple <em>manipulations</em> to a single participant. An exactly analogous decision comes up for <em>measures</em>! And our take-home will be similar: unless there are specific difficulties that come up, it’s usually a very good idea to make multiple measurements (via multiple experimental <strong>trials</strong>) for each participant in each condition.</p>
<p>You can create a between-participants design where you administer your manipulation and then measure multiple times. This scenario is pictured in <a href="#fig-design-rm-between">Figure&nbsp;<span>9.8</span></a>). Sometimes this works quite well. For example, imagine a transcranial magnetic stimulation (TMS) experiment: participants receive neural stimulation for a period of time, targeted at a particular region. Then they perform some measurement task repeatedly until it wears off. The more times they perform the measurement task, the better the estimate of whatever effect (when compared to a control of TMS to another region, say).</p>
<div id="fig-design-rm-between" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/rm-between.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.8: A between-participants, repeated-measures design.</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>Sometimes this design is called a <strong>repeated measures</strong> design, but terminology here is tricky again. The term “repeated measures” refers to any experiment where each participant is measured more than once, including both between-participants <em>and</em> within-participants designs.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> Our advice is <em>both</em> to use within-participants designs <em>and</em> to get multiple measurements from each participant.</p><div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;We’re talking about multiple trials with the same measure, not multiple distinct measures. As we discussed in <a href="008-measurement.html"><span>Chapter&nbsp;8</span></a>, we tend to be against measuring lots of different things in a single experiment – in part because of the concerns that we’re articulating in this chapter: if you have time, it’s better to make more precise measures of what you care about most. Measuring one thing well is hard enough. Much better to measure one thing well than many things badly.</p></li></div></div>
<p>Why? In the last subsection, we described how variability in our estimates in a between-participants design depend on three components:</p>
<ol type="1">
<li>true condition differences,</li>
<li>sampling variation between conditions, and</li>
<li>measurement error.</li>
</ol>
<p>Within-participants designs are good because they don’t include (2). Repeated measurements reduce (3): the more times you measure, the lower your measurement error, leading to greater measure reliability!</p>
<p>There are problems with repeating the same measure many times, however. Some measures can’t be repeated without altering the response. To take an obvious example, we can’t give the exact same math problem twice and get two useful measurements of mathematical ability! The typical solution to this problem is to create multiple <strong>items</strong>. In the case of a math assessment, you create multiple problems that you believe test the same concept but have different numbers or other superficial characteristics.</p>
<p>Using multiple items for measurement is good for two reasons. First, it reduces measurement error by allowing responses to be combined across items. But second, it increases the generalizability of the measurement. An effect that is consistent across many different items is more likely to be an effect that can be generalized to a whole class of stimuli – in precisely the same way that the use of multiple participants can license generalizations across a population of people <span class="citation" data-cites="clark1973">(<a href="#ref-clark1973" role="doc-biblioref">Clark 1973</a>)</span>.</p>
<div id="fig-design-pre-post" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/pre-post.png" class="img-fluid figure-img" style="width:55.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.9: A between-participants, pre-post design.</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>One variation on the repeated measures, between-participants design is a specific version where the measure is administered both before (pre-) and after (post-) intervention, as in <a href="#fig-design-pre-post">Figure&nbsp;<span>9.9</span></a>. This design is sometimes known as a <strong>pre-post</strong> design. It is extremely common in cases where the intervention is larger-scale and harder to give within-participants, such as in a field experiment where a policy or curriculum is given to one sample and not to another. The pre measurements can be used to subtract out participant-level variability and recover a more precise estimate of the treatment effect. Recall that our treatment effect in a pure between participants design is <span class="math inline">\(\beta = \theta_{T} - \theta_{C}\)</span>. In a pre-post design, we can do better by computing <span class="math inline">\(\beta = (\theta_{T_{post}} - \theta_{T_{pre}}) - (\theta_{C_{post}} - \theta_{C_{pre}})\)</span>. This equation says “how much more did the treatment group go up than the control group?<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn12"><p><sup>12</sup>&nbsp;This estimate is sometimes called a “difference in differences.” The basic idea is widely used in the field of econometrics, both in experimental and quasi-experimental cases <span class="citation" data-cites="cunningham2021">(<a href="#ref-cunningham2021" role="doc-biblioref">Cunningham 2021</a>)</span>. In practice, though, we recommend using the pre-treatment measurements as a covariate in a model-based analysis, not just doing the simple subtraction.</p></li></div></div>
<p>In sum, within-participants, repeated measurement designs are the bread and butter of most research in perception, psychophysics, and cognitive psychology. When both manipulations and measures can be repeated, these designs afford high measurement precision even with small sample sizes; they are recommended whenever possible.</p>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<section id="stimulus-specific-effects" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="stimulus-specific-effects">Stimulus-specific effects</h2>
<p>Imagine you’re a psycholinguist who has the hypothesis that nouns are processed faster than verbs. You run an experiment where you pick out ten verbs and ten nouns, then measure a large sample of participants’ reading time for each of these. You find strong evidence for the predicted effect and publish a paper on your claim. The only problem is that, at the same time, someone else has done exactly the same study – with different nouns and verbs – and published a paper making the opposite claim. When this happens, it is possible that each effect is driven by the specific experimental items that were chosen, rather than a generalization that is true of nouns and verbs in general <span class="citation" data-cites="clark1973">(<a href="#ref-clark1973" role="doc-biblioref">Clark 1973</a>)</span>.</p>
<p>The problem of generalization from sample to population is not new – as we discussed in <a href="006-inference.html"><span>Chapter&nbsp;6</span></a>, we are constantly making this kind of inference with the samples of people that participate in our experiments. Our classic statistical techniques are designed to quantify our ability to generalize from a sample of participants to a population, so we recognize that a very small sample size leads to a weak generalization. The exact same issue comes up with <em>items</em>: a very small sample of experimental items leads to a weak generalization to the population of items.</p>
<p>Item effects are kind of like accidentally finding a group of ten people whose left toes are longer than their right ones. If you continued to measure the same group’s toes, you could continue to replicate the difference in length. But that doesn’t mean it’s true of the population as a whole.</p>
<p>This kind of <strong>stimulus generalizability</strong> problem comes up across many different areas of psychology. In one example, hundreds of papers were written about a phenomenon called the “risky shift” – in which groups deliberating about a decision would produce riskier decisions than individuals. Unfortunately, this phenomenon appeared to be completely driven by the specific choice of vignettes that groups deliberated about, with some stories producing a risky shift and others producing a more conservative shift <span class="citation" data-cites="westfall2015">(<a href="#ref-westfall2015" role="doc-biblioref">Westfall, Judd, and Kenny 2015</a>)</span>.</p>
<p>Another example comes from the memory literature, where a classic paper by <span class="citation" data-cites="baddeley1975">Baddeley, Thomson, and Buchanan (<a href="#ref-baddeley1975" role="doc-biblioref">1975</a>)</span> suggested that words that take longer to pronounce (“tycoon” or “morphine”) would be remembered worse than words that took a shorter amount of time (“ember” or “wicket”) even when they had the same number of syllables. This effect also appears to be driven by the specific sets of words chosen in the original paper. It’s very replicable with that particular stimulus set but not generalizable across other sets <span class="citation" data-cites="lovatt2000">(<a href="#ref-lovatt2000" role="doc-biblioref">Lovatt, Avons, and Masterson 2000</a>)</span>.</p>
<p>The implication of these examples is clear: experimenters need to take care in both their experimental design and analysis to avoid overgeneralizing from their stimuli to a broader construct. Three primary steps can help experimenters avoid this pitfall:</p>
<ol type="1">
<li>To maximize generality, use samples of experimental items – words, pictures, or vignettes – that are comparable in size to your samples of participants.</li>
<li>When replicating an experiment, consider taking a new sample of items as well as a new sample of participants. It’s more work to draft new items, but it will lead to more robust conclusions.</li>
<li>When experimental items are sampled at random from a broader population, use a statistical model that includes this sampling process (e.g., mixed effects models with random intercepts for items from <a href="007-models.html"><span>Chapter&nbsp;7</span></a>).</li>
</ol>
</section>
</div>
</div>
</section>
<section id="discrete-and-continuous-experimental-manipulations" class="level3 page-columns page-full" data-number="9.1.5">
<h3 data-number="9.1.5" class="anchored"><span class="header-section-number">9.1.5</span> Discrete and continuous experimental manipulations</h3>
<p>Most experimental designs in psychology use discrete condition manipulations: treatment vs.&nbsp;control. In our view, this decision often leads to a lost opportunity relative to a more continuous manipulation of the strength of the treatment. The goal of an experiment is to estimate a causal effect; ideally, this estimate can be generalized to other contexts and used as a basis for theory. Measuring not just one effect but instead a <strong>dose-response</strong> relationship – how the measure changes as the strength of the manipulation is changed – has a number of benefits in helping to achieve this goal.</p>
<div id="fig-design-dose-schema" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/design/dose-response.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption margin-caption">Figure&nbsp;9.10: Three schematic designs. (left) Control and treatment are two levels of a nominal variable. (middle) Control is compared to ordered levels of a treatment. (right) Treatment level is an interval or ratio variable such that points can be connected and a parametric curve can be extrapolated.</figcaption>
</figure>
</div>
<div class="page-columns page-full"><p>Many manipulations can be <strong>titrated</strong> – that is, their strength can be varied continuously – with a little creativity on the part of an experimenter. A curriculum intervention can be applied at different levels of intensity, perhaps by changing the number of sessions in which it is taught. For a priming manipulation, the frequency or duration of prime stimuli can be varied. Two stimuli can be morphed continuously so that categorization boundaries can be examined.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn13"><p><sup>13</sup>&nbsp;These methods are extremely common in perception and psychophysics research, in part because the dimensions being studied are often continuous in nature. It would be basically impossible to estimate a participant’s visual contrast sensitivity <em>without</em> continuously manipulating the contrast of the stimulus!</p></li></div></div>
<div class="page-columns page-full"><p>Dose-response designs are useful because they provide insight into the shape of the function mapping your manipulation to your measure. Knowing this shape can inform your theoretical understanding! Consider the examples given in <a href="#fig-design-dose-schema">Figure&nbsp;<span>9.10</span></a>. If you only have two conditions in your experiment, then the most you can say about the relationship between your manipulation and your measure is that it produces an effect of a particular magnitude; in essence, you are assuming that condition is a nominal variable. If you have multiple ordered levels of treatment, you can start to speculate about the nature of the relationship between treatment and effect magnitude. But if you can measure the strength of your treatment, then you can begin to describe the nature of the relationship between the strength of treatment and strength of effect via a parametric function (e.g., a linear regression, a sigmoid, or other function.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> These parametric functions can in turn allow you to generalize from your experiment, making predictions about what would happen under intervention conditions that you didn’t measure directly!</p><div class="no-row-height column-margin column-container"><li id="fn14"><p><sup>14</sup>&nbsp;These assumptions are theory-laden, of course – the choice of a linear function or a sigmoid is not necessary: nothing guarantees that simple, smooth, or monotonic functions are the right ones. The important point is that choosing a function makes explicit your assumptions about the nature of the treatment-effect relationship.</p></li></div></div>
<div class="callout callout-style-default callout-note callout-titled" title="depth">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
depth
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="tradeoffs-associated-with-titrated-designs" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="tradeoffs-associated-with-titrated-designs">Tradeoffs associated with titrated designs</h2>
<p>Like adults, babies like to look at more interesting, complex stimuli. But do they uniformly prefer complex stimuli, or do they search for stimuli at an appropriate level of complexity for their processing abilities? To test this hypothesis, <span class="citation" data-cites="brennan1966">Brennan, Ames, and Moore (<a href="#ref-brennan1966" role="doc-biblioref">1966</a>)</span> exposed infants in three different age groups (3, 8, and 14 weeks, N=30) to black and white checkerboard stimuli with three different levels of complexity (2x2, 8x8, and 24x24).</p>
<p>Their findings are plotted in <a href="#fig-design-dose-ex">Figure&nbsp;<span>9.11</span></a>: the youngest infants preferred the simplest stimuli, while infants at an intermediate age preferred stimuli of intermediate complexity, and the oldest infants preferred the most complex stimuli. These findings help to motivate the theory that infants attend preferentially to stimuli that provide appropriate learning input for their processing ability <span class="citation" data-cites="kidd2012">(<a href="#ref-kidd2012" role="doc-biblioref">Kidd, Piantadosi, and Aslin 2012</a>)</span>.</p>
<div class="cell" data-hash="009-design_cache/html/fig-design-dose-ex_edbfff38d0d81c19d5ad698148090e87">
<div class="cell-output-display">
<div id="fig-design-dose-ex" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="009-design_files/figure-html/fig-design-dose-ex-1.png" class="img-fluid figure-img" style="width:55.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9.11: Infants’ looking time, plotted by stimulus complexity and infant age. Data from <span class="citation" data-cites="brennan1966">Brennan, Ames, and Moore (<a href="#ref-brennan1966" role="doc-biblioref">1966</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>If your goal is simply to detect whether an effect is zero or non-zero, then dose-response designs do not achieve the maximum statistical power. For example, if <span class="citation" data-cites="brennan1966">Brennan, Ames, and Moore (<a href="#ref-brennan1966" role="doc-biblioref">1966</a>)</span> simply wanted to achieve maximal statistical power, they probably should have only tested two age groups and two levels of complexity (say, 3 and 14 week infants and 2x2 and 24x24 checkerboards). That would have been enough to show an interaction of complexity and age, and their greater resources devoted to these four (as opposed to nine) conditions would mean more precise estimates of each. But their findings would be less clearly supportive of the view that infants prefer stimuli that are appropriate to their processing ability, because no group would have preferred an intermediate level of complexity (as the 9-week-olds apparently did). By seeking to measure intermediate conditions, they provided a stronger test of their theory.</p>
</section>
</div>
</div>
</section>
</section>
<section id="choosing-your-manipulation" class="level2 page-columns page-full" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="choosing-your-manipulation"><span class="header-section-number">9.2</span> Choosing your manipulation</h2>
<p>In the previous section, we reviewed a host of common experimental designs. These designs provide a palette of common options for combining manipulations and measures. But your choice must be predicated on the specific manipulation you are interested in! In this section, we discuss considerations for experimenters as they design manipulations.</p>
<p>In <a href="008-measurement.html"><span>Chapter&nbsp;8</span></a>, we talked about <em>measurement</em> validity, but the idea of validity concept can be applied to manipulations as well as measures. In particular, a manipulation is valid if it corresponds to the construct that the experimenter intends to intervene on. In this context, <em>internal</em> validity threats to manipulations tend to refer to cases where factors in the experimental design keep the intended manipulation from actually intervening on the construct of interest. In contrast, <em>external</em> validity threats to manipulations tend to be cases where the manipulation simply doesn’t line up well with the construct of interest.</p>
<section id="internal-validity-threats-confounding" class="level3 page-columns page-full" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="internal-validity-threats-confounding"><span class="header-section-number">9.2.1</span> Internal validity threats: Confounding</h3>
<p>First and foremost, manipulations must actually manipulate the construct whose causal effect is being estimated. If they <em>actually</em> manipulate something else instead, they are <strong>confounded</strong>. This term is used widely in psychology, but it’s worth revisiting what it means. An <strong>experimental confound</strong> is a variable that is created in the course of the experimental design that is both causally related to the predictor and potentially also related to the outcome. As such, it is a threat to <strong>internal validity</strong>.</p>
<p>Let’s go back to our discussion of causal inference in <a href="001-experiments.html"><span>Chapter&nbsp;1</span></a>. Our goal was to use a randomized experiment to estimate the causal effect of money on happiness. But just giving people money is a big intervention that involves contact with researchers – contact alone can lead to an experimental effect even if your manipulation fails. For that reason, many studies that provide money to participants either give a small amount of money or a large amount of money. This design keeps researcher contact consistent in both conditions, implying that the difference in outcomes between these two conditions should be due to the amount of money received (unless there are other confounds!).</p>
<p>Suppose you were designing an experiment of this sort and you wanted to follow our advice and use a within-participants design. You could measure happiness, give participants $100, wait a month and measure happiness again, give participants $1000, wait a month, and then measure happiness for the third time. The trouble is, this design has an obvious experimental confound (<a href="#fig-design-money1">Figure&nbsp;<span>9.12</span></a>): the order of the monetary gifts. Maybe happiness just went up more over time, irrespective of getting the second gift.</p>

<div class="no-row-height column-margin column-container"><div id="fig-design-money1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/design/money1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9.12: Confounding order and condition assignment means that you can’t make an inference about the link between money and happiness.</figcaption>
</figure>
</div></div><p>If you think your experimental design might have a confound, you should think about ways to remove it. A first option is <strong>elimination</strong>, which we described above: basically, matching a particular variable across different conditions. This should be our first option for most confounds. Unfortunately, in our within-participants money-happiness study, order is confounded with condition so if we match orders we have eliminated our condition manipulation entirely.</p>
<div class="page-columns page-full"><p>A second option is <strong>counterbalancing</strong>, in which we vary a confounding factor systematically across participants so its average effect is zero across the whole experiment. In the case of our example, counterbalancing order across participants is a very safe choice. Some participants get $100 first and others get $1000 first. That way, you are guaranteed that the order of conditions will have no effect of the confound on your average effect. The effect of this counterbalancing is that it “snips” the causal dependency between condition assignment and later time. We notate this on our causal diagram with a scissors icon (<a href="#fig-design-money2">Figure&nbsp;<span>9.13</span></a>).<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> Time can still have an effect on happiness, but the effect is independent from the effect of condition and hence your experiment can still yield an unbiased estimate of the condition effect.</p><div class="no-row-height column-margin column-container"><li id="fn15"><p><sup>15</sup>&nbsp;In practice, counterbalancing is like adding an additional factor to your factorial design! But because the factor is a <strong>nuisance factor</strong> – basically, one we don’t care about – we don’t discuss it as a true condition manipulation. Despite that, it’s a good practice to check for effects of these sorts of nuisance factors in your preliminary analysis. Even though your average effect won’t be biased by it, it introduces variation that you might want to understand to interpret other effects and plan new studies.</p></li></div></div>

<div class="no-row-height column-margin column-container"><div id="fig-design-money2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/design/money2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9.13: Confounding between a specific condition and the time at which it’s administered can be removed by counterbalancing or randomization of order.</figcaption>
</figure>
</div></div><p>Counterbalancing gets trickier when you have too many levels on a variable or multiple confounding variables. In that case, it may not be possible to do a full counterbalance so that all combinations of these factors are seen by equal numbers of participants. You may have to rely on partial counterbalancing schemes or Latin square designs (see Depth box above; in this case, the Latin squares are used to create orderings of stimuli such that the position of each treatment in the order is controlled across two other confounding variables).</p>
<p>A final option, especially useful for such tricky cases is <strong>randomization</strong>, that is, choosing which level of a nuisance variable to administer to the participant via a random choice. Randomization is increasingly common now that many experimental interventions are delivered by software. If you <em>can</em> randomize experimental confounds, you probably should. The only time you really get in trouble with randomization is when you have a large number of options, a small number of participants, or some combination of the two. Then you can end up with unbalanced levels of the randomized factors. Averaging across many experiments, a lack of balance will come out in the wash, but in a single experiment, it can lead to unfortunate bias in numbers.</p>
<p>A good approach to thinking through your experimental design is to walk through the experiment step by step and think about potential confounds. For each of these confounds, consider how it might be removed via counterbalancing or randomization. As our case study shows, confounds are not always obvious, especially in complex paradigms. There is no sure-fire way to ensure that you have spotted every one – sometimes the best way to avoid them is simply to present your candidate design to a skeptical friend.</p>
</section>
<section id="internal-validity-threats-placebo-demand-and-expectancy" class="level3 page-columns page-full" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored"><span class="header-section-number">9.2.2</span> Internal validity threats: Placebo, demand, and expectancy</h3>
<p>A second class of important threats to internal validity comes from cases where the research design is confounded by factors related to how the manipulation is administered, or even <em>that</em> a manipulation is administered. In some cases, these create confounds that can be controlled; in others they must simply be understood and guarded against. <span class="citation" data-cites="rosnow1997">Rosnow and Rosenthal (<a href="#ref-rosnow1997" role="doc-biblioref">1997</a>)</span> called these “artifacts”: systematic errors related to research <em>on</em> people, conducted <em>by</em> people.</p>
<p>A <strong>placebo</strong> effect is a positive effect on the measure that comes as a result of participants’ expectations about a treatment in the context of research study. The classic example of a placebo is medical: giving an inactive sugar pill as a “treatment” leads some patients to report a reduction in whatever symptom they are being treated for. Placebo effects are a major concern in medical research as well as a fixture in experimental designs in medicine <span class="citation" data-cites="benedetti2020">(<a href="#ref-benedetti2020" role="doc-biblioref">Benedetti 2020</a>)</span>. The key insight is that treatments must not simply be compared to a baseline of no treatment but rather to a baseline in which the psychological aspects of treatment are present but the “active ingredient” is not. In the terms we have been using, the experience of receiving a treatment (independent of the content of the treatment) is a confounding factor when you simply compare treatment to no treatment conditions.</p>
<div class="callout callout-style-default callout-note callout-titled" title="accident report">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
accident report
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<section id="brain-training" class="level2 unnumbered callout-body-container callout-body">
<h2 class="unnumbered anchored" data-anchor-id="brain-training">Brain training?</h2>
<p>Can doing challenging cognitive tasks make you smarter? In the late 2000s and early 2010s, a large industry for “brain training” emerged. Companies like Lumos Labs, CogMed, BrainHQ, and CogniFit offered games, often modeled on cognitive psychology tasks, that claimed to lead to gains in memory, attention, and problem solving.</p>
<p>These companies were basing their claims in part on a scientific literature reporting that concerted training on difficult cognitive tasks could lead to benefits that <strong>transferred</strong> to other cognitive domains. Among the most influential of these was a study by <span class="citation" data-cites="jaeggi2008">Jaeggi et al. (<a href="#ref-jaeggi2008" role="doc-biblioref">2008</a>)</span>. They conducted four experiments in which participants (N=70 across the studies) were assigned to either working memory training via a difficult working memory task (the “dual N-back”) or a no-training control, with training varying from 8 days all the way to 19 days.</p>
<p>The finding from this study excited a tremendous amount of interest because they reported not only gains in performance on the specific training task but also on a general intelligence task that the participants had trained on. While the control group’s scores on these tasks improved, presumably just from being tested twice, there was a condition by time (pre- vs.&nbsp;post) interaction such that the scores of the trained groups (consolidated across all four training experiments) grew significantly more over the training period (<a href="#fig-design-jaeggi">Figure&nbsp;<span>9.14</span></a>). These results were interpreted as supporting transfer – whereby training on one task leads to broader gains – a key goal for “brain training.”</p>
<div class="cell" data-hash="009-design_cache/html/fig-design-jaeggi_6e3ad67400c65be855e3f8ec74b0f94c">
<div class="cell-output-display">
<div id="fig-design-jaeggi" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="009-design_files/figure-html/fig-design-jaeggi-1.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9.14: The primary outcome graph for data from <span class="citation" data-cites="jaeggi2008">Jaeggi et al. (<a href="#ref-jaeggi2008" role="doc-biblioref">2008</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>Careful readers of the original paper noticed signs of analytic flexibility (as discussed in Chapters <a href="003-replication.html"><span>3</span></a> and <a href="006-inference.html"><span>6</span></a>), however. For example, the key statistical model was fit to dataset created by post-hoc consolidation of experiments, which yielded <span class="math inline">\(p = .025\)</span> on the key interaction <span class="citation" data-cites="redick2013">(<a href="#ref-redick2013" role="doc-biblioref">Redick et al. 2013</a>)</span>. When data were disaggregated, it was clear that the measures and effects had differed in each of the different sub-experiments (<a href="#fig-design-jaeggi-disagg">Figure&nbsp;<span>9.15</span></a>).</p>
<div class="cell" data-hash="009-design_cache/html/fig-design-jaeggi-disagg_b4cf0e7f18fd4f0394d9b221567477d3">
<div class="cell-output-display">
<div id="fig-design-jaeggi-disagg" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="009-design_files/figure-html/fig-design-jaeggi-disagg-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9.15: The four sub-experiments of <span class="citation" data-cites="jaeggi2008">Jaeggi et al. (<a href="#ref-jaeggi2008" role="doc-biblioref">2008</a>)</span>, now disaggregated. Panels show 8-, 12-, 17-, and 19-session studies. Note the different measures: RAPM = Raven’s Advanced Progressive Matrices; BOMAT = Bochumer Matrizentest. Based on <span class="citation" data-cites="redick2013">Redick et al. (<a href="#ref-redick2013" role="doc-biblioref">2013</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
<p>Several replications by the same group addressed some of these issues, but still failed to show convincing evidence of transfer. In particular, there was no comparison to an <strong>active control group</strong> in which participants did some kind of alternative activity for the same amount of time <span class="citation" data-cites="simons2016">(<a href="#ref-simons2016" role="doc-biblioref">Simons et al. 2016</a>)</span>. Such a comparison is critical because a comparison to a <strong>passive control group</strong> (a group that does no intervention) confounds participants’ general effort and involvement in the study with the specific training being used. Successful transfer compared to passive control could be the result of participants’ involvement, expectations, or motivation rather than brain training per se.</p>
<p>A careful replication of the training study (N=74) with an active control group and a wide range of outcome measures failed to find any transfer effects from working-memory training <span class="citation" data-cites="redick2013">(<a href="#ref-redick2013" role="doc-biblioref">Redick et al. 2013</a>)</span>. A meta-analysis of 23 studies concluded that their findings cast doubt on working memory training for increasing cognitive functioning <span class="citation" data-cites="melby-lervag2013">(<a href="#ref-melby-lervag2013" role="doc-biblioref">Melby-Lervåg and Hulme 2013</a>)</span>. In one convincing test of the cognitive transfer theory, a BBC show (“Bang Goes The Theory”) encouraged its listeners to participate in a six week online brain training study. More than 11,000 listeners completed the pre- and post-tests and at least two training sessions. Neither focused training of planning and reasoning nor broader training on memory, attention and mathematics led to transfer to untrained tasks.</p>
<p>Placebo effects are one plausible explanation for some positive findings in the brain training literature. <span class="citation" data-cites="foroughi2016">Foroughi et al. (<a href="#ref-foroughi2016" role="doc-biblioref">2016</a>)</span> recruited participants to participate via two different advertisements. The first advertised that “numerous studies have shown working memory training can increase fluid intelligence” (“placebo treatment” group) while the second simply offered experimental credits (control group). After a single training session, the placebo treatment group showed significant improvements to their matrix reasoning abilities. Participants in the placebo treatment group realized gains from training out of proportion with any they could have realized through training. Further, those participants who responded to the placebo treatment ad tended to endorse statements about the malleability of intelligence, suggesting that they might have been especially likely to self-select into the intervention.</p>
<p>Summarizing the voluminous literature on brain training, <span class="citation" data-cites="simons2016">Simons et al. (<a href="#ref-simons2016" role="doc-biblioref">2016</a>)</span> wrote: “Despite marketing claims from brain-training companies of ‘proven benefits’… we find the evidence of benefits from cognitive brain training to be ‘inadequate.’”</p>
</section>
</div>
</div>
<p>If placebo effects reflect what participants expect from a treatment then <strong>demand characteristics</strong> reflect what participants think <em>experimenters</em> want and their desire to help the experimenters achieve that goal <span class="citation" data-cites="orne1962">(<a href="#ref-orne1962" role="doc-biblioref">Orne 1962</a>)</span>. Demand characteristics are often raised as a reason for avoiding within-participants designs – if participants become alert to the presence of an intervention, they may then respond in a way that they believe is helpful to the experimenter. Typical tools for controlling or identifying demand characteristics include using a cover story to mask the purpose of an experiment, using a debriefing procedure to probe whether participants typically guessed the purpose of an experiment, and (perhaps most effectively) creating a control condition with similar demand characteristics but missing a key component of the experimental intervention. Note that if you use a cover story to mask the purpose of your experiment, it’s worth thinking about whether you are using deception, which can raise ethical issues (see <a href="004-ethics.html"><span>Chapter&nbsp;4</span></a>). Certainly you should be sure to debrief participants about the true function of the experiment!</p>
<p>The final entry into this list of internal validity threats is <strong>experimenter expectancy effects</strong>, where the experimenter’s behavior biases participants in a way that results in the appearance of condition differences where no true difference exists. The classic example of such effects is from the animal learning literature and the story of Clever Hans. Clever Hans was a horse who appeared able to do arithmetic by tapping out solutions with his hoof. On deeper investigation, it became apparent he was being cued by his trainer’s posture (apparently without the trainer’s knowledge) to stop tapping when the desired answer was reached. The horse knew nothing about math, but the experimenter’s expectations were altering the horse’s behavior across different conditions.</p>
<p>In any experiment delivered by human experimenters who know what condition they are delivering, condition differences can result from experimenters imparting their expectations. <a href="#tbl-rosenthal">Table&nbsp;<span>9.2</span></a> shows the results of a meta-analysis estimating sizes of expectancy effects in a range of domains – the magnitudes are shocking. There’s no question that experimenter expectancy is sufficient to “create” many interesting phenomena artifactually. The mechanisms of expectancy are an interesting research topic in their own right; in many cases expectancies appear to be communicated non-verbally in much the same way that Clever Hans learned <span class="citation" data-cites="rosnow1997">(<a href="#ref-rosnow1997" role="doc-biblioref">Rosnow and Rosenthal 1997</a>)</span>.</p>
<div id="tbl-rosenthal" class="anchored">
<table class="table">
<caption>Table&nbsp;9.2: Magnitudes of expectancy effects. Based on <span class="citation" data-cites="rosenthal1994">Rosenthal (<a href="#ref-rosenthal1994" role="doc-biblioref">1994</a>)</span>.</caption>
<colgroup>
<col style="width: 23%">
<col style="width: 5%">
<col style="width: 4%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Domain</th>
<th style="text-align: right;">d</th>
<th style="text-align: right;">r</th>
<th style="text-align: left;">Example of type of study</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Laboratory interviews</td>
<td style="text-align: right;">0.14</td>
<td style="text-align: right;">.07</td>
<td style="text-align: left;">Effects of sensory restriction on reports of hallucinatory experiences</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reaction time</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">.08</td>
<td style="text-align: left;">Latency of word associations to certain stimulus words</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Learning and ability</td>
<td style="text-align: right;">0.54</td>
<td style="text-align: right;">.26</td>
<td style="text-align: left;">IQ test scores, verbal conditioning (learning)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Person perception</td>
<td style="text-align: right;">0.55</td>
<td style="text-align: right;">.27</td>
<td style="text-align: left;">Perception of other people’s success</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Inkblot tests</td>
<td style="text-align: right;">0.84</td>
<td style="text-align: right;">.39</td>
<td style="text-align: left;">Ratio of animal to human Rorschach responses</td>
</tr>
<tr class="even">
<td style="text-align: left;">Everyday situations</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">.40</td>
<td style="text-align: left;">Symbol learning, athletic performance</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Psychophysical judgments</td>
<td style="text-align: right;">1.05</td>
<td style="text-align: right;">.46</td>
<td style="text-align: left;">Ability to discriminate tones</td>
</tr>
<tr class="even">
<td style="text-align: left;">Animal learning</td>
<td style="text-align: right;">1.73</td>
<td style="text-align: right;">.65</td>
<td style="text-align: left;">Learning in mazes and Skinner boxes</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Weighted mean</em></td>
<td style="text-align: right;">0.70</td>
<td style="text-align: right;">.33</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><em>Unweighted mean</em></td>
<td style="text-align: right;">0.74</td>
<td style="text-align: right;">.35</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><em>Median</em></td>
<td style="text-align: right;">0.70</td>
<td style="text-align: right;">.33</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div class="page-columns page-full"><p>In medical research, the gold standard is an experimental design where neither patients nor experimenters know which condition the patients are in.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> Results from other designs are treated with suspicion because of their vulnerability to demand and expectancy effects. In psychology, the most common modern protection against experimenter expectancy is the delivery of interventions by a computer platform that can give instructions in a coherent and uniform way across conditions.</p><div class="no-row-height column-margin column-container"><li id="fn16"><p><sup>16</sup>&nbsp;These are commonly referred to as <strong>double blind</strong> designs (though the term <strong>masked</strong> is now often preferred).</p></li></div></div>
<p>In the case of interventions that must be delivered by experimenters, ideally experimenters should be unaware of which condition they are delivering. On the other hand, the logistics of maintaining experimenter ignorance can be quite complicated in psychology. For this reason, many researchers opt for lesser degrees of control, for example, choosing to standardize delivery of an intervention via a script. These designs are sometimes necessary for practical reasons but should be scrutinized closely. “How can you rule out experimenter expectancy effects?” is an uncomfortable question that should be asked more frequently in seminars and paper reviews.</p>
</section>
<section id="external-validity-of-manipulations" class="level3 page-columns page-full" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="external-validity-of-manipulations"><span class="header-section-number">9.2.3</span> External validity of manipulations</h3>
<div class="page-columns page-full"><p>The goal of a specific experimental manipulation is to operationalize a particular causal relationship of interest. Just as the relationship between measure and construct can be more or less valid, so too can the relationship between manipulation and construct. How can you tell? Just like in the case of measures, there’s no one royal road to validity. You need to make a validity argument <span class="citation" data-cites="kane1992">(<a href="#ref-kane1992" role="doc-biblioref">Kane 1992</a>)</span>.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn17"><p><sup>17</sup>&nbsp;One caveat is that the validity of a manipulation incorporates the validity of the manipulation <em>and</em> the measure. You can’t really have a good estimate of a causal effect if the measurement is invalid.</p></li></div></div>
<!-- Let's see how these arguments might look for some of the examples we've discussed in this chapter.  -->
<p>For testing the effect of money on happiness, our manipulation was to give participants $1000. This manipulation is clearly face valid. But how often do people just receive a windfall of cash, versus getting a raise at work or inheriting money from a relative? Is the effect caused by <em>having</em> the money, or <em>receiving</em> the money with no strings attached? We might have to do more experiments to figure out what aspect of the money manipulation was most important. Even in straightforward cases like this one, we need to be careful about the breadth of the claims we make.</p>
<p>Sometimes validity arguments are made based on the success of the manipulation in producing some change in the measurement. In the the implicit theory of mind case study we began with, the stimulus contained an animated Smurf character, and the argument was that participants took the Smurf’s beliefs into account in making their judgments. This stimulus choice seems surprising – not only would participants have to track the implicit beliefs of other <em>people</em>, they would also have to be tracking the beliefs of depictions of non-human, animated characters. On the other hand, based on the success of the manipulation, the authors made an <em>a fortiori</em> argument: if people track even an animated Smurf’s beliefs, then they <em>must</em> be tracking the beliefs of real humans.</p>
<div class="page-columns page-full"><p>Let’s look at one last example to think more about manipulation validity. <span class="citation" data-cites="walton2011">Walton and Cohen (<a href="#ref-walton2011" role="doc-biblioref">2011</a>)</span> conducted a short intervention in which college students (N=92) read about social belonging and the challenges of the transition to college and then reframed their own experiences using these ideas. This intervention led to long-lasting changes in grades and well-being. While the intervention undoubtedly had a basis in theory, part of our understanding of the validity of the intervention comes from its efficacy: sense of belonging <em>must</em> be a powerful factor if intervening on it causes such big changes in the outcome measures.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> The only danger is when the argument becomes circular – a theory is correct because the intervention yielded a success, and the intervention is presumed to be valid because of the theory. The way out of this circle is through replication and generalization of the intervention. If the intervention repeatably produces the outcome, as has been shown in replications of the sense of belonging intervention <span class="citation" data-cites="walton2020">(<a href="#ref-walton2020" role="doc-biblioref">Walton, Brady, and Crum 2020</a>)</span>, then the manipulation becomes an intriguing target for future theories. The next step in such a research program is to understand the limitations of such interventions (sometimes called <strong>boundary conditions</strong>).</p><div class="no-row-height column-margin column-container"><li id="fn18"><p><sup>18</sup>&nbsp;On the other hand, if the manipulation <em>doesn’t</em> produce a change in your measure, maybe the manipulation is invalid, but the construct still exists. Sense of belonging could still be important even if my particular intervention failed to alter it!</p></li></div></div>
</section>
</section>
<section id="summary-experimental-design" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="summary-experimental-design"><span class="header-section-number">9.3</span> Summary: Experimental design</h2>
<p>In this chapter, we started by examining some common experimental designs that allow us to measure effects associated with one or more manipulations. Our advice, in brief, was: “keep it simple!” The failure mode of many experiments is that they contain too many manipulations, and these manipulations are measured with too little precision.</p>
<p>Start with just a single manipulation, and measure it carefully. Ideally this measurement should be done via a within-participants design unless the manipulation is completely incompatible with this design. And if this design can incorporate a dose-response manipulation, it is more likely to provide a basis for quantitative theorizing.</p>
<p>How do you ensure that your manipulation is valid? A careful experimenter needs to consider possible confounds and ensure that these are controlled or randomized. They must also consider other artifacts including placebo, demand, and expectancy effects. Finally, they must begin thinking about the relation of their manipulation to the broader theoretical construct whose causal role they hope to test.</p>
<div class="callout callout-style-default callout-note callout-titled" title="discussion questions">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
discussion questions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Choose a classic study in your area of psychology. Analyze the design choices: how many factors were manipulated? How many measures were taken? Did it use a within-participants or between-participants design? Were measures repeated? Can you justify these choices with respect to trade-offs (e.g., carry-over effects, fatigue, etc.)?</p></li>
<li><p>Consider the same study. Design an alternative version that varies one of these design parameters (e.g., drops a manipulation or measure, changes within- to between-participants, etc.). What are the pros and cons of this change? Do you think your design improves on the original?</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="readings">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
readings
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Much of this material is covered in more depth in the classic text on research methods: Rosenthal, R. &amp; Rosnow, R. L. 2008. <em>Essentials of Behavioral Research: Methods and Data Analysis</em>. Third Edition. New York: McGraw-Hill. <a href="http://dx.doi.org/10.34944/dspace/66" class="uri">http://dx.doi.org/10.34944/dspace/66</a>.</li>
</ul>
</div>
</div>
</div>
<!-- \refs -->


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-baddeley1975" class="csl-entry" role="listitem">
Baddeley, Alan D, Neil Thomson, and Mary Buchanan. 1975. <span>“Word Length and the Structure of Short-Term Memory.”</span> <em>Journal of Verbal Learning and Verbal Behavior</em> 14 (6): 575–89.
</div>
<div id="ref-benedetti2020" class="csl-entry" role="listitem">
Benedetti, Fabrizio. 2020. <em>Placebo Effects</em>. Oxford University Press.
</div>
<div id="ref-boyce2023" class="csl-entry" role="listitem">
Boyce, Veronica, Maya Mathur, and Michael C Frank. 2023. <span>“Eleven Years of Student Replication Projects Provide Evidence on the Correlates of Replicability in Psychology.”</span> <em>Royal Society Open Science</em> 10 (11): 231240.
</div>
<div id="ref-brennan1966" class="csl-entry" role="listitem">
Brennan, Wendy M, Elinor W Ames, and Ronald W Moore. 1966. <span>“Age Differences in Infants’ Attention to Patterns of Different Complexities.”</span> <em>Science</em> 151 (3708): 354–56.
</div>
<div id="ref-clark1973" class="csl-entry" role="listitem">
Clark, Herbert H. 1973. <span>“The Language-as-Fixed-Effect Fallacy: A Critique of Language Statistics in Psychological Research.”</span> <em>Journal of Verbal Learning and Verbal Behavior</em> 12 (4): 335–59.
</div>
<div id="ref-cunningham2021" class="csl-entry" role="listitem">
Cunningham, Scott. 2021. <em>Causal Inference</em>. Yale University Press.
</div>
<div id="ref-el-kaddouri2020" class="csl-entry" role="listitem">
El Kaddouri, Rachida, Lara Bardi, Diana De Bremaeker, Marcel Brass, and Roeljan Wiersema. 2020. <span>“Measuring Spontaneous Mentalizing with a Ball Detection Task: Putting the Attention-Check Hypothesis by Phillips and Colleagues (2015) to the Test.”</span> <em>PSYCHOLOGICAL RESEARCH-PSYCHOLOGISCHE FORSCHUNG</em> 84 (6): 1749–57.
</div>
<div id="ref-foroughi2016" class="csl-entry" role="listitem">
Foroughi, Cyrus K, Samuel S Monfort, Martin Paczynski, Patrick E McKnight, and PM Greenwood. 2016. <span>“Placebo Effects in Cognitive Training.”</span> <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7470–74.
</div>
<div id="ref-gelman2017" class="csl-entry" role="listitem">
Gelman, Andrew. 2017. <span>“Poisoning the Well with a Within-Person Design? What’s the Risk?”</span> 2017. <a href="https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/">https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/</a>.
</div>
<div id="ref-greenwald1976" class="csl-entry" role="listitem">
Greenwald, Anthony G. 1976. <span>“Within-Subjects Designs: To Use or Not to Use?”</span> <em>Psychological Bulletin</em> 83 (2): 314.
</div>
<div id="ref-jaeggi2008" class="csl-entry" role="listitem">
Jaeggi, Susanne M, Martin Buschkuehl, John Jonides, and Walter J Perrig. 2008. <span>“Improving Fluid Intelligence with Training on Working Memory.”</span> <em>Proceedings of the National Academy of Sciences</em> 105 (19): 6829–33.
</div>
<div id="ref-kane1992" class="csl-entry" role="listitem">
Kane, Michael T. 1992. <span>“An Argument-Based Approach to Validity.”</span> <em>Psychological Bulletin</em> 112 (3): 527.
</div>
<div id="ref-kidd2012" class="csl-entry" role="listitem">
Kidd, Celeste, Steven T Piantadosi, and Richard N Aslin. 2012. <span>“The Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple nor Too Complex.”</span> <em>PloS One</em> 7 (5): e36399.
</div>
<div id="ref-kovacs2010" class="csl-entry" role="listitem">
Kovács, Ágnes Melinda, Ernő Téglás, and Ansgar Denis Endress. 2010. <span>“The Social Sense: Susceptibility to Others’ Beliefs in Human Infants and Adults.”</span> <em>Science</em> 330 (6012): 1830–34.
</div>
<div id="ref-lakens2016" class="csl-entry" role="listitem">
Lakens, Daniel. 2016. <span>“Why Within-Subject Designs Require Fewer Participants Than Between-Subject Designs.”</span> 2016. <a href="https://daniellakens.blogspot.com/2016/11/why-within-subject-designs-require-less.html">https://daniellakens.blogspot.com/2016/11/why-within-subject-designs-require-less.html</a>.
</div>
<div id="ref-lovatt2000" class="csl-entry" role="listitem">
Lovatt, Peter, Steve E Avons, and Jackie Masterson. 2000. <span>“The Word-Length Effect and Disyllabic Words.”</span> <em>The Quarterly Journal of Experimental Psychology: Section A</em> 53 (1): 1–22.
</div>
<div id="ref-mcclelland1993" class="csl-entry" role="listitem">
McClelland, Gary H, and Charles M Judd. 1993. <span>“Statistical Difficulties of Detecting Interactions and Moderator Effects.”</span> <em>Psychological Bulletin</em> 114 (2): 376.
</div>
<div id="ref-melby-lervag2013" class="csl-entry" role="listitem">
Melby-Lervåg, Monica, and Charles Hulme. 2013. <span>“Is Working Memory Training Effective? A Meta-Analytic Review.”</span> <em>Developmental Psychology</em> 49 (2): 270.
</div>
<div id="ref-myung2009" class="csl-entry" role="listitem">
Myung, Jay I, and Mark A Pitt. 2009. <span>“Optimal Experimental Design for Model Discrimination.”</span> <em>Psychological Review</em> 116 (3): 499.
</div>
<div id="ref-orne1962" class="csl-entry" role="listitem">
Orne, Martin T. 1962. <span>“On the Social Psychology of the Psychological Experiment: With Particular Reference to Demand Characteristics and Their Implications.”</span> <em>American Psychologist</em> 17 (11): 776.
</div>
<div id="ref-phillips2015" class="csl-entry" role="listitem">
Phillips, Jonathan, Desmond C Ong, Andrew DR Surtees, Yijing Xin, Samantha Williams, Rebecca Saxe, and Michael C Frank. 2015. <span>“A Second Look at Automatic Theory of Mind: Reconsidering Kov<span>á</span>cs, t<span>é</span>gl<span>á</span>s, and Endress (2010).”</span> <em>Psychological Science</em> 26 (9): 1353–67.
</div>
<div id="ref-redick2013" class="csl-entry" role="listitem">
Redick, Thomas S, Zach Shipstead, Tyler L Harrison, Kenny L Hicks, David E Fried, David Z Hambrick, Michael J Kane, and Randall W Engle. 2013. <span>“No Evidence of Intelligence Improvement After Working Memory Training: A Randomized, Placebo-Controlled Study.”</span> <em>Journal of Experimental Psychology: General</em> 142 (2): 359.
</div>
<div id="ref-rosenthal1994" class="csl-entry" role="listitem">
Rosenthal, Robert. 1994. <span>“Interpersonal Expectancy Effects: A 30-Year Perspective.”</span> <em>Current Directions in Psychological Science</em> 3 (6): 176–79.
</div>
<div id="ref-rosnow1997" class="csl-entry" role="listitem">
Rosnow, Ralph, and Robert Rosenthal. 1997. <em>People Studying People: Artifacts and Ethics in Behavioral Research</em>. WH Freeman.
</div>
<div id="ref-simons2016" class="csl-entry" role="listitem">
Simons, Daniel J, Walter R Boot, Neil Charness, Susan E Gathercole, Christopher F Chabris, David Z Hambrick, and Elizabeth AL Stine-Morrow. 2016. <span>“Do <span>‘Brain-Training’</span> Programs Work?”</span> <em>Psychological Science in the Public Interest</em> 17 (3): 103–86.
</div>
<div id="ref-walton2020" class="csl-entry" role="listitem">
Walton, Gregory M, Shannon T Brady, and AJ Crum. 2020. <span>“The Social-Belonging Intervention.”</span> <em>Handbook of Wise Interventions: How Social Psychology Can Help People Change</em>, 36–62.
</div>
<div id="ref-walton2011" class="csl-entry" role="listitem">
Walton, Gregory M, and Geoffrey L Cohen. 2011. <span>“A Brief Social-Belonging Intervention Improves Academic and Health Outcomes of Minority Students.”</span> <em>Science</em> 331 (6023): 1447–51.
</div>
<div id="ref-westfall2015" class="csl-entry" role="listitem">
Westfall, Jacob, Charles M Judd, and David A Kenny. 2015. <span>“Replicating Studies in Which Samples of Participants Respond to Samples of Stimuli.”</span> <em>Perspectives on Psychological Science</em> 10 (3): 390–99.
</div>
<div id="ref-young2007" class="csl-entry" role="listitem">
Young, Liane, Fiery Cushman, Marc Hauser, and Rebecca Saxe. 2007. <span>“The Neural Basis of the Interaction Between Theory of Mind and Moral Judgment.”</span> <em>Proceedings of the National Academy of Sciences</em> 104 (20): 8235–40.
</div>
</div>
</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./008-measurement.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Measurement</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./010-sampling.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sampling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>