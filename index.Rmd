--- 
title: "Experimentology"
subtitle: "An Open Science Approach to Experimental Psychology Methods "
author: "Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams"

site: bookdown::bookdown_site
documentclass: book
classoption: twoside,symmetric

bibliography: experimentology.bib
biblio-style: apalike
link-citations: yes
---

# {.unlisted .unnumbered}

<!-- ## Summary {-} -->

How do we extract generalizable insights about the vast complexity of human behavior? The goal of this book is to provide an introduction to the workflow of the experimental researcher in the psychological sciences. The organization is sequential, from the planning stages of the research process through design, data collection, analysis, and reporting. We introduce these concepts via narrative examples from a range of sub-disciplines, including cognitive, developmental, and social psychology. Throughout, we also illustrate the pitfalls that led to the “replication crisis” in psychology. Across chapters, the book will emphasize four themes of successful experimental research: transparency, precision, bias reduction, and generalizability. The book takes an open-science based approach, providing readers with tutorials and justifications of practices such as preregistration, data sharing, and reproducible workflows. The audience for the book is graduate students, advanced undergraduates, or highly-motivated self-learners with some domain knowledge in psychology and a basic grasp of linear regression. For relevant chapters, examples will be presented using code from R (the free statistical programming language), Git (a version control system), and the Open Science Framework (a data sharing/preregistration site), and appendices will provide tutorials on these tools. The author group has collaborated for years in both teaching and research contexts and has broad expertise in experimental methods pedagogy as well as meta-science, statistical analysis and meta-analysis, and ethics. Our hope is that this book provides a practical introduction to how to do robust and replicable work as an experimental psychologist. 

## Introduction {-}

Experimental Methods (Psych 251) is the foundational course for incoming graduate students in the Stanford psychology department. For the last ten years, one of us (Frank) has taught this course and most of us (Hawkins, Cachia, Hardwicke, Mathur, Williams) have TA’d, taken, or otherwise contributed to the course. The goal is to orient students to the nuts and bolts of doing behavioral experiments, including how to plan and design a solid experiment and how to avoid common pitfalls regarding design, measurement, and sampling. Almost all of students’ coursework both before and in graduate school deals with the content of their research, including theories and results in their areas of focus. In contrast, the course is sometimes the only one that deals with the *process* of research, from big questions about why we do experiments and what it means to make a causal inference all the way to the tiny details of organization, like what to name your directories and how to make sure you don’t lose your data in a computer crash. 

This observation leads to our book's title. **Experimentology** is not psychology, cognitive science, or any other body of content knowledge – but rather the set of practices, findings, and approaches that help to enable the construction of robust, precise, and generalizable experiments. 

The centerpiece of the Experimental Methods course is a replication project, a model first described in @frank2012 and later expanded on in @hawkins2018. Each student chooses a published experiment in the literature and collects new data on a pre-registered version of the same experimental paradigm, comparing their result to the original publication. Over the course of the quarter, we walk through how to set up a replication experiment, how to pre-register confirmatory analyses, and how to write a reproducible report on the findings. The project provides numerous object lessons for teaching concepts like reliability and validity, which allow students to analyze choices that the original experimenters made -- often choices that could have been made differently in hindsight! 

At the end of the course, we reap the harvest of projects. The project presentations are a wonderful demonstration of both how much the students can accomplish in a quarter and also how tricky it can be to reproduce (redo calculations in the original data) and replicate (recover similar results in new data) the published literature. Often our replication rate for the course hovers just above 50%, an outcome that can be disturbing or distressing for students who assume that the published literature reports the absolute truth! 

In this book, we will tell the story of the major shifts in psychology that have come about in the last ten years, including both the "replication crisis" narrative [@osc2015 et seq.] and the positive methodological reforms that have resulted from it. Using this story as motivation, we will highlight the importance of transparency during all aspects of the experimental process from planning to dissemination of materials, data, and code. 

## What this book is and isn't about {-}

Experimentology is organized into five main sections, mirroring the timeline of an experiment: 1) Before You Begin, 2) Design and Planning, 3) Doing the Experiment, 4) Analysis and Reporting, and 5) Contextualizing. We hope that organization makes it well-suited for teaching or for use as a reference book for self-study, and we provide a number of resources for instructors of a graduate course (Appendix \@ref(instructors) and Appendix \@ref(students)). 

We also hope that some readers will come to specific chapters of the book because of an interest in specific topics like measurement (Chapter \@ref(measurement)) or sampling (Chapter \@ref(sampling)) and will be able to use those chapters as standalone references. And finally, for those interested in the “replication crisis” and reforms that have taken place in the behavioral sciences in the wake of it, Chapters \@ref(intro), \@ref(replication), \@ref(prereg), and \@ref(conclusion) will be especially interesting. 

```{marginfigure, echo=TRUE}
This book has fun stuff going on in the margins!
<img src="images/dog.jpeg"/>
```

We want to give you what you need to plan and execute your own study! Instead of enumerating different approaches, we try to provide a single coherent – and often quite opinionated – perspective, using marginal notes and references to give pointers to more advanced materials or alternative approaches. 

This book is about the process of doing simple randomized psychology experiments. These will be typically be short studies conducted online or in a single visit to a lab, often with a convenience population. We won't go into depth about the many fascinating methodological and statistical issues brought up by single-participant case studies, longitudinal research, field studies, or other methodological variants. Many of the concerns we raise are still important for these types of studies, but some of our advice won't transfer to these critical but more unusual cases.^[For example, it's hard to do a full pilot study on a two year longitudinal intervention!] 

In our writing, we presuppose that readers have some background in psychology, at least at an introductory level. We also presuppose a basic undergraduate statistics background so that we can draw on concepts like *distribution*, *mean*, and *standard deviation*, and basic tools like the *t-test* and the *correlation coefficient*. Finally, our examples are written in the R statistical programming language, and for chapters on statistics and visualization especially (Chapters \@ref(inference), \@ref(models), \@ref(viz), \@ref(eda), and \@ref(meta)), some familiarity with R will be helpful. None of these prerequisites are strictly necessary, however -- a bit of googling will often be enough to fill in any gap. Specific R skills like tidyverse and ggplot are covered in the Appendices. 

Each chapter of the book will start with a narrative case study in which we describe an experiment or series of experiments, using these to illustrate and motivate the specific issues that the chapter deals with, often with a focus on pitfalls or challenges in previous research. These will draw from a broad set of subfields. In addition to narrative examples that begin the chapters, we include several other instructional elements with specific examples. Throughout we share “accident reports,” short cases drawn from the published literature where a particular practice led to an error or faulty inference. We also include “ethics boxes” where we discuss ethical issues arising from the content of a chapter. 

## Themes {-}

We try to highlight four major cross-cutting themes for the book: **transparency**, **precision**, **bias reduction**, and **generalizability**. 

* **Transparency**: For experiments to be reproducible, other researchers need to be able to determine exactly what you did. Thus, every stage of the research process should be guided by a primary concern for transparency. For example, preregistration creates transparency into the researcher’s evolving expectations and thought processes; releasing open materials and analysis scripts creates transparency into the details of the procedure. 
* **Precision**: We want researchers to start planning an experiment by thinking “what causal effect do I want to measure” and to make their planning, sampling, design, and analytic choices to maximize the precision of this measurement. A downstream consequence of this mindset is that we move away from a focus on dichotomized inference, like p-value significance, and towards analytic and meta-analytic models that focus on continuous effect sizes and confidence intervals (Cumming 2014).
* **Bias reduction**: Precision is not enough if the estimate is biased. In our samples, analyses, experimental designs, and in the literature, we need to think carefully about sources of bias in the quantity being estimated. This kind of thinking also reveals key weaknesses of dichotomous, “significance”-based reasoning.
* **Generalizability**: Complex behaviors are rarely universal across all settings and populations, and any given experiment can only hope to cover a small slice of the possible conditions where the behavior takes place (Yarkoni 2020). Behavioral scientists must therefore consider the generalizability of their findings at every stage of the process, from stimulus selection, sampling procedures, and analytic methods, to how findings are reported.

Throughout we will return to the important relationships between these four concepts, and how the decisions made by the experimenter at every stage of design, data collection, and analysis bear on the inferences that can be made about the results. Importantly, discussions of reproducibility and replicability have often proceeded without consideration of issues like precision, bias reduction, and generalizability, leading to a number of deep critiques of the methodological reform movement that we will cover in some detail. 

## The software toolkit of the behavioral researcher (and of this book) {-}

We introduce and advocate for an approach to reproducible study planning, analysis, and writing. This approach depends on an ecosystem of open-source software tools. 

* The R statistical programming language and the [R Studio](http://rstudio.org) integrated development environment. R is a free and open platform for statistical programming. Because of its large userbase, almost all extent analysis and visualization methods are implemented in packages that can be downloaded from [CRAN](https://cran.r-project.org), the comprehensive R archive network, 
* The `tidyverse` family of R packages, which extend the basic functionality of R with simple tools for data wrangling, analysis, and visualization (including the `ggplot2` package). 
* Version control using `git` and [github](http://github.com). Version control tools allow one or more users to collaborate on text documents like code, prose, and data, storing and integrating contributions over time. The github provides a centralized hosting and project management platform for version-controlled projects, making it ideal for collaboration and dissemination.  
* While github is an excellent real-time collaboration platform, it does not provide archival guarantees or the ability to provide time-stamped registrations of projects. For these functions, we use the [Open Science Framework](http://osf.io), a project management platform designed for scientific projects. 


<!-- ## Integrating this book into an experimental methods course {-} -->

<!-- The project-based approach (argument for doing replication/reproducibility study as part of learning methods)  -->
<!-- Each chapter ends with a mixture of discussion questions, exercises, and project milestones that can be integrated into course assignments.  -->
<!-- We include links to appendices, references, and recurring boxes with ethical content and ‘accident reports’ from documented problems in the literature.  -->
