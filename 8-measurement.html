
<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 8 Measurement | Experimentology" />
<meta property="og:type" content="book" />




<meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Chapter 8 Measurement | Experimentology">

<title>Chapter 8 Measurement | Experimentology</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<link rel="stylesheet" type="text/css" href="/assets/src/index.page.client.jsx.50f4142f.css"></head>
<body>



<div class="row">
<div class="col-sm-12">
<div id="island_0"><header class="_toc_1lnsy_1" id="toc"><a class="_book_title_1lnsy_24" href="/">Experimentology: An Open Science Approach to Experimental Psychology Methods</a><nav><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Preliminaries</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="1-experiments">Experiments</a><a class="_chapter_title_1lnsy_32" href="2-theories">Theories</a><a class="_chapter_title_1lnsy_32" href="3-replication">Replication</a><a class="_chapter_title_1lnsy_32" href="4-ethics">Ethics</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Statistics</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="5-estimation">Estimation</a><a class="_chapter_title_1lnsy_32" href="6-inference">Inference</a><a class="_chapter_title_1lnsy_32" href="7-models">Models</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Design</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="8-measurement">Measurement</a><a class="_chapter_title_1lnsy_32" href="9-design">Design</a><a class="_chapter_title_1lnsy_32" href="10-sampling">Sampling</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Execution</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="11-prereg">Preregistration</a><a class="_chapter_title_1lnsy_32" href="12-collection">Data collection</a><a class="_chapter_title_1lnsy_32" href="13-management">Project management</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Reporting</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="14-writing">Writing</a><a class="_chapter_title_1lnsy_32" href="15-viz">Visualization</a><a class="_chapter_title_1lnsy_32" href="16-meta">Meta-analysis</a><a class="_chapter_title_1lnsy_32" href="17-conclusions">Conclusions</a></div></div><div class="_part_1lnsy_16"><div class="_part_title_1lnsy_24"><div class="_part_title_first_1lnsy_59">Appendices</div><div class="_part_title_rest_1lnsy_32"></div></div><div class="_dropdown_1lnsy_16"><a class="_chapter_title_1lnsy_32" href="A-git">GitHub</a><a class="_chapter_title_1lnsy_32" href="B-rmarkdown">R Markdown</a><a class="_chapter_title_1lnsy_32" href="C-tidyverse">Tidyverse</a><a class="_chapter_title_1lnsy_32" href="D-ggplot">ggplot</a><a class="_chapter_title_1lnsy_32" href="E-instructors">Instructor’s guide</a></div></div></nav></header></div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="measurement" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> Measurement</h1>
<div id="island_1"><div class="box learning_goals"><div class="Collapsible"><span id="collapsible-trigger-1664301173540" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173540" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="apple-whole" class="svg-inline--fa fa-apple-whole " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M224 112c-8.8 0-16-7.2-16-16V80c0-44.2 35.8-80 80-80h16c8.8 0 16 7.2 16 16V32c0 44.2-35.8 80-80 80H224zM0 288c0-76.3 35.7-160 112-160c27.3 0 59.7 10.3 82.7 19.3c18.8 7.3 39.9 7.3 58.7 0c22.9-8.9 55.4-19.3 82.7-19.3c76.3 0 112 83.7 112 160c0 128-80 224-160 224c-16.5 0-38.1-6.6-51.5-11.3c-8.1-2.8-16.9-2.8-25 0c-13.4 4.7-35 11.3-51.5 11.3C80 512 0 416 0 288z"></path></svg>Learning goals<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173540" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173540"><div class="Collapsible__contentInner">
<ul>
<li>Discuss the reliability and validity of psychological measures</li>
<li>Reason about tradeoffs between different measures and measure types</li>
<li>Identify the characteristics of well-constructed survey questions</li>
<li>Articulate risks of measurement flexibility and the costs and benefits of multiple measures</li>
</ul>
</div></div></div></div></div>

<p>Throughout the history of science, advances in measurement have gone hand in hand with advances in knowledge.<label for="tufte-sn-105" class="margin-toggle sidenote-number">105</label><input type="checkbox" id="tufte-sn-105" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">105</span> As such, measurement is a perennially controversial topic in philosophy of science. For an overview of competing frameworks, see <span class="citation">Tal (<a href="#ref-sep-measurement-science" role="doc-biblioref">2020</a>)</span> or <span class="citation">Maul et al. (<a href="#ref-maul2016philosophical" role="doc-biblioref">2016</a>)</span>, which focuses specifically on measurement in psychology.</span> Telescopes revolutionized astronomy, microscopes revolutionized biology, and patch clamping revolutionized physiology. But measurement isn’t easy. Even the humble thermometer, allowing reliable measurement of temperature, required centuries of painstaking effort to perfect <span class="citation">(<a href="#ref-chang2004inventing" role="doc-biblioref">Chang, 2004</a>)</span>. Psychology and the behavioral sciences are no different – we need reliable instruments to measure the things we care about. In this next section of the book, we’re going to discuss the challenges facing measurement in psychology, and the properties that distinguish good instruments from bad.</p>
<p>What does it mean to measure something? Intuitively, we know that a ruler measures the quantity of length, and a scale measures the quantity of weight <span class="citation">(<a href="#ref-kisch1965scales" role="doc-biblioref">Kisch, 1965</a>)</span>. But what does it mean to measure a psychological construct – a hypothesized theoretical quantity inside the head? According to <span class="citation">Stevens (<a href="#ref-stevens1946" role="doc-biblioref">1946</a>)</span>, measurement is simply the practice of assigning numbers to things. But, to paraphrase <span class="citation">Norman R. Campbell &amp; Jeffreys (<a href="#ref-campbell1938symposium" role="doc-biblioref">1938</a>)</span>, not every assignment of numbers is measurement! Roughly speaking, we want the numbers we assign to behave like the constructs we’re assigning them to (in the ways that matter). And not all measurement instruments are created equal.</p>
<p>This point is obvious when you think about physical measurement instruments: a caliper will give you a much more precise estimate of the thickness of a small object than a ruler. One way to see that the measurement is more precise is by repeating it a bunch of times. The measurements from the caliper will likely be more similar to one another, reflecting the fact that the amount of error in each individual measurement is smaller. We can do the same thing with a psychological measurement – repeat and assess variation – though as we’ll see below it’s a little trickier. Measurement instruments that have less error are called more <strong>reliable</strong> instruments.<label for="tufte-sn-106" class="margin-toggle sidenote-number">106</label><input type="checkbox" id="tufte-sn-106" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">106</span> Is <strong>reliability</strong> the same as <strong>precision</strong>? Yes, more or less. Confusingly, different fields call these concepts different things <span class="citation">(there’s a helpful table of these names in <a href="#ref-brandmaier2018" role="doc-biblioref">Brandmaier et al., 2018</a>)</span>. Here we’ll talk about reliability as a property of instruments specifically while using the term precision to talk about the measurements themselves.</span></p>
<p>When we have a physical quantity of interest, we can assess how well an instrument measures that quantity. But things are much trickier when the construct we are trying to measure can’t be assessed directly. We have to measure something observable – our operationalization of the construct – and then make an argument about how the measure relates to the construct of interest. This is an argument for the <strong>validity</strong> of measurements from the instrument.<label for="tufte-sn-107" class="margin-toggle sidenote-number">107</label><input type="checkbox" id="tufte-sn-107" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">107</span> We are also going to talk in Chapter <a href="9-design.html#design">9</a> about the validity of manipulations. The way you identify a causal effect on some measure is by operationalizing some construct as well. If this is done badly, the manipulation can be invalid – meaning the causal effect that’s measured doesn’t map onto the construct.</span></p>
<p>These two concepts, reliability and validity, provide a conceptual toolkit for assessing how good a psychological measurement instrument is.</p>
<div id="island_2"><div class="box case_study"><div class="Collapsible"><span id="collapsible-trigger-1664301173542" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173542" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="microscope" class="svg-inline--fa fa-microscope " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M168 32c0-17.7 14.3-32 32-32h16c17.7 0 32 14.3 32 32h8c17.7 0 32 14.3 32 32V288c0 17.7-14.3 32-32 32h-8c0 17.7-14.3 32-32 32H200c-17.7 0-32-14.3-32-32h-8c-17.7 0-32-14.3-32-32V64c0-17.7 14.3-32 32-32l8 0zM32 448H320c70.7 0 128-57.3 128-128s-57.3-128-128-128V128c106 0 192 86 192 192c0 49.2-18.5 94-48.9 128H480c17.7 0 32 14.3 32 32s-14.3 32-32 32H320 32c-17.7 0-32-14.3-32-32s14.3-32 32-32zm80-64H304c8.8 0 16 7.2 16 16s-7.2 16-16 16H112c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg>Case study<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173542" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173542"><div class="Collapsible__contentInner"><p class="title">A reliable and valid measure of children’s vocabulary</p>

<p>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes <span class="citation">(<a href="#ref-marchman2008" role="doc-biblioref">Marchman &amp; Fernald, 2008</a>)</span>. Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.<label for="tufte-sn-108" class="margin-toggle sidenote-number">108</label><input type="checkbox" id="tufte-sn-108" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">108</span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref">Frank et al., 2021</a>)</span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related – you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!</span></p>
<p>Because bringing children into a lab can be expensive, one popular option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (the first page of an English form is shown in Figure <a href="8-measurement.html#fig:measurement-cdi">8.1</a>. But is parent report a reliable or valid measure of children’s early language?</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cdi"></span>
<img src="images/measurement/cdi.jpg" alt="The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language." width="\linewidth" />
Figure 8.1: The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children’s early language.
</span>
</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-psycho-cors"></span>
<img src="images/measurement/psycho-cors2.png" alt="Longitudinal correlations between a child's score on one administration of the CDI and another one several months later. From Frank et al. (2021). " width="\linewidth" />
Figure 8.2: Longitudinal correlations between a child’s score on one administration of the CDI and another one several months later. From Frank et al. (2021). 
</span>
</p>
<p>One test of the reliability of the CDI is a <strong>test-retest</strong> correlation, where we compute the correlation within children between two different administrations of the form. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! Figure @ref(fig:measurement-psycho-cors longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref">Frank et al., 2021</a>)</span>.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cdi-validity"></span>
<img src="images/measurement/cdi-validity.png" alt="Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights [@bornstein1998]." width="\linewidth" />
Figure 8.3: Relations between an early form of the CDI (the ELI) and several other measurements of children’s early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights <span class="citation">(<a href="#ref-bornstein1998" role="doc-biblioref">Bornstein &amp; Haynes, 1998</a>)</span>.
</span>
</p>
<p>Given that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? <span class="citation">Bornstein &amp; Haynes (<a href="#ref-bornstein1998" role="doc-biblioref">1998</a>)</span> collected many different measures of children’s language – including the ELI (an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. Figure <a href="8-measurement.html#fig:measurement-cdi-validity">8.3</a> shows the results of a structural equation model that measures the shared variance between these measures and a hypothesized central construct (“vocabulary competence”). The ELI (CDI) score correlated closely with the shared variance among all the different measures, suggesting that it was a valid measure of the construct.</p>
<p>The combination of reliability and validity evidence suggests that CDI are a useful (and relatively inexpensive source) of data about children’s early language, and indeed they have become one of the most common assessments for this age group!</p>

</div></div></div></div></div>
<div id="reliability" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Reliability</h2>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-brandmaier"></span>
<img src="images/measurement/reliability-validity.png" alt="Reliability and validity visualized. The reliability of an instrument is its expected precision. The bias of measurements from an instrument also provide a metaphor for its validity." width="\linewidth" />
Figure 8.4: Reliability and validity visualized. The reliability of an instrument is its expected precision. The bias of measurements from an instrument also provide a metaphor for its validity.
</span>
</p>
<p>Reliability is a way of describing the extent to which a measure yields signal relative to noise. Intuitively, if there’s less noise, then there will be more similarity between different measurements of the same quantity, illustrated in Figure <a href="8-measurement.html#fig:measurement-brandmaier">8.4</a> as a tighter grouping of points on the bulls-eye. But how do we measure signal and noise?</p>
<div id="island_3"><div class="box depth"><div class="Collapsible"><span id="collapsible-trigger-1664301173543" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173543" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="landmark" class="svg-inline--fa fa-landmark " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M240.1 4.2c9.8-5.6 21.9-5.6 31.8 0l171.8 98.1L448 104l0 .9 47.9 27.4c12.6 7.2 18.8 22 15.1 36s-16.4 23.8-30.9 23.8H32c-14.5 0-27.2-9.8-30.9-23.8s2.5-28.8 15.1-36L64 104.9V104l4.4-1.6L240.1 4.2zM64 224h64V416h40V224h64V416h48V224h64V416h40V224h64V420.3c.6 .3 1.2 .7 1.8 1.1l48 32c11.7 7.8 17 22.4 12.9 35.9S494.1 512 480 512H32c-14.1 0-26.5-9.2-30.6-22.7s1.1-28.1 12.9-35.9l48-32c.6-.4 1.2-.7 1.8-1.1V224z"></path></svg>Depth<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173543" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173543"><div class="Collapsible__contentInner"><p class="title">Early controversies over psychological measurement</p>

<blockquote>
<p>“Psychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of […] measurement” <span class="citation">(<a href="#ref-cattel1890mental" role="doc-biblioref">Cattel, 1890</a>)</span>.</p>
</blockquote>
<p>It is no coincidence that the founders of experimental psychology were obsessed with measurement <span class="citation">(<a href="#ref-heidelberger2004nature" role="doc-biblioref">Heidelberger, 2004</a>)</span>.
It was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science.
For example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor), was a 1887 philosophical treatise entitled “Zahlen und Messen” (“Counting and Measuring”; see <span class="citation">Darrigol (<a href="#ref-darrigol2003number" role="doc-biblioref">2003</a>)</span>).
In the same year, <span class="citation">Fechner (<a href="#ref-fechner1987my" role="doc-biblioref">1987</a>)</span> explicitly grappled with the foundations of measurement in “Uber die psychischen Massprincipien” (“On Psychic Measurement Principles”).</p>
<p>Many of the early debates over measurement revolved around the emerging area of <em>psychophysics</em>, the problem of relating objective, physical stimuli (e.g. light or sound or pressure) to the subjective sensations they produce in the mind.
For example, <span class="citation">Fechner (<a href="#ref-fechner1860elemente" role="doc-biblioref">1860</a>)</span> was interested in a quantity called the “just noticeable difference” (JND), the smallest change in a stimulus that can be discriminated by our senses.
He argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the reported intensity (up to some constant).
In other words, sensation was <em>measurable</em> via instruments like the JND.</p>
<p>It may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated.
But this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative ‘measurements’ of anything at all <span class="citation">(<a href="#ref-moscati2018measuring" role="doc-biblioref">Moscati, 2018</a>)</span>.
It was unable to reach a conclusion, with physicists and psychologists deadlocked:</p>
<blockquote>
<p>Having found that individual sensations have an order, they [some psychologists] assume that they are <em>measurable</em>. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws […] which, if they mean anything, are certainly false. <span class="citation">(<a href="#ref-ferguson1940" role="doc-biblioref">Ferguson &amp; Tucker, 1940</a>)</span></p>
</blockquote>
<p>The heart of the disagreement was rooted in the classical definition of quantity requiring strictly <em>additive</em> structure.
An attribute was only considered measurable in light of a meaningful concatenation operation.
For example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rock on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones.)
Norman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined <em>fundamental</em> measurement in this way <span class="citation">(e.g. see <a href="#ref-campbell1928account" role="doc-biblioref">Norman Robert Campbell, 1928</a>)</span>, contrasting it with <em>derived measurement</em> which was some function of fundamental measures.
According to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any <em>fundamental</em> scale with this kind of additive operation.
It just didn’t make sense to break up holistic sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined <span class="citation">(<a href="#ref-cattell1962relational" role="doc-biblioref">Cattell, 1962</a>)</span>.
Even the intuitive additive logic of <span class="citation">Donders (<a href="#ref-donders1969speed" role="doc-biblioref">1868</a>)</span>’s “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g. in an early textbook, <span class="citation">Woodworth (<a href="#ref-woodworth1938" role="doc-biblioref">1938</a>)</span> claimed “we cannot break up the reaction into successive acts and obtain the time for each act.”)</p>
<p>The primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments.
Exiled from classical frameworks of measurement, he went about developing an alternative “operational” framework <span class="citation">(<a href="#ref-stevens1946" role="doc-biblioref">Stevens, 1946</a>)</span>, where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see <a href="8-measurement.html#tab:measurement-stevens-table">8.1</a> below).
Stevens’ framework quickly spread, leading to an explosion of proposed measures.
However, operationalism remains controversial outside psychology <span class="citation">(<a href="#ref-michell1999measurement" role="doc-biblioref">Michell, 1999</a>)</span>.
The most extreme version of his stance (“measurement is the assignment of numerals to objects or events according to rule”) permits researchers to <em>define</em> constructs operationally in terms of a measure <span class="citation">(<a href="#ref-hardcastle1995ss" role="doc-biblioref">Hardcastle, 1995</a>)</span>.
For example, one may say that the construct of intelligence is simply <em>whatever it is</em> that IQ measures.
It is then left up to the researcher to decide which scale type their proposed measure should belong to.</p>
<p>In Chapter <a href="2-theories.html#theories">2</a>, we outlined a somewhat different view, closer to a kind of constructive realism <span class="citation">Putnam (<a href="#ref-putnam1999threefold" role="doc-biblioref">2000</a>)</span>.
Psychological constructs like working memory or theory of mind are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct.
In other words, we are not free to assign numerals however we like.
Whether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.</p>
<p>The next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations.
This effort culminated in the three-volume Foundations of Measurement series <span class="citation">Robert Duncan Luce et al. (<a href="#ref-luce2007foundations" role="doc-biblioref">1990</a>)</span>, which has become the canonical text for every psychology student seeking to understand measurement in the non-physical sciences.<label for="tufte-sn-109" class="margin-toggle sidenote-number">109</label><input type="checkbox" id="tufte-sn-109" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">109</span> It is worth noting that 20th century physics has also seriously challenged the classical additive understanding of measurement. For example, velocities are revealed to be non-additive under general relativity, and properties of quantum particles are only measurable under a complex probabilistic framework.</span>
One of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) <em>effects</em> of constructs in conjunction with one another:</p>
<blockquote>
<p>When no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive. <span class="citation">(<a href="#ref-luce1964simultaneous" role="doc-biblioref">R. Duncan Luce &amp; Tukey, 1964</a>)</span>.</p>
</blockquote>
<p>This modern viewpoint broadly informs the view we describe here.</p>
</div></div></div></div></div>
<div id="measurement-scales" class="section level3" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Measurement scales</h3>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-cv"></span>
<img src="images/measurement/cv.png" alt="Computing the coefficient of variation (CV)." width="\linewidth" />
Figure 8.5: Computing the coefficient of variation (CV).
</span>
</p>
<p>In the physical sciences, it’s common to measure the precision of an instrument by quantifying its coefficient of variation <span class="citation">(<a href="#ref-brandmaier2018" role="doc-biblioref">Brandmaier et al., 2018</a>)</span>:</p>
<p><span class="math display">\[CV = \frac{\sigma_w}{\mu_w}\]</span>
where <span class="math inline">\(\sigma_w\)</span> is the standard deviation of the measurements within an individual and <span class="math inline">\(\mu_w\)</span> is the mean of those measurements (Figure <a href="8-measurement.html#fig:measurement-cv">8.5</a>).</p>
<p>Imagine we measure the height of a person five times, resulting in measurements of 171cm, 172cm, 171cm, 173cm, and 172cm. These are the combination of the person’s true height (we assume they have one!) and some <strong>measurement error</strong>. Now we can use these measurements to compute the coefficient of variation, which is 0.005. Why can’t we just do that with psychological measurements?</p>
<p><span class="marginnote shownote"><span id="tab:measurement-stevens-table">Table 8.1: </span>Stevens (1946) table of scale types and their associated operations and statistics.</span></p>
<table><thead><tr><th style="text-align: left;">
Scale
</th><th style="text-align: left;">
Definition
</th><th style="text-align: left;">
Operations
</th><th style="text-align: left;">
Statistics
</th></tr></thead><tbody><tr><td style="text-align: left;">
Nominal
</td><td style="text-align: left;">
Unordered list
</td><td style="text-align: left;">
Equality
</td><td style="text-align: left;">
Mode
</td></tr><tr><td style="text-align: left;">
Ordinal
</td><td style="text-align: left;">
Ordered list
</td><td style="text-align: left;">
Greater than or less than
</td><td style="text-align: left;">
Median
</td></tr><tr><td style="text-align: left;">
Interval
</td><td style="text-align: left;">
Numerical
</td><td style="text-align: left;">
Equality of intervals
</td><td style="text-align: left;">
Mean, SD
</td></tr><tr><td style="text-align: left;">
Ratio
</td><td style="text-align: left;">
Numerical with zero
</td><td style="text-align: left;">
Equality of ratios
</td><td style="text-align: left;">
Coefficient of variation
</td></tr></tbody></table>
<p>Thinking about this question takes us on a detour through the different kinds of measurement scales used in psychological research <span class="citation">(<a href="#ref-stevens1946" role="doc-biblioref">Stevens, 1946</a>)</span>. The height measurements in our example are on what is known as a <strong>ratio</strong> scale: a scale in which numerical measurements are equally spaced and on which there is a true zero point. These scales are common for physical quantities but actually quite infrequent in psychology. More common are <strong>interval</strong> scales, in which there is no true zero point. For example, IQ (and other standardized scores) are intended to capture interval variation on some dimension but 0 is meaningless – an IQ of 0 does not correspond to any particular interpretation.<label for="tufte-sn-110" class="margin-toggle sidenote-number">110</label><input type="checkbox" id="tufte-sn-110" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">110</span> It can actually be shown in a suitably rigorous sense that ratio and interval scales (and another lying in between) are the <em>only</em> scales possible for the real numbers <span class="citation">(<a href="#ref-narens1986measurement" role="doc-biblioref">Narens &amp; Luce, 1986</a>)</span></span></p>
<p><strong>Ordinal</strong> scales are also commonly used. These are scales that are ordered but are not necessarily spaced equally. For example, levels of educational achievement (“Elementary”,“High school”,“Some college”,“College”,“Graduate school”) are ordered, but there is no sense in which “High school” is as far from “Elementary” as “Graduate school” is from “College.” The last type in Stevens’ hierarchy is <strong>nominal</strong> scales, in which no ordering is possible either. For example, race is an unordered scale in which multiple categories are present but there is no inherent ordering of these categories. The full hierarchy is presented in Table <a href="8-measurement.html#tab:measurement-stevens-table">8.1</a>.</p>
<p>Critically, different summary measures work for each scale type. If you have an unordered list like a list of options for a question about race on a survey, you can present the modal response (the most likely one). It doesn’t even make sense to think about what the median was – there’s no ordering! For ordered levels of education, a median is possible but you can’t compute a mean. And for interval variables like “number of correct answers on a math test” you can compute a mean and a standard deviation.<label for="tufte-sn-111" class="margin-toggle sidenote-number">111</label><input type="checkbox" id="tufte-sn-111" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">111</span> You might be tempted to think that “number of correct answers” is a ratio variable – but is zero really meaningful? Does it truly correspond to “no math knowledge” or is it just a stand-in for “less math knowledge than this test requires”?</span></p>
<p>Now we’re ready to answer our initial question about why we can’t quantify reliability using the coefficient of variation. Unless you have a ratio scale with a true zero, you can’t compute a coefficient of variation. Think about it for IQ scores: currently, by convention, standardized IQ scores are set to have a mean of 100. If we tested someone multiple times and found the standard deviation of their test scores was 4 points, then we could estimate the precision of their measurements as “CV” of 4/100 = .04. But since IQ of 0 isn’t meaningful, we could just set the mean IQ for the population to 200. Our test would be the same, and so the CV would be 4/200 = .02. On that logic we just doubled the precision of our measurements by rescaling the test! That doesn’t make any sense.</p>

</div>
<div id="measuring-reliability" class="section level3" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Measuring reliability</h3>
<p>So then how do we measure signal and noise when we don’t have a true zero? We can still look at the variation between repeated measurement, but rather than comparing that variation between measurements to the mean, we can compare it to some other kind of variation, for example, variation between people. In what follows, we’ll discuss reliability on interval scales, but many of the same tools have been developed for ordinal and nominal scales.
</p>
<p>Imagine that you are developing an instrument to measure some cognitive ability. We assume that every participant has a true ability, <span class="math inline">\(t\)</span>, just the same way that they have a true height in the example above. Every time we measure this true ability with our instrument, however, it gets messed up by some measurement error. Let’s specify that error is normally distributed with a mean of zero – so it doesn’t <strong>bias</strong> the measurements, it just adds noise. The result is our observed score, <span class="math inline">\(o\)</span>.<label for="tufte-sn-112" class="margin-toggle sidenote-number">112</label><input type="checkbox" id="tufte-sn-112" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">112</span> The approach we use to introduce this set of ideas is called <strong>classical test theory</strong>. There are other – more modern – alternative approaches, but CTT (as it’s called) is a good starting point for thinking through the concepts.</span></p>
<p>Taking this approach, we could define a relative version of the coefficient of variation. The idea is that the reliability of a measurement is the amount of variance attributable to the true score variance (signal), rather than the observed score variance (which includes noise). If <span class="math inline">\(\sigma^2_t\)</span> is the variance of the true scores and <span class="math inline">\(\sigma^2_o\)</span> is the variance of the observed scores, then this ratio is</p>
<p><span class="math display">\[
R = \frac{\sigma^2_t}{\sigma^2_o}.
\]</span>
When noise is high, then the denominator is going to be big and <span class="math inline">\(R\)</span> will go down to 0; when noise is low, the numerator and the denominator will be almost the same and <span class="math inline">\(R\)</span> will approach 1.</p>
<p>This all sounds great, except for one problem: we can’t compute reliability using this formula without knowing true ability scores. But if we knew those, we wouldn’t need to measure anything at all! To get around this fundamental issue, there are two main approaches to computing reliability from data.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-trt"></span>
<img src="images/measurement/trt.png" alt="Computing test-retest reliability." width="\linewidth" />
Figure 8.6: Computing test-retest reliability.
</span>
</p>
<p><strong>Test-retest reliability</strong>. Imagine you have two parallel versions of your instrument that are the same difficulty and hence reflect the same true score for each participant you assess. In that case, you can use these two measurement to compute the reliability of the instrument by simply computing the correlation between the two scores. The logic is that, if both variants reflect the same true score, then the covariance between them is the same as the covariance between the true score and itself – which is just <span class="math inline">\(\sigma^2_t\)</span>, the true score variance (the variable that we wanted but didn’t have). Test-retest reliability is thus a very convenient way to measure reliability (Figure <a href="8-measurement.html#fig:measurement-trt">8.6</a>).</p>
<p><strong>Internal reliability</strong>. If you don’t have two parallel versions of an instrument, or you can’t give the test twice for whatever reason, then you have another option. Assuming your instrument has multiple items – e.g., multiple survey questions or multiple math problems – then you can split the test in pieces and treat the scores from each of these sub-parts as parallel versions of the instrument. The simplest way to do this is to split the instrument in half and compute the correlation between participants’ scores on the two halves – this quantity is called <strong>split half reliability</strong>.<label for="tufte-sn-113" class="margin-toggle sidenote-number">113</label><input type="checkbox" id="tufte-sn-113" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">113</span> The problem is that each half is… half as long as the original instrument. To get around this, there is a correction called the Spearman-Brown correction that can be applied to estimate the expected correlation for the full-length instrument.</span></p>
<p>Another method for computing the internal consistency to treat each item as a sub-instrument and compute the average split-half correlation over all splits, a method which yields the statistic <strong>Cronbach’s alpha</strong>. Alpha is a widely reported statistic, but it is also widely misinterpreted <span class="citation">(<a href="#ref-sijtsma2009" role="doc-biblioref">Sijtsma, 2009</a>)</span>. First, it is actually a lower bound on reliability rather than a good estimate of reliability itself. And second, it is often misinterpreted as evidence that an instrument yields scores that are “internally consistent,” which it does not; it’s not an accurate summary of dimensionality. Alpha is a standard statistic, but it should be used with caution.</p>





<div id="island_4"><div class="box depth"><div class="Collapsible"><span id="collapsible-trigger-1664301173544" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173544" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="landmark" class="svg-inline--fa fa-landmark " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M240.1 4.2c9.8-5.6 21.9-5.6 31.8 0l171.8 98.1L448 104l0 .9 47.9 27.4c12.6 7.2 18.8 22 15.1 36s-16.4 23.8-30.9 23.8H32c-14.5 0-27.2-9.8-30.9-23.8s2.5-28.8 15.1-36L64 104.9V104l4.4-1.6L240.1 4.2zM64 224h64V416h40V224h64V416h48V224h64V416h40V224h64V420.3c.6 .3 1.2 .7 1.8 1.1l48 32c11.7 7.8 17 22.4 12.9 35.9S494.1 512 480 512H32c-14.1 0-26.5-9.2-30.6-22.7s1.1-28.1 12.9-35.9l48-32c.6-.4 1.2-.7 1.8-1.1V224z"></path></svg>Depth<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173544" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173544"><div class="Collapsible__contentInner"><p class="title">Reliability paradoxes!</p>

<p>There’s a major issue with calculating reliabilities using the approaches we described here: reliability will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!</p>
<p>Let’s think about the CDI data we were talking about earlier, which showed high test-retest reliability. Now imagine we restricted our sample to only 16 – 18 month-olds (our prior sample had 16 – 30-month-olds) with low maternal education. Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change <em>within</em> a child would be larger relative to the differences <em>between</em> children. That would make our test-retest reliability score go down, even though we would just be computing it on a subset of the same data.</p>
<p>We can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually <em>increase</em> our estimate of split-half reliability – while the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox” – sloppier data collection can actually lead to higher reliabilities.<label for="tufte-sn-114" class="margin-toggle sidenote-number">114</label><input type="checkbox" id="tufte-sn-114" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">114</span> If you get interested in this topic, take a look at <span class="citation">(<a href="#ref-luck2018" role="doc-biblioref"><strong>luck2018?</strong></a>)</span>. There’s also a fascinating article by <span class="citation">Hedge et al. (<a href="#ref-hedge2018" role="doc-biblioref">2018</a>)</span> that shows why many highly replicable cognitive tasks like the Stroop task nevertheless have low reliability: they don’t vary very much between individuals!</span></p>
<p>More generally, we need to be sensitive to the sources of variability we’re quantifying reliability over – both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs. across individuals (from some population). Both of these sampling decisions affect reliability – if the population is more variable <em>or</em> the questions are less variable, we’ll get higher reliability.</p>
</div></div></div></div></div>
</div>
<div id="practical-advice-for-computing-reliability" class="section level3" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Practical advice for computing reliability</h3>
<p>Ignorance is not bliss. If you don’t know the reliability of your measures for an experiment, you risk wasting your and your participants’ time. A higher reliability measure will lead to more precise measurements of a causal effect of interest and hence smaller sample sizes.<label for="tufte-sn-115" class="margin-toggle sidenote-number">115</label><input type="checkbox" id="tufte-sn-115" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">115</span> Low-reliability measures also limit your ability to detect correlations between measurements. One of us spent several fruitless months in graduate school running dozens of participants through batteries of language processing tasks and correlating the results across tasks. This exercise was a waste of time because most of the tasks were of such low reliability that, even had they been highly correlated with another task, this relationship would have been almost impossible to detect without a huge sample size. One rule of thumb that’s helpful for individual difference designs of this sort is that the maximal correlation that can be observed between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the square root of the product of their reliabilities: <span class="math inline">\(\sqrt{r_x r_y}\)</span>. So if you have two measures that are reliable at .25, the maximal measured correlation between them is .25 as well! This kind of method is now frequently used in cognitive neuroscience (and other fields as well) to compute the so-called <strong>noise ceiling</strong> for a measure: the maximum amount of signal that in principle <em>could</em> be predicted <span class="citation">(<a href="#ref-lage-castellanos2019" role="doc-biblioref">Lage-Castellanos et al., 2019</a>)</span>.</span></p>
<p>Test-retest reliability is generally the most conservative measure of reliability. Although these quantities don’t feature into the classical analysis, test-retest reliability estimates include not only measurement error but also participants’ state variation across different testing sessions and variance due to differences between versions of your instrument. These real-world quantities are absent from internal reliability estimates, which may make you erroneously think that there is more signal present in your instrument than there is. It’s hard work to measure test-retest reliability estimates, but if you plan on using an instrument more than once or twice, it will likely be worthwhile!</p>
<p>Finally, if you have multiple measurement items as part of your instrument, make sure you evaluate how they contribute to the reliability of the instrument. Perhaps you have several questions in a survey that you’d like to use to measure the same construct; perhaps multiple experimental vignettes that vary in content or difficulty. Some of these items may not contribute to your instrument’s reliability – and some may even detract. At a bare minimum, you should always visualize the distribution of responses across items to scan for <strong>floor and ceiling effects</strong> – when items always yield responses bunched at the bottom or top of the scale, limiting their usefulness – and take a look at whether there are particular items on which items do not relate to the others. If you are thinking about developing an instrument that you use repeatedly, it may be useful to use more sophisticated psychometric models to estimate the dimensionality of responses on your instrument as well as the properties of the individual items <span class="citation">(<a href="#ref-embretson2013" role="doc-biblioref">Embretson &amp; Reise, 2013</a>; <a href="#ref-furr2021" role="doc-biblioref">Furr, 2021</a>)</span>.</p>
</div>
</div>
<div id="validity" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Validity</h2>
<p>In Chapter <a href="2-theories.html#theories">2</a>, we talked about the process of theory building as a process of describing the relationships between constructs. But for the theory to be tested, the constructs must be measured so that you can test the relationships between them! Measurement and measure construction is therefore intimately related to theory construction, and the notion of validity is central.</p>
<p>A valid instrument measures the construct of interest. In Figure <a href="8-measurement.html#fig:measurement-brandmaier">8.4</a>, invalidity is pictured as bias – the holes in the target are tightly grouped but in the wrong place.<label for="tufte-sn-116" class="margin-toggle sidenote-number">116</label><input type="checkbox" id="tufte-sn-116" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">116</span> This metaphor is a good rough guide but it doesn’t distinguish an instrument that is systematically biased (for example, by estimating scores too low for one group) and one that is invalid (because it measures the wrong construct).</span> How can you tell if a measure is valid, given that the construct of interest is unobserved? There is no single test of the validity of a measure <span class="citation">(<a href="#ref-cronbach1955" role="doc-biblioref">Cronbach &amp; Meehl, 1955</a>)</span>. Rather, the measure is valid if there is evidence that it fits into the nomological network – the network of predicted relationships with other constructs and their measures.</p>
<p>Validity is typically established via an argument that calls on different sources of support <span class="citation">(<a href="#ref-kane1992" role="doc-biblioref">Kane, 1992</a>)</span>. Here are some of the ways that you might support the relationship between a measure and a construct:</p>
<ul>
<li><strong>Face validity</strong>: The measure looks like the construct, such that intuitively it is reasonable that it measures the construct.</li>
<li><strong>Ecological validity</strong>: The measure incorporates how the construct is used in people’s lives.<br />
</li>
<li><strong>Internal validity</strong>: Usually used negatively. A “challenge to internal validity” is a description of a case where the measure is administered in such a way as to weaken the relationship between measure and construct.<label for="tufte-sn-117" class="margin-toggle sidenote-number">117</label><input type="checkbox" id="tufte-sn-117" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">117</span> Sometimes this concept is described as only being relevant to the validity of a manipulation, e.g. when the manipulation of the construct is confounded and some other psychological variable is manipulated as well.</span></li>
<li><strong>Convergent validity</strong>: The classic strategy for showing validity is to show that a measure relates (usually, correlates) with other putative measures of the same construct. When these relationships are measured concurrently, this is sometimes called <strong>concurrent validity</strong>. This evidence is most convincing when the other measures themselves have validity evidence.<label for="tufte-sn-118" class="margin-toggle sidenote-number">118</label><input type="checkbox" id="tufte-sn-118" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">118</span> This idea of convergent validity is precisely the circularity of Cronbach and Meehl’s “nomological network” idea – a measure is valid if it relates to other valid measures, which themselves are only valid if the first one is! The measures are valid because the theory works, and the theory works because the measures are valid.</span></li>
<li><strong>Predictive validity</strong>. If the measure predicts other later measures of the construct; often used in lifespan and developmental studies where it is particularly prized for a measure to be able to predict meaningful life outcomes in the future.</li>
<li><strong>Divergent validity</strong>. If the measure can be shown to be distinct from measure(s) of a different construct, this evidence can help establish that the measure is specifically linked to the target construct.</li>
</ul>
<div id="validity-arguments-in-practice" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Validity arguments in practice</h3>
<p>Let’s take a look at how we might make an argument about the validity of the CDI, the vocabulary instrument that we used for our case study.</p>
<p>First, the CDI is face valid – it is clearly about early language ability. In contrast, even though a child’s height would likely be correlated with their early language ability, we should be skeptical of this measure due to its lack of face validity. In addition, the CDI shows good concurrent and predictive validity. Concurrently, the CDI correlates well with evidence from transcripts of children’s actual speech and from standardized language assessments (as discussed in the case study above). And predictively, CDI scores at age 2 actually relate to reading scores during elementary school <span class="citation">(<a href="#ref-marchman2008" role="doc-biblioref">Marchman &amp; Fernald, 2008</a>)</span>.</p>
<p>On the other hand, users of the CDI must avoid challenges to the internal validity of the data they collect. For example, some CDI data are compromised by confusing instructions or poor data collection processes <span class="citation">(<a href="#ref-frank2021" role="doc-biblioref">Frank et al., 2021</a>)</span>. Further, advocates and critics of the CDI argue about its ecological validity. There is something quite ecologically valid about asking parents and caregivers – who are experts on their own child – to report on their child’s abilities. On the other hand, the actual experience of filling out a structured form estimating language ability might be more familiar to some families from high education backgrounds than it would be for others from lower education backgrounds. Thus, a critic could reasonably say that comparisons of CDI scores across socioeconomic strata would be an invalid usage.</p>
</div>
<div id="avoid-questionable-measurement-practices" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Avoid questionable measurement practices!</h3>
<p>Experimentalists sometimes have a tendency to make up ad hoc measures on the fly. It’s fine to invent new measures, but the next step is to think about what evidence there is that it’s valid! Table <a href="8-measurement.html#tab:flake-questions">8.2</a> gives a set of questions to guide thoughtful reporting of measurement practices <span class="citation">(adapted from <a href="#ref-flake2020" role="doc-biblioref">Flake &amp; Fried, 2020</a>)</span>.</p>
<p><span class="marginnote shownote"><span id="tab:flake-questions">Table 8.2: </span>Questions about measurement that every reseacher should answer in their paper. Adapted from Flake &amp; Fried (2020).</span></p>
<table><thead><tr><th style="text-align: left;">
Question
</th><th style="text-align: left;">
Information to Report
</th></tr></thead><tbody><tr><td style="text-align: left;">
What is your construct?
</td><td style="text-align: left;">
Define construct, describe theory and research.
</td></tr><tr><td style="text-align: left;">
What measure did you use to operationalize your construct?
</td><td style="text-align: left;">
Describe measure and justify operationalization.
</td></tr><tr><td style="text-align: left;">
Did you select your measure from the literature or create it from scratch?
</td><td style="text-align: left;">
Justify measure selection and review evidence on reliability and validity (or disclose the lack of such evidence).
</td></tr><tr><td style="text-align: left;">
Did you modify your measure during the process?
</td><td style="text-align: left;">
Describe and justify any modifications; note whether they occurred before or after data collection.
</td></tr><tr><td style="text-align: left;">
How did you quantify your measure?
</td><td style="text-align: left;">
Describe decisions underlying the calculation of scores on the measure; note whether these were established before or after data collection and whether they are based on standards from previous literature.
</td></tr></tbody></table>
<p>One big issue to be careful about is that researchers have been known to modify their scales and their scale scoring practices (say, omitting items from a survey or rescaling responses) after data collection. This kind of post-hoc alteration of the measurement instrument can sometimes be justified by features of the data, but it can also look a lot like <span class="math inline">\(p\)</span>-hacking! If researchers modify their measurement strategy after seeing their data, this decision needs to be disclosed, and it may undermine their statistical inferences.</p>
<div id="island_5"><div class="box accident_report"><div class="Collapsible"><span id="collapsible-trigger-1664301173545" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173545" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="person-falling-burst" class="svg-inline--fa fa-person-falling-burst " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M256 32c0-17.7-14.3-32-32-32s-32 14.3-32 32l0 9.8c0 39-23.7 74-59.9 88.4C71.6 154.5 32 213 32 278.2V352c0 17.7 14.3 32 32 32s32-14.3 32-32l0-73.8c0-10 1.6-19.8 4.5-29L261.1 497.4c9.6 14.8 29.4 19.1 44.3 9.5s19.1-29.4 9.5-44.3L222.6 320H224l80 0 38.4 51.2c10.6 14.1 30.7 17 44.8 6.4s17-30.7 6.4-44.8l-43.2-57.6C341.3 263.1 327.1 256 312 256l-71.5 0-56.8-80.2-.2-.3c44.7-29 72.5-79 72.5-133.6l0-9.8zM96 80c0-26.5-21.5-48-48-48S0 53.5 0 80s21.5 48 48 48s48-21.5 48-48zM464 286.1l58.6 53.9c4.8 4.4 11.9 5.5 17.8 2.6s9.5-9 9-15.5l-5.6-79.4 78.7-12.2c6.5-1 11.7-5.9 13.1-12.2s-1.1-13-6.5-16.7l-65.6-45.1L603 92.2c3.3-5.7 2.7-12.8-1.4-17.9s-10.9-7.2-17.2-5.3L508.3 92.1l-29.4-74C476.4 12 470.6 8 464 8s-12.4 4-14.9 10.1l-29.4 74L343.6 68.9c-6.3-1.9-13.1 .2-17.2 5.3s-4.6 12.2-1.4 17.9l39.5 69.1-65.6 45.1c-5.4 3.7-8 10.3-6.5 16.7c.1 .3 .1 .6 .2 .8l19.4 0c20.1 0 39.2 7.5 53.8 20.8l18.4 2.9L383 265.3l36.2 48.3c2.1 2.8 3.9 5.7 5.5 8.6L464 286.1z"></path></svg>Accident report<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173545" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173545"><div class="Collapsible__contentInner"><p class="title">Talk about flexible measurement!</p>

<p>The Competitive Reaction Time Task (CRTT) is a lab-based measure of aggression. Participants are told that they are playing a reaction-time game against another player and are asked to set the parameters of a noise blast that will be played to their opponent. Unfortunately, in an analysis of the literature using CRTT, <span class="citation">Elson et al. (<a href="#ref-elson2014" role="doc-biblioref">2014</a>)</span> found that different papers using the CRTT use dramatically different methods for scoring the task. Across trials, both the volume and duration of the noise blast were sometimes analyzed. Sometimes these scores were transformed (via logarithms) or thresholded. Sometimes they were combined into a single score. Elson was so worried by this flexibility, he created a website, <a href>http://flexiblemeasures.com</a>, to document the variation he observed.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:crtt"></span>
<img src="images/measurement/CRTT.png" alt="Data on the number of publications using CRTT and the number of different quantifications of CRTT, plotted cumulatively until 2016. Image from [http://flexiblemeasures.com]()." width="\linewidth" />
Figure 8.7: Data on the number of publications using CRTT and the number of different quantifications of CRTT, plotted cumulatively until 2016. Image from <a href>http://flexiblemeasures.com</a>.
</span>
</p>
<p>As of 2016, Elson had found 130 papers using the CRTT. And across these papers, he documented an astonishing 157 quantification strategies. One paper reported ten different strategies for extracting numbers from this measure! More worrisome still, Elson and colleagues found that when they tried out some of these strategies on their own data, different strategies led to very different effect sizes and levels of statistical significance. They could effectively make a finding appear bigger or smaller depending on which scoring they chose.</p>
<p>This examination of the use of the CRTT measure has several implications. First, and most troublingly, there may have been undisclosed flexibility in the analysis of CRTT data across the literature, with investigators taking advantage of the lack of standardization to try many different analysis variants and report the one most favorable to their own hypothesis. Second, it is unknown which quantification of CRTT behavior is in fact most reliable and valid. Since some of these variants are presumably better than others, researchers are effectively “leaving money on the table” by using suboptimal quantifications. Finally, as a consequence, when if researchers adopt the CRTT, they find much less guidance from the literature on what quantification to adopt.</p>
</div></div></div></div></div>
</div>
</div>
<div id="how-to-select-a-good-measure" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> How to select a good measure?</h2>
<p>Ideally you want a measure that is reliable and valid. How do you get one? An important first principle is to use a pre-existing measure. Perhaps someone else has done the hard work of compiling evidence on reliability and validity, and in that case you will most likely want to piggyback on that work. Standardized measures are typically broad in their application and so the tendency can be to discard these because they are not tailored for our studies specifically. But the benefits of a standardized measure are substantial. Not only can you justify the measure using the prior literature, you also have an important index of population variability by comparing absolute scores to other reports.<label for="tufte-sn-119" class="margin-toggle sidenote-number">119</label><input type="checkbox" id="tufte-sn-119" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">119</span> Comparing absolute measurements is a really important trick for “sanity-checking” your data. If your participants are way less accurate than the ones in the paper you’re following up, that may be a signal that something has gone wrong.</span></p>
<p>If you don’t use someone else’s measure, you’ll need to make one up yourself. Most experimenters go down this route at some point, but if you do, remember that you will need to figure out how to estimate its reliability and also how to make an argument for its validity!</p>
<p>We can assign numbers to almost anything people do. We could run an experiment on children’s exploratory play and count the number of times they interact with another child <span class="citation">(<a href="#ref-ross1989" role="doc-biblioref">H. S. Ross &amp; Lollis, 1989</a>)</span>, or run an experiment on aggression where we quantify the amount of hot sauce participants serve <span class="citation">(<a href="#ref-lieberman1999" role="doc-biblioref">Lieberman et al., 1999</a>)</span>. Yet most of the time we choose a relatively small set of operational variables: asking survey questions, collecting choices and reaction times, and measuring physiological variables like eye-movements. Besides following these conventions, how do we choose the right measurement type for a particular experiment?</p>
<p>There’s no hard and fast rule about what aspect of behavior to measure, but here we will focus on two dimensions that can help us organize the broad space of possible measure targets.<label for="tufte-sn-120" class="margin-toggle sidenote-number">120</label><input type="checkbox" id="tufte-sn-120" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">120</span> Some authors differentiate between “self-report” and “observational” measures. This distinction seems simple on its face, but actually gets kind of complicated. Is a facial expression a “self-report”? Language is not the only way that people communicate with one another – many actions are intended to be communicative <span class="citation">(<a href="#ref-shafto2012" role="doc-biblioref">Shafto et al., 2012</a>)</span>.</span> The first of these is the continuum between simple and complex behaviors. The second is the focus on explicit, voluntary behaviors vs. implicit or involuntary behaviors.</p>
<div id="simple-vs.-complex-behaviors" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Simple vs. complex behaviors</h3>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-considerations"></span>
<img src="images/measurement/measure-considerations.png" alt="Often choosing a measure can be consolidated into a choice along a continuum from simple measures that provide a small amount of information but are quick and easy to repeat and those that provide much richer information but require more time." width="\linewidth" />
Figure 8.8: Often choosing a measure can be consolidated into a choice along a continuum from simple measures that provide a small amount of information but are quick and easy to repeat and those that provide much richer information but require more time.
</span>
</p>
<p>Figure <a href="8-measurement.html#fig:measurement-considerations">8.8</a> shows the continuum between simple and complex behaviors. The simplest measurable behaviors tend to be button presses, for example:</p>
<ul>
<li>pressing a key to advance to the next word in a word-by-word self-paced reading study</li>
<li>selecting “yes” or “no” in a lexical decision task</li>
<li>making a forced choice between different alternatives to indicate which has been seen before</li>
</ul>
<p>These specific measures – and many more like them – are the bread and butter of many cognitive psychology studies. Because they are quick and easy to explain, these tasks can be repeated over many trials. They can also be executed with a wider variety of populations including with young children and sometimes even with non-human animals with appropriate adaptation. (A further benefit of these paradigms is that they can yield useful reaction time data, which we discuss further below).</p>
<p>In contrast, a huge range of complex behaviors have been studied by psychologists, including:</p>
<ul>
<li>open-ended verbal interviews</li>
<li>written expression, e.g. via handwriting or writing style</li>
<li>body movements, including gestures, walking, or dance</li>
<li>drawing or artifact building</li>
</ul>
<p>There are many reasons to study these kinds of behaviors. First, the behaviors themselves may be examples of tasks of interest (e.g., studies of drawing that seek to understand the origins of artistic expression). Or, the behavior may stand in for other even more complex behaviors of interest, as in studies of typing that use this behavior as a proxy for lexical knowledge <span class="citation">(<a href="#ref-rumelhart1982" role="doc-biblioref">Rumelhart &amp; Norman, 1982</a>)</span>.</p>
<p>Complex behaviors typically afford a huge variety of different measurement strategies. So any experiment that uses a particular measurement of a complex behavior will typically need to do significant work up front to justify the choice of that measurement strategy – how to quantify dances or gestures or typing errors – and provide some assurance about its reliability. Further, it is often much more difficult to have a participant repeat a complex behavior many times under the same conditions. Imagine asking someone to draw hundreds of sketches as opposed to pressing a key hundreds of times! Thus, the choice of a complex behavior is often a choice to forego a large number of simple trials for a small number of more complex trials.</p>
<p>Complex behaviors can be especially useful to study either at the beginning or the end of a set of experiments. At the beginning of a set of experiments, they can provide inspiration about the richness of the target behavior and insight into the many factors that influence it. And at the end of a set of experiments, they can provide an ecologically valid measure to complement a reliable but more artificial, lab-based behavior.</p>
<p>The more complex the behavior, however, the more it will vary across individuals and the more environmental and situational factors will affect it. These can be important parts of the phenomenon, but they will also be nuisances that are difficult to get under experimental control. Simple measures are typically easier to use and hence easier to deploy repeatedly in a set of experiments where you iterate your manipulation to test a causal theory.</p>
</div>
<div id="implicit-vs.-explicit-behaviors" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Implicit vs. explicit behaviors</h3>
<p>A second important dimension of organization for measures is the difference between implicit and explicit measures. An explicit measure provides a measurement of a behavior that a participant has conscious awareness of – for example, the answer to a question. In contrast, implicit measures provide measurements of psychological processes that participants are unable to report (or occasionally, unwilling to).<label for="tufte-sn-121" class="margin-toggle sidenote-number">121</label><input type="checkbox" id="tufte-sn-121" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">121</span> Implicit/explicit is likely more of a continuum, but one cut-point is whether the participants’ behavior is considered intentional: that is, participants <em>intend</em> to press a key to register a decision, but they likely do not intend to react in 300 as opposed to 350 milliseconds due to having seen a prime.</span> Implicit measures, especially reaction time, have long been argued to reflect internal psychological processes <span class="citation">(<a href="#ref-donders1969" role="doc-biblioref">Donders, 1969</a>)</span>. They also have been proposed as measures of qualities such as racial bias that participants may have motivation not to disclose <span class="citation">(<a href="#ref-greenwald1998" role="doc-biblioref">Greenwald et al., 1998</a>)</span>.
There are also of course a host of physiological measurements available. Some of these measure eye-movements, heart rate, or skin conductance, which can be linked to aspects of cognitive process. Others reflect underlying brain activity via the signals associated with MRI, MEG, NIRS, and EEG measurements. These methods are outside the scope of this book, though we note that the measurement concerns we discuss here are entirely germane <span class="citation">(e.g., <a href="#ref-zuo2019" role="doc-biblioref">Zuo et al., 2019</a>)</span>.</p>
<p>Many tasks produce both accuracy and reaction time data. Often these trade off with one another in a classic <strong>speed-accuracy tradeoff</strong>: the faster participants respond, the less accurate they are. For example, to investigate racial bias in policing, <span class="citation">Payne (<a href="#ref-payne2001" role="doc-biblioref">2001</a>)</span> showed US college students a series of pictures of tools and guns, proceeded by a prime of either a White face or a Black face. In a first study, participants were faster to identify weapons when primed by a Black face but had similar accuracies. A second study added a response deadline to speed up judgments: this manipulation resulted in equal reaction times across conditions but greater errors in weapon identification after Black prime faces. These studies likely revealed the same phenomenon – some sort of bias to associate Black faces with weapons – but the design of the task moved participants along a speed accuracy tradeoff, yielding effects on different measures.<label for="tufte-sn-122" class="margin-toggle sidenote-number">122</label><input type="checkbox" id="tufte-sn-122" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">122</span> One way of describing the information processing underlying this tradeoff is given by drift diffusion models, which allow joint analysis of accuracy and reaction time <span class="citation">(<a href="#ref-voss2013" role="doc-biblioref">Voss et al., 2013</a>)</span>. Used appropriately, drift diffusion models can provide a way to remove speed-accuracy tradeoffs and extract more reliable signals from tasks where accuracy and reaction time are both measured (see <span class="citation">Johnson et al. (<a href="#ref-johnson2017" role="doc-biblioref">2017</a>)</span> for an example of DDM on a weapon-decision task).</span></p>
<p>Simple, explicit behaviors are often a good starting point. Work using these measures – often the least ecologically valid – can then be enriched with implicit measures or measurements of more complex behaviors.</p>
<div id="island_6"><div class="box depth"><div class="Collapsible"><span id="collapsible-trigger-1664301173546" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173546" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="landmark" class="svg-inline--fa fa-landmark " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M240.1 4.2c9.8-5.6 21.9-5.6 31.8 0l171.8 98.1L448 104l0 .9 47.9 27.4c12.6 7.2 18.8 22 15.1 36s-16.4 23.8-30.9 23.8H32c-14.5 0-27.2-9.8-30.9-23.8s2.5-28.8 15.1-36L64 104.9V104l4.4-1.6L240.1 4.2zM64 224h64V416h40V224h64V416h48V224h64V416h40V224h64V420.3c.6 .3 1.2 .7 1.8 1.1l48 32c11.7 7.8 17 22.4 12.9 35.9S494.1 512 480 512H32c-14.1 0-26.5-9.2-30.6-22.7s1.1-28.1 12.9-35.9l48-32c.6-.4 1.2-.7 1.8-1.1V224z"></path></svg>Depth<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173546" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173546"><div class="Collapsible__contentInner"><p class="title">Survey measures</p>

<p>Sometimes the easiest way to elicit information from participants is simply to ask. Survey questions are an important part of experimental measurement, so we’ll share a few best practices, primarily derived from <span class="citation">Krosnick &amp; Presser (<a href="#ref-krosnick2010" role="doc-biblioref">2010</a>)</span>.</p>
<p>Treat survey questions as a conversation. The easier your items are to understand, the better. Don’t repeat variations on the same question unless you want different answers! Try to make the order reasonable. The more you include “tricky” items the more you invite tricky answers to straightforward questions.<label for="tufte-sn-123" class="margin-toggle sidenote-number">123</label><input type="checkbox" id="tufte-sn-123" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">123</span> We’ll talk in Chapter <a href="12-collection.html#collection">12</a> about manipulation checks and their strengths and weaknesses.</span></p>
<p>Open-ended survey questions can be quite rich and informative, especially when an appropriate coding scheme is developed in advance and responses are categorized into a relatively small number of types. On the other hand, they present practical obstacles because they require coding (often by multiple coders to ensure reliability of the coding). Further, they tend to yield nominal data, which are often less useful for quantitative theorizing. Open-ended questions are a useful tool to add nuance and color to the interpretation of an experiment.</p>
<p>
<span class="marginnote shownote">
<span style="display: block;" id="fig:measurement-likert"></span>
<img src="images/measurement/likert.png" alt="Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled." width="\linewidth" />
Figure 8.9: Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled.
</span>
</p>
<p>Especially given their ubiquity in commercial survey research, Likert scales with a fixed number of response items are a simple and conventional way of gathering data on attitude and judgment questions (Figure <a href="8-measurement.html#fig:measurement-likert">8.9</a>). Bipolar scales are those in which the endpoints represent opposites, for example the continuum between “strongly dislike” and “strongly like.” Unipolar scales have one neutral endpoint, like the continuum between “no pain” and “very intense pain.” Survey best practices suggest that reliability is maximized when bipolar scales have seven points and unipolar scales have five. Labeling every point on the scale with verbal labels is preferable to labeling only the endpoints.<label for="tufte-sn-124" class="margin-toggle sidenote-number">124</label><input type="checkbox" id="tufte-sn-124" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">124</span> One important question is whether to treat data from Likert scales as ordinal or interval. It’s extremely common (and convenient) to make the assumption that Likert ratings are interval, allowing the use of standard statistical tools like means, standard deviations, linear regression, and the like. The risk in this practice comes from the possibility that scale items are not evenly spaced – for example, on a scale labeled “never”,“seldom”, “occasionally”,“often”,“always,” the distance from “often” to “always” may be larger than the distance from “seldom” to “occasionally.”
In practice, you can choose to use regression variants that are appropriate, e.g. ordinal logistic regression and its variants, or they can attempt to assess and mitigate the risks of treating the data as interval. If you choose the second option, it’s definitely a good idea to look carefully at the raw distributions for individual items (see Chapter <a href="15-viz.html#viz">15</a>) to see if their distribution appears approximately normal and not highly skewed or censored. You should also consider the names you give to your scale up front to try to minimize these issues.</span> Recently some researchers have begun to use “visual analog scales” (or sliders) as a solution. We don’t recommend these – the distribution of the resulting data is often anchored at the starting point or endpoints <span class="citation">(<a href="#ref-matejka2016" role="doc-biblioref">Matejka et al., 2016</a>)</span>, and a meta-analysis shows that are quite a bit lower than Likert scales in reliability <span class="citation">(<a href="#ref-krosnick2010" role="doc-biblioref">Krosnick &amp; Presser, 2010</a>)</span>.</p>
<p>It rarely helps matters to add a “don’t know” or “other” option to survey questions. These are some of a variety of practices that encourage <strong>satisficing</strong>, where survey takers give answers that are good enough but don’t reflect substantial thought about the question. Another behavior that results from satisficing is “straight-lining” – that is, picking the same option for every question. In general, the best way to prevent straight-lining is to make surveys relatively short, engaging, and well-compensated. The practice of “reverse coding” to make the expected answers to some questions more negative can block straight-lining, but at the cost of making items more confusing [often by introducing pragmatically infelicitous negation; <span class="citation">Nieuwland &amp; Kuperberg (<a href="#ref-nieuwland2008" role="doc-biblioref">2008</a>)</span>]. Some obvious formatting options can reduce straight-lining as well, for example placing scales further apart or on subsequent (web) pages.</p>
<p>In sum, survey questions can be a helpful tool for eliciting graded judgments about explicit questions. The best way to execute them well is to try and make them as clear and easy to answer as possible.</p>
</div></div></div></div></div>
</div>
</div>
<div id="the-temptation-to-measure-lots-of-things" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> The temptation to measure lots of things</h2>
<p>If one measure is good, shouldn’t two be better? Many experimenters add multiple measurements to their experiments, reasoning that more data is better than less. But that’s not always true!</p>
<p>The decision whether to include multiple measures is an aesthetic and practical issue as well as a scientific one. Throughout this book we have been advocating for a viewpoint in which experiments should be as simple as possible. For us, the best experiment is one that shows that a simple and valid manipulation affects a single, reliable and valid measure.<label for="tufte-sn-125" class="margin-toggle sidenote-number">125</label><input type="checkbox" id="tufte-sn-125" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">125</span> In an entertaining article called “things I have learned (so far)”, <span class="citation">Cohen (<a href="#ref-cohen1990" role="doc-biblioref">1990</a>)</span> quips that he leans so far in the direction of large numbers of observations and small numbers of measures, that some students think his perfect study has 10,000 participants and no measures.</span> If you are tempted to include more than one measure, see if we can talk you out of it.<label for="tufte-sn-126" class="margin-toggle sidenote-number">126</label><input type="checkbox" id="tufte-sn-126" class="margin-toggle" /><span class="sidenote"><span class="sidenote-number">126</span> As usual, we want to qualify that we are only talking about randomized experiments here! In observational studies, often the point is to measure the associations between multiple measures so you typically <em>have</em> to include more than one. Additionally, some of the authors of this book have advocated for measuring multiple outcomes in longitudinal observational studies, which could reduce investigator bias, encourage reporting null effects, enable comparison of effect sizes, and improve research efficiency <span class="citation">(<a href="#ref-vanderweele2020outcome" role="doc-biblioref">VanderWeele et al., 2020</a>)</span>. We’ve also done plenty of descriptive studies – these can be very valuable. In a descriptive context, often the goal is to include as many measures as possible so as to have a holistic picture of the phenomenon of interest.</span></p>
<p>First, make sure that including more measures doesn’t compromise each individual measure. This can happen via fatigue or carryover effects. For example, if a brief attitude induction is followed by multiple questionnaire measures, it is a good bet that there is likely to be “fade-out” of the effect over time, so it won’t have the same effect on the first questionnaire as the last one. Further, even if a condition manipulation has a long duration effect on participants, survey fatigue may lead to less meaningful responses to later questions <span class="citation">(<a href="#ref-herzog1981" role="doc-biblioref">Herzog &amp; Bachman, 1981</a>)</span>.</p>
<p>Second, consider whether you have a strong prediction for each measure, or whether you are simply looking for more ways to see an effect of your manipulation. As we’ve discussed in Chapter <a href="2-theories.html#theories">2</a>, we think of an experiment as a “bet.” On that viewpoint, theories are best tested by observing measurements that they predict but that are low probability according to others. The more measures you add, the more bets you are making but the less value you are putting on each. In essence, you are hedging your bets and so the success of any one bet is less convincing.</p>
<p>Third, if you include multiple measures in your experiment, you need to think about how you will interpret inconsistent results. Imagine you have experimental participants engage in a brief written reflection that is hypothesized to affect a construct (vs a control writing exercise, say listing meals). If you include two measures of the construct of interest and one shows a larger effect, what will you conclude? It may be tempting to assume that the one that shows a larger effect is the “better measure” but the logic is circular – it’s only better if the manipulation affected the construct of interest, which is what you were testing in the first place! Including multiple measures because you’re uncertain which one is more related to the construct indulges in this circular logic, since the experiment often can’t resolve the situation. A much better move in this case is to do a preliminary study of the reliability and validity of the two measures so as to be able to select one as the experiment’s primary endpoint.</p>
<p>Finally, if you do include multiple measures, selective reporting of significant or hypothesis-aligned measures becomes a real risk. For this reason, preregistration and transparent reporting of all measures becomes even more important.</p>
<p>There are some cases where more measures are better. The more expensive the experiment, the less likely it is to be repeated to gather a new measurement of the effects of the same manipulation. Thus, larger studies present a stronger rationale for including multiple measures. Clinical trials often involve interventions that can have effects on many different measures; imagine a cancer treatment that might affect mortality rates, quality of life, tumor growth rates, etc. Further, such trials are extremely expensive and difficult to repeat. Thus, there is a strong rationale for including more measures in such studies.</p>


</div>
<div id="chapter-summary-measurement" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Chapter summary: Measurement</h2>
<p>In olden times, all the psychologists went to the same conferences and worried about the same things. But then a split formed between different groups. Educational psychologists and psychometricians thought a lot about how different problems on tests had different measurement properties. They began exploring how to select good and bad items, and how to figure out people’s ability abstracted away from specific items. This led to a profusion of interesting ideas about measurement and modeling, but these ideas rarely percolated into day-to-day practice in other areas.
For example, cognitive psychologists collected lots of trials and measured quantities of interest with high precision, but worried less about measure validity or experimental reliability.
Social psychologists spent more time worrying about issues of ecological validity in their experiments, but often used <em>ad hoc</em> scales with poor psychometric properties.</p>
<p>These sociological differences between fields has led to an unfortunate divergence, where experimentalists often do not recognize the value of the conceptual tools developed to aid measurement, and hence fail to reason about the reliability and validity of their measures in ways that can help them make better inferences. The fundamental insight of the psychometric perspective is that the constructs we study as psychologists are latent, rather than directly observed. So when we attempt to measure these constructs, we need to understand the properties of our instruments and how we hypothesize that they connect to the constructs of interest. As we said in our discussion of reliability, ignorance is not bliss. Even if you fail to make explicit assumptions about how your measure functions and how it connects to your construct of interest, your adoption of defaults still constitutes a choice. Much better to think these choices through!</p>



<div id="island_7"><div class="box discussion_questions"><div class="Collapsible"><span id="collapsible-trigger-1664301173547" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173547" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="pen-ruler" class="svg-inline--fa fa-pen-ruler " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M469.3 19.3l23.4 23.4c25 25 25 65.5 0 90.5l-56.4 56.4L322.3 75.7l56.4-56.4c25-25 65.5-25 90.5 0zM44.9 353.2L299.7 98.3 413.7 212.3 158.8 467.1c-6.7 6.7-15.1 11.6-24.2 14.2l-104 29.7c-8.4 2.4-17.4 .1-23.6-6.1s-8.5-15.2-6.1-23.6l29.7-104c2.6-9.2 7.5-17.5 14.2-24.2zM249.4 103.4L103.4 249.4 16 161.9c-18.7-18.7-18.7-49.1 0-67.9L94.1 16c18.7-18.7 49.1-18.7 67.9 0l19.8 19.8c-.3 .3-.7 .6-1 .9l-64 64c-6.2 6.2-6.2 16.4 0 22.6s16.4 6.2 22.6 0l64-64c.3-.3 .6-.7 .9-1l45.1 45.1zM408.6 262.6l45.1 45.1c-.3 .3-.7 .6-1 .9l-64 64c-6.2 6.2-6.2 16.4 0 22.6s16.4 6.2 22.6 0l64-64c.3-.3 .6-.7 .9-1L496 350.1c18.7 18.7 18.7 49.1 0 67.9L417.9 496c-18.7 18.7-49.1 18.7-67.9 0l-87.4-87.4L408.6 262.6z"></path></svg>Discussion questions<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173547" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173547"><div class="Collapsible__contentInner">

</div></div></div></div></div>
<div id="island_8"><div class="box readings"><div class="Collapsible"><span id="collapsible-trigger-1664301173581" class="Collapsible__trigger is-closed" aria-expanded="false" aria-disabled="false" aria-controls="collapsible-content-1664301173581" role="button"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="book" class="svg-inline--fa fa-book " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M96 0C43 0 0 43 0 96V416c0 53 43 96 96 96H384h32c17.7 0 32-14.3 32-32s-14.3-32-32-32V384c17.7 0 32-14.3 32-32V32c0-17.7-14.3-32-32-32H384 96zm0 384H352v64H96c-17.7 0-32-14.3-32-32s14.3-32 32-32zm32-240c0-8.8 7.2-16 16-16H336c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16zm16 48H336c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg>Readings<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="angles-down" class="svg-inline--fa fa-angles-down " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M246.6 470.6c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 402.7 361.4 265.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3l-160 160zm160-352l-160 160c-12.5 12.5-32.8 12.5-45.3 0l-160-160c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L224 210.7 361.4 73.4c12.5-12.5 32.8-12.5 45.3 0s12.5 32.8 0 45.3z"></path></svg></span><div id="collapsible-content-1664301173581" class="Collapsible__contentOuter" style="height: 0px; -webkit-transition: height 300ms ease; ms-transition: height 300ms ease; transition: height 300ms ease; overflow: hidden;" role="region" aria-labelledby="collapsible-trigger-1664301173581"><div class="Collapsible__contentInner">
<ul>
<li><p>A classic textbook on psychometrics that introduces the concepts of reliability and validity in a simple and readable way: Furr, R. M. (2021). <em>Psychometrics: an introduction</em>. SAGE publications.</p></li>
<li><p>A great primer on questionnaire design: Krosnick, J.A. (2018). Improving Question Design to Maximize Reliability and Validity. In: Vannette, D., Krosnick, J. (eds) The Palgrave Handbook of Survey Research. Palgrave Macmillan, Cham. <a href>https://doi.org/10.1007/978-3-319-54395-6_13</a></p></li>
<li><p>Introduction to general issues in measurement and why they shouldn’t be ignored: Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456-465. <a href>https://doi.org/10.1177/2515245920952393</a></p></li>
</ul>
</div></div></div></div></div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-bornstein1998" class="csl-entry">
Bornstein, M. H., &amp; Haynes, O. M. (1998). Vocabulary competence in early childhood: Measurement, latent construct, and predictive validity. <em>Child Development</em>, <em>69</em>(3), 654–671.
</div>
<div id="ref-brandmaier2018" class="csl-entry">
Brandmaier, A. M., Wenger, E., Bodammer, N. C., Kühn, S., Raz, N., &amp; Lindenberger, U. (2018). Assessing reliability in neuroimaging research through intra-class effect decomposition (ICED). <em>Elife</em>, <em>7</em>, e35718.
</div>
<div id="ref-campbell1928account" class="csl-entry">
Campbell, Norman Robert. (1928). <em>An account of the principles of measurement and calculation</em>. Longmans, Green; Company, Limited.
</div>
<div id="ref-campbell1938symposium" class="csl-entry">
Campbell, Norman R., &amp; Jeffreys, H. (1938). Symposium: Measurement and its importance for philosophy. <em>Proceedings of the Aristotelian Society, Supplementary Volumes</em>, <em>17</em>, 121–151.
</div>
<div id="ref-cattel1890mental" class="csl-entry">
Cattel, J. M. (1890). Mental tests and measurements. <em>Mind</em>, <em>15</em>, 373–380.
</div>
<div id="ref-cattell1962relational" class="csl-entry">
Cattell, R. B. (1962). The relational simplex theory of equal interval and absolute scaling. <em>Acta Psychologica</em>, <em>20</em>, 139–158.
</div>
<div id="ref-chang2004inventing" class="csl-entry">
Chang, H. (2004). <em>Inventing temperature: Measurement and scientific progress</em>. Oxford University Press.
</div>
<div id="ref-cohen1990" class="csl-entry">
Cohen, J. (1990). Things i have learned (so far). <em>American Psychologist</em>, <em>45</em>, 1304–1312.
</div>
<div id="ref-cronbach1955" class="csl-entry">
Cronbach, L. J., &amp; Meehl, P. E. (1955). Construct validity in psychological tests. <em>Psychol. Bull.</em>, <em>52</em>(4), 281–302.
</div>
<div id="ref-darrigol2003number" class="csl-entry">
Darrigol, O. (2003). Number and measure: Hermann von helmholtz at the crossroads of mathematics, physics, and psychology. <em>Studies in History and Philosophy of Science Part A</em>, <em>34</em>(3), 515–573.
</div>
<div id="ref-donders1969speed" class="csl-entry">
Donders, F. C. (1868). On the speed of mental processes. <em>Acta Psychologica</em>, <em>30</em>, 412–431.
</div>
<div id="ref-donders1969" class="csl-entry">
Donders, F. C. (1969). On the speed of mental processes. <em>Acta Psychologica</em>, <em>30</em>, 412–431.
</div>
<div id="ref-elson2014" class="csl-entry">
Elson, M., Mohseni, M. R., Breuer, J., Scharkow, M., &amp; Quandt, T. (2014). Press <span>CRTT</span> to measure aggressive behavior: The unstandardized use of the competitive reaction time task in aggression research. <em>Psychological Assessment</em>, <em>26</em>(2), 419–432. <a href="https://doi.org/10.1037/a0035569">https://doi.org/10.1037/a0035569</a>
</div>
<div id="ref-embretson2013" class="csl-entry">
Embretson, S. E., &amp; Reise, S. P. (2013). <em>Item response theory</em>. Psychology Press.
</div>
<div id="ref-fechner1860elemente" class="csl-entry">
Fechner, G. T. (1860). <em>Elemente der psychophysik</em> (Vol. 2). Breitkopf u. H<span>ä</span>rtel.
</div>
<div id="ref-fechner1987my" class="csl-entry">
Fechner, G. T. (1987). My own viewpoint on mental measurement (1887). <em>Psychological Research</em>, <em>49</em>(4), 213–219.
</div>
<div id="ref-ferguson1940" class="csl-entry">
Ferguson, M., A., &amp; Tucker, W. S. (1940). Quantitative estimates of sensory events, final report. <em>Report of the British Association for the Advancement of Science</em>, 331–349.
</div>
<div id="ref-flake2020" class="csl-entry">
Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. <em>Advances in Methods and Practices in Psychological Science</em>, <em>3</em>(4), 456–465.
</div>
<div id="ref-frank2021" class="csl-entry">
Frank, M. C., Braginsky, M., Yurovsky, D., &amp; Marchman, V. A. (2021). <em>Variability and consistency in early language learning: The wordbank project</em>. MIT Press.
</div>
<div id="ref-furr2021" class="csl-entry">
Furr, R. M. (2021). <em>Psychometrics: An introduction</em>. SAGE publications.
</div>
<div id="ref-greenwald1998" class="csl-entry">
Greenwald, A. G., McGhee, D. E., &amp; Schwartz, J. L. (1998). Measuring individual differences in implicit cognition: The implicit association test. <em>Journal of Personality and Social Psychology</em>, <em>74</em>(6), 1464.
</div>
<div id="ref-hardcastle1995ss" class="csl-entry">
Hardcastle, G. L. (1995). SS stevens and the origins of operationism. <em>Philosophy of Science</em>, 404–424.
</div>
<div id="ref-hedge2018" class="csl-entry">
Hedge, C., Powell, G., &amp; Sumner, P. (2018). The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences. <em>Behavior Research Methods</em>, <em>50</em>(3), 1166–1186.
</div>
<div id="ref-heidelberger2004nature" class="csl-entry">
Heidelberger, M. (2004). <em>Nature from within: Gustav theodor fechner and his psychophysical worldview</em>. University of Pittsburgh Pre.
</div>
<div id="ref-herzog1981" class="csl-entry">
Herzog, A. R., &amp; Bachman, J. G. (1981). Effects of questionnaire length on response quality. <em>Public Opinion Quarterly</em>, <em>45</em>(4), 549–559.
</div>
<div id="ref-johnson2017" class="csl-entry">
Johnson, D. J., Hopwood, C. J., Cesario, J., &amp; Pleskac, T. J. (2017). Advancing research on cognitive processes in social and personality psychology: A hierarchical drift diffusion model primer. <em>Social Psychological and Personality Science</em>, <em>8</em>(4), 413–423.
</div>
<div id="ref-kane1992" class="csl-entry">
Kane, M. T. (1992). An argument-based approach to validity. <em>Psychological Bulletin</em>, <em>112</em>(3), 527.
</div>
<div id="ref-kisch1965scales" class="csl-entry">
Kisch, B. (1965). <em>Scales and weights: A historical outline</em>. Yale University Press.
</div>
<div id="ref-krosnick2010" class="csl-entry">
Krosnick, J. A., &amp; Presser, S. (2010). Question and questionnaire design. <em>Handbook of Survey Research</em>, 263.
</div>
<div id="ref-lage-castellanos2019" class="csl-entry">
Lage-Castellanos, A., Valente, G., Formisano, E., &amp; De Martino, F. (2019). Methods for computing the maximum performance of computational models of fMRI responses. <em>PLoS Computational Biology</em>, <em>15</em>(3), e1006397.
</div>
<div id="ref-lieberman1999" class="csl-entry">
Lieberman, J. D., Solomon, S., Greenberg, J., &amp; McGregor, H. A. (1999). A hot new way to measure aggression: Hot sauce allocation. <em>Aggressive Behavior: Official Journal of the International Society for Research on Aggression</em>, <em>25</em>(5), 331–348.
</div>
<div id="ref-luce2007foundations" class="csl-entry">
Luce, Robert Duncan, Krantz, D. H., Suppes, P., &amp; Tversky, A. (1990). <em>Foundations of measurement III: Representation, axiomatization, and invariance</em>. Courier Corporation.
</div>
<div id="ref-luce1964simultaneous" class="csl-entry">
Luce, R. Duncan, &amp; Tukey, J. W. (1964). Simultaneous conjoint measurement: A new type of fundamental measurement. <em>Journal of Mathematical Psychology</em>, <em>1</em>(1), 1–27.
</div>
<div id="ref-marchman2008" class="csl-entry">
Marchman, V. A., &amp; Fernald, A. (2008). Speed of word recognition and vocabulary knowledge in infancy predict cognitive and language outcomes in later childhood. <em>Developmental Science</em>, <em>11</em>(3), F9–F16.
</div>
<div id="ref-matejka2016" class="csl-entry">
Matejka, J., Glueck, M., Grossman, T., &amp; Fitzmaurice, G. (2016). The effect of visual appearance on the performance of continuous sliders and visual analogue scales. <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>, 5421–5432.
</div>
<div id="ref-maul2016philosophical" class="csl-entry">
Maul, A., Irribarra, D. T., &amp; Wilson, M. (2016). On the philosophical foundations of psychological measurement. <em>Measurement</em>, <em>79</em>, 311–320.
</div>
<div id="ref-michell1999measurement" class="csl-entry">
Michell, J. (1999). <em>Measurement in psychology: A critical history of a methodological concept</em> (Vol. 53). Cambridge University Press.
</div>
<div id="ref-moscati2018measuring" class="csl-entry">
Moscati, I. (2018). <em>Measuring utility: From the marginal revolution to behavioral economics</em>. Oxford University Press.
</div>
<div id="ref-narens1986measurement" class="csl-entry">
Narens, L., &amp; Luce, R. D. (1986). Measurement: The theory of numerical assignments. <em>Psychological Bulletin</em>, <em>99</em>(2), 166.
</div>
<div id="ref-nieuwland2008" class="csl-entry">
Nieuwland, M. S., &amp; Kuperberg, G. R. (2008). When the truth is not too hard to handle: An event-related potential study on the pragmatics of negation. <em>Psychological Science</em>, <em>19</em>(12), 1213–1218.
</div>
<div id="ref-payne2001" class="csl-entry">
Payne, B. K. (2001). Prejudice and perception: The role of automatic and controlled processes in misperceiving a weapon. <em>Journal of Personality and Social Psychology</em>, <em>81</em>(2), 181.
</div>
<div id="ref-putnam1999threefold" class="csl-entry">
Putnam, H. (2000). <em>The threefold cord: Mind, body, and world</em>. Columbia Univ. Press.
</div>
<div id="ref-ross1989" class="csl-entry">
Ross, H. S., &amp; Lollis, S. P. (1989). A social relations analysis of toddler peer relationships. <em>Child Development</em>, 1082–1091.
</div>
<div id="ref-rumelhart1982" class="csl-entry">
Rumelhart, D. E., &amp; Norman, D. A. (1982). Simulating a skilled typist: A study of skilled cognitive-motor performance. <em>Cognitive Science</em>, <em>6</em>(1), 1–36.
</div>
<div id="ref-shafto2012" class="csl-entry">
Shafto, P., Goodman, N. D., &amp; Frank, M. C. (2012). Learning from others: The consequences of psychological reasoning for human learning. <em>Perspectives on Psychological Science</em>, <em>7</em>(4), 341–351.
</div>
<div id="ref-sijtsma2009" class="csl-entry">
Sijtsma, K. (2009). On the use, the misuse, and the very limited usefulness of cronbach’s alpha. <em>Psychometrika</em>, <em>74</em>(1), 107.
</div>
<div id="ref-stevens1946" class="csl-entry">
Stevens, S. S. (1946). On the theory of scales of measurement. <em>Science</em>, <em>103</em>(2684), 677–680.
</div>
<div id="ref-sep-measurement-science" class="csl-entry">
Tal, E. (2020). <span class="nocase">Measurement in Science</span>. In E. N. Zalta (Ed.), <em>The <span>Stanford</span> encyclopedia of philosophy</em> (<span>F</span>all 2020). <a href="https://plato.stanford.edu/archives/fall2020/entries/measurement-science/">https://plato.stanford.edu/archives/fall2020/entries/measurement-science/</a>; Metaphysics Research Lab, Stanford University.
</div>
<div id="ref-vanderweele2020outcome" class="csl-entry">
VanderWeele, T. J., Mathur, M. B., &amp; Chen, Y. (2020). Outcome-wide longitudinal designs for causal inference: A new template for empirical studies. <em>Statistical Science</em>, <em>35</em>(3), 437–466.
</div>
<div id="ref-voss2013" class="csl-entry">
Voss, A., Nagler, M., &amp; Lerche, V. (2013). Diffusion models in experimental psychology. <em>Experimental Psychology</em>, <em>60</em>(6), 385–402. <a href="https://doi.org/10.1027/1618-3169/a000218">https://doi.org/10.1027/1618-3169/a000218</a>
</div>
<div id="ref-woodworth1938" class="csl-entry">
Woodworth, R. S. (1938). <em>Experimental psychology.</em>
</div>
<div id="ref-zuo2019" class="csl-entry">
Zuo, X.-N., Xu, T., &amp; Milham, M. P. (2019). Harnessing reliability for neuroscience research. <em>Nature Human Behaviour</em>, <em>3</em>(8), 768–771. <a href="https://doi.org/10.1038/s41562-019-0655-x">https://doi.org/10.1038/s41562-019-0655-x</a>
</div>
</div>

</div>
</div>



<script type="module" src="/assets/src/index.page.client.jsx.5b23722a.js"></script><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","islands":[{"id":"island_0","name":"TOC","props":{}},{"id":"island_1","name":"Box","props":{"title":"!undefined","type":"learning_goals","content":"\n\u003cul>\n\u003cli>Discuss the reliability and validity of psychological measures\u003c/li>\n\u003cli>Reason about tradeoffs between different measures and measure types\u003c/li>\n\u003cli>Identify the characteristics of well-constructed survey questions\u003c/li>\n\u003cli>Articulate risks of measurement flexibility and the costs and benefits of multiple measures\u003c/li>\n\u003c/ul>\n"}},{"id":"island_2","name":"Box","props":{"title":"A reliable and valid measure of children’s vocabulary","type":"case_study","content":"\n\n\u003cp>Anyone who has worked with little children or had children of their own can attest to how variable their early language is. Some children speak clearly and produce long sentences from an early age, while others struggle; this variation appears to be linked to later school outcomes \u003cspan class=\"citation\">(\u003ca href=\"#ref-marchman2008\" role=\"doc-biblioref\">Marchman &amp; Fernald, 2008\u003c/a>)\u003c/span>. Thus, there are many reasons why you’d want to make precise measurements of children’s early language ability as a latent construct of interest.\u003clabel for=\"tufte-sn-108\" class=\"margin-toggle sidenote-number\">108\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-108\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">108\u003c/span> Of course, you can also ask if early language is a single construct, or whether it is multi-dimensional! For example, does grammar develop separately from vocabulary? It turns out the two are very closely coupled \u003cspan class=\"citation\">(\u003ca href=\"#ref-frank2021\" role=\"doc-biblioref\">Frank et al., 2021\u003c/a>)\u003c/span>. This point illustrates the general idea that, especially in psychology, measurement and theory building are intimately related – you need data to inform your theory, but the measurement instruments you use to collect your data in turn presuppose some theory!\u003c/span>\u003c/p>\n\u003cp>Because bringing children into a lab can be expensive, one popular option for measuring child language is the MacArthur Bates Communicative Development Inventory (CDI for short), a form which asks parents to mark words that their child says or understands. CDI forms are basically long checklists of words (the first page of an English form is shown in Figure \u003ca href=\"8-measurement.html#fig:measurement-cdi\">8.1\u003c/a>. But is parent report a reliable or valid measure of children’s early language?\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-cdi\">\u003c/span>\n\u003cimg src=\"images/measurement/cdi.jpg\" alt=\"The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children's early language.\" width=\"\\linewidth\" />\nFigure 8.1: The first page of the MacArthur-Bates Communicative Development Inventory, a parent report instrument for measuring children’s early language.\n\u003c/span>\n\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-psycho-cors\">\u003c/span>\n\u003cimg src=\"images/measurement/psycho-cors2.png\" alt=\"Longitudinal correlations between a child's score on one administration of the CDI and another one several months later. From Frank et al. (2021). \" width=\"\\linewidth\" />\nFigure 8.2: Longitudinal correlations between a child’s score on one administration of the CDI and another one several months later. From Frank et al. (2021). \n\u003c/span>\n\u003c/p>\n\u003cp>One test of the reliability of the CDI is a \u003cstrong>test-retest\u003c/strong> correlation, where we compute the correlation within children between two different administrations of the form. Unfortunately, this analysis has one issue: the longer you wait between observations the more the child has changed! Figure @ref(fig:measurement-psycho-cors longitudinal test-retest correlations for two CDIs, showing how correlations start off high and drop off as the gap between observations increases \u003cspan class=\"citation\">(\u003ca href=\"#ref-frank2021\" role=\"doc-biblioref\">Frank et al., 2021\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-cdi-validity\">\u003c/span>\n\u003cimg src=\"images/measurement/cdi-validity.png\" alt=\"Relations between an early form of the CDI (the ELI) and several other measurements of children's early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights [@bornstein1998].\" width=\"\\linewidth\" />\nFigure 8.3: Relations between an early form of the CDI (the ELI) and several other measurements of children’s early language from both transcripts and direct assessments. Arrows indicate loadings of different measures (boxes) onto a single construct (circle). Numbers give standardized weights \u003cspan class=\"citation\">(\u003ca href=\"#ref-bornstein1998\" role=\"doc-biblioref\">Bornstein &amp; Haynes, 1998\u003c/a>)\u003c/span>.\n\u003c/span>\n\u003c/p>\n\u003cp>Given that CDI forms are relatively reliable instruments, are they valid? That is, do they really measure the construct of interest, namely children’s early language ability? \u003cspan class=\"citation\">Bornstein &amp; Haynes (\u003ca href=\"#ref-bornstein1998\" role=\"doc-biblioref\">1998\u003c/a>)\u003c/span> collected many different measures of children’s language – including the ELI (an early CDI form) and other “gold standard” measures like transcribed samples of children’s speech. Figure \u003ca href=\"8-measurement.html#fig:measurement-cdi-validity\">8.3\u003c/a> shows the results of a structural equation model that measures the shared variance between these measures and a hypothesized central construct (“vocabulary competence”). The ELI (CDI) score correlated closely with the shared variance among all the different measures, suggesting that it was a valid measure of the construct.\u003c/p>\n\u003cp>The combination of reliability and validity evidence suggests that CDI are a useful (and relatively inexpensive source) of data about children’s early language, and indeed they have become one of the most common assessments for this age group!\u003c/p>\n\n"}},{"id":"island_3","name":"Box","props":{"title":"Early controversies over psychological measurement","type":"depth","content":"\n\n\u003cblockquote>\n\u003cp>“Psychology cannot attain the certainty and exactness of the physical sciences, unless it rests on a foundation of […] measurement” \u003cspan class=\"citation\">(\u003ca href=\"#ref-cattel1890mental\" role=\"doc-biblioref\">Cattel, 1890\u003c/a>)\u003c/span>.\u003c/p>\n\u003c/blockquote>\n\u003cp>It is no coincidence that the founders of experimental psychology were obsessed with measurement \u003cspan class=\"citation\">(\u003ca href=\"#ref-heidelberger2004nature\" role=\"doc-biblioref\">Heidelberger, 2004\u003c/a>)\u003c/span>.\nIt was viewed as the primary obstacle facing psychology on its road to becoming a legitimate quantitative science.\nFor example, one of the final pieces written by Hermann von Helmholtz (Wilhelm Wundt’s doctoral advisor), was a 1887 philosophical treatise entitled “Zahlen und Messen” (“Counting and Measuring”; see \u003cspan class=\"citation\">Darrigol (\u003ca href=\"#ref-darrigol2003number\" role=\"doc-biblioref\">2003\u003c/a>)\u003c/span>).\nIn the same year, \u003cspan class=\"citation\">Fechner (\u003ca href=\"#ref-fechner1987my\" role=\"doc-biblioref\">1987\u003c/a>)\u003c/span> explicitly grappled with the foundations of measurement in “Uber die psychischen Massprincipien” (“On Psychic Measurement Principles”).\u003c/p>\n\u003cp>Many of the early debates over measurement revolved around the emerging area of \u003cem>psychophysics\u003c/em>, the problem of relating objective, physical stimuli (e.g. light or sound or pressure) to the subjective sensations they produce in the mind.\nFor example, \u003cspan class=\"citation\">Fechner (\u003ca href=\"#ref-fechner1860elemente\" role=\"doc-biblioref\">1860\u003c/a>)\u003c/span> was interested in a quantity called the “just noticeable difference” (JND), the smallest change in a stimulus that can be discriminated by our senses.\nHe argued for a lawful (logarithmic) relationship: a logarithmic change in the intensity of, say, brightness corresponded to a linear change in the reported intensity (up to some constant).\nIn other words, sensation was \u003cem>measurable\u003c/em> via instruments like the JND.\u003c/p>\n\u003cp>It may be surprising to modern ears that the basic claim of measurability was controversial, even if the precise form of the psychophysical function would continue to be debated.\nBut this claim led to a deeply rancorous debate, culminating with the so-called Ferguson Committee, formed by the British Association for the Advancement of Science in 1932 to investigate whether such psychophysical procedures could count as quantitative ‘measurements’ of anything at all \u003cspan class=\"citation\">(\u003ca href=\"#ref-moscati2018measuring\" role=\"doc-biblioref\">Moscati, 2018\u003c/a>)\u003c/span>.\nIt was unable to reach a conclusion, with physicists and psychologists deadlocked:\u003c/p>\n\u003cblockquote>\n\u003cp>Having found that individual sensations have an order, they [some psychologists] assume that they are \u003cem>measurable\u003c/em>. Having travestied physical measurement in order to justify that assumption, they assume that their sensation intensities will be related to stimuli by numerical laws […] which, if they mean anything, are certainly false. \u003cspan class=\"citation\">(\u003ca href=\"#ref-ferguson1940\" role=\"doc-biblioref\">Ferguson &amp; Tucker, 1940\u003c/a>)\u003c/span>\u003c/p>\n\u003c/blockquote>\n\u003cp>The heart of the disagreement was rooted in the classical definition of quantity requiring strictly \u003cem>additive\u003c/em> structure.\nAn attribute was only considered measurable in light of a meaningful concatenation operation.\nFor example, weight was a measurable attribute because putting a bag of three rocks on a scale yields the same number as putting each of the three rock on separate scales and then summing up those numbers (in philosophy of science, attributes with this concatenation property are known as “extensive” attributes, as opposed to “intensive” ones.)\nNorman Campbell, one of the most prominent members of the Ferguson Committee, had recently defined \u003cem>fundamental\u003c/em> measurement in this way \u003cspan class=\"citation\">(e.g. see \u003ca href=\"#ref-campbell1928account\" role=\"doc-biblioref\">Norman Robert Campbell, 1928\u003c/a>)\u003c/span>, contrasting it with \u003cem>derived measurement\u003c/em> which was some function of fundamental measures.\nAccording to the physicists on the Ferguson Committee, measuring mental sensations was impossible because they could never be grounded in any \u003cem>fundamental\u003c/em> scale with this kind of additive operation.\nIt just didn’t make sense to break up holistic sensations into parts the way we would weights or lengths: they didn’t come in “amounts” or “quantities” that could be combined \u003cspan class=\"citation\">(\u003ca href=\"#ref-cattell1962relational\" role=\"doc-biblioref\">Cattell, 1962\u003c/a>)\u003c/span>.\nEven the intuitive additive logic of \u003cspan class=\"citation\">Donders (\u003ca href=\"#ref-donders1969speed\" role=\"doc-biblioref\">1868\u003c/a>)\u003c/span>’s “method of subtraction” for measuring the speed of mental processes was viewed skeptically on the same grounds by the time of the committee (e.g. in an early textbook, \u003cspan class=\"citation\">Woodworth (\u003ca href=\"#ref-woodworth1938\" role=\"doc-biblioref\">1938\u003c/a>)\u003c/span> claimed “we cannot break up the reaction into successive acts and obtain the time for each act.”)\u003c/p>\n\u003cp>The primary target of the Ferguson Committee’s investigation was the psychologist S. S. Stevens, who had claimed to measure the sensation of loudness using psychophysical instruments.\nExiled from classical frameworks of measurement, he went about developing an alternative “operational” framework \u003cspan class=\"citation\">(\u003ca href=\"#ref-stevens1946\" role=\"doc-biblioref\">Stevens, 1946\u003c/a>)\u003c/span>, where the classical ratio scale recognized by physicists was only one of several ways of assigning numbers to things (see \u003ca href=\"8-measurement.html#tab:measurement-stevens-table\">8.1\u003c/a> below).\nStevens’ framework quickly spread, leading to an explosion of proposed measures.\nHowever, operationalism remains controversial outside psychology \u003cspan class=\"citation\">(\u003ca href=\"#ref-michell1999measurement\" role=\"doc-biblioref\">Michell, 1999\u003c/a>)\u003c/span>.\nThe most extreme version of his stance (“measurement is the assignment of numerals to objects or events according to rule”) permits researchers to \u003cem>define\u003c/em> constructs operationally in terms of a measure \u003cspan class=\"citation\">(\u003ca href=\"#ref-hardcastle1995ss\" role=\"doc-biblioref\">Hardcastle, 1995\u003c/a>)\u003c/span>.\nFor example, one may say that the construct of intelligence is simply \u003cem>whatever it is\u003c/em> that IQ measures.\nIt is then left up to the researcher to decide which scale type their proposed measure should belong to.\u003c/p>\n\u003cp>In Chapter \u003ca href=\"2-theories.html#theories\">2\u003c/a>, we outlined a somewhat different view, closer to a kind of constructive realism \u003cspan class=\"citation\">Putnam (\u003ca href=\"#ref-putnam1999threefold\" role=\"doc-biblioref\">2000\u003c/a>)\u003c/span>.\nPsychological constructs like working memory or theory of mind are taken to exist independent of any given operationalization, putting us on firmer ground to debate the pros and cons associated with different ways of measuring the same construct.\nIn other words, we are not free to assign numerals however we like.\nWhether a particular construct or quantity is measurable on a particular scale should be treated as an empirical question.\u003c/p>\n\u003cp>The next major breakthrough in measurement theory emerged with the birth of mathematical psychology in the 1960s, which aimed to put psychological measurement on more rigorous foundations.\nThis effort culminated in the three-volume Foundations of Measurement series \u003cspan class=\"citation\">Robert Duncan Luce et al. (\u003ca href=\"#ref-luce2007foundations\" role=\"doc-biblioref\">1990\u003c/a>)\u003c/span>, which has become the canonical text for every psychology student seeking to understand measurement in the non-physical sciences.\u003clabel for=\"tufte-sn-109\" class=\"margin-toggle sidenote-number\">109\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-109\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">109\u003c/span> It is worth noting that 20th century physics has also seriously challenged the classical additive understanding of measurement. For example, velocities are revealed to be non-additive under general relativity, and properties of quantum particles are only measurable under a complex probabilistic framework.\u003c/span>\nOne of the key breakthroughs was to shift the burden from measuring (additive) constructs themselves to measuring (additive) \u003cem>effects\u003c/em> of constructs in conjunction with one another:\u003c/p>\n\u003cblockquote>\n\u003cp>When no natural concatenation operation exists, one should try to discover a way to measure factors and responses such that the ‘effects’ of different factors are additive. \u003cspan class=\"citation\">(\u003ca href=\"#ref-luce1964simultaneous\" role=\"doc-biblioref\">R. Duncan Luce &amp; Tukey, 1964\u003c/a>)\u003c/span>.\u003c/p>\n\u003c/blockquote>\n\u003cp>This modern viewpoint broadly informs the view we describe here.\u003c/p>\n"}},{"id":"island_4","name":"Box","props":{"title":"Reliability paradoxes!","type":"depth","content":"\n\n\u003cp>There’s a major issue with calculating reliabilities using the approaches we described here: reliability will always be relative to the variation in the sample. So if a sample has less variability, reliability will decrease!\u003c/p>\n\u003cp>Let’s think about the CDI data we were talking about earlier, which showed high test-retest reliability. Now imagine we restricted our sample to only 16 – 18 month-olds (our prior sample had 16 – 30-month-olds) with low maternal education. Within this more restricted subset, overall vocabularies would be lower and more similar to one another, and so the average amount of change \u003cem>within\u003c/em> a child would be larger relative to the differences \u003cem>between\u003c/em> children. That would make our test-retest reliability score go down, even though we would just be computing it on a subset of the same data.\u003c/p>\n\u003cp>We can construct a much more worrisome version of the same problem. Say we are very sloppy in our administration of the CDI and create lots of between-participants variability, perhaps by giving different instructions to different families. This practice will actually \u003cem>increase\u003c/em> our estimate of split-half reliability – while the within-participant variability will remain the same, the between-participant variability will go up! You could call this a “reliability paradox” – sloppier data collection can actually lead to higher reliabilities.\u003clabel for=\"tufte-sn-114\" class=\"margin-toggle sidenote-number\">114\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-114\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">114\u003c/span> If you get interested in this topic, take a look at \u003cspan class=\"citation\">(\u003ca href=\"#ref-luck2018\" role=\"doc-biblioref\">\u003cstrong>luck2018?\u003c/strong>\u003c/a>)\u003c/span>. There’s also a fascinating article by \u003cspan class=\"citation\">Hedge et al. (\u003ca href=\"#ref-hedge2018\" role=\"doc-biblioref\">2018\u003c/a>)\u003c/span> that shows why many highly replicable cognitive tasks like the Stroop task nevertheless have low reliability: they don’t vary very much between individuals!\u003c/span>\u003c/p>\n\u003cp>More generally, we need to be sensitive to the sources of variability we’re quantifying reliability over – both the numerator and the denominator. If we’re computing split-half reliabilities, typically we’re looking at variability across test questions (from some question bank) vs. across individuals (from some population). Both of these sampling decisions affect reliability – if the population is more variable \u003cem>or\u003c/em> the questions are less variable, we’ll get higher reliability.\u003c/p>\n"}},{"id":"island_5","name":"Box","props":{"title":"Talk about flexible measurement!","type":"accident_report","content":"\n\n\u003cp>The Competitive Reaction Time Task (CRTT) is a lab-based measure of aggression. Participants are told that they are playing a reaction-time game against another player and are asked to set the parameters of a noise blast that will be played to their opponent. Unfortunately, in an analysis of the literature using CRTT, \u003cspan class=\"citation\">Elson et al. (\u003ca href=\"#ref-elson2014\" role=\"doc-biblioref\">2014\u003c/a>)\u003c/span> found that different papers using the CRTT use dramatically different methods for scoring the task. Across trials, both the volume and duration of the noise blast were sometimes analyzed. Sometimes these scores were transformed (via logarithms) or thresholded. Sometimes they were combined into a single score. Elson was so worried by this flexibility, he created a website, \u003ca href>http://flexiblemeasures.com\u003c/a>, to document the variation he observed.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:crtt\">\u003c/span>\n\u003cimg src=\"images/measurement/CRTT.png\" alt=\"Data on the number of publications using CRTT and the number of different quantifications of CRTT, plotted cumulatively until 2016. Image from [http://flexiblemeasures.com]().\" width=\"\\linewidth\" />\nFigure 8.7: Data on the number of publications using CRTT and the number of different quantifications of CRTT, plotted cumulatively until 2016. Image from \u003ca href>http://flexiblemeasures.com\u003c/a>.\n\u003c/span>\n\u003c/p>\n\u003cp>As of 2016, Elson had found 130 papers using the CRTT. And across these papers, he documented an astonishing 157 quantification strategies. One paper reported ten different strategies for extracting numbers from this measure! More worrisome still, Elson and colleagues found that when they tried out some of these strategies on their own data, different strategies led to very different effect sizes and levels of statistical significance. They could effectively make a finding appear bigger or smaller depending on which scoring they chose.\u003c/p>\n\u003cp>This examination of the use of the CRTT measure has several implications. First, and most troublingly, there may have been undisclosed flexibility in the analysis of CRTT data across the literature, with investigators taking advantage of the lack of standardization to try many different analysis variants and report the one most favorable to their own hypothesis. Second, it is unknown which quantification of CRTT behavior is in fact most reliable and valid. Since some of these variants are presumably better than others, researchers are effectively “leaving money on the table” by using suboptimal quantifications. Finally, as a consequence, when if researchers adopt the CRTT, they find much less guidance from the literature on what quantification to adopt.\u003c/p>\n"}},{"id":"island_6","name":"Box","props":{"title":"Survey measures","type":"depth","content":"\n\n\u003cp>Sometimes the easiest way to elicit information from participants is simply to ask. Survey questions are an important part of experimental measurement, so we’ll share a few best practices, primarily derived from \u003cspan class=\"citation\">Krosnick &amp; Presser (\u003ca href=\"#ref-krosnick2010\" role=\"doc-biblioref\">2010\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>Treat survey questions as a conversation. The easier your items are to understand, the better. Don’t repeat variations on the same question unless you want different answers! Try to make the order reasonable. The more you include “tricky” items the more you invite tricky answers to straightforward questions.\u003clabel for=\"tufte-sn-123\" class=\"margin-toggle sidenote-number\">123\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-123\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">123\u003c/span> We’ll talk in Chapter \u003ca href=\"12-collection.html#collection\">12\u003c/a> about manipulation checks and their strengths and weaknesses.\u003c/span>\u003c/p>\n\u003cp>Open-ended survey questions can be quite rich and informative, especially when an appropriate coding scheme is developed in advance and responses are categorized into a relatively small number of types. On the other hand, they present practical obstacles because they require coding (often by multiple coders to ensure reliability of the coding). Further, they tend to yield nominal data, which are often less useful for quantitative theorizing. Open-ended questions are a useful tool to add nuance and color to the interpretation of an experiment.\u003c/p>\n\u003cp>\n\u003cspan class=\"marginnote shownote\">\n\u003cspan style=\"display: block;\" id=\"fig:measurement-likert\">\u003c/span>\n\u003cimg src=\"images/measurement/likert.png\" alt=\"Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled.\" width=\"\\linewidth\" />\nFigure 8.9: Likert scales based on survey best practices: a bipolar opinion scale with seven points and a unipolar frequency scale with five points. Both have all points labeled.\n\u003c/span>\n\u003c/p>\n\u003cp>Especially given their ubiquity in commercial survey research, Likert scales with a fixed number of response items are a simple and conventional way of gathering data on attitude and judgment questions (Figure \u003ca href=\"8-measurement.html#fig:measurement-likert\">8.9\u003c/a>). Bipolar scales are those in which the endpoints represent opposites, for example the continuum between “strongly dislike” and “strongly like.” Unipolar scales have one neutral endpoint, like the continuum between “no pain” and “very intense pain.” Survey best practices suggest that reliability is maximized when bipolar scales have seven points and unipolar scales have five. Labeling every point on the scale with verbal labels is preferable to labeling only the endpoints.\u003clabel for=\"tufte-sn-124\" class=\"margin-toggle sidenote-number\">124\u003c/label>\u003cinput type=\"checkbox\" id=\"tufte-sn-124\" class=\"margin-toggle\" />\u003cspan class=\"sidenote\">\u003cspan class=\"sidenote-number\">124\u003c/span> One important question is whether to treat data from Likert scales as ordinal or interval. It’s extremely common (and convenient) to make the assumption that Likert ratings are interval, allowing the use of standard statistical tools like means, standard deviations, linear regression, and the like. The risk in this practice comes from the possibility that scale items are not evenly spaced – for example, on a scale labeled “never”,“seldom”, “occasionally”,“often”,“always,” the distance from “often” to “always” may be larger than the distance from “seldom” to “occasionally.”\nIn practice, you can choose to use regression variants that are appropriate, e.g. ordinal logistic regression and its variants, or they can attempt to assess and mitigate the risks of treating the data as interval. If you choose the second option, it’s definitely a good idea to look carefully at the raw distributions for individual items (see Chapter \u003ca href=\"15-viz.html#viz\">15\u003c/a>) to see if their distribution appears approximately normal and not highly skewed or censored. You should also consider the names you give to your scale up front to try to minimize these issues.\u003c/span> Recently some researchers have begun to use “visual analog scales” (or sliders) as a solution. We don’t recommend these – the distribution of the resulting data is often anchored at the starting point or endpoints \u003cspan class=\"citation\">(\u003ca href=\"#ref-matejka2016\" role=\"doc-biblioref\">Matejka et al., 2016\u003c/a>)\u003c/span>, and a meta-analysis shows that are quite a bit lower than Likert scales in reliability \u003cspan class=\"citation\">(\u003ca href=\"#ref-krosnick2010\" role=\"doc-biblioref\">Krosnick &amp; Presser, 2010\u003c/a>)\u003c/span>.\u003c/p>\n\u003cp>It rarely helps matters to add a “don’t know” or “other” option to survey questions. These are some of a variety of practices that encourage \u003cstrong>satisficing\u003c/strong>, where survey takers give answers that are good enough but don’t reflect substantial thought about the question. Another behavior that results from satisficing is “straight-lining” – that is, picking the same option for every question. In general, the best way to prevent straight-lining is to make surveys relatively short, engaging, and well-compensated. The practice of “reverse coding” to make the expected answers to some questions more negative can block straight-lining, but at the cost of making items more confusing [often by introducing pragmatically infelicitous negation; \u003cspan class=\"citation\">Nieuwland &amp; Kuperberg (\u003ca href=\"#ref-nieuwland2008\" role=\"doc-biblioref\">2008\u003c/a>)\u003c/span>]. Some obvious formatting options can reduce straight-lining as well, for example placing scales further apart or on subsequent (web) pages.\u003c/p>\n\u003cp>In sum, survey questions can be a helpful tool for eliciting graded judgments about explicit questions. The best way to execute them well is to try and make them as clear and easy to answer as possible.\u003c/p>\n"}},{"id":"island_7","name":"Box","props":{"title":"!undefined","type":"discussion_questions","content":"\n\n"}},{"id":"island_8","name":"Box","props":{"title":"!undefined","type":"readings","content":"\n\u003cul>\n\u003cli>\u003cp>A classic textbook on psychometrics that introduces the concepts of reliability and validity in a simple and readable way: Furr, R. M. (2021). \u003cem>Psychometrics: an introduction\u003c/em>. SAGE publications.\u003c/p>\u003c/li>\n\u003cli>\u003cp>A great primer on questionnaire design: Krosnick, J.A. (2018). Improving Question Design to Maximize Reliability and Validity. In: Vannette, D., Krosnick, J. (eds) The Palgrave Handbook of Survey Research. Palgrave Macmillan, Cham. \u003ca href>https://doi.org/10.1007/978-3-319-54395-6_13\u003c/a>\u003c/p>\u003c/li>\n\u003cli>\u003cp>Introduction to general issues in measurement and why they shouldn’t be ignored: Flake, J. K., &amp; Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456-465. \u003ca href>https://doi.org/10.1177/2515245920952393\u003c/a>\u003c/p>\u003c/li>\n\u003c/ul>\n"}}]}}</script></body>
</html>
